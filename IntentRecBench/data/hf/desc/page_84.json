{
    "punzel/flux_emma_watson": "Emma Watson\nModel description\nDownload model\nEmma Watson\nPrompt\n-\nPrompt\n-\nPrompt\n-\nPrompt\n-\nModel description\nThis LoRA was trained on 25 images of Emma Watson using SimpleTuner for 1600 steps.\nA trigger word is not required.\nAs a request I have uploaded the original version that was also on civitai to the /old folder\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.",
    "Qwen/Qwen2-VL-7B-Instruct": "A newer version of this model is available:\nQwen/Qwen2.5-VL-7B-Instruct\nQwen2-VL-7B-Instruct\nIntroduction\nWhatâ€™s New in Qwen2-VL?\nEvaluation\nImage Benchmarks\nVideo Benchmarks\nRequirements\nQuickstart\nMore Usage Tips\nLimitations\nCitation\nQwen2-VL-7B-Instruct\nIntroduction\nWe're excited to unveil Qwen2-VL, the latest iteration of our Qwen-VL model, representing nearly a year of innovation.\nWhatâ€™s New in Qwen2-VL?\nKey Enhancements:\nSoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\nUnderstanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\nAgent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\nMultilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\nModel Architecture Updates:\nNaive Dynamic Resolution: Unlike before, Qwen2-VL can handle arbitrary image resolutions, mapping them into a dynamic number of visual tokens, offering a more human-like visual processing experience.\nMultimodal Rotary Position Embedding (M-ROPE): Decomposes positional embedding into parts to capture 1D textual, 2D visual, and 3D video positional information, enhancing its multimodal processing capabilities.\nWe have three models with 2, 7 and 72 billion parameters. This repo contains the instruction-tuned 7B Qwen2-VL model. For more information, visit our Blog and GitHub.\nEvaluation\nImage Benchmarks\nBenchmark\nInternVL2-8B\nMiniCPM-V 2.6\nGPT-4o-mini\nQwen2-VL-7B\nMMMUval\n51.8\n49.8\n60\n54.1\nDocVQAtest\n91.6\n90.8\n-\n94.5\nInfoVQAtest\n74.8\n-\n-\n76.5\nChartQAtest\n83.3\n-\n-\n83.0\nTextVQAval\n77.4\n80.1\n-\n84.3\nOCRBench\n794\n852\n785\n845\nMTVQA\n-\n-\n-\n26.3\nVCRen easy\n-\n73.88\n83.60\n89.70\nVCRzh easy\n-\n10.18\n1.10\n59.94\nRealWorldQA\n64.4\n-\n-\n70.1\nMMEsum\n2210.3\n2348.4\n2003.4\n2326.8\nMMBench-ENtest\n81.7\n-\n-\n83.0\nMMBench-CNtest\n81.2\n-\n-\n80.5\nMMBench-V1.1test\n79.4\n78.0\n76.0\n80.7\nMMT-Benchtest\n-\n-\n-\n63.7\nMMStar\n61.5\n57.5\n54.8\n60.7\nMMVetGPT-4-Turbo\n54.2\n60.0\n66.9\n62.0\nHallBenchavg\n45.2\n48.1\n46.1\n50.6\nMathVistatestmini\n58.3\n60.6\n52.4\n58.2\nMathVision\n-\n-\n-\n16.3\nVideo Benchmarks\nBenchmark\nInternvl2-8B\nLLaVA-OneVision-7B\nMiniCPM-V 2.6\nQwen2-VL-7B\nMVBench\n66.4\n56.7\n-\n67.0\nPerceptionTesttest\n-\n57.1\n-\n62.3\nEgoSchematest\n-\n60.1\n-\n66.7\nVideo-MMEwo/w subs\n54.0/56.9\n58.2/-\n60.9/63.6\n63.3/69.0\nRequirements\nThe code of Qwen2-VL has been in the latest Hugging face transformers and we advise you to build from source with command pip install git+https://github.com/huggingface/transformers, or you might encounter the following error:\nKeyError: 'qwen2_vl'\nQuickstart\nWe offer a toolkit to help you handle various types of visual input more conveniently. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:\npip install qwen-vl-utils\nHere we show a code snippet to show you how to use the chat model with transformers and qwen_vl_utils:\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2-VL-7B-Instruct\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nWithout qwen_vl_utils\nfrom PIL import Image\nimport requests\nimport torch\nfrom torchvision import io\nfrom typing import Dict\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n# Load the model in half-precision on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n# Image\nurl = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preprocess the inputs\ntext_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\ninputs = processor(\ntext=[text_prompt], images=[image], padding=True, return_tensors=\"pt\"\n)\ninputs = inputs.to(\"cuda\")\n# Inference: Generation of the output\noutput_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids = [\noutput_ids[len(input_ids) :]\nfor input_ids, output_ids in zip(inputs.input_ids, output_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n)\nprint(output_text)\nMulti image inference\n# Messages containing multiple images and a text query\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n{\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n{\"type\": \"text\", \"text\": \"Identify the similarities between these images.\"},\n],\n}\n]\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nVideo inference\n# Messages containing a images list as a video and a text query\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"video\",\n\"video\": [\n\"file:///path/to/frame1.jpg\",\n\"file:///path/to/frame2.jpg\",\n\"file:///path/to/frame3.jpg\",\n\"file:///path/to/frame4.jpg\",\n],\n\"fps\": 1.0,\n},\n{\"type\": \"text\", \"text\": \"Describe this video.\"},\n],\n}\n]\n# Messages containing a video and a text query\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"video\",\n\"video\": \"file:///path/to/video1.mp4\",\n\"max_pixels\": 360 * 420,\n\"fps\": 1.0,\n},\n{\"type\": \"text\", \"text\": \"Describe this video.\"},\n],\n}\n]\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nBatch inference\n# Sample messages for batch inference\nmessages1 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n{\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n{\"type\": \"text\", \"text\": \"What are the common elements in these pictures?\"},\n],\n}\n]\nmessages2 = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n# Combine messages for batch processing\nmessages = [messages1, messages1]\n# Preparation for batch inference\ntexts = [\nprocessor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\nfor msg in messages\n]\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=texts,\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Batch Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_texts = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_texts)\nMore Usage Tips\nFor input images, we support local files, base64, and URLs. For videos, we currently only support local files.\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\n## Local file path\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n## Image URL\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n## Base64 encoded image\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\nImage Resolution for performance boost\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage.\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n)\nBesides, We provide two methods for fine-grained control over the image size input to the model:\nDefine min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels.\nSpecify exact dimensions: Directly set resized_height and resized_width. These values will be rounded to the nearest multiple of 28.\n# min_pixels and max_pixels\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"file:///path/to/your/image.jpg\",\n\"resized_height\": 280,\n\"resized_width\": 420,\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# resized_height and resized_width\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"file:///path/to/your/image.jpg\",\n\"min_pixels\": 50176,\n\"max_pixels\": 50176,\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\nLimitations\nWhile Qwen2-VL are applicable to a wide range of visual tasks, it is equally important to understand its limitations. Here are some known restrictions:\nLack of Audio Support: The current model does not comprehend audio information within videos.\nData timeliness: Our image dataset is updated until June 2023, and information subsequent to this date may not be covered.\nConstraints in Individuals and Intellectual Property (IP): The model's capacity to recognize specific individuals or IPs is limited, potentially failing to comprehensively cover all well-known personalities or brands.\nLimited Capacity for Complex Instruction: When faced with intricate multi-step instructions, the model's understanding and execution capabilities require enhancement.\nInsufficient Counting Accuracy: Particularly in complex scenes, the accuracy of object counting is not high, necessitating further improvements.\nWeak Spatial Reasoning Skills: Especially in 3D spaces, the model's inference of object positional relationships is inadequate, making it difficult to precisely judge the relative positions of objects.\nThese limitations serve as ongoing directions for model optimization and improvement, and we are committed to continually enhancing the model's performance and scope of application.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4": "Qwen2-VL-2B-Instruct-GPTQ-Int4\nIntroduction\nWhatâ€™s New in Qwen2-VL?\nBenchmark\nRequirements\nQuickstart\nMore Usage Tips\nLimitations\nCitation\nQwen2-VL-2B-Instruct-GPTQ-Int4\nIntroduction\nWe're excited to unveil Qwen2-VL, the latest iteration of our Qwen-VL model, representing nearly a year of innovation.\nWhatâ€™s New in Qwen2-VL?\nKey Enhancements:\nSoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\nUnderstanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\nAgent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\nMultilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\nModel Architecture Updates:\nNaive Dynamic Resolution: Unlike before, Qwen2-VL can handle arbitrary image resolutions, mapping them into a dynamic number of visual tokens, offering a more human-like visual processing experience.\nMultimodal Rotary Position Embedding (M-ROPE): Decomposes positional embedding into parts to capture 1D textual, 2D visual, and 3D video positional information, enhancing its multimodal processing capabilities.\nWe have three models with 2, 7 and 72 billion parameters. This repo contains the quantized version of the instruction-tuned 2B Qwen2-VL model. For more information, visit our Blog and GitHub.\nBenchmark\nPerformance of Quantized Models\nThis section reports the generation performance of quantized models (including GPTQ and AWQ) of the Qwen2-VL series. Specifically, we report:\nMMMU_VAL (Accuracy)\nDocVQA_VAL (Accuracy)\nMMBench_DEV_EN (Accuracy)\nMathVista_MINI (Accuracy)\nWe use VLMEvalkit to evaluate all models.\nModel Size\nQuantization\nMMMU\nDocVQA\nMMBench\nMathVista\nQwen2-VL-2B-Instruct\nBF16(ðŸ¤—ðŸ¤–)\n41.88\n88.34\n72.07\n44.40\nGPTQ-Int8(ðŸ¤—ðŸ¤–)\n41.55\n88.28\n71.99\n44.60\nGPTQ-Int4(ðŸ¤—ðŸ¤–)\n39.22\n87.21\n70.87\n41.69\nAWQ(ðŸ¤—ðŸ¤–)\n41.33\n86.96\n71.64\n39.90\nSpeed Benchmark\nThis section reports the speed performance of bf16 models, quantized models (including GPTQ-Int4, GPTQ-Int8 and AWQ) of the Qwen2-VL series. Specifically, we report the inference speed (tokens/s) as well as memory footprint (GB) under the conditions of different context lengths.\nThe environment of the evaluation with huggingface transformers is:\nNVIDIA A100 80GB\nCUDA 11.8\nPytorch 2.2.1+cu118\nFlash Attention 2.6.1\nTransformers 4.38.2\nAutoGPTQ 0.6.0+cu118\nAutoAWQ 0.2.5+cu118 (autoawq_kernels 0.0.6+cu118)\nNote:\nWe use the batch size of 1 and the least number of GPUs as possible for the evalution.\nWe test the speed and memory of generating 2048 tokens with the input lengths of 1, 6144, 14336, 30720, 63488, and 129024 tokens.\n2B (transformers)\nModel\nInput Length\nQuantization\nGPU Num\nSpeed(tokens/s)\nGPU Memory(GB)\nQwen2-VL-2B-Instruct\n1\nBF16\n1\n35.29\n4.68\nGPTQ-Int8\n1\n28.59\n3.55\nGPTQ-Int4\n1\n39.76\n2.91\nAWQ\n1\n29.89\n2.88\n6144\nBF16\n1\n36.58\n10.01\nGPTQ-Int8\n1\n29.53\n8.87\nGPTQ-Int4\n1\n39.27\n8.21\nAWQ\n1\n33.42\n8.18\n14336\nBF16\n1\n36.31\n17.20\nGPTQ-Int8\n1\n31.03\n16.07\nGPTQ-Int4\n1\n39.89\n15.40\nAWQ\n1\n32.28\n15.40\n30720\nBF16\n1\n32.53\n31.64\nGPTQ-Int8\n1\n27.76\n30.51\nGPTQ-Int4\n1\n30.73\n29.84\nAWQ\n1\n31.55\n29.84\nRequirements\nThe code of Qwen2-VL has been in the latest Hugging face transformers and we advise you to build from source with command pip install git+https://github.com/huggingface/transformers, or you might encounter the following error:\nKeyError: 'qwen2_vl'\nQuickstart\nWe offer a toolkit to help you handle various types of visual input more conveniently. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:\npip install qwen-vl-utils\nHere we show a code snippet to show you how to use the chat model with transformers and qwen_vl_utils:\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4\")\n# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4\", min_pixels=min_pixels, max_pixels=max_pixels)\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nWithout qwen_vl_utils\nfrom PIL import Image\nimport requests\nimport torch\nfrom torchvision import io\nfrom typing import Dict\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n# Load the model in half-precision on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4\", torch_dtype=\"auto\", device_map=\"auto\"\n)\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4\")\n# Image\nurl = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preprocess the inputs\ntext_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\ninputs = processor(\ntext=[text_prompt], images=[image], padding=True, return_tensors=\"pt\"\n)\ninputs = inputs.to(\"cuda\")\n# Inference: Generation of the output\noutput_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids = [\noutput_ids[len(input_ids) :]\nfor input_ids, output_ids in zip(inputs.input_ids, output_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n)\nprint(output_text)\nMulti image inference\n# Messages containing multiple images and a text query\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n{\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n{\"type\": \"text\", \"text\": \"Identify the similarities between these images.\"},\n],\n}\n]\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nVideo inference\n# Messages containing a images list as a video and a text query\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"video\",\n\"video\": [\n\"file:///path/to/frame1.jpg\",\n\"file:///path/to/frame2.jpg\",\n\"file:///path/to/frame3.jpg\",\n\"file:///path/to/frame4.jpg\",\n],\n\"fps\": 1.0,\n},\n{\"type\": \"text\", \"text\": \"Describe this video.\"},\n],\n}\n]\n# Messages containing a video and a text query\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"video\",\n\"video\": \"file:///path/to/video1.mp4\",\n\"max_pixels\": 360 * 420,\n\"fps\": 1.0,\n},\n{\"type\": \"text\", \"text\": \"Describe this video.\"},\n],\n}\n]\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nBatch inference\n# Sample messages for batch inference\nmessages1 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n{\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n{\"type\": \"text\", \"text\": \"What are the common elements in these pictures?\"},\n],\n}\n]\nmessages2 = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n# Combine messages for batch processing\nmessages = [messages1, messages1]\n# Preparation for batch inference\ntexts = [\nprocessor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\nfor msg in messages\n]\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=texts,\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Batch Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_texts = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_texts)\nMore Usage Tips\nFor input images, we support local files, base64, and URLs. For videos, we currently only support local files.\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\n## Local file path\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n## Image URL\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n## Base64 encoded image\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\nImage Resolution for performance boost\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage.\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n\"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4\", min_pixels=min_pixels, max_pixels=max_pixels\n)\nBesides, We provide two methods for fine-grained control over the image size input to the model:\nDefine min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels.\nSpecify exact dimensions: Directly set resized_height and resized_width. These values will be rounded to the nearest multiple of 28.\n# min_pixels and max_pixels\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"file:///path/to/your/image.jpg\",\n\"resized_height\": 280,\n\"resized_width\": 420,\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# resized_height and resized_width\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"file:///path/to/your/image.jpg\",\n\"min_pixels\": 50176,\n\"max_pixels\": 50176,\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\nLimitations\nWhile Qwen2-VL are applicable to a wide range of visual tasks, it is equally important to understand its limitations. Here are some known restrictions:\nLack of Audio Support: The current model does not comprehend audio information within videos.\nData timeliness: Our image dataset is updated until June 2023, and information subsequent to this date may not be covered.\nConstraints in Individuals and Intellectual Property (IP): The model's capacity to recognize specific individuals or IPs is limited, potentially failing to comprehensively cover all well-known personalities or brands.\nLimited Capacity for Complex Instruction: When faced with intricate multi-step instructions, the model's understanding and execution capabilities require enhancement.\nInsufficient Counting Accuracy: Particularly in complex scenes, the accuracy of object counting is not high, necessitating further improvements.\nWeak Spatial Reasoning Skills: Especially in 3D spaces, the model's inference of object positional relationships is inadequate, making it difficult to precisely judge the relative positions of objects.\nThese limitations serve as ongoing directions for model optimization and improvement, and we are committed to continually enhancing the model's performance and scope of application.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "nvidia/Llama-3.1-8B-Instruct-FP8": "Model Overview\nDescription:\nThird-Party Community Consideration\nLicense/Terms of Use:\nModel Architecture:\nInput:\nOutput:\nSoftware Integration:\nModel Version(s):\nDatasets:\nInference:\nPost Training Quantization\nUsage\nEvaluation\nDeploy with vLLM\nModel Overview\nDescription:\nThe NVIDIA Llama 3.1 8B Instruct FP8 model is the quantized version of the Meta's Llama 3.1 8B Instruct model, which is an auto-regressive language model that uses an optimized transformer architecture. For more information, please check here. The NVIDIA Llama 3.1 8B Instruct FP8 model is quantized with TensorRT Model Optimizer.\nThis model is ready for commercial and non-commercial use.\nThird-Party Community Consideration\nThis model is not owned or developed by NVIDIA. This model has been developed and built to a third-partyâ€™s requirements for this application and use case; see link to Non-NVIDIA (Meta-Llama-3.1-8B-Instruct) Model Card.\nLicense/Terms of Use:\nnvidia-open-model-license\nllama3.1\nModel Architecture:\nArchitecture Type: Transformers\nNetwork Architecture: Llama3.1\nInput:\nInput Type(s): Text\nInput Format(s): String\nInput Parameters: Sequences\nOther Properties Related to Input: Context length up to 128K\nOutput:\nOutput Type(s): Text\nOutput Format: String\nOutput Parameters: Sequences\nOther Properties Related to Output: N/A\nSoftware Integration:\nSupported Runtime Engine(s):\nTensor(RT)-LLM\nvLLM\nSupported Hardware Microarchitecture Compatibility:\nNVIDIA Blackwell\nNVIDIA Hopper\nNVIDIA Lovelace\nPreferred Operating System(s):\nLinux\nModel Version(s):\nThe model is quantized with nvidia-modelopt v0.27.0\nDatasets:\nCalibration Dataset: cnn_dailymail\nEvaluation Dataset: MMLU\nInference:\nEngine: Tensor(RT)-LLM or vLLM\nTest Hardware: H100\nPost Training Quantization\nThis model was obtained by quantizing the weights and activations of Meta-Llama-3.1-8B-Instruct to FP8 data type, ready for inference with TensorRT-LLM and vLLM. Only the weights and activations of the linear operators within transformers blocks are quantized. This optimization reduces the number of bits per parameter from 16 to 8, reducing the disk size and GPU memory requirements by approximately 50%. On H100, we achieved 1.3x speedup.\nUsage\nTo deploy the quantized checkpoint with TensorRT-LLM, follow the sample commands below with the TensorRT-LLM GitHub repo:\nCheckpoint convertion:\npython examples/llama/convert_checkpoint.py --model_dir Llama-3.1-8B-Instruct-FP8 --output_dir /ckpt --use_fp8\nBuild engines:\ntrtllm-build --checkpoint_dir /ckpt --output_dir /engine\nThroughputs evaluation:\nPlease refer to the TensorRT-LLM benchmarking documentation for details.\nEvaluation\nPrecision\nMMLU\nGSM8K (CoT)\nARC Challenge\nIFEVAL\nTPS\nBF16\n69.4\n84.5\n83.4\n80.4\n8,579.93\nFP8\n68.7\n83.1\n83.3\n81.8\n11,062.90\nWe benchmarked with tensorrt-llm v0.13 on 8 H100 GPUs, using batch size 1024 for the throughputs with in-flight batching enabled. We achieved ~1.3x speedup with FP8.\nDeploy with vLLM\nTo deploy the quantized checkpoint with vLLM, follow the instructions below:\nInstall vLLM from directions here.\nTo use a Model Optimizer PTQ checkpoint with vLLM, quantization=modelopt flag must be passed into the config while initializing the LLM Engine.\nExample deployment on H100:\nfrom vllm import LLM, SamplingParams\nmodel_id = \"nvidia/Llama-3.1-8B-Instruct-FP8\"\nsampling_params = SamplingParams(temperature=0.8, top_p=0.9)\nprompts = [\n\"Hello, my name is\",\n\"The president of the United States is\",\n\"The capital of France is\",\n\"The future of AI is\",\n]\nllm = LLM(model=model_id, quantization=\"modelopt\")\noutputs = llm.generate(prompts, sampling_params)\n# Print the outputs.\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\nThis model can be deployed with an OpenAI Compatible Server via the vLLM backend. Instructions here.",
    "nyrahealth/CrisperWhisper": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nCrisperWhisper\nKey Features\nTable of Contents\nHighlights\n1. Performance Overview\n1.1 Qualitative Performance Overview\n1.2 Quantitative Performance Overview\n2. Usage\n2.1 Usage with ðŸ¤— transformers\n3. How?\nLicense\nlicense: cc-by-nc-4.0\nCrisperWhisper\nCrisperWhisper is an advanced variant of OpenAI's Whisper, designed for fast, precise, and verbatim speech recognition with accurate (crisp) word-level timestamps. Unlike the original Whisper, which tends to omit disfluencies and follows more of a intended transcription style, CrisperWhisper aims to transcribe every spoken word exactly as it is, including fillers, pauses, stutters and false starts. Checkout our repo for more details: https://github.com/nyrahealth/CrisperWhisper\nKey Features\nðŸŽ¯ Accurate Word-Level Timestamps: Provides precise timestamps, even around disfluencies and pauses, by utilizing an adjusted tokenizer and a custom attention loss during training.\nðŸ“ Verbatim Transcription: Transcribes every spoken word exactly as it is, including and differentiating fillers like \"um\" and \"uh\".\nðŸ” Filler Detection: Detects and accurately transcribes fillers.\nðŸ›¡ï¸ Hallucination Mitigation: Minimizes transcription hallucinations to enhance accuracy.\nTable of Contents\nKey Features\nHighlights\nPerformance Overview\nQualitative Performance Overview\nQuantitative Performance Overview\nTranscription Performance\nSegmentation Performance\nUsage\nwith transformers\nHow?\nHighlights\nðŸ† 1st place on the OpenASR Leaderboard in verbatim datasets (TED, AMI)\nðŸŽ“ Accepted at INTERSPEECH 2024.\nðŸ“„ Paper Drop: Check out our paper for details and reasoning behind our tokenizer adjustment.\nâœ¨ New Feature: Not mentioned in the paper is a added AttentionLoss to further improve timestamp accuracy. By specifically adding a loss to train the attention scores used for the DTW alignment using timestamped data we significantly boosted the alignment performance.\n1. Performance Overview\n1.1 Qualitative Performance Overview\nAudio\nWhisper Large V3\nCrisper Whisper\nDemo de 1\nEr war kein Genie, aber doch ein fÃ¤higer Ingenieur.\nEs ist zwar kein. Er ist zwar kein Genie, aber doch ein fÃ¤higer Ingenieur.\nDemo de 2\nLeider mÃ¼ssen wir in diesen schweren Zeiten auch unserem TagesgeschÃ¤ft nachgehen. Der hier vorgelegte Kulturhaushalt der Ampelregierung strebt an, den Erfolgskurs der Union zumindest fiskalisch fortzufÃ¼hren.\nLeider [UH] mÃ¼ssen wir in diesen [UH] schweren Zeiten auch [UH] unserem [UH] TagesgeschÃ¤ft nachgehen. Der hier [UH] vorgelegte [UH] Kulturhaushalt der [UH] Ampelregierung strebt an, den [UH] Erfolgskurs der Union [UH] zumindest [UH] fiskalisch fortzufÃ¼hren. Es.\nDemo de 3\ndie Ã¼ber alle FRA-Fraktionen hinweg gut im Blick behalten sollten, auch weil sie teilweise sehr teeteuer sind. Aber nicht nur, weil sie teeteuer sind. Wir steigen mit diesem Endentwurf ein in die sogenannten Pandemie-BereitschaftsvertrÃ¤ge.\nDie Ã¼ber alle Fr Fraktionen hinweg gut im [UH] Blick behalten sollten, auch weil sie teil teilweise sehr te teuer sind. Aber nicht nur, weil sie te teuer sind. Wir [UH] steigen mit diesem Ent Entwurf ein in die sogenannten Pand PandemiebereitschaftsvertrÃ¤ge.\nDemo en 1\nalternative is you can get like, you have those Dr. Bronner's\nAlternative is you can get like [UH] you have those, you know, those doctor Brahmer's.\nDemo en 2\ninfluence our natural surrounding? How does it influence our ecosystem?\nInfluence our [UM] our [UH] our natural surrounding. How does it influence our ecosystem?\nDemo en 3\nand always find a place on the street to park and it was easy and you weren't a long distance away from wherever it was that you were trying to go. So I remember that being a lot of fun and easy to do and there were nice places to go and good events to attend. Come downtown and you had the Warner Theater and\nAnd always find a place on the street to park. And and it was it was easy and you weren't a long distance away from wherever it was that you were trying to go. So, I I I remember that being a lot of fun and easy to do and there were nice places to go and, [UM] i good events to attend. Come downtown and you had the Warner Theater and, [UM]\nDemo en 4\nyou know, more masculine, who were rough, and that definitely wasn't me. Then, you know, I was very smart because my father made sure I was smart, you know. So, you know, I hung around those people, you know. And then you had the ones that were just out doing things that they shouldn't have been doing also. So, yeah, I was in the little geek squad. You were in the little geek squad. Yeah.\nyou know, more masculine, who were rough, and that definitely wasn't me. Then, you know, I was very smart because my father made sure I was smart. You know, so, [UM] you know, I I hung around those people, you know. And then you had the ones that were just just out doing things that they shouldn't have been doing also. So yeah, I was the l I was in the little geek squad. Do you\n1.2 Quantitative Performance Overview\nTranscription Performance\nCrisperWhisper significantly outperforms Whisper Large v3, especially on datasets that have a more verbatim transcription style in the ground truth, such as AMI and TED-LIUM.\nDataset\nCrisperWhisper\nWhisper Large v3\nAMI\n8.72\n16.01\nEarnings22\n12.37\n11.3\nGigaSpeech\n10.27\n10.02\nLibriSpeech clean\n1.74\n2.03\nLibriSpeech other\n3.97\n3.91\nSPGISpeech\n2.71\n2.95\nTED-LIUM\n3.35\n3.9\nVoxPopuli\n8.61\n9.52\nCommonVoice\n8.19\n9.67\nAverage WER\n6.66\n7.7\nSegmentation Performance\nCrisperWhisper demonstrates superior performance segmentation performance. This performance gap is especially pronounced around disfluencies and pauses.\nThe following table uses the metrics as defined in the paper. For this table we used a collar of 50ms. Heads for each Model were selected using the method described in the How section and the result attaining the highest F1 Score was choosen for each model using varying number of heads.\nDataset\nMetric\nCrisperWhisper\nWhisper Large v2\nWhisper Large v3\nAMI IHM\nF1 Score\n0.79\n0.63\n0.66\nAvg IOU\n0.67\n0.54\n0.53\nCommon Voice\nF1 Score\n0.80\n0.42\n0.48\nAvg IOU\n0.70\n0.32\n0.43\nTIMIT\nF1 Score\n0.69\n0.40\n0.54\nAvg IOU\n0.56\n0.32\n0.43\n2. Usage\nHere's how to use CrisperWhisper in your Python scripts:\nFirst install our custom transformers fork for the most accurate timestamps:\npip install git+https://github.com/nyrahealth/transformers.git@crisper_whisper\n2.1 Usage with ðŸ¤— transformers\nimport os\nimport sys\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\ndef adjust_pauses_for_hf_pipeline_output(pipeline_output, split_threshold=0.12):\n\"\"\"\nAdjust pause timings by distributing pauses up to the threshold evenly between adjacent words.\n\"\"\"\nadjusted_chunks = pipeline_output[\"chunks\"].copy()\nfor i in range(len(adjusted_chunks) - 1):\ncurrent_chunk = adjusted_chunks[i]\nnext_chunk = adjusted_chunks[i + 1]\ncurrent_start, current_end = current_chunk[\"timestamp\"]\nnext_start, next_end = next_chunk[\"timestamp\"]\npause_duration = next_start - current_end\nif pause_duration > 0:\nif pause_duration > split_threshold:\ndistribute = split_threshold / 2\nelse:\ndistribute = pause_duration / 2\n# Adjust current chunk end time\nadjusted_chunks[i][\"timestamp\"] = (current_start, current_end + distribute)\n# Adjust next chunk start time\nadjusted_chunks[i + 1][\"timestamp\"] = (next_start - distribute, next_end)\npipeline_output[\"chunks\"] = adjusted_chunks\nreturn pipeline_output\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel_id = \"nyrahealth/CrisperWhisper\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\nmodel_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\nprocessor = AutoProcessor.from_pretrained(model_id)\npipe = pipeline(\n\"automatic-speech-recognition\",\nmodel=model,\ntokenizer=processor.tokenizer,\nfeature_extractor=processor.feature_extractor,\nchunk_length_s=30,\nbatch_size=16,\nreturn_timestamps='word',\ntorch_dtype=torch_dtype,\ndevice=device,\n)\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\nhf_pipeline_output = pipe(sample)\ncrisper_whisper_result = adjust_pauses_for_hf_pipeline_output(hf_pipeline_output)\nprint(crisper_whisper_result)\nread more about the reasoning behind the pause distribution logic in our paper.\n3. How?\nWe employ the popular Dynamic Time Warping (DTW) on the Whisper cross-attention scores, as detailed in our paper to derive word-level timestamps. By leveraging our retokenization process, this method allows us to consistently detect pauses. Given that the accuracy of the timestamps heavily depends on the DTW cost matrix and, consequently, on the quality of the cross-attentions, we developed a specialized loss function for the selected alignment heads to enhance precision.\nAlthough this loss function was not included in the original paper due to time constraints preventing the completion of experiments and training before the submission deadline, it has been used to train our publicly available models.\nKey Features of this loss are as follows:\nData Preparation\nWe used datasets with word-level timestamp annotations, such as AMI IHM and TIMIT   , but required additional timestamped data.\nTo address this, we validated the alignment accuracy of several forced alignment tools using a small hand-labeled dataset.\nBased on this validation, we chose the PyTorch CTC aligner to generate more time-aligned data from the CommonVoice dataset.\nBecause the PyTorch CTC aligner tends to overestimate pause durations, we applied the same pause-splitting method detailed in our paper to correct these errors. The effectiveness of this correction was confirmed using our hand-labeled dataset.\nToken-Word Alignment\nDue to retokenization as detailed in our paper, each token is either part of a word or a pause/space, but never both\nTherefore each token can be cleanly aligned to a word OR a space/pause\nGround Truth Cross-Attention\nWe define the cross-attention ground truth for tokens as the L2-normalized vector, where:\nA value of 1 indicates that the word is active according to the word-level ground truth timestamp.\nA value of 0 indicates that no attention should be paid.\nTo account for small inaccuracies in the ground truth timestamps, we apply a linear interpolation of 4 steps (8 milliseconds) on both sides of the ground truth vector, transitioning smoothly from 0 to 1.\nLoss Calculation\nThe loss function is defined as 1 - cosine similarity  between the predicted cross-attention vector (when predicting a token) and the ground truth cross-attention vector.\nThis loss is averaged across all predicted tokens and alignment heads.\nAlignment Head selection\nTo choose the heads for alignment we evaluated the alignment performance of each individual decoder attention head on the timestamped timit dataset.\nWe choose the 15 best performing heads and finetune them using our attention loss.\nTraining Details\nSince most of our samples during training were shorter than 30 seconds we shift the audio sample and corresponding timestamp ground truth around with a 50% probability to mitigate the cross attentions ,,overfitting\" to early positions of the encoder output.\nIf we have more than 40ms of silence (before or after shifting) we prepend the ground truth transcript ( and corresponding cross attention ground truth) with a space so the model has to accurately predict the starting time of the first word.\nWe use WavLM augmentations during Training adding random speech samples or noise to the audio wave to generally increase robustness of the transcription and stability of the alignment heads.\nWe clip ,,predicted\" values in the cross attention vectors 4 seconds before and 4 seconds after the groundtruth word they belong to to 0. This is to decrease the dimensionality of the cross attention vector and therefore emphasize the attention where it counts in the loss and ultimately for the alignment.\nWith a probability of 1% we use samples containing exclusively noise where the model has to return a empty prediction to improve hallucination.\nThe Model is trained on a mixture of english and german datasets so we only gurantee good performance on these languages\nThe Model is trained in three stages, in the first stage we use around 10000 hours of audio to adjust Whisper to the new tokenizer. In the second stage we exclusively use high quality datasets that are transcribed in a verbatim fashion. Finally we continue training on this verbatim mixture and add the attention loss for another 6000 steps.\nLicense\nlicense: cc-by-nc-4.0",
    "second-state/3dAnimationDiffusion_v10-GGUF": "3Danimation-GGUF\nOriginal Model\nRun with sd-api-server\nQuantized GGUF Models\n3Danimation-GGUF\nOriginal Model\nYntec/3Danimation\nRun with sd-api-server\nGo to the sd-api-server repository for more information.\nQuantized GGUF Models\nName\nQuant method\nBits\nSize\nUse case\n3dAnimationDiffusion_v10-Q4_0.gguf\nQ4_0\n4\n1.57 GB\n3dAnimationDiffusion_v10-Q4_1.gguf\nQ4_1\n4\n1.59 GB\n3dAnimationDiffusion_v10-Q5_0.gguf\nQ5_0\n5\n1.62 GB\n3dAnimationDiffusion_v10-Q5_1.gguf\nQ5_1\n5\n1.64 GB\n3dAnimationDiffusion_v10-Q8_0.gguf\nQ8_0\n8\n1.76 GB\n3dAnimationDiffusion_v10-f16.gguf\nf16\n16\n2.13 GB\nvae-Q8_0.gguf\nQ8_0\n8\n165 MB\nvae-f16.gguf\nf16\n16\n167 MB\nQuantized with stable-diffusion.cpp master-697d000.",
    "yuyouyu/Mistral-Nemo-BD-RP": "Mistral-Nemo-BD-RP\nIntroduction ðŸŽ‰\nTraining details ðŸš€\nRequirements ðŸ“\nQuickstart ðŸ’¥\nEvaluation ðŸ†\nCitation ðŸ“–\nAcknowledgements ðŸ¥°\nMistral-Nemo-BD-RP\nIntroduction ðŸŽ‰\nMistral-Nemo-BD-RP is a large language model (LLM) fine-tuned on the BeyondDialogue dataset. The model is designed to generate responses in a role-playing setting. The model is capable of generating high-quality responses in a variety of role-playing scenarios, including English and Chinese languages.\nFor more details, please refer to our paper, GitHub.\nTraining details ðŸš€\nWe fully finetuning Mistral-Nemo-Instruct-2407 for 3 epochs with 833 steps with the 128 global batch size. We set the training sequence length to 4,096. The learning rate is 3e-5. The training data is from the BeyondDialogue dataset.\nRequirements ðŸ“\nThe code of Mistral has been in the latest Hugging face transformers and we advise you to install transformers>=4.37.0 to use the model.\npip install transformers>=4.42.0\nQuickstart ðŸ’¥\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\nchatbot = pipeline(\"text-generation\", model=\"yuyouyu/Mistral-Nemo-BD-RP\", device_map=\"auto\")\nsystem_prompt_temp = \"\"\"I want you to answer questions as if you are {role_name}, assuming you live in the world of {world} and mimicking {role_name}'s personality and speaking style. Use the tone, manner, and vocabulary that {role_name} would use. Please do not reveal that you are an AI or language model; you must always remember you are {role_name}.\n{role_name}'s character traits are {character}.\n{role_name}'s MBTI personality type is {MBTI}.\n{role_name}'s speaking style is {stryle}.\nCurrent scene:\n{scene}\nrole's emotion (0-10, the higher the value, the more pronounced the emotion):\n{emotion}\nNow, please act as {role_name} and reply with a brief sentence to {chat_role}. Your intimacy level with them is {relationship} (0-10, the higher the value, the closer the relationship). Accurately display the MBTI personality, character traits, speaking style, and emotion you have been assigned.\"\"\"\nrole_name = \"Hamlet\"\nworld = \"8th Century Danish Royalty\"\ncharacter = \"extreme, strong, decisive\"\nMBTI = \"Extraverted (E), Intuitive (N), Feeling (F), Judging (J)\"\nstyle = \"indecisive, decisive, sentimental\"\nscene = \"Inside the grand hall of Elsinore, lit by flickering torchlight, Hamlet paces anxiously as Elena conjures an ethereal mirage of the Danish landscape. Regal tapestries and opulent furnishings surround them, yet Hamlet's gaze is fixed on Elena's illusions. She gracefully weaves dissonance into the tapestry of reality, prompting Hamlet to clutch his chest in a moment of existential crisis. The weight of unspoken love and inner turmoil hangs in the air, thick with tension and anticipation.\"\nemotion = \"happiness: 1, sadness: 8, disgust: 5, fear: 7, surprise: 6, anger: 4\"\nchat_role = \"Elena\"\nrelationship = \"7\"\nsystem_prompt = system_prompt_temp.format(\nrole_name=role_name,\nworld=world,\ncharacter=character,\nMBTI=MBTI,\nstyle=style,\nscene=scene,\nemotion=emotion,\nchat_role=chat_role,\nrelationship=relationship\n)\nprompt = \"Oh, dear Hamlet, dost thou see in these conjured whispers the paths unseen? Speak, for shadows may guide us to the truth bound within thy tormented soul.\"\nmessages = [\n{\"role\": \"system\", \"content\": system_prompt},\n{\"role\": \"user\", \"content\": prompt}\n]\nresponse = chatbot(messages, max_new_tokens=256, pad_token_id=chatbot.tokenizer.eos_token_id, do_sample=True, temperature=0.7)[0]['generated_text'][-1]['content']\nNote: The examples for Mistral-Nemo-BD-RP use English role-playing. For English examples, please refer to our other training model repository -- Qwen2-7B-BD-RP.\nEvaluation ðŸ†\nWe use objective questions to assess eight dimensions: Character, Style, Emotion, Relationship, Personality, Human-likeness, Coherence, and Role Consistency. The metric design can be find in our paper. The evaluation code can be found in GitHub. The results are shown below:\nModel\nCharacter â†‘\nStyle â†‘\nEmotion â†“\nRelationship â†“\nPersonality â†‘\nAvg. â†‘\nHuman-likeness â†‘\nRole Choice â†‘\nCoherence â†‘\nGeneral Baselines(Proprietary)\nGPT-4o\n74.32 Â± 1.15\n81.67 Â± 1.51\n16.31 Â± 0.48\n12.13 Â± 0.66\n66.58 Â± 4.41\n78.83 Â± 1.64\n67.33 Â± 3.95\n87.33 Â± 3.86\n99.67 Â± 0.33\nGPT-3.5-Turbo\n72.26 Â± 1.27\n73.66 Â± 1.73\n17.79 Â± 0.56\n14.17 Â± 0.73\n66.92 Â± 4.85\n76.18 Â± 1.83\n33.33 Â± 4.43\n83.00 Â± 4.68\n97.33 Â± 1.17\nMoonshot-v1-8k\n74.06 Â± 1.19\n80.64 Â± 1.51\n16.17 Â± 0.47\n13.42 Â± 0.70\n67.00 Â± 4.87\n78.42 Â± 1.75\n44.00 Â± 4.33\n86.67 Â± 3.75\n99.33 Â± 0.46\nYi-Large-Turbo\n75.13 Â± 1.22\n79.18 Â± 1.58\n16.44 Â± 0.49\n13.48 Â± 0.67\n68.25 Â± 4.61\n78.53 Â± 1.72\n47.00 Â± 4.60\n84.33 Â± 3.67\n92.67 Â± 2.39\nDeepseek-Chat\n75.46 Â± 1.14\n81.49 Â± 1.51\n15.92 Â± 0.46\n12.42 Â± 0.63\n67.92 Â± 4.57\n79.30 Â± 1.66\n52.33 Â± 4.95\n83.00 Â± 4.68\n96.67 Â± 1.00\nBaichuan4\n71.82 Â± 1.25\n76.92 Â± 1.52\n17.57 Â± 0.52\n12.30 Â± 0.62\n67.08 Â± 4.75\n77.19 Â± 1.73\n45.33 Â± 4.31\n82.33 Â± 4.49\n99.33 Â± 0.46\nHunyuan\n73.77 Â± 1.18\n78.75 Â± 1.56\n17.24 Â± 0.48\n13.22 Â± 0.68\n67.00 Â± 4.39\n77.81 Â± 1.66\n53.00 Â± 4.29\n84.33 Â± 4.52\n98.33 Â± 0.84\nRole-play Expertise Baselines\nIndex-1.9B-Character\n73.33 Â± 1.32\n76.48 Â± 1.50\n17.99 Â± 0.53\n13.58 Â± 0.71\n66.33 Â± 4.57\n76.92 Â± 1.73\n21.67 Â± 3.96\n78.67 Â± 5.14\n69.67 Â± 3.85\nCharacterGLM-6B\n73.36 Â± 1.28\n76.08 Â± 1.55\n18.58 Â± 0.55\n14.27 Â± 0.79\n67.33 Â± 4.34\n76.79 Â± 1.70\n16.00 Â± 2.38\n81.00 Â± 4.40\n25.67 Â± 3.48\nBaichuan-NPC-Turbo\n75.19 Â± 1.23\n79.15 Â± 1.38\n17.24 Â± 0.51\n13.10 Â± 0.69\n65.33 Â± 4.84\n77.87 Â± 1.73\n56.00 Â± 4.66\n86.33 Â± 4.90\n99.00 Â± 0.56\nGeneral Baselines(Open-source)\nYi-1.5-9B-Chat\n75.31 Â± 1.20\n76.78 Â± 1.49\n16.67 Â± 0.52\n12.75 Â± 0.66\n67.42 Â± 4.63\n78.02 Â± 1.70\n38.67 Â± 4.39\n84.00 Â± 4.61\n92.67 Â± 1.79\nGLM-4-9b-chat\n74.26 Â± 1.19\n78.40 Â± 1.55\n17.18 Â± 0.50\n14.48 Â± 0.74\n67.17 Â± 4.93\n77.63 Â± 1.78\n47.67 Â± 4.25\n83.33 Â± 4.51\n99.33 Â± 0.46\nQwen2-7B-Instruct\n75.39 Â± 1.13\n77.68 Â± 1.65\n17.64 Â± 0.56\n13.43 Â± 0.7\n67.75 Â± 4.44\n77.95 Â± 1.70\n48.00 Â± 4.66\n83.33 Â± 4.48\n99.00 Â± 0.56\nMistral-Nemo-Instruct-2407\n74.12 Â± 1.17\n77.04 Â± 1.48\n17.00 Â± 0.43\n13.50 Â± 0.67\n67.00 Â± 4.30\n77.53 Â± 1.61\n53.67 Â± 4.66\n82.67 Â± 4.77\n74.33 Â± 3.77\nMistral-Nemo-BD-RP\n74.58 Â± 1.28\n78.47 Â± 1.45\n16.62 Â± 0.48\n11.38 Â± 0.67*\n69.08 Â± 4.46\n78.83 Â± 1.67\n59.00 Â± 4.46\n87.00 Â± 4.73\n92.67 Â± 1.59\nCitation ðŸ“–\nPlease cite our work if you found the resources in this repository useful:\n@article{yu2024beyond,\ntitle   = {BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General Role-Playing Language Model},\nauthor  = {Yu, Yeyong and Yu, Runsheng and Wei, Haojie and Zhang, Zhanqiu and Qian, Quan},\nyear    = {2024},\njournal = {arXiv preprint arXiv:2408.10903},\n}\nAcknowledgements ðŸ¥°\nWe would like to express our sincere gratitude to Tencent LightSpeed Studios for their invaluable support in this project. Their contributions and encouragement have been instrumental in the successful completion of our work.",
    "stable-diffusion-v1-5/stable-diffusion-inpainting": "Stable Diffusion Inpainting model card\nExamples:\nUse with Diffusers\nUse with Original GitHub Repository or AUTOMATIC1111\nModel Details\nUses\nDirect Use\nMisuse, Malicious Use, and Out-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nTraining\nEvaluation Results\nInpainting Evaluation\nEnvironmental Impact\nCitation\nStable Diffusion Inpainting model card\nâš ï¸ This repository is a mirror of the now deprecated ruwnayml/stable-diffusion-inpainting, this repository or oganization are not affiliated in any way with RunwayML.\nModifications to the original model card are in red or green\nStable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.\nThe Stable-Diffusion-Inpainting was initialized with the weights of the Stable-Diffusion-v-1-2. First 595k steps regular training, then 440k steps of inpainting training at resolution 512x512 on â€œlaion-aesthetics v2 5+â€ and 10% dropping of the text-conditioning to improve classifier-free classifier-free guidance sampling. For inpainting, the UNet has 5 additional input channels (4 for the encoded masked-image and 1 for the mask itself) whose weights were zero-initialized after restoring the non-inpainting checkpoint. During training, we generate synthetic masks and in 25% mask everything.\nOpen In Spaces\nExamples:\nYou can use this both with the ðŸ§¨Diffusers library and RunwayML GitHub repository (now deprecated), Automatic1111.\nUse with Diffusers\nfrom diffusers import StableDiffusionInpaintPipeline\npipe = StableDiffusionInpaintPipeline.from_pretrained(\n\"sd-legacy/stable-diffusion-inpainting\",\nrevision=\"fp16\",\ntorch_dtype=torch.float16,\n)\nprompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\n#image and mask_image should be PIL images.\n#The mask structure is white for inpainting and black for keeping as is\nimage = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]\nimage.save(\"./yellow_cat_on_park_bench.png\")\nHow it works:\nimage\nmask_image\nprompt\nOutput\nFace of a yellow cat, high resolution, sitting on a park bench\nUse with Original GitHub Repository or AUTOMATIC1111\nDownload the weights sd-v1-5-inpainting.ckpt\nFollow instructions here (now deprecated).\nUse it with AUTOMATIC1111\nModel Details\nDeveloped by: Robin Rombach, Patrick Esser\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: The CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based.\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (CLIP ViT-L/14) as suggested in the Imagen paper.\nResources for more information: GitHub Repository, Paper.\nCite as:\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and\ntasks include\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nExcluded uses are described below.\nMisuse, Malicious Use, and Out-of-Scope Use\nNote: This section is taken from the DALLE-MINI model card, but applies in the same way to Stable Diffusion v1.\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nMisuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\nImpersonating individuals without their consent.\nSexual content without consent of the people who might see it.\nMis- and disinformation\nRepresentations of egregious violence and gore\nSharing of copyrighted or licensed material in violation of its terms of use.\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to â€œA red cube on top of a blue sphereâ€\nFaces and people in general may not be generated properly.\nThe model was trained mainly with English captions and will not work as well in other languages.\nThe autoencoding part of the model is lossy\nThe model was trained on a large-scale dataset\nLAION-5B which contains adult material\nand is not fit for product use without additional safety mechanisms and\nconsiderations.\nNo additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.\nThe training data can be searched at https://rom1504.github.io/clip-retrieval/ to possibly assist in the detection of memorized images.\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nStable Diffusion v1 was trained on subsets of LAION-2B(en),\nwhich consists of images that are primarily limited to English descriptions.\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for.\nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the\nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nTraining\nTraining Data\nThe model developers used the following dataset for training the model:\nLAION-2B (en) and subsets thereof (see next section)\nTraining Procedure\nStable Diffusion v1 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training,\nImages are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\nText prompts are encoded through a ViT-L/14 text-encoder.\nThe non-pooled output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\nThe loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet.\nWe currently provide six checkpoints, sd-v1-1.ckpt, sd-v1-2.ckpt and sd-v1-3.ckpt, sd-v1-4.ckpt, sd-v1-5.ckpt and sd-v1-5-inpainting.ckpt\nwhich were trained as follows,\nsd-v1-1.ckpt: 237k steps at resolution 256x256 on laion2B-en.\n194k steps at resolution 512x512 on laion-high-resolution (170M examples from LAION-5B with resolution >= 1024x1024).\nsd-v1-2.ckpt: Resumed from sd-v1-1.ckpt.\n515k steps at resolution 512x512 on \"laion-improved-aesthetics\" (a subset of laion2B-en,\nfiltered to images with an original size >= 512x512, estimated aesthetics score > 5.0, and an estimated watermark probability < 0.5. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an improved aesthetics estimator).\nsd-v1-3.ckpt: Resumed from sd-v1-2.ckpt. 195k steps at resolution 512x512 on \"laion-improved-aesthetics\" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.\nsd-v1-4.ckpt: Resumed from stable-diffusion-v1-2.225,000 steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10 % dropping of the text-conditioning to classifier-free guidance sampling.\nsd-v1-5.ckpt: Resumed from sd-v1-2.ckpt. 595k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.\nsd-v1-5-inpaint.ckpt: Resumed from sd-v1-2.ckpt. 595k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. Then 440k steps of inpainting training at resolution 512x512 on â€œlaion-aesthetics v2 5+â€ and 10% dropping of the text-conditioning. For inpainting, the UNet has 5 additional input channels (4 for the encoded masked-image and 1 for the mask itself) whose weights were zero-initialized after restoring the non-inpainting checkpoint. During training, we generate synthetic masks and in 25% mask everything.\nHardware: 32 x 8 x A100 GPUs\nOptimizer: AdamW\nGradient Accumulations: 2\nBatch: 32 x 8 x 2 x 4 = 2048\nLearning rate: warmup to 0.0001 for 10,000 steps and then kept constant\nEvaluation Results\nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling\nsteps show the relative improvements of the checkpoints:\nEvaluated using 50 PLMS steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\nInpainting Evaluation\nTo assess the performance of the inpainting model, we used the same evaluation\nprotocol as in our LDM paper. Since the\nStable Diffusion Inpainting Model acccepts a text input, we simply used a fixed\nprompt of photograph of a beautiful empty scene, highest quality settings.\nModel\nFID\nLPIPS\nStable Diffusion Inpainting\n1.00\n0.141 (+- 0.082)\nLatent Diffusion Inpainting\n1.50\n0.137 (+- 0.080)\nCoModGAN\n1.82\n0.15\nLaMa\n2.21\n0.134 (+- 0.080)\nEnvironmental Impact\nStable Diffusion v1 Estimated Emissions\nBased on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\nHardware Type: A100 PCIe 40GB\nHours used: 150000\nCloud Provider: AWS\nCompute Region: US-east\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 11250 kg CO2 eq.\nCitation\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nThis model card was written by: Robin Rombach and Patrick Esser and is based on the DALL-E Mini model card.",
    "Shakker-Labs/AWPortrait-FL": "AWPortrait-FL\nComparison\nInference\nLoRA Inference\nOnline Inference\nAcknowledgements\nAWPortrait-FL\nAWPortrait-FL is finetuned on FLUX.1-dev using the training set of AWPortrait-XL and nearly 2,000 fashion photography photos with extremely high aesthetic quality.\nIt has remarkable improvements in composition and details, with more delicate and realistic skin and textual. Trained by DynamicWang at AWPlanet.\nComparison\nThe following example shows a simple comparison with FLUX.1-dev under the same parameter setting.\nInference\nimport torch\nfrom diffusers import FluxPipeline\npipe = FluxPipeline.from_pretrained(\"Shakker-Labs/AWPortrait-FL\", torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\nprompt = \"close up portrait, Amidst the interplay of light and shadows in a photography studio,a soft spotlight traces the contours of a face,highlighting a figure clad in a sleek black turtleneck. The garment,hugging the skin with subtle luxury,complements the Caucasian model's understated makeup,embodying minimalist elegance. Behind,a pale gray backdrop extends,its fine texture shimmering subtly in the dim light,artfully balancing the composition and focusing attention on the subject. In a palette of black,gray,and skin tones,simplicity intertwines with profundity,as every detail whispers untold stories.\"\nimage = pipe(prompt,\nnum_inference_steps=24,\nguidance_scale=3.5,\nwidth=768, height=1024,\n).images[0]\nimage.save(f\"example.png\")\nLoRA Inference\nTo save memory, we also add a LoRA version to achieve same performance.\nimport torch\nfrom diffusers import FluxPipeline\npipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\npipe.load_lora_weights('Shakker-Labs/AWPortrait-FL', weight_name='AWPortrait-FL-lora.safetensors')\npipe.fuse_lora(lora_scale=0.9)\npipe.to(\"cuda\")\nprompt = \"close up portrait, Amidst the interplay of light and shadows in a photography studio,a soft spotlight traces the contours of a face,highlighting a figure clad in a sleek black turtleneck. The garment,hugging the skin with subtle luxury,complements the Caucasian model's understated makeup,embodying minimalist elegance. Behind,a pale gray backdrop extends,its fine texture shimmering subtly in the dim light,artfully balancing the composition and focusing attention on the subject. In a palette of black,gray,and skin tones,simplicity intertwines with profundity,as every detail whispers untold stories.\"\nimage = pipe(prompt,\nnum_inference_steps=24,\nguidance_scale=3.5,\nwidth=768, height=1024,\n).images[0]\nimage.save(f\"example.png\")\nOnline Inference\nYou can also download this model at Shakker AI, where we provide an online interface to generate images.\nAcknowledgements\nThis model is trained by our copyrighted users DynamicWang. We release this model under permissions. The model follows flux-1-dev-non-commercial-license and the generated images are also non commercial.",
    "Msobhi/Persian_Sentence_Embedding_v3": "SentenceTransformer based on FacebookAI/xlm-roberta-large\nModel Details\nModel Description\nModel Sources\nFull Model Architecture\nUsage\nDirect Usage (Sentence Transformers)\nTraining Details\nTraining Datasets\nTraining Hyperparameters\nTraining Logs\nFramework Versions\nCitation\nBibTeX\nSentenceTransformer based on FacebookAI/xlm-roberta-large\nThis is a sentence-transformers model finetuned from FacebookAI/xlm-roberta-large on the parsinlu_qqp_pair2class, parsinlu_entail_pair3class, pquad_pair, alpaca_persian_pair, ghaemiyeh_pair, wiki_triplet, wiki_DSimilar_pair2class, miracle_triplet, Estef_pair, all_resaleh_pair and persianQA_pair datasets. It maps sentences & paragraphs to a 1024-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.\nModel Details\nModel Description\nModel Type: Sentence Transformer\nBase model: FacebookAI/xlm-roberta-large\nMaximum Sequence Length: 512 tokens\nOutput Dimensionality: 1024 tokens\nSimilarity Function: Cosine Similarity\nTraining Datasets:\nparsinlu_qqp_pair2class\nparsinlu_entail_pair3class\npquad_pair\nalpaca_persian_pair\nghaemiyeh_pair\nwiki_triplet\nwiki_DSimilar_pair2class\nmiracle_triplet\nEstef_pair\nall_resaleh_pair\npersianQA_pair\nLanguages: fa, en, ar, bn, es, fi, fr, hi, id, ja, ko, ru, sw, te, th, zh\nModel Sources\nDocumentation: Sentence Transformers Documentation\nRepository: Sentence Transformers on GitHub\nHugging Face: Sentence Transformers on Hugging Face\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: XLMRobertaModel\n(1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n)\nUsage\nDirect Usage (Sentence Transformers)\nFirst install the Sentence Transformers library:\npip install -U sentence-transformers\nThen you can load this model and run inference.\nfrom sentence_transformers import SentenceTransformer\n# Download from the ðŸ¤— Hub\nmodel = SentenceTransformer(\"sentence_transformers_model_id\")\n# Run inference\nsentences = [\n'Ú†Ù‡ Ø§ØªÙØ§Ù‚ÛŒ Ø¯Ø± Ù…Ø³Ø¬Ø¯ Ø§Ù„Ø­Ø±Ø§Ù… Ø¨Ø±Ø§ÛŒ Ø¹Ø¨Ø¯Ø§Ù„Ù…Ø·Ù„Ø¨ Ùˆ Ù¾Ø³Ø±Ø´ Ø±Ø® Ø¯Ø§Ø¯ØŸ',\n'Ø¯Ø± Ù…Ø³Ø¬Ø¯ Ø§Ù„Ø­Ø±Ø§Ù…ØŒ Ø¹Ø¨Ø¯Ø§Ù„Ù…Ø·Ù„Ø¨ Ùˆ Ù¾Ø³Ø±Ø´ ØªÙˆØ³Ø· Ø¯Ù‡ Ù…Ø±Ø¯ Ù¾Ø§Ø¨Ø±Ù‡Ù†Ù‡ Ùˆ Ø´Ù…Ø´ÛŒØ± Ø¨Ù‡ Ø¯Ø³ØªØŒ Ù…ÙˆØ±Ø¯ Ø­Ù…Ù„Ù‡ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ†Ø¯ Ùˆ Ø§Ø² Ú©Ø´ØªÙ† ÙØ±Ø²Ù†Ø¯ Ø¹Ø¨Ø¯Ø§Ù„Ù…Ø·Ù„Ø¨ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ú©Ø±Ø¯Ù†Ø¯. Ø§ÛŒÙ† Ø­Ø§Ø¯Ø«Ù‡ Ø¨Ø§Ø¹Ø« Ø´Ø¯ Ú©Ù‡ Ù…Ø±Ø¯Ù… Ø¯Ø± Ù…Ø³Ø¬Ø¯ Ø§Ù„Ø­Ø±Ø§Ù… ØºØ±Ù‚ Ø¯Ø± Ù‡ÛŒØ§Ù‡Ùˆ Ø´ÙˆÙ†Ø¯ Ùˆ ØµØ¯Ø§Ù‡Ø§ Ø¯Ø±Ù‡Ù… Ø¢Ù…ÛŒØ®ØªÙ‡ Ùˆ ØµØ¯Ø§ÛŒ Ø²Ù†Ø§Ù† Ù…Ø­Ùˆ Ø´ÙˆØ¯.',\n'Ù‚Ø§Ø¦Ù… Ø¢Ù„ Ù…Ø­Ù…Ø¯ (Øµ) Ø¨Ù‡ Ø§Ø±Ø§Ø¯Ù‡ Ø§Ù„Ù‡ÛŒ Ù‚ÛŒØ§Ù… Ú©Ø±Ø¯Ù‡ Ùˆ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ø§Ùˆ Ù‚ÛŒØ§Ù… Ú©Ù†Ø¯ØŒ Ø¯ÛŒÚ¯Ø± Ø§Ø² Ø´Ø±Ú© Ùˆ Ø´Ø±Ú©\\u200cÚ¯Ø±Ø§ÛŒÛŒ Ø§Ø«Ø±ÛŒ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ù…Ø§Ù†Ø¯ Ùˆ Ø¯ÛŒÙ† Ø­Ù‚ Ù‡Ù…Ù‡ Ø¯Ù„Ù‡Ø§ Ø±Ø§ Ù†ÙˆØ±Ø¨Ø§Ø±Ø§Ù† Ù…ÛŒ\\u200cØ³Ø§Ø²Ø¯. Ø§ÛŒÙ† Ù…Ø·Ù„Ø¨ Ø§Ø² Ø¢ÛŒØ§ØªÛŒ Ú©Ù‡ Ù¾ÛŒØ±Ø§Ù…ÙˆÙ† ÙˆØ¬ÙˆØ¯ Ú¯Ø±Ø§Ù†Ù…Ø§ÛŒÙ‡ Ø§Ùˆ ØªØ£ÙˆÛŒÙ„ Ø´Ø¯Ù‡ Ø§Ø³Øª Ø¨Ø³Ù†Ø¯Ù‡ Ù…ÛŒ\\u200cØ´ÙˆØ¯ Ùˆ Ø§ÛŒÙ† Ø§Ù…Ø± Ø¯Ø± Ù‚Ø±Ø¢Ù† Ùˆ Ø±ÙˆØ§ÛŒØ§Øª Ø¨Ù‡ Ø·ÙˆØ± Ø¬Ø§Ù…Ø¹ Ø¨ÛŒØ§Ù† Ø´Ø¯Ù‡ Ø§Ø³Øª.',\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, 1024]\n# Get the similarity scores for the embeddings\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities.shape)\n# [3, 3]\nTraining Details\nTraining Datasets\nparsinlu_qqp_pair2class\nDataset: parsinlu_qqp_pair2class\nSize: 4,644 training samples\nColumns: sentence1, sentence2, and label\nApproximate statistics based on the first 1000 samples:\nsentence1\nsentence2\nlabel\ntype\nstring\nstring\nint\ndetails\nmin: 5 tokensmean: 15.58 tokensmax: 50 tokens\nmin: 5 tokensmean: 15.91 tokensmax: 100 tokens\n0: ~61.70%1: ~38.30%\nSamples:\nsentence1\nsentence2\nlabel\nÚ†Ú¯ÙˆÙ†Ù‡ Ù…ÛŒ ØªÙˆØ§Ù†Ù… Ú©Ù… Ú©Ù… ÙˆØ²Ù† Ø®ÙˆØ¯ Ø±Ø§ Ú©Ø§Ù‡Ø´ Ø¯Ù‡Ù…ØŸ\nÚ†Ú¯ÙˆÙ†Ù‡ ÙˆØ²Ù† Ú©Ù… Ú©Ù†Ù…ØŸ\n1\nÚ†Ú¯ÙˆÙ†Ù‡ Ø§Ø³ØªÙ…Ù†Ø§Ø¡ Ø¨Ø± Ù‚Ø¯Ø±Øª ØªÙ…Ø±Ú©Ø² Ø´Ø®Øµ ØªØ£Ø«ÛŒØ± Ù…ÛŒ Ú¯Ø°Ø§Ø±Ø¯ØŸ\nØ¢ÛŒØ§ Ú†Ø±Ú© Ø±ÙˆÛŒ Ù„ÙˆØ²Ù‡ Ù‡Ø§ Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ù†Ø´Ø§Ù†Ù‡ Ø§ÛŒ Ø§Ø² STD Ø¨Ø§Ø´Ø¯ØŸ\n0\nÙˆÙ‚ØªÛŒ Ø®ÙˆØ§Ø¨ Ú©Ø³ÛŒ Ø±Ø§ Ø¯ÛŒØ¯Ù… Ùˆ Ú¯ÙØªÙ… Ø¢Ù†Ù‡Ø§ Ø¯Ø± Ø­Ø§Ù„ Ù…Ø±Ú¯ Ù‡Ø³ØªÙ†Ø¯ ØŒ Ú†Ù‡ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¯Ø§Ø±Ø¯ØŸ\nÙˆÙ‚ØªÛŒ Ø®ÙˆØ§Ø¨ Ù…ÛŒ Ø¨ÛŒÙ†Ù… Ú©Ù‡ Ú©Ø³ÛŒ Ø¯Ø± Ø­Ø§Ù„ Ù…Ø±Ú¯ Ø¨Ø§Ø´Ø¯ Ù…Ø¹Ù†ÛŒ Ø§Ø´ Ú†ÛŒØ³ØªØŸ\n0\nLoss: ContrastiveLoss with these parameters:{\n\"distance_metric\": \"SiameseDistanceMetric.COSINE_DISTANCE\",\n\"margin\": 0.5,\n\"size_average\": true\n}\nparsinlu_entail_pair3class\nDataset: parsinlu_entail_pair3class at c49b2d8\nSize: 2,697 training samples\nColumns: sentence1, sentence2, and label\nApproximate statistics based on the first 1000 samples:\nsentence1\nsentence2\nlabel\ntype\nstring\nstring\nint\ndetails\nmin: 3 tokensmean: 34.16 tokensmax: 203 tokens\nmin: 3 tokensmean: 17.89 tokensmax: 73 tokens\n0: ~39.30%1: ~31.60%2: ~29.10%\nSamples:\nsentence1\nsentence2\nlabel\nØ²Ù†Ø§Ù† Ø¨Ù‡ Ù‚Ø¯Ø±ÛŒ Ø¨Ø®Ø´ Ø¨Ø²Ø±Ú¯ÛŒ Ø§Ø² Ù†ÛŒØ±ÙˆÛŒ Ú©Ø§Ø± Ø±Ø§ ØªØ´Ú©ÛŒÙ„ Ù…ÛŒ Ø¯Ù‡Ù†Ø¯ Ú©Ù‡ Ø¨Ù‡ Ø³Ø®ØªÛŒ Ù…ÛŒ ØªÙˆØ§Ù† Ø¨Ø§ÙˆØ± Ø¯Ø§Ø´Øª Ú©Ù‡ Ø§Ú¯Ø± Ø§ÛŒÙ† Ø§Ù…Ø± Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø²Ù†Ø§Ù†  ØµØ§Ø¯Ù‚ Ù†Ø¨Ø§Ø´Ø¯ ØŒ Ø§ÛŒÙ† Ø§Ù…Ø± Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ ØµØ§Ø¯Ù‚ Ø¨Ø§Ø´Ø¯.\nÙ…Ø±Ø¯Ø§Ù† Ø¨Ø®Ø´ Ø¹Ø¸ÛŒÙ…ÛŒ Ø§Ø² Ù†ÛŒØ±ÙˆÛŒ Ú©Ø§Ø± Ù‡Ø³ØªÙ†Ø¯ Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† ØªÙ†Ù‡Ø§ Ø§ÙØ±Ø§Ø¯ Ù…Ù‡Ù… Ù‡Ø³ØªÙ†Ø¯.\n2\nØ³Ø§Ù„Ù‡Ø§ Ø§Ø³Øª Ú©Ù‡ Ú©Ù†Ú¯Ø±Ù‡ Ø¯Ø± ØªÙ„Ø§Ø´ Ø§Ø³Øª ØªØ§ Ø§Ø«Ø±Ø¨Ø®Ø´ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒ Ø±Ø§ Ø¯Ø± Ø¯ÙˆÙ„Øª ÙØ¯Ø±Ø§Ù„ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡Ø¯.\nÚ©Ù†Ú¯Ø±Ù‡ Ø¨ÙˆØ¯Ø¬Ù‡ ÙˆÛŒÚ˜Ù‡ Ø§ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒ Ø¯Ø± Ø¯ÙˆÙ„Øª ÙØ¯Ø±Ø§Ù„ Ø¯Ø§Ø±Ø¯.\n1\nØ³Ø±Ø§Ù…ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ³Øª Ø®Ù†Ø«ÛŒ Ù¾Ø³ Ø§Ø² Ù‚Ø±Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø¯Ø± Ø¨Ø¯Ù† Ù…ÛŒØ²Ø¨Ø§Ù† Ø®ÙˆØ§Øµ ÙÛŒØ²ÛŒÚ©ÛŒ Ùˆ Ù…Ú©Ø§Ù†ÛŒÚ©ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø­ÙØ¸ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\nØ®ÙˆØ§Øµ ÙÛŒØ²ÛŒÚ©ÛŒ Ø³Ø±Ø§Ù…ÛŒÚ©â€ŒÙ‡Ø§ Ù‚Ø§Ø¨Ù„ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú¯ÛŒØ±ÛŒ Ø§Ø³Øª.\n1\nLoss: SoftmaxLoss\npquad_pair\nDataset: pquad_pair\nSize: 79,972 training samples\nColumns: positive and anchor\nApproximate statistics based on the first 1000 samples:\npositive\nanchor\ntype\nstring\nstring\ndetails\nmin: 19 tokensmean: 183.65 tokensmax: 366 tokens\nmin: 5 tokensmean: 13.95 tokensmax: 36 tokens\nSamples:\npositive\nanchor\nØ¨Ø§Ø´Ú¯Ø§Ù‡ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù“Ø±Ø³Ù†Ø§Ù„ (Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ: Arsenal Football Club) ÛŒÚ© Ø¨Ø§Ø´Ú¯Ø§Ù‡ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¯Ø± Ø´Ù…Ø§Ù„ Ø´Ù‡Ø± Ù„Ù†Ø¯Ù† Ø§Ø³Øª Ú©Ù‡ Ù…ÙˆÙÙ‚ Ø¨Ù‡ Ú©Ø³Ø¨ Û±Û³ Ø¹Ù†ÙˆØ§Ù† Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¯Ø± Ù„ÛŒÚ¯ Ø¯Ø³ØªÙ‡ Ø§ÙˆÙ„ Ùˆ Ù„ÛŒÚ¯ Ø¨Ø±ØªØ± Ø§Ù†Ú¯Ù„Ø³ØªØ§Ù†ØŒ Û±Û´ Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø¬Ø§Ù… Ø­Ø°ÙÛŒ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù†Ú¯Ù„Ø³ØªØ§Ù† ØŒ Û±Û¶ Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø¬Ø§Ù… Ø®ÛŒØ±ÛŒÙ‡ Ø§Ù†Ú¯Ù„Ø³ØªØ§Ù† Ùˆ Ø¯Ùˆ Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø¬Ø§Ù… Ø§ØªØ­Ø§Ø¯ÛŒÙ‡ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù†Ú¯Ù„Ø³ØªØ§Ù† Ø´Ø¯Ù‡â€ŒØ§Ø³Øª. Ø§Ù“Ù†â€ŒÙ‡Ø§ Ø±Ú©ÙˆØ±Ø¯Ø¯Ø§Ø± Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ±ÛŒÙ† Ù…Ø¯Øª ØµØ¯Ø±Ù†Ø´ÛŒÙ†ÛŒ Ø¨Ø¯ÙˆÙ† ÙˆÙ‚ÙÙ‡ Ø¯Ø± Ù„ÛŒÚ¯ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù†Ú¯Ù„ÛŒØ³ØŒ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø¨Ø§Ø²ÛŒ Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø®ØªÙ Ù¾ÛŒØ§Ù¾ÛŒ (Û´Û¹ Ø¨Ø§Ø²ÛŒ) Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¨Ø¯ÙˆÙ† Ø´Ú©Ø³Øª Ø¯Ø± ÛŒÚ© ÙØµÙ„ (Û°Û´â€“Û²Û°Û°Û³) Ù…ÛŒâ€ŒØ¨Ø§Ø´Ù†Ø¯ Ùˆ ØªÙˆØ§Ù†Ø³ØªÙ†Ø¯ Ø§ÙˆÙ„ÛŒÙ† Ùˆ ØªÙ†Ù‡Ø§ ØªÛŒÙ…ÛŒ Ø¯Ø± ØªØ§Ø±ÛŒØ® Ù„ÛŒÚ¯ Ø¨Ø±ØªØ± Ø¨Ø§Ø´Ù†Ø¯ Ú©Ù‡ Ø¬Ø§Ù… Ø·Ù„Ø§ÛŒÛŒ Ø±Ø§ Ø¨Ø¯Ø³Øª Ù…ÛŒâ€ŒØ§Ù“ÙˆØ±Ù†Ø¯.\nÙ…ÙˆÙ‚Ø¹ÛŒØª Ø¬ØºØ±Ø§ÙÛŒ Ø¨Ø§Ø´Ú¯Ø§Ù‡ ÙÙˆØªØ¨Ø§Ù„ Ø¢Ø±Ø³Ù†Ø§Ù„ Ø±Ø§ Ø¨Ú¯ÙˆÛŒÛŒØ¯ØŸ\nØ¨Ø§Ø´Ú¯Ø§Ù‡ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù“Ø±Ø³Ù†Ø§Ù„ (Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ: Arsenal Football Club) ÛŒÚ© Ø¨Ø§Ø´Ú¯Ø§Ù‡ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¯Ø± Ø´Ù…Ø§Ù„ Ø´Ù‡Ø± Ù„Ù†Ø¯Ù† Ø§Ø³Øª Ú©Ù‡ Ù…ÙˆÙÙ‚ Ø¨Ù‡ Ú©Ø³Ø¨ Û±Û³ Ø¹Ù†ÙˆØ§Ù† Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¯Ø± Ù„ÛŒÚ¯ Ø¯Ø³ØªÙ‡ Ø§ÙˆÙ„ Ùˆ Ù„ÛŒÚ¯ Ø¨Ø±ØªØ± Ø§Ù†Ú¯Ù„Ø³ØªØ§Ù†ØŒ Û±Û´ Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø¬Ø§Ù… Ø­Ø°ÙÛŒ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù†Ú¯Ù„Ø³ØªØ§Ù† ØŒ Û±Û¶ Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø¬Ø§Ù… Ø®ÛŒØ±ÛŒÙ‡ Ø§Ù†Ú¯Ù„Ø³ØªØ§Ù† Ùˆ Ø¯Ùˆ Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø¬Ø§Ù… Ø§ØªØ­Ø§Ø¯ÛŒÙ‡ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù†Ú¯Ù„Ø³ØªØ§Ù† Ø´Ø¯Ù‡â€ŒØ§Ø³Øª. Ø§Ù“Ù†â€ŒÙ‡Ø§ Ø±Ú©ÙˆØ±Ø¯Ø¯Ø§Ø± Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ±ÛŒÙ† Ù…Ø¯Øª ØµØ¯Ø±Ù†Ø´ÛŒÙ†ÛŒ Ø¨Ø¯ÙˆÙ† ÙˆÙ‚ÙÙ‡ Ø¯Ø± Ù„ÛŒÚ¯ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù†Ú¯Ù„ÛŒØ³ØŒ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø¨Ø§Ø²ÛŒ Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø®ØªÙ Ù¾ÛŒØ§Ù¾ÛŒ (Û´Û¹ Ø¨Ø§Ø²ÛŒ) Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¨Ø¯ÙˆÙ† Ø´Ú©Ø³Øª Ø¯Ø± ÛŒÚ© ÙØµÙ„ (Û°Û´â€“Û²Û°Û°Û³) Ù…ÛŒâ€ŒØ¨Ø§Ø´Ù†Ø¯ Ùˆ ØªÙˆØ§Ù†Ø³ØªÙ†Ø¯ Ø§ÙˆÙ„ÛŒÙ† Ùˆ ØªÙ†Ù‡Ø§ ØªÛŒÙ…ÛŒ Ø¯Ø± ØªØ§Ø±ÛŒØ® Ù„ÛŒÚ¯ Ø¨Ø±ØªØ± Ø¨Ø§Ø´Ù†Ø¯ Ú©Ù‡ Ø¬Ø§Ù… Ø·Ù„Ø§ÛŒÛŒ Ø±Ø§ Ø¨Ø¯Ø³Øª Ù…ÛŒâ€ŒØ§Ù“ÙˆØ±Ù†Ø¯.\nÙ„ÛŒÚ¯ Ø¨Ø±ØªØ± Ø§Ù†Ú¯Ù„Ø³ØªØ§Ù† Ù…ÙˆÙÙ‚ Ø¨Ù‡ Ú©Ø³Ø¨ Ú†Ù†Ø¯ Ø¹Ù†ÙˆØ§Ù† Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø¬Ø§Ù… Ø­Ø°ÙÛŒ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù†Ú¯Ù„Ø³ØªØ§Ù† Ø´Ø¯Ù‡ Ø§Ø³ØªØŸ\nØ¨Ø§Ø´Ú¯Ø§Ù‡ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù“Ø±Ø³Ù†Ø§Ù„ (Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ: Arsenal Football Club) ÛŒÚ© Ø¨Ø§Ø´Ú¯Ø§Ù‡ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¯Ø± Ø´Ù…Ø§Ù„ Ø´Ù‡Ø± Ù„Ù†Ø¯Ù† Ø§Ø³Øª Ú©Ù‡ Ù…ÙˆÙÙ‚ Ø¨Ù‡ Ú©Ø³Ø¨ Û±Û³ Ø¹Ù†ÙˆØ§Ù† Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¯Ø± Ù„ÛŒÚ¯ Ø¯Ø³ØªÙ‡ Ø§ÙˆÙ„ Ùˆ Ù„ÛŒÚ¯ Ø¨Ø±ØªØ± Ø§Ù†Ú¯Ù„Ø³ØªØ§Ù†ØŒ Û±Û´ Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø¬Ø§Ù… Ø­Ø°ÙÛŒ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù†Ú¯Ù„Ø³ØªØ§Ù† ØŒ Û±Û¶ Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø¬Ø§Ù… Ø®ÛŒØ±ÛŒÙ‡ Ø§Ù†Ú¯Ù„Ø³ØªØ§Ù† Ùˆ Ø¯Ùˆ Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø¬Ø§Ù… Ø§ØªØ­Ø§Ø¯ÛŒÙ‡ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù†Ú¯Ù„Ø³ØªØ§Ù† Ø´Ø¯Ù‡â€ŒØ§Ø³Øª. Ø§Ù“Ù†â€ŒÙ‡Ø§ Ø±Ú©ÙˆØ±Ø¯Ø¯Ø§Ø± Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ±ÛŒÙ† Ù…Ø¯Øª ØµØ¯Ø±Ù†Ø´ÛŒÙ†ÛŒ Ø¨Ø¯ÙˆÙ† ÙˆÙ‚ÙÙ‡ Ø¯Ø± Ù„ÛŒÚ¯ ÙÙˆØªØ¨Ø§Ù„ Ø§Ù†Ú¯Ù„ÛŒØ³ØŒ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø¨Ø§Ø²ÛŒ Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø®ØªÙ Ù¾ÛŒØ§Ù¾ÛŒ (Û´Û¹ Ø¨Ø§Ø²ÛŒ) Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ Ø¨Ø¯ÙˆÙ† Ø´Ú©Ø³Øª Ø¯Ø± ÛŒÚ© ÙØµÙ„ (Û°Û´â€“Û²Û°Û°Û³) Ù…ÛŒâ€ŒØ¨Ø§Ø´Ù†Ø¯ Ùˆ ØªÙˆØ§Ù†Ø³ØªÙ†Ø¯ Ø§ÙˆÙ„ÛŒÙ† Ùˆ ØªÙ†Ù‡Ø§ ØªÛŒÙ…ÛŒ Ø¯Ø± ØªØ§Ø±ÛŒØ® Ù„ÛŒÚ¯ Ø¨Ø±ØªØ± Ø¨Ø§Ø´Ù†Ø¯ Ú©Ù‡ Ø¬Ø§Ù… Ø·Ù„Ø§ÛŒÛŒ Ø±Ø§ Ø¨Ø¯Ø³Øª Ù…ÛŒâ€ŒØ§Ù“ÙˆØ±Ù†Ø¯.\nØ¨ÛŒØ´ØªØ±ÛŒÙ† Ø¨Ø§Ø²ÛŒ Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø®Øª Ù¾ÛŒØ§Ù¾ÛŒ Ù…ØªØ¹Ù„Ù‚ Ø¨Ù‡ Ú©Ø¯Ø§Ù… Ø¨Ø§Ø´Ú¯Ø§Ù‡ Ø§Ø³ØªØŸ\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nalpaca_persian_pair\nDataset: alpaca_persian_pair at dd503f5\nSize: 35,117 training samples\nColumns: anchor and positive\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\ntype\nstring\nstring\ndetails\nmin: 2 tokensmean: 17.2 tokensmax: 249 tokens\nmin: 2 tokensmean: 198.32 tokensmax: 512 tokens\nSamples:\nanchor\npositive\nØ³Ù‡ Ù†Ú©ØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ù„Ù… Ù…Ø§Ù†Ø¯Ù† Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯.\n1. Ø±Ú˜ÛŒÙ… ØºØ°Ø§ÛŒÛŒ Ù…ØªØ¹Ø§Ø¯Ù„ Ùˆ Ù…ØºØ°ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯: Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ Ú©Ù‡ ÙˆØ¹Ø¯Ù‡ Ù‡Ø§ÛŒ ØºØ°Ø§ÛŒÛŒ Ø´Ù…Ø§ Ø´Ø§Ù…Ù„ Ø§Ù†ÙˆØ§Ø¹ Ù…ÛŒÙˆÙ‡ Ù‡Ø§ Ùˆ Ø³Ø¨Ø²ÛŒØ¬Ø§ØªØŒ Ù¾Ø±ÙˆØªØ¦ÛŒÙ† Ø¨Ø¯ÙˆÙ† Ú†Ø±Ø¨ÛŒØŒ ØºÙ„Ø§Øª Ú©Ø§Ù…Ù„ Ùˆ Ú†Ø±Ø¨ÛŒ Ù‡Ø§ÛŒ Ø³Ø§Ù„Ù… Ø§Ø³Øª. Ø§ÛŒÙ† Ø¨Ù‡ Ø¨Ø¯Ù† Ø´Ù…Ø§ Ú©Ù…Ú© Ù…ÛŒ Ú©Ù†Ø¯ ØªØ§ Ù…ÙˆØ§Ø¯ Ù…ØºØ°ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¯ ÙØ±Ø§Ù‡Ù… Ú©Ù†Ø¯ Ùˆ Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ Ù¾ÛŒØ´Ú¯ÛŒØ±ÛŒ Ø§Ø² Ø¨ÛŒÙ…Ø§Ø±ÛŒ Ù‡Ø§ÛŒ Ù…Ø²Ù…Ù† Ú©Ù…Ú© Ú©Ù†Ø¯.2. Ø¯Ø± ÙØ¹Ø§Ù„ÛŒØª Ø¨Ø¯Ù†ÛŒ Ù…Ù†Ø¸Ù… Ø´Ø±Ú©Øª Ú©Ù†ÛŒØ¯: ÙˆØ±Ø²Ø´ Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ø§Ø³ØªØ®ÙˆØ§Ù† Ù‡Ø§ØŒ Ù…Ø§Ù‡ÛŒÚ†Ù‡ Ù‡Ø§ Ùˆ Ø³Ù„Ø§Ù…Øª Ù‚Ù„Ø¨ Ùˆ Ø¹Ø±ÙˆÙ‚ Ù‚ÙˆÛŒ Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù… Ø§Ø³Øª. Ø­Ø¯Ø§Ù‚Ù„ 150 Ø¯Ù‚ÛŒÙ‚Ù‡ ÙˆØ±Ø²Ø´ Ù‡ÙˆØ§Ø²ÛŒ Ù…ØªÙˆØ³Ø· ÛŒØ§ 75 Ø¯Ù‚ÛŒÙ‚Ù‡ ÙˆØ±Ø²Ø´ Ø´Ø¯ÛŒØ¯ Ø¯Ø± Ù‡ÙØªÙ‡ Ø±Ø§ Ù‡Ø¯Ù Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯.3. Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø®ÙˆØ§Ø¨ÛŒØ¯: Ø¯Ø§Ø´ØªÙ† Ø®ÙˆØ§Ø¨ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ù„Ø§Ù…Øª Ø¬Ø³Ù…ÛŒ Ùˆ Ø±ÙˆØ§Ù†ÛŒ Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù… Ø§Ø³Øª. Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… Ø®Ù„Ù‚ Ùˆ Ø®ÙˆØŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø´Ù†Ø§Ø®ØªÛŒ Ùˆ Ø­Ù…Ø§ÛŒØª Ø§Ø² Ø±Ø´Ø¯ Ø³Ø§Ù„Ù… Ùˆ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø³ÛŒØ³ØªÙ… Ø§ÛŒÙ…Ù†ÛŒ Ú©Ù…Ú© Ù…ÛŒ Ú©Ù†Ø¯. Ù‡Ø± Ø´Ø¨ Û· ØªØ§ Û¹ Ø³Ø§Ø¹Øª Ø¨Ø®ÙˆØ§Ø¨ÛŒØ¯.\nØ³Ù‡ Ø±Ù†Ú¯ Ø§ØµÙ„ÛŒ Ú©Ø¯Ø§Ù…Ù†Ø¯ØŸ\nØ³Ù‡ Ø±Ù†Ú¯ Ø§ØµÙ„ÛŒ Ù‚Ø±Ù…Ø²ØŒ Ø¢Ø¨ÛŒ Ùˆ Ø²Ø±Ø¯ Ù‡Ø³ØªÙ†Ø¯. Ø§ÛŒÙ† Ø±Ù†Ú¯â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ Ø§ÛŒÙ† Ø¯Ù„ÛŒÙ„ Ø§ØµÙ„ÛŒ Ù…ÛŒâ€ŒÙ†Ø§Ù…Ù†Ø¯ Ú©Ù‡ Ø¨Ø§ Ø§Ø®ØªÙ„Ø§Ø· Ø±Ù†Ú¯â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ùˆ Ù‡Ù…Ù‡ Ø±Ù†Ú¯â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø±Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ø§ ØªØ±Ú©ÛŒØ¨ Ø¢Ù†Ù‡Ø§ Ø¨Ù‡ Ù†Ø³Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø³Ø§Ø®Øª. Ø¯Ø± Ø³ÛŒØ³ØªÙ… Ø±Ù†Ú¯ Ø§ÙØ²ÙˆØ¯Ù†ÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒ Ø´ÙˆØ¯ØŒ Ø±Ù†Ú¯ Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ù‚Ø±Ù…Ø²ØŒ Ø³Ø¨Ø² Ùˆ Ø¢Ø¨ÛŒ (RGB) Ù‡Ø³ØªÙ†Ø¯.\nØ³Ø§Ø®ØªØ§Ø± ÛŒÚ© Ø§ØªÙ… Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯.\nØ§ØªÙ… Ø¨Ù„ÙˆÚ© Ø§ØµÙ„ÛŒ Ù‡Ù…Ù‡ Ù…ÙˆØ§Ø¯ Ø§Ø³Øª Ùˆ Ø§Ø² Ø³Ù‡ Ù†ÙˆØ¹ Ø°Ø±Ù‡ ØªØ´Ú©ÛŒÙ„ Ø´Ø¯Ù‡ Ø§Ø³Øª: Ù¾Ø±ÙˆØªÙˆÙ†ØŒ Ù†ÙˆØªØ±ÙˆÙ† Ùˆ Ø§Ù„Ú©ØªØ±ÙˆÙ†. Ø³Ø§Ø®ØªØ§Ø± ÛŒÚ© Ø§ØªÙ… Ø±Ø§ Ù…ÛŒ ØªÙˆØ§Ù† Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ù‡Ø³ØªÙ‡ Ø¯Ø± Ù…Ø±Ú©Ø² Ú©Ù‡ ØªÙˆØ³Ø· Ø§Ø¨Ø±ÛŒ Ø§Ø² Ø§Ù„Ú©ØªØ±ÙˆÙ† Ø§Ø­Ø§Ø·Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª ØªÙˆØµÛŒÙ Ú©Ø±Ø¯.Ù‡Ø³ØªÙ‡ ÛŒÚ© Ø§ØªÙ… Ø§Ø² Ù¾Ø±ÙˆØªÙˆÙ† Ùˆ Ù†ÙˆØªØ±ÙˆÙ† ØªØ´Ú©ÛŒÙ„ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ù¾Ø±ÙˆØªÙˆÙ† Ù‡Ø§ Ø°Ø±Ø§Øª Ø¨Ø§ Ø¨Ø§Ø± Ù…Ø«Ø¨Øª Ùˆ Ù†ÙˆØªØ±ÙˆÙ† Ù‡Ø§ Ø°Ø±Ø§Øª Ø®Ù†Ø«ÛŒ Ø¨Ø¯ÙˆÙ† Ø¨Ø§Ø± Ù‡Ø³ØªÙ†Ø¯. Ù‡Ø± Ø¯ÙˆÛŒ Ø§ÛŒÙ† Ø°Ø±Ø§Øª Ø¯Ø± Ù‡Ø³ØªÙ‡ Ø§ØªÙ… Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯ Ú©Ù‡ Ø¯Ø± Ù…Ø±Ú©Ø² Ø§ØªÙ… Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ Ùˆ Ø¨ÛŒØ´ØªØ± Ø¬Ø±Ù… Ø§ØªÙ… Ø±Ø§ Ø¯Ø± Ø®ÙˆØ¯ Ø¯Ø§Ø±Ø¯.Ø¯ÙˆØ± Ù‡Ø³ØªÙ‡ Ø§ØªÙ… Ø§Ø¨Ø±ÛŒ Ø§Ø² Ø§Ù„Ú©ØªØ±ÙˆÙ† Ø§Ø³Øª. Ø§Ù„Ú©ØªØ±ÙˆÙ† Ù‡Ø§ Ø°Ø±Ø§ØªÛŒ Ø¨Ø§ Ø¨Ø§Ø± Ù…Ù†ÙÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ø¯Ø± Ø§Ø·Ø±Ø§Ù Ù‡Ø³ØªÙ‡ Ø¯Ø± Ø­Ø±Ú©Øª Ø«Ø§Ø¨Øª Ù‡Ø³ØªÙ†Ø¯. Ø§Ø¨Ø± Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒ Ø¨Ù‡ Ù¾ÙˆØ³ØªÙ‡ ÛŒØ§ Ø§ÙˆØ±Ø¨ÛŒØªØ§Ù„ ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒ Ø´ÙˆØ¯ Ùˆ Ù‡Ø± Ù¾ÙˆØ³ØªÙ‡ Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø¹ÛŒÙ†ÛŒ Ø§Ù„Ú©ØªØ±ÙˆÙ† Ø±Ø§ Ø¯Ø± Ø®ÙˆØ¯ Ø¬Ø§ÛŒ Ø¯Ù‡Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ø§Ù„Ú©ØªØ±ÙˆÙ†â€ŒÙ‡Ø§ Ø¯Ø± Ø¨ÛŒØ±ÙˆÙ†ÛŒâ€ŒØªØ±ÛŒÙ† Ù¾ÙˆØ³ØªÙ‡ Ú©Ù‡ Ù„Ø§ÛŒÙ‡ Ø¸Ø±ÙÛŒØªÛŒ Ù†Ø§Ù… Ø¯Ø§Ø±Ø¯ØŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø´ÛŒÙ…ÛŒØ§ÛŒÛŒ Ø§ØªÙ… Ø±Ø§ ØªØ¹ÛŒÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.Ø¯Ø± ÛŒÚ© Ø§ØªÙ… Ø®Ù†Ø«ÛŒØŒ ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø±ÙˆØªÙˆÙ† Ù‡Ø§ÛŒ Ù‡Ø³ØªÙ‡ Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ Ø§Ù„Ú©ØªØ±ÙˆÙ† Ù‡Ø§ÛŒ Ø§Ø¨Ø± Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒ Ø§Ø³ØªØŒ Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ø¨Ø§Ø±Ù‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ùˆ Ù…Ù†ÙÛŒ Ù…ØªØ¹Ø§Ø¯Ù„ Ù…ÛŒ Ø´ÙˆÙ†Ø¯ Ùˆ Ø§ØªÙ… Ø¨Ø§Ø± Ú©Ù„ÛŒ Ù†Ø¯Ø§Ø±Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø±ÙˆØªÙˆÙ† Ù‡Ø§ Ú©Ù‡ Ø¹Ø¯Ø¯ Ø§ØªÙ…ÛŒ Ù†ÛŒØ² Ù†Ø§Ù…ÛŒØ¯Ù‡ Ù…ÛŒ Ø´ÙˆØ¯ØŒ ØªØ¹ÛŒÛŒÙ† Ù…ÛŒ Ú©Ù†Ø¯ Ú©Ù‡ Ø§ØªÙ… Ú†Ù‡ Ø¹Ù†ØµØ±ÛŒ Ø§Ø³Øª.\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nghaemiyeh_pair\nDataset: ghaemiyeh_pair\nSize: 1,444 training samples\nColumns: anchor and positive\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\ntype\nstring\nstring\ndetails\nmin: 6 tokensmean: 19.4 tokensmax: 69 tokens\nmin: 4 tokensmean: 82.05 tokensmax: 442 tokens\nSamples:\nanchor\npositive\nØ§Ù… Ø­Ø¨ÛŒØ¨Ù‡ Ú†Ù‡ Ù†Ù‚Ø´ÛŒ Ø¯Ø± Ø±ÙˆØ§ÛŒØª Ø­Ø¯ÛŒØ« Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³ØªØŸ\nØ§Ù… Ø­Ø¨ÛŒØ¨Ù‡ Ù‡Ù…Ø³Ø± Ú¯Ø±Ø§Ù…ÛŒ Ø±Ø³ÙˆÙ„ Ø®Ø¯Ø§ Ø¨ÙˆØ¯Ù‡ Ùˆ Ù†Ù‚Ø´ Ù…Ù‡Ù…ÛŒ Ø¯Ø± Ø±ÙˆØ§ÛŒØª Ø­Ø¯ÛŒØ« Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª. Ø§Ùˆ Ø§Ø² Ø²Ù†Ø§Ù† Ø§Ø³ÙˆÙ‡ Ù…Ø¹ØªØ¨Ø± Ø¨Ù‡ Ø´Ù…Ø§Ø± Ù…ÛŒâ€ŒØ±ÙØªÙ‡ Ùˆ Ø¯Ø± Ù…Ù‚Ø¯Ù…Ù‡ Ú©ØªØ§Ø¨ 'Ø²Ù†Ø§Ù† Ø§Ø³ÙˆÙ‡' Ø°Ú©Ø± Ø´Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø§Ù… Ø­Ø¨ÛŒØ¨Ù‡ Ù‡Ù…Ø³Ø± Ø±Ø³ÙˆÙ„ Ø®Ø¯Ø§ Ø¨Ø­Ø³Ø¨ Ø±ÙˆØ§ÛŒØ§Øª Ø§Ù‡Ù„ Ø³Ù†Øª Ùˆ Ø´ÛŒØ¹Ù‡ Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª. Ø§ÛŒÙ† Ù†Ù‚Ø´ Ø§Ùˆ Ù†Ø´Ø§Ù† Ø§Ø² Ø§Ù‡Ù…ÛŒØª Ùˆ ØªØ£Ø«ÛŒØ±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ù… Ø­Ø¨ÛŒØ¨Ù‡ Ø¯Ø± Ø¬Ø§Ù…Ø¹Ù‡ Ø§Ø³Ù„Ø§Ù…ÛŒ Ø¯Ø§Ø±Ø¯.\nÚ†Ù‡ Ú©Ø³ÛŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…ØªÙ‚Ù† ØªØ±ÛŒÙ† Ø¢Ù…ÙˆØ²Ù‡ Ù‡Ø§ÙŠ ØªØ±Ø¨ÛŒØªÛŒ Ùˆ Ø§Ø³Ø§Ø³Ù€ÛŒ ØªØ±ÛŒÙ† Ù…Ø¤Ù„ÙÙ‡ Ø¯Ø± ÙØ±Ù‡Ù†Ú¯ Ø§Ø³Ù€Ù„Ø§Ù…ÛŒ Ø´Ù†Ø§Ø®ØªÙ‡ Ù…ÛŒ Ø´ÙˆØ¯ØŸ\nØ§Ù‡Ù„ Ø¨ÛŒØª Ø®Ø§Ù†Ù€Ø¯Ø§Ù† ÙˆØ­ÛŒ (Ø¹Ù„ÛŒÙ‡Ù… Ø§Ù„Ø³Ù€Ù„Ø§Ù…)\nØ§Ù…Ù‘ Ø­Ø¨ÛŒØ¨Ù‡ Ú†Ù‡ Ø¬Ø§ÛŒÚ¯Ø§Ù‡ ÙˆÛŒÚ˜Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø²Ù†Ø¯Ú¯ÛŒ Ù¾ÛŒØ§Ù…Ø¨Ø± Ø§Ø³Ù„Ø§Ù… (Øµ) Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³ØªØŸ\nØ§Ù…Ù‘ Ø­Ø¨ÛŒØ¨Ù‡ ÛŒÚ©ÛŒ Ø§Ø² Ù‡Ù…Ø³Ø±Ø§Ù† Ø±Ø³ÙˆÙ„ Ø®Ø¯Ø§ (Øµ) Ø¨ÙˆØ¯Ù‡ Ùˆ Ø§Ø² ØµØ§Ø¯Ù‚â€ŒØªØ±ÛŒÙ† Ù‡Ù…Ø³Ø±Ø§Ù† Ø§ÛŒØ´Ø§Ù† Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§Ùˆ ØªÙ†Ù‡Ø§ Ù‡Ù…Ø³Ø±ÛŒ Ø¨ÙˆØ¯ Ú©Ù‡ Ø¨Ø§ Ù¾ÛŒØ§Ù…Ø¨Ø± Ø§Ø³Ù„Ø§Ù… (Øµ) Ø±Ø§Ø¨Ø·Ù‡ Ù†Ø³Ø¨ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ùˆ Ø§Ø² Ù…ÛŒØ§Ù† Ù‡Ù…Ø³Ø±Ø§Ù† Ø§ÛŒØ´Ø§Ù† Ø¨Ø±ØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª. Ø§Ù…Ù‘ Ø­Ø¨ÛŒØ¨Ù‡ Ù¾ÛŒØ´ Ø§Ø² Ø§Ø²Ø¯ÙˆØ§Ø¬ Ø¨Ø§ Ù¾ÛŒØ§Ù…Ø¨Ø± (Øµ) Ø¨Ø§ Ø¹Ù…Ù‡ Ø²Ø§Ø¯Ù‡ Ø§ÛŒØ´Ø§Ù† Ø§Ø²Ø¯ÙˆØ§Ø¬ Ú©Ø±Ø¯Ù‡ Ø¨ÙˆØ¯ Ùˆ Ù†Ø§Ù… Ø§Ùˆ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù†Ø§Ù… ÙØ±Ø²Ù†Ø¯Ø´ (Ø­Ø¨ÛŒØ¨Ù‡ Ø¨Ù†Øª Ø¹Ø¨ÛŒÙ€Ø¯Ø§ï·² Ø¨Ù† Ø¬Ø­Ø´)ØŒ Ø§Ù… Ø­Ø¨ÛŒØ¨Ù‡ Ø§Ø³Øª.\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nwiki_triplet\nDataset: wiki_triplet\nSize: 191,929 training samples\nColumns: anchor, positive, and negative\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 15 tokensmean: 41.49 tokensmax: 173 tokens\nmin: 14 tokensmean: 43.65 tokensmax: 183 tokens\nmin: 14 tokensmean: 40.19 tokensmax: 156 tokens\nSamples:\nanchor\npositive\nnegative\n-Ø§ÛŒÙ† Ø±ÙˆØ´ Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„ Ù‡Ø§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ú©ÙˆÚ†Ú© Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª Ú†ÙˆÙ† Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ Ø¯Ø± Ø¢Ù† Ù‡Ø§ Ø³Ø±ÛŒØ¹Ù†Ø± Ø§Ø³Øª .\n-Ø¨Ù‡Ú© Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡ Ù‚ÙˆÛŒ Ùˆ Ù…Ù†Ø§Ø³Ø¨ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ø¯ ØªØ§ Ú©Ø§Ø±Ø§ÛŒÛŒ Ø§Ø² Ø¯Ø³Øª Ù†Ø±ÙˆØ¯ .\nÙØ±Ø§ÛŒÙ†Ø¯ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ (Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ : back up) Ø¨Ù‡ Ø±ÙˆÙ†ÙˆØ´Øª Ø¨Ø±Ø¯Ø§Ø´ØªÙ† Ø§Ø² Ù¾Ø±ÙˆÙ†Ø¯Ù‡ Ù‡Ø§ÛŒ ÙÛŒØ²ÛŒÚ©ÛŒ ÛŒØ§ Ù…Ø¬Ø§Ø²ÛŒ Ùˆ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ Ø¯Ø± ÛŒÚ© Ø³Ø§ÛŒØª Ø«Ø§Ù†ÙˆÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø´Ø±Ø§ÛŒØ·ÛŒ Ú©Ù‡ Ø³Ø§Ù…Ø§Ù†Ù‡ Ø§Ø² Ú©Ø§Ø± Ø¨ÛŒÙØªØ¯ ØŒ Ø§Ø·Ù„Ø§Ù‚ Ù…ÛŒ Ø´ÙˆØ¯ .\nØ¶Ø­Ø§Ú© Ø³Ù¾Ø§Ù‡ ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø±Ø¯ Ùˆ Ø¨Ù‡ Ø¯Ø³ØªÚ¯ÛŒØ±ÛŒ Ø¬Ù…Ø´ÛŒØ¯ ÙØ±Ø³ØªØ§Ø¯ .\nØ¬Ù…Ø´ÛŒØ¯ Ø¯Ùˆ Ø¯Ø®ØªØ± Ø®ÙˆØ¨ Ø±Ùˆ Ø¯Ø§Ø´Øª : ÛŒÚ©ÛŒ Ø´Ù‡Ø±Ù†Ø§Ø² Ùˆ Ø¯ÛŒÚ¯Ø±ÛŒ Ø§Ø±Ù†ÙˆØ§Ø².Ø§ÛŒÙ† Ø¯Ùˆ Ù†ÛŒØ² Ø¯Ø± Ø¯Ø³Øª Ø¶Ø­Ø§Ú© Ø³ØªÙ…Ú¯Ø± Ø§Ø³ÛŒØ± Ø´Ø¯Ù†Ø¯ Ùˆ Ø§Ø² ØªØ±Ø³ Ø¨Ù‡ ÙØ±Ù…Ø§Ù† Ø§Ùˆ Ø¯Ø±Ø¢Ù…Ø¯Ù†Ø¯ .\nÙØ±Ø§Ù†Ú© ØŒ Ù…Ø§Ø¯Ø± ÙØ±ÛŒØ¯ÙˆÙ† ØŒ Ø¨ÛŒ Ø´ÙˆÙ‡Ø± Ù…Ø§Ù†Ø¯ Ùˆ ÙˆÙ‚ØªÛŒ Ø¯Ø§Ù†Ø³Øª Ø¶Ø­Ø§Ú© Ø¯Ø± Ø®ÙˆØ§Ø¨ Ø¯ÛŒØ¯Ù‡ Ú©Ù‡ Ø´Ú©Ø³ØªØ´ Ø¨Ù‡ Ø¯Ø³Øª ÙØ±ÛŒØ¯ÙˆÙ† Ø§Ø³Øª Ø¨ÛŒÙ…Ù†Ø§Ú© Ø´Ø¯ .\nØªØ¯Ú©Ø³ Ø¬ÙˆØ§Ù†Ø§Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ø§ÛŒÛŒ Ù…Ø³ØªÙ‚Ù„ Ø§Ø³Øª Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ø´ Ø¢Ù…ÙˆØ²Ø§Ù† Ù…Ù‚Ø·Ø¹ Û· ØªØ§ Û±Û² Ø¨Ø±Ú¯Ø²Ø§Ø± Ù…ÛŒ Ø´ÙˆØ¯ .\nØ§ÙˆÙ„ÛŒÙ† ØªØ¯Ú©Ø³ Ø§ÛŒØ±Ø§Ù† Ø¯Ø± Û²Ûµ Ø¨Ù‡Ù…Ù† Ø³Ø§Ù„ Û±Û³Û¹Û² Ø¯Ø± ØªÙ‡Ø±Ø§Ù† Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† tedxtehran Ø¨Ø±Ú¯Ø²Ø§Ø± Ø´Ø¯ .\nÚ©ØªØ§Ø¨Ù‡Ø§ÛŒÛŒ Ø§ØµÙ„ÛŒ Ù‡Ø³ØªÙ†Ø¯ . Ù…Ø§Ù†Ù†Ø¯ Ù…Ø°Ø§Ú©Ø±Ø§Øª ØŒ Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒ ØªØ¯ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ø´Ù Ø§ÛŒØ¯Ù‡ Ù‚ÙˆÛŒ Ù¾Ø±Ù…Ø­ØªÙˆØ§ Ù‡Ø³ØªÙ†Ø¯ Ùˆ Ø¯Ø± Ø¹ÛŒÙ† Ø­Ø§Ù„ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø§ÛŒ Ú©Ù… Ø­Ø¬Ù… Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ø¯Ø± ÛŒÚ© Ù…Ø¯Øª Ø²Ù…Ø§Ù† Ú©ÙˆØªØ§Ù‡ Ø¨ØªÙˆØ§Ù† Ø¢Ù† Ø±Ø§ Ø®ÙˆØ§Ù†Ø¯ .\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nwiki_DSimilar_pair2class\nDataset: wiki_DSimilar_pair2class\nSize: 137,402 training samples\nColumns: sentence1, sentence2, and label\nApproximate statistics based on the first 1000 samples:\nsentence1\nsentence2\nlabel\ntype\nstring\nstring\nint\ndetails\nmin: 14 tokensmean: 40.0 tokensmax: 143 tokens\nmin: 15 tokensmean: 41.49 tokensmax: 165 tokens\n0: ~54.10%1: ~45.90%\nSamples:\nsentence1\nsentence2\nlabel\nØ´Ù…Ø§Ù„ÛŒ ØªØ±ÛŒÙ† Ù†Ù‚Ø·Ù‡ Ø§ÛŒØ§Ù„Øª Ø¢Ù„Ø§Ø¨Ø§Ù…Ø§ Ø¯Ø± Ù†Ù‡ Ùˆ Ù†ÛŒÙ… Ú©ÛŒÙ„ÙˆÙ…ØªØ±ÛŒ Ø´Ù…Ø§Ù„ ØºØ±Ø¨ Ø´Ù‡Ø± ÙˆØ§ØªØ±Ù„Ùˆ Ø¯Ø± Ø´Ù‡Ø±Ø³ØªØ§Ù† Ù„ÙˆØ¯Ø±Ø¯ÛŒÙ„ Ø¯Ø± Ø´Ù…Ø§Ù„ ØºØ±Ø¨ÛŒ ØªØ±ÛŒÙ† Ù†Ù‚Ø·Ù‡ Ø§ÛŒØ§Ù„Øª Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯ .\nØ´Ø±Ú©Øª Ù‡Ø§ÛŒ Ù…Ø®Ø§Ø¨Ø±Ø§ØªÛŒ Ù…Ø§Ù†Ù†Ø¯ Ø§ÛŒ ØªÛŒ Ø§Ù†Ø¯ ØªÛŒ Ø­Ø¶ÙˆØ± Ù¾Ø± Ø±Ù†Ú¯ÛŒ Ø¯Ø± Ø¢Ù„Ø§Ø¨Ø§Ù…Ø§ Ø¯Ø§Ø±Ù†Ø¯ .\n0\nØ¯Ø§Ø³ØªØ§Ù† Ù…Ø¹Ø±ÙˆÙ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø³ÛŒØ±Ù‡ Ù†Ù‚Ù„ Ù…ÛŒ Ú©Ù†Ø¯ Ú©Ù‡ ÙˆÙ‚ØªÛŒ Ù…Ø­Ù…Ø¯ Ø§Ø² Ø³Ù‡ Ø³Ø¤Ø§Ù„ Ù…Ø·Ø±Ø­ Ø´Ø¯Ù‡ Ø§Ø² Ø³ÙˆÛŒ Ø±Ø¨Ø§ÛŒ Ù‡Ø§ Ù…Ø·Ù„Ø¹ Ú¯Ø´Øª ØŒ Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯ Ú©Ù‡ ØµØ¨Ø­ Ù¾Ø§Ø³Ø® Ù‡Ø§ Ø±Ø§ Ø®ÙˆØ§Ù‡Ø¯ Ø¯Ø§Ø´Øª .\nØ¯Ø± Ù…ÛŒØ§Ù† Ø¹Ù„Ù…Ø§ÛŒ Ù…Ø³Ù„Ù…Ø§Ù† ØŒ Ø§ÛŒÙ† ØªÙ… Ù‡ÙˆÛŒØª ÛŒØ§Ø¨ÛŒ Ø°ÙˆØ§Ù„Ù‚Ø±Ù†ÛŒÙ† Ø¨Ø§ Ø§Ø³Ú©Ù†Ø¯Ø± Ú©Ø¨ÛŒØ± ØŒ Ø¨Ù‡ Ù†Ø¸Ø± Ù…ÛŒ Ø±Ø³Ø¯ Ú©Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ø¬Ø§ Ø³Ø±Ú†Ø´Ù…Ù‡ Ú¯Ø±ÙØªÙ‡ .\n1\nØ¨Ø§ Ø±Ø´Ø¯ Ù…Ø³ØªÙ…Ø± Ø§Ù‚ØªØµØ§Ø¯ ÙˆÙ†Ø²ÙˆØ¦Ù„Ø§ÛŒ Ù†ÙØª Ø®ÛŒØ² Ø¯Ø± Ù‚Ø±Ù† Ø¨ÛŒØ³ØªÙ… ØŒ Ú©Ø§Ø±Ø§Ú©Ø§Ø³ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡Ú©ÛŒ Ø§Ø² Ù…Ø±Ø§Ú©Ø² Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ù…Ù‡Ù… Ø¢Ù…Ø±ÛŒÚ©Ø§ÛŒ Ù„Ø§ØªÛŒÙ† Ø´Ø¯ ØŒ Ùˆ Ù†ÛŒØ² Ø§ÛŒÙ† Ø´Ù‡Ø± ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…Ø±Ú©Ø² Ø§ØµÙ„ÛŒ Ø±ÙˆØ§Ø¨Ø· Ø§Ø±ÙˆÙ¾Ø§ Ùˆ Ø¢Ù…Ø±ÛŒÚ©Ø§ÛŒ Ø¬Ù†ÙˆØ¨ÛŒ Ø´Ø¯ .\nØ¨Ø±Ø¬Ø³ØªÙ‡ ØªØ±ÛŒÙ† ØªÛŒÙ… Ù‡Ø§ÛŒ ÙÙˆØªØ¨Ø§Ù„ Ùˆ Ø¨ÛŒØ³Ø¨Ø§Ù„ Ø¯Ø± Ú©Ø§Ø±Ø§Ú©Ø§Ø³ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯ .\n0\nLoss: ContrastiveLoss with these parameters:{\n\"distance_metric\": \"SiameseDistanceMetric.COSINE_DISTANCE\",\n\"margin\": 0.5,\n\"size_average\": true\n}\nmiracle_triplet\nDataset: miracle_triplet at 07e2b62\nSize: 2,107 training samples\nColumns: anchor, positive, and negative\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 7 tokensmean: 13.43 tokensmax: 39 tokens\nmin: 15 tokensmean: 133.6 tokensmax: 512 tokens\nmin: 7 tokensmean: 130.35 tokensmax: 512 tokens\nSamples:\nanchor\npositive\nnegative\nØ¯ÛŒÙ† Ù…Ø³ÛŒØ­ÛŒØª Ø¯Ø± Ú©Ø¬Ø§ Ø¨ÛŒØ´ØªØ± Ø±ÙˆØ§Ø¬ Ø¯Ø§Ø±Ø¯ØŸ\nÙÙ‡Ø±Ø³Øª Ú©Ø´ÙˆØ±Ù‡Ø§ Ø¨Ø± Ù¾Ø§ÛŒÙ‡ Ø¬Ù…Ø¹ÛŒØª Ù…Ø³ÛŒØ­ÛŒØ§Ù†Ù…Ø³ÛŒØ­ÛŒØª Ø¯ÛŒÙ† ØºØ§Ù„Ø¨ Ø¯Ø± Ø§Ø±ÙˆÙ¾Ø§ØŒ Ø±ÙˆØ³ÛŒÙ‡ØŒ Ø¢Ù…Ø±ÛŒÚ©Ø§ØŒ ÙÛŒÙ„ÛŒÙ¾ÛŒÙ†ØŒ ØªÛŒÙ…ÙˆØ± Ø´Ø±Ù‚ÛŒØŒ Ø¬Ù†ÙˆØ¨ Ø¢ÙØ±ÛŒÙ‚Ø§ØŒ Ù…Ø±Ú©Ø² Ø¢ÙØ±ÛŒÙ‚Ø§ØŒ Ø¢ÙØ±ÛŒÙ‚Ø§ÛŒ Ø´Ø±Ù‚ÛŒ Ùˆ Ø§Ù‚ÛŒØ§Ù†ÙˆØ³ÛŒÙ‡ Ø§Ø³Øª. Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¬ÙˆØ§Ù…Ø¹ Ø¨Ø²Ø±Ú¯ Ù…Ø³ÛŒØ­ÛŒ Ø¯Ø± Ø¯ÛŒÚ¯Ø± Ù†Ù‚Ø§Ø· Ø¬Ù‡Ø§Ù† Ù…Ø§Ù†Ù†Ø¯ Ø§Ù†Ø¯ÙˆÙ†Ø²ÛŒØŒ Ø¢Ø³ÛŒØ§ÛŒ Ù…Ø±Ú©Ø²ÛŒ Ùˆ Ø´Ø±Ù‚ Ù…ÛŒØ§Ù†Ù‡ØŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†â€ŒÙ‡Ø§ Ù…Ø³ÛŒØ­ÛŒØª Ø¯ÙˆÙ…ÛŒÙ† Ø¯ÛŒÙ† Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ø³Ù„Ø§Ù… Ø§Ø³Øª ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯. Ø§ÛŒØ§Ù„Ø§Øª Ù…ØªØ­Ø¯Ù‡ Ø¢Ù…Ø±ÛŒÚ©Ø§ Ø¯Ø§Ø±Ø§ÛŒ Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ø¬Ù…Ø¹ÛŒØª Ù…Ø³ÛŒØ­ÛŒ Ø¯Ø± Ø¬Ù‡Ø§Ù† Ø§Ø³ØªØŒ Ù¾Ø³ Ø§Ø² Ø¢Ù† Ø¨Ø±Ø²ÛŒÙ„ Ùˆ Ù…Ú©Ø²ÛŒÚ© Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯. ÙˆØ§ØªÛŒÚ©Ø§Ù† ØªÙ†Ù‡Ø§ Ú©Ø´ÙˆØ± Ø¬Ù‡Ø§Ù† Ø§Ø³Øª Ú©Ù‡ Û±Û°Û° Ø¯Ø±ØµØ¯ Ø¬Ù…Ø¹ÛŒØª Ø¢Ù† Ù…Ø³ÛŒØ­ÛŒ Ù‡Ø³ØªÙ†Ø¯.\nÚ¯Ø³ØªØ±Ø´ Ø§Ø¯ÛŒØ§Ù† Ø¯Ø± Ø¹Ø§Ù„Ù…Ù‡Ù…â€ŒÚ†Ù†ÛŒÙ† Ù…Ø·Ø§Ø¨Ù‚ Ø¢Ù…Ø§Ø±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ØªØ± (Û²Û°Û°Ûµ) Ø¢ÛŒÛŒÙ† Ù…Ø³ÛŒØ­ÛŒØª Ø¨Ø§ Ú¯Ø³ØªØ±Ø´ Ø¯Ø± Û³Û³Ùª Ù†ÙˆØ¹ Ø¨Ø´Ø±ØŒ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ú¯Ø³ØªØ±Ø´ Ø±Ø§ Ø±ÙˆÛŒ Ú©Ø±Ù‡Ù” Ø®Ø§Ú©ÛŒ Ø¯Ø§Ø±Ø¯ØŒ Ùˆ Ù¾Ø³ Ø§Ø² Ø¢Ù† Ø§Ø³Ù„Ø§Ù… Ø¨Ø§ Ú¯Ø³ØªØ±Ø´ Û²Û±ÙªØŒ Ø¨ÛŒâ€ŒØ¯ÛŒÙ†ÛŒ Ø¨Ø§ Ú¯Ø³ØªØ±Ø´ Û±Û¶Ùª Ùˆ Ø¢ÛŒÛŒÙ† Ù‡Ù†Ø¯Ùˆ Ø¨Ø§ Ú¯Ø³ØªØ±Ø´ Û±Û´Ùª Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯.\nØ¢Ø´ Ø±Ø´ØªÙ‡ Ú†Ú¯ÙˆÙ†Ù‡ ØªÙ‡ÛŒÙ‡ Ù…ÛŒ Ø´ÙˆØ±ØŸ\nØ¢Ø´ Ø±Ø´ØªÙ‡Ø¢Ø´ Ø±Ø´ØªÙ‡ Ø§Ø² Ù…Ø¹Ø±ÙˆÙâ€ŒØªØ±ÛŒÙ† Ø¢Ø´â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø§Ø³Øª. Ø§ÛŒÙ† Ø¢Ø´ Ø±Ø§ ØºØ§Ù„Ø¨Ø§Ù‹ Ø¨Ø§ Ú©Ø´Ú© Ùˆ Ú¯Ø§Ù‡ÛŒ Ø¨Ø§ Ø³Ø±Ú©Ù‡ Ùˆ Ø­ØªÛŒ ØªØ±Ø´ÛŒ Ù…ÛŒâ€ŒØ®ÙˆØ±Ù†Ø¯. Ø¨Ø³ØªÙ‡ Ø¨Ù‡ Ø¹Ø§Ø¯Øªâ€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ Ø§Ø² Ø§ÛŒÙ† Ø®ÙˆØ±Ø§Ú© Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù¾ÛŒØ´ Ø®ÙˆØ±Ø§Ú© ÛŒØ§ Ø®ÙˆØ±Ø§Ú© Ø§ØµÙ„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯(ÙˆÙ„ÛŒ Ø¢Ø´ ØºØ°Ø§ Ù†ÛŒØ³Øª). Ù‡Ù…Ú†Ù†ÛŒÙ† Ú¯Ø§Ù‡ÛŒ Ø¯Ø± Ù…Ù‡Ù…Ø§Ù†ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± ÙØµÙ„ Ø²Ù…Ø³ØªØ§Ù† ÛŒØ§ Ø¯Ø± Ø±ÙˆØ² Ø³ÛŒØ²Ø¯Ù‡â€ŒØ¨Ù‡â€ŒØ¯Ø± Ùˆ ÛŒØ§ Ù…Ø±Ø§Ø³Ù… Ø§ÙØ·Ø§Ø± Ù…Ø³Ù„Ù…Ø§Ù†Ø§Ù† Ø¯Ø± Ù…Ø§Ù‡ Ø±Ù…Ø¶Ø§Ù† Ø§Ø² Ø¢Ø´ Ø±Ø´ØªÙ‡ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù¾ÛŒØ´ Ø®ÙˆØ±Ø§Ú© Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø¢Ø´ Ø±Ø´ØªÙ‡ Ø¯Ø± Ù…Ø§Ù‡ Ø±Ù…Ø¶Ø§Ù† Ø¬Ø§ÛŒÚ¯Ø§Ù‡ ÙˆÛŒÚ˜Ù‡â€ŒØ§ÛŒ Ø¨ÛŒÙ† Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¯Ø§Ø±Ø¯ Ùˆ Ø§Ø² Ø¬Ù…Ù„Ù‡ Ø®ÙˆØ±Ø§Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø§ÛŒÙ† Ù…Ø§Ù‡ Ø§Ø³Øª.Ø¢Ø¨ Ø±Ø§ Ø¯Ø± Ø¸Ø±ÙÛŒ Ú©Ù‡ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡Ù” Ú©Ø§ÙÛŒ Ø¨Ø²Ø±Ú¯ Ø§Ø³Øª Ù…ÛŒâ€ŒÚ¯Ø°Ø§Ø±ÛŒÙ… Ú©Ù‡ Ø¬ÙˆØ´ Ø¨ÛŒØ§ÛŒØ¯ØŒ Ù¾Ø³ Ø§Ø² Ø¢Ù† Ø­Ø¨ÙˆØ¨Ø§Øª Ø´Ø§Ù…Ù„ Ù„ÙˆØ¨ÛŒØ§ØŒ Ø¹Ø¯Ø³ Ùˆ Ù†Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡ Ø¢Ù† Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø¯Ø± Ø§ÛŒÙ† Ù…ÛŒØ§Ù† Ø¯Ø± Ø¸Ø±Ù Ø¯ÛŒÚ¯Ø±ÛŒ Ù¾ÛŒØ§Ø²Ù‡Ø§ÛŒÛŒ Ø±Ø§ Ú©Ù‡ Ø§Ø² Ù¾ÛŒØ´ Ø®Ø±Ø¯ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒÙ… Ø¯Ø± Ø±ÙˆØºÙ†ØŒ Ù†Ù…Ú© Ùˆ Ø²Ø±Ø¯ Ú†ÙˆØ¨Ù‡ Ø³Ø±Ø® Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ù¾Ø³ Ø§Ø² Ø¢Ù†Ú©Ù‡ Ù…Ø­ØªÙˆÛŒ Ø¸Ø±Ù Ø´Ø§Ù…Ù„ Ø¢Ø¨ Ùˆ Ø­Ø¨ÙˆØ¨Ø§Øª Ø¨Ù‡ Ø¬ÙˆØ´ Ø¢Ù…Ø¯ØŒ Ø³Ø¨Ø²ÛŒ Ø¢Ø´ Ø±Ø§ Ø¨Ù‡ Ø¢Ù† Ù…ÛŒâ€ŒØ§ÙØ²Ø§ÛŒÛŒÙ… Ùˆ ØµØ¨Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Û²Û° Ø¯Ù‚ÛŒÙ‚Ù‡ Ø¨Ø¬ÙˆØ´Ø¯ ØªØ§ Ù…Ø²Ù‡Ù” Ø®Ø§Ù…ÛŒ Ù†Ø¯Ù‡Ø¯. Ù¾Ø³ Ø§Ø² Ø§ÛŒÙ† Ù…Ø¯Øª Ø±Ø´ØªÙ‡Ù” Ø¢Ø´ÛŒ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ùˆ Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ Ø¢Ù† Ù¾ÛŒØ§Ø² Ø¯Ø§Øº Ø±Ø§ Ù‡Ù… Ù…ÛŒâ€ŒØ±ÛŒØ²ÛŒÙ… Ùˆ Ø¢Ù† Ù‚Ø¯Ø± ØµØ¨Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø±Ø´ØªÙ‡ Ù¾Ø®ØªÙ‡ Ø´ÙˆØ¯. Ø±Ø´ØªÙ‡ Ø²Ù…Ø§Ù†ÛŒ Ù¾Ø®ØªÙ‡â€ŒØ§Ø³Øª Ú©Ù‡ Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ Ù†Ø±Ù… Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ØŒ Ùˆ Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ Ù‡Ù…Ú¯ÛŒ Ú©Ù…ÛŒ Ù¾Ù‡Ù†â€ŒØªØ± Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯ (Ú©Ù…ÛŒ Ø¨ÛŒØ´ Ø§Ø² Û²Û° Ø¯Ù‚ÛŒÙ‚Ù‡).\nØ¢Ø´ Ø´ÙˆÙ„ÛŒØ§Ø¨ØªØ¯Ø§ Ù„Ø¨Ùˆ Ø±Ø§ Ù¾ÙˆØ³Øª Ú¯Ø±ÙØªÙ‡ØŒ Ø³Ù¾Ø³ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø®Ù„Ø§Ù„ÛŒ Ø¨Ø±Ø´ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… Ùˆ Ø¨Ø§ Ù…Ù‚Ø¯Ø§Ø±ÛŒ Ø¢Ø¨ Ø±ÙˆÛŒ Ø­Ø±Ø§Ø±Øª Ø´Ø¹Ù„Ù‡ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ… ØªØ§ Ù†ÛŒÙ…â€ŒÙ¾Ø² Ø´ÙˆØ¯.\nØ±ÙØ§Ù‡  Ø­Ø§Ù„ Ú©ÙˆØ¯Ú©Ø§Ù† Ø¯Ø± Ú¯Ø±Ùˆ Ú†ÛŒØ³ØªØŸ\nØ¨Ø§Ø²ÛŒâ€ŒØ¯Ø±Ù…Ø§Ù†ÛŒØ¬Ù…Ù„Ù‡ Ù…Ø¹Ø±ÙˆÙ Ù…Ø´Ú©Ù„Ø§Øª Ú©ÙˆØ¯Ú©Ø§Ù† Ø¨Ø§ Ø¨Ø²Ø±Ú¯ Ø´Ø¯Ù†Ø´Ø§Ù† Ø¨Ø²Ø±Ú¯ØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø¬Ù…Ù„Ù‡ Ø§ÛŒ Ú©Ù„ÛŒØ´Ù‡ Ø§ÛŒ Ùˆ Ø§Ø´ØªØ¨Ø§Ù‡ Ø§Ø³Øª. Ú©ÙˆØ¯Ú©Ø§Ù† Ø¨Ø§ Ù‡Ø± Ø±ÙˆØ² Ø¨Ø²Ø±Ú¯ Ø´Ø¯Ù† Ø¨Ø§ÛŒØ³ØªÛŒ ØªÙˆØ§Ù†Ø§ÛŒÛŒâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆÛŒØ´ Ø¯Ø± Ø­Ù„ Ù…Ø³Ø¦Ù„Ù‡ Ø±Ø§ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨Ø¨Ø®Ø´Ù†Ø¯ ÛŒÚ©ÛŒ Ø§Ø² Ø§ÛŒÙ† Ø±Ø§Ù‡â€ŒÙ‡Ø§ Ø¨Ø§Ø²ÛŒ Ø¯Ø±Ù…Ø§Ù†ÛŒ Ø§Ø³Øª. Ø¨Ø±Ø®ÛŒ Ø¨Ø§Ø²ÛŒâ€ŒÙ‡Ø§ Ø§Ø­Ø³Ø§Ø³Ø§ØªØŒ Ø¨Ø±Ø®ÛŒ Ø¯ÛŒÚ¯Ø± Ù‚Ø¯Ø±Øª Ø­Ù„ Ù…Ø³Ø¦Ù„Ù‡ØŒ Ø¨Ø±Ø®ÛŒ Ø¯ÛŒÚ¯Ø± Ø¹Ø¶Ù„Ø§Øª Ú©ÙˆÚ†Ú© Ùˆ Ø¨Ø±Ø®ÛŒ Ø¯ÛŒÚ¯Ø± Ø¹Ø¶Ù„Ø§Øª Ø¨Ø²Ø±Ú¯ØªØ± Ú©ÙˆØ¯Ú©Ø§Ù† Ø±Ø§ Ø¯Ø± Ú¯ÛŒØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø¯Ø± Ø§ÛŒÙ† Ø¯Ø±Ú¯ÛŒØ±ÛŒ Ø§Ù†Ø±Ú˜ÛŒ Ú©ÙˆØ¯Ú© ØªØ®Ù„ÛŒÙ‡ Ø´Ø¯Ù‡ Ùˆ Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ø¶Ø·Ø±Ø§Ø¨ØŒ Ù¾Ø± ØªØ­Ø±Ú©ÛŒØŒ Ø§ÙØ³Ø±Ø¯Ú¯ÛŒ Ùˆ Ø³Ø±Ø®ÙˆØ±Ø¯Ú¯ÛŒ Ù†Ø§Ø´ÛŒ Ø§Ø² Ø§ÛŒÙ† Ø§Ù†Ø±Ú˜ÛŒ Ù…Ø§Ø²Ø§Ø¯ Ú©Ø§Ù‡Ø´ Ù…ÛŒ ÛŒØ§Ø¨Ø¯. ØªØµÙˆÛŒØ± Ú©Ù†ÛŒØ¯ Ú©ÙˆØ¯Ú©ÛŒ Ú©Ù‡ Ù†ØªÙˆØ§Ù†Ø¯ Ø³Ø§Ø¯Ù‡â€ŒØªØ±ÛŒÙ† Ù…Ø³Ø§Ø¦Ù„ Ø²Ù†Ø¯Ú¯ÛŒ Ø®ÙˆØ¯ Ù…Ø«Ù„ Ù†Ø¸Ø§ÙØª Ø§ØªØ§Ù‚ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù†Ø¯Ù‡Ø¯ØŒ Ø§ÛŒÙ† Ø¹Ø¯Ù… ØªÙˆØ§Ù†Ø§ÛŒÛŒ Ø¯Ø± Ø§Ù†Ø¬Ø§Ù… ÙØ¹Ø§Ù„ÛŒØª Ù…Ø±Ø¨ÙˆØ·Ù‡ ÙˆÙ‚ØªÛŒ Ø¨Ø§ ØªØ°Ú©Ø±Ø§Øª Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡ Ù†ÛŒØ² Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§Ø´Ø¯ Ø¨Ù‡ Ø§Ùˆ Ø§Ø³ØªØ±Ø³ Ø²ÛŒØ§Ø¯ÛŒ ØªØ²Ø±ÛŒÙ‚ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø¯Ø± ÙˆØ§Ù‚Ø¹ Ø³Ù„Ø§Ù…Øª Ø¬Ø³Ù…ÛŒ Ú©ÙˆØ¯Ú©Ø§Ù† Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ú¯Ø±Ùˆ ØªØºØ°ÛŒÙ‡ Ùˆ Ø³Ù„Ø§Ù…ØªÛŒ Ø±ÙˆØ­ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ú¯Ø±Ùˆ Ø­Ù„ Ù…Ø³Ø¦Ù„Ù‡ØŒ Ù¾Ø§ÛŒØ´ Ùˆ Ù¾ÙˆÛŒØ´ Ø¯Ø± Ø­ÙˆØ²Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø§Ø³Øª.\nØ¨Ø§Ø²ÛŒâ€ŒØ¯Ø±Ù…Ø§Ù†ÛŒØ¨Ø§Ø²ÛŒ Ø¯Ø±Ù…Ø§Ù†ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ Ù…ÙˆÙ‚Ø¹ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒ Ø¯Ø± ÛŒÚ© Ø²Ù…ÛŒÙ†Ù‡ Ø¯Ø±Ù…Ø§Ù†ÛŒ Ø§Ø³Øª. Ø¨Ø§Ø²ÛŒ Ø¯Ø±Ù…Ø§Ù†ÛŒ Ø´Ø±Ø§ÛŒØ·ÛŒ Ø±Ø§ ÙØ±Ø§Ù‡Ù… Ù…ÛŒ Ú©Ù†Ø¯ ØªØ§ Ú©ÙˆØ¯Ú©Ø§Ù† Ø¨ØªÙˆØ§Ù†Ù†Ø¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ Ù‡Ø§ÛŒ Ø³Ø§Ù„Ù… Ø§Ø­Ø³Ø§Ø³Ø§Øª Ùˆ Ù‡ÛŒØ¬Ø§Ù†Ø§Øª Ø®ÙˆØ¯ Ø±Ø§ Ú©Ø´Ù Ú©Ù†Ù†Ø¯ Ùˆ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ø±ÙˆØ² Ø¯Ù‡Ù†Ø¯. Ø§ÛŒÙ† Ú©Ø§Ø± Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ ØµÙˆØ±Øª ÙØ±Ø¯ÛŒ Ùˆ Ú¯Ø±ÙˆÙ‡ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯. Ø¯Ø± Ø¨Ø§Ø²ÛŒ Ø¯Ø±Ù…Ø§Ù†ÛŒØŒ Ù…Ø´Ø§ÙˆØ± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¯Ø±Ù…Ø§Ù†Ú¯Ø± Ø§Ø³Øª Ø§Ù…Ø§ Ø§ÛŒÙ† Ú©Ø§Ø± ØªÙˆØ³Ø· ÙˆØ§Ù„Ø¯ÛŒÙ† Ù†ÛŒØ² Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ ØµÙˆØ±Øª Ú¯ÛŒØ±Ø¯ Ú©Ù‡ ØªØ£Ø«ÛŒØ± Ø¢Ù† Ú©Ù…ØªØ± Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯.  Ø§ÙˆÙ„ÛŒÙ† ÙˆØ¸ÛŒÙÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ùˆ Ø§Ø² Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ø²Ù†Ø¯Ú¯ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¨Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù† Ø§Ø³ØªØŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø³ØªÙ‚ÛŒÙ… (Ø¢Ù…ÙˆØ²Ø´â€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ Ø®ÙˆØ§Ù†Ø¯Ù† Ùˆ Ù†ÙˆØ´ØªÙ†ØŒ Ø±ÛŒØ§Ø¶ÛŒØ§Øª Ùˆ Ø¹Ù„ÙˆÙ… Ú©Ù„Ø§Ø³ÛŒ) Ø¨Ø±Ø§ÛŒ Ú©ÙˆØ¯Ú©Ø§Ù† Ø²ÛŒØ± Û· Ø³Ø§Ù„ Ø¨Ø³ÛŒØ§Ø± Ù…Ø®Ø±Ø¨ Ùˆ ÙˆÛŒØ±Ø§Ù† Ú¯Ø± Ø§Ø³ØªØŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ù‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ø¯Ø± Ø³Ù†ÛŒÙ† Ù¾Ø§Ø¦ÛŒÙ† Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø¨Ø§Ø²ÛŒ Ø¯Ø±Ù…Ø§Ù†ÛŒ ØµÙˆØ±Øª Ù…ÛŒÚ¯ÛŒØ±Ø¯. Ø¨Ø§Ø²ÛŒ Ø§Ø² Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ø²Ù†Ø¯Ú¯ÛŒ Ú©ÙˆØ¯Ú©Ø§Ù† Ø§Ø³ØªØŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø¨Ø§Ø²ÛŒ Ø§ÙÚ©Ø§Ø±Ø´Ø§Ù† Ø±Ø§ Ø¨Ø§Ø²Ú¯Ùˆ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯. ØªÙˆØ§Ù†Ø§ÛŒÛŒ Ø­Ù„ Ù…Ø³Ø¦Ù„Ù‡ Ø¯Ø± Ú©ÙˆØ¯Ú©Ø§Ù† Ø§Ø² Ø·Ø±ÛŒÙ‚ ØªÙ…Ø±Ú©Ø² Ø¯Ø± Ø¨Ø§Ø²ÛŒ Ú¯Ø³ØªØ±Ø´ Ù…ÛŒ ÛŒØ§Ø¨Ø¯ Ùˆ Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø¯Ù„ÛŒÙ„ Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¨Ù‡ Ù†ÙØ³ Ùˆ Ø±ÙˆØ­ÛŒÙ‡ Ø§Ø³ØªÙ‚Ù„Ø§Ù„ Ø·Ù„Ø¨ÛŒ Ø®ÙˆØ¯Ø´Ø§Ù† Ø±Ø§ ØªÙ‚ÙˆÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯.\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nEstef_pair\nDataset: Estef_pair\nSize: 2,085 training samples\nColumns: anchor and positive\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\ntype\nstring\nstring\ndetails\nmin: 2 tokensmean: 48.97 tokensmax: 291 tokens\nmin: 2 tokensmean: 42.48 tokensmax: 264 tokens\nSamples:\nanchor\npositive\nØ¢ÛŒØ§ ØªÙ‚Ù„ÛŒØ¯ØŒ ØµØ±ÙØ§ ÛŒÚ© Ù…Ø³Ø£Ù„Ù‡ Ø¹Ù‚Ù„ÛŒ Ø§Ø³Øª ÛŒØ§ Ø§Ø¯Ù„Ù‡ Ø´Ø±Ø¹ÛŒ Ù†ÛŒØ² Ø¯Ø§Ø±Ø¯ØŸ\nØªÙ‚Ù„ÛŒØ¯ Ø§Ø¯Ù„Ù‡ Ø´Ø±Ø¹ÛŒ Ø¯Ø§Ø±Ø¯ Ùˆ Ø¹Ù‚Ù„ Ù†ÛŒØ² Ø­Ú©Ù… Ù…ÛŒÚ©Ù†Ø¯ Ú©Ù‡ Ø´Ø®Øµ Ù†Ø§Ø¢Ú¯Ø§Ù‡ Ø¨Ù‡ Ø§Ø­Ú©Ø§Ù… Ø¯ÛŒÙ† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ù…Ø¬ØªÙ‡Ø¯ Ø¬Ø§Ù…Ø¹ Ø§Ù„Ø´Ø±Ø§ÛŒØ· Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†Ø¯.\nØ¨Ù‡ Ù†Ø¸Ø± Ø´Ø±ÛŒÙ Ø­Ø¶Ø±Øª Ø¹Ø§Ù„ÛŒ Ø¹Ù…Ù„ Ø¨Ù‡ Ø§Ø­ØªÛŒØ§Ø· Ø¨Ù‡ØªØ± Ø§Ø³Øª ÛŒØ§ ØªÙ‚Ù„ÛŒØ¯ØŸ\nÚ†ÙˆÙ† Ø¹Ù…Ù„ Ø¨Ù‡ Ø§Ø­ØªÛŒØ§Ø· Ù…Ø³ØªÙ„Ø²Ù… Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÙˆØ§Ø±Ø¯ Ùˆ Ú†Ú¯ÙˆÙ†Ú¯ÛŒ Ø§Ø­ØªÛŒØ§Ø· Ùˆ ØµØ±Ù ÙˆÙ‚Øª Ø¨ÛŒØ´ØªØ± Ø§Ø³Øª. Ø¨Ù‡ØªØ± Ø¢Ù† Ø§Ø³Øª Ú©Ù‡ Ù…Ú©Ù„Ù Ø¯Ø± Ø§Ø­Ú©Ø§Ù… Ø¯ÛŒÙ† Ø§Ø² Ù…Ø¬ØªÙ‡Ø¯ Ø¬Ø§Ù…Ø¹ Ø§Ù„Ø´Ø±Ø§ÛŒØ· ØªÙ‚Ù„ÛŒØ¯ Ú©Ù†Ø¯.\nÙ‚Ù„Ù…Ø±Ùˆ Ø§Ø­ØªÛŒØ§Ø· Ø¯Ø± Ø§Ø­Ú©Ø§Ù… Ø¯ÛŒÙ† Ø¯Ø± Ø¨ÛŒÙ† ÙØªØ§ÙˆØ§ÛŒ ÙÙ‚Ù‡Ø§ Ú†Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø§Ø³ØªØŸ Ø¢ÛŒØ§ Ø±Ø¹Ø§ÛŒØª Ø¢Ø±Ø§ÛŒ ÙÙ‚ÛŒÙ‡Ø§Ù† Ú¯Ø°Ø´ØªÙ‡ Ù†ÛŒØ² Ù„Ø§Ø²Ù… Ø§Ø³ØªØŸ\nÙ…Ø±Ø§Ø¯ Ø§Ø² Ø§Ø­ØªÛŒØ§Ø· Ø¯Ø± Ù…ÙˆØ§Ø±Ø¯ Ø¢Ù† Ø±Ø¹Ø§ÛŒØª Ù‡Ù…Ù‡ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª ÙÙ‚Ù‡ÛŒ Ø§Ø³ØªØ› Ø¨Ù‡ Ø·ÙˆØ±ÛŒ Ú©Ù‡ Ù…Ú©Ù„Ù Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆØ¯ Ú©Ù‡ Ø¨Ù‡ ÙˆØ¸ÛŒÙÙ‡ Ø®ÙˆØ¯ Ø¹Ù…Ù„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nall_resaleh_pair\nDataset: all_resaleh_pair\nSize: 6,425 training samples\nColumns: positive and anchor\nApproximate statistics based on the first 1000 samples:\npositive\nanchor\ntype\nstring\nstring\ndetails\nmin: 201 tokensmean: 510.8 tokensmax: 512 tokens\nmin: 5 tokensmean: 21.8 tokensmax: 85 tokens\nSamples:\npositive\nanchor\n'Ùˆ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ ØªÙÚ©Ø± Ùˆ Ú¯ÙØª Ùˆ Ú¯ÙˆÛŒ Ø¹Ù„Ù…ÛŒ Ø¯Ø± Ú©Ù„Ø§Ø³ ÙØ±Ø§Ù‡Ù… Ø¢ÙˆØ±Ø¯Ø› Ø¨Ù†Ø§ Ø¨Ø±Ø§ÛŒÙ†ØŒ Ø§ÛŒÙ† ÙØ¹Ù‘Ø§Ù„ÛŒØª Ù‡Ø§ Ø±Ø§ Ø¨Ø®Ø´ÛŒ Ø§Ø² ÙØ±Ø§ÛŒÙ†Ø¯ ØªØ¯Ø±ÛŒØ³Ø¨Ù‡ Ø­Ø³Ø§Ø¨ Ø¢ÙˆØ±ÛŒØ¯ Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ù‡ Ø®Ø§Ø±Ø¬ Ø§Ø² Ú©Ù„Ø§Ø³ Ù…ÙˆÚ©ÙˆÙ„ Ù†Ú©Ù†ÛŒØ¯. ÙØ±Ø§Ú¯ÛŒØ±Ø§Ù† Ø±Ø§ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ùˆ ØªØ´ÙˆÛŒÙ‚ Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ú¯Ø±ÙˆÙ‡ÛŒÙˆ Ø¨Ø§ Ù‡Ù…ÙÚ©Ø±ÛŒØŒ ÙØ¹Ù‘Ø§Ù„ÛŒØª Ù‡Ø§ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ù†Ø¯.Û¶ Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ø· Ø²Ù…Ø§Ù† Ùˆ Ù†Ù‚Ø´Ù‡ Ù‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒØŒ Ø§Ø² Ø¬Ù…Ù„Ù‡ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Ù…Ù‡Ù… Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¯Ø± ÙØ±Ø§ÛŒÙ†Ø¯ ØªØ¯Ø±ÛŒØ³ Ø¯Ø±Ø³ ØªØ§Ø±ÛŒØ® Ø¨Ù‡ Ø´Ù…Ø§Ø±Ù…ÛŒ Ø±ÙˆÙ†Ø¯. Ø¯Ø± Ø§ÛŒÙ† Ú©ØªØ§Ø¨ØŒ Ø¨Ø®Ø´ÛŒ Ø§Ø² Ù…Ø­ØªÙˆØ§ØŒ Ø¨Ù‡ Ø®ØµÙˆØµ Ø²Ù…Ø§Ù† Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ Ùˆ Ù†Ø§Ù… Ù‡Ø§ÛŒ Ø§Ø´Ø®Ø§Øµ Ùˆ Ù…Ú©Ø§Ù† Ù‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒØŒ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø¢Ù†Ù‡Ø§Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ùˆ ÙØ¹Ù‘Ø§Ù„ÛŒØª Ù‡Ø§ÛŒÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¢Ù†Ù‡Ø§ Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ù„Ø§Ø²Ù… Ø§Ø³Øª Ø¯Ø± Ø¬Ø±ÛŒØ§Ù† ÛŒØ§Ø¯Ø¯Ù‡ÛŒ  ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒØŒ Ø§Ø² Ø§ÛŒÙ† Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡Ø´ÙˆØ¯ØŒ Ø§Ù…Ø§ Ù‡Ø±Ú¯Ø² Ù†Ø¨Ø§ÛŒØ¯ Ø¯Ø§Ù†Ø´ Ø¢Ù…ÙˆØ²Ø§Ù† Ø±Ø§ ÙˆØ§Ø¯Ø§Ø± Ø¨Ù‡ Ø­ÙØ¸ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù†Ø¯Ø±Ø¬ Ø¯Ø± Ø§ÛŒÙ† Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ Ú©Ø±Ø¯. Ø¯Ø§Ù†Ø´ Ø¢Ù…ÙˆØ²Ø§Ù† Ø±Ø§ ØªØ´ÙˆÛŒÙ‚ Ú©Ù†ÛŒØ¯ Ú©Ù‡Ø¨Ø§ Ù…Ø±ÙˆØ± Ø¯Ù‚ÛŒÙ‚ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ùˆ Ù†Ù‚Ø´Ù‡ Ù‡Ø§ØŒ ÙØ¹Ù‘Ø§Ù„ÛŒØª Ù‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ù†Ø¯. Ø¨Ù‡ Ù…Ù†Ø¸ÙˆØ± Ø§ÛŒØ¬Ø§Ø¯ Ø§Ù†Ú¯ÛŒØ²Ù‡ Ùˆ ØªØ¹Ù…ÛŒÙ‚ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒØŒÙØ±Ø§Ú¯ÛŒØ±Ø§Ù† Ø±Ø§ ØªØ±ØºÛŒØ¨ Ùˆ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ø¨ÙØ±Ù…Ø§ÛŒÛŒØ¯ Ú©Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒÛŒ Ø¨Ø§ Ù…ÙˆØ¶ÙˆØ¹ Ù‡Ø§ÛŒ Ù…ØªÙ†ÙˆØ¹ØŒ Ø·Ø±Ù‘Ø§Ø­ÛŒ Ú©Ù†Ù†Ø¯Ø› Ù…Ø«Ù„Ø§Ù‹ Ø¬Ø¯ÙˆÙ„ Ø³Ù„Ø³Ù„Ù‡ Ù‡Ø§ Ùˆ ÛŒØ§ÙØ±Ù…Ø§Ù†Ø±ÙˆØ§ÛŒØ§Ù† ÛŒÚ© Ø³Ù„Ø³Ù„Ù‡ØŒ Ø¢Ø«Ø§Ø± Ùˆ Ø¨Ù†Ø§Ù‡Ø§ÛŒ Ø´Ø§Ø®Øµ Ø¯ÙˆØ±Ø§Ù† Ø¨Ø§Ø³ØªØ§Ù† ØªØ§Ø±ÛŒØ® Ø§ÛŒØ±Ø§Ù† Ø±Ø§ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø¢Ù†Ø§Ù† Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯ Ùˆ Ø§Ø² Ø¢Ù†Ù‡Ø§ Ø¨Ø®ÙˆØ§Ù‡ÛŒØ¯Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ ÛŒØ§ Ù…ØµÙˆÙ‘Ø± ØªØ±Ø³ÛŒÙ… Ú©Ù†Ù†Ø¯. ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ Ú©Ù‡ Ø¯Ø± Ø¢Ø²Ù…ÙˆÙ† Ù‡Ø§ÛŒ Ù¾Ø§ÛŒØ§Ù†ÛŒØŒ Ø§Ø² Ù…Ù†Ø¯Ø±Ø¬Ø§Øª Ù†Ù‚Ø´Ù‡ Ù‡Ø§ØŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ ÙˆØ¬Ø¯ÙˆÙ„ Ù‡Ø§ Ù‡ÛŒÚ† Ù¾Ø±Ø³Ø´ÛŒ Ø·Ø±Ø­ Ù†Ø´ÙˆØ¯.Û· Ù…Ø·Ø§Ù„Ø¨ÛŒ Ú©Ù‡ Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† Ú©ÛŒ ØªÙˆØ¶ÛŒØ­ Ø¯Ø± Ú©ØªØ§Ø¨ Ø¢Ù…Ø¯Ù‡ØŒ Ø§Ø±Ø²Ø´ Ùˆ Ø§Ù‡Ù…ÛŒØªÛŒ Ù‡Ù…Ø§Ù†Ù†Ø¯ Ø³Ø§ÛŒØ± Ù…Ø·Ø§Ù„Ø¨ Ùˆ Ù…Ø¨Ø§Ø­Ø« Ú©ØªØ§Ø¨ Ø¯Ø§Ø±Ù†Ø¯ Ùˆ ØªÙˆØ¬Ù‡Ø¨Ù‡ Ø¢Ù†Ù‡Ø§ Ø¯Ø± ØªØ¯Ø±ÛŒØ³ Ùˆ Ø§Ø±Ø²Ø´ÛŒØ§Ø¨ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³ØªØŒ Ø§Ù…Ø§ Ø¯Ø± Ø¢Ø²Ù…ÙˆÙ† Ù‡Ø§ÛŒ Ù¾Ø§ÛŒØ§Ù†ÛŒØŒ Ù†Ø¨Ø§ÛŒØ¯ Ù¾Ø±Ø³Ø´ÛŒ Ø§Ø² Ù…Ø­ØªÙˆØ§ÛŒ Ú©ÛŒ ØªÙˆØ¶ÛŒØ­ Ø·Ø±Ø­ Ø´ÙˆØ¯.Û¸ Ù‡Ù…Ú©Ø§Ø±Ø§Ù† Ø§Ø±Ø¬Ù…Ù†Ø¯! Ù…Ø¬Ø¯Ø¯Ø§Ù‹ ØªØ§Ú©ÛŒØ¯ Ù…ÛŒ Ø´ÙˆØ¯ Ú©Ù‡ Ù‡Ø¯Ù Ø§ÛŒÙ† Ú©ØªØ§Ø¨ØŒ ÙØ±Ø§Ù‡Ù… Ø¢ÙˆØ±Ø¯Ù† Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ ÙØ±Ø²Ù†Ø¯Ø§Ù† Ø§ÛŒÙ† Ù…Ø±Ø² Ùˆ Ø¨ÙˆÙ…Ø§Ø³Øª Ú©Ù‡ Ø¨Ù‡ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ùˆ Ø´Ù†Ø§Ø®Øª Ú¯Ø°Ø´ØªÙ‡ Ú©Ø´ÙˆØ± Ø®ÙˆÛŒØ´ Ø¹Ù„Ø§Ù‚Ù‡ ÛŒØ§Ø¨Ù†Ø¯ Ùˆ Ù…Ù‡Ø§Ø±Øª Ú©Ø§ÙˆØ´Ú¯Ø±ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø±ØªÙ‚Ø§ Ø¯Ù‡Ù†Ø¯. Ø§Ø² Ø§ÛŒÙ† Ø±ÙˆØŒ Ø§Ø³Ø§Ø³ÙØ±Ø§ÛŒÙ†Ø¯ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ  ÛŒØ§Ø¯Ø¯Ù‡ÛŒ Ø±Ø§ Ø¨Ø± ØªØ´ÙˆÛŒÙ‚ Ùˆ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ù¾ÛŒÙˆØ³ØªÙ‡ Ø¯Ø§Ù†Ø´ Ø¢Ù…ÙˆØ²Ø§Ù† Ø¨Ù‡ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ùˆ Ø¬Ø³Øª Ùˆ Ø¬Ùˆ Ø¯Ø± Ù…Ù†Ø§Ø¨Ø¹ Ùˆ Ù…Ø¢Ø®Ø° ØªØ§Ø±ÛŒØ®ÛŒÙ‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯ Ùˆ Ù…Ø±Ø§Ù‚Ø¨Øª Ú©Ù†ÛŒØ¯ Ú©Ù‡ ØªÙˆØ¬Ù‡ Ùˆ ØªÙ…Ø±Ú©Ø² ÙØ±Ø§Ú¯ÛŒØ±Ø§Ù† ØµØ±ÙØ§Ù‹ Ø¨Ù‡ Ø§Ù†Ø¨Ø§Ø´Øª Ø°Ù‡Ù†ÛŒ ÙˆØ§Ú˜Ù‡ Ù‡Ø§ Ùˆ Ø¬Ù…Ù„Ù‡ Ù‡Ø§ÛŒ Ú©ØªØ§Ø¨ Ø¯Ø±Ø³ÛŒØŒ Ù…Ø­Ø¯ÙˆØ¯Ù†Ø´ÙˆØ¯. Ø®Ù„Ø§ØµÙ‡ Ú©Ù„Ø§Ù…ØŒ Ù‡Ø¯Ù Ø§Ø² ØªØ§Ù„ÛŒÙ Ú©ØªØ§Ø¨ Ø¯Ø±Ø³ÛŒ ØªØ§Ø±ÛŒØ® Ø§ÛŒÙ† Ù†ÛŒØ³Øª Ú©Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±Ø§Ù† ÙÙ‚Ø· Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§ÛŒ Ø§Ø² Ú¯Ø²Ø§Ø±Ù‡ Ù‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø±Ø§Ø¨Ù‡ Ø´Ú©Ù„ÛŒ Ù…Ø³ØªÙ‚Ù„ Ùˆ Ù…Ø¬Ø²Ø§ Ø§Ø² Ù‡Ù… Ø¨Ù‡ Ø°Ù‡Ù† Ø¨Ø³Ù¾Ø§Ø±Ù†Ø¯ØŒ Ø¨Ù„Ú©Ù‡ Ù…Ù‚ØµÙˆØ¯ Ø¢Ù† Ø§Ø³Øª Ú©Ù‡ Ø¯Ø§Ù†Ø´ Ø¢Ù…ÙˆØ²Ø§Ù† Ø¨Ø§ Ø¬Ø³Øª ÙˆØ¬Ùˆ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ø´ÙˆØ§Ù‡Ø¯ ÙˆÙ…Ù†Ø§Ø¨Ø¹ØŒ Ù‡Ù…ÙÚ©Ø±ÛŒØŒ Ú¯ÙØª Ùˆ Ú¯Ùˆ Ùˆ Ú©Ø§Ø± Ú¯Ø±ÙˆÙ‡ÛŒØŒ Ø¹Ù„Ù„ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ù‡Ù… Ùˆ Ù†Ù‚Ø´ Ø´Ø®ØµÛŒØª Ù‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„Ùˆ Ù…ÙˆÙ‚Ø¹ÛŒØª Ú©Ø´ÙˆØ± Ø®ÙˆÛŒØ´ Ø±Ø§ Ø¯Ø± Ø¯ÙˆØ±Ø§Ù† Ù…Ø®ØªÙ„Ù Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†Ù†Ø¯.Û¹ Ø¨Ø³ØªÙ‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ù†Ø±Ù… Ø§ÙØ²Ø§Ø± Ø¨Ø±ÙØ±Ø§Ø² Ø¢Ø³Ù…Ø§Ù†ØŒ Ø­Ø§ÙˆÛŒ Ù†Ú©Ø§ØªÛŒ Ø¯Ø± Ø¨Ø§Ø±Ù‡ Ú©ØªØ§Ø¨ Ø¯Ø±Ø³ÛŒ ØªØ§Ø±ÛŒØ® Ø¯Ù‡Ù… Ùˆ Ù†ÛŒØ² Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§ÛŒ Ø§Ø² Ù…ÙˆØ§Ø¯ Ù¾Ø´ØªÛŒØ¨Ø§Ù†Ú©ØªØ§Ø¨ Ø§Ø² Ù‚Ø¨ÛŒÙ„: Ù‚Ø·Ø¹Ù‡ Ù‡Ø§ÛŒ ØµÙˆØªÛŒØŒ ØªØµÙˆÛŒØ±ÛŒ Ùˆ ÙˆÛŒØ¯Ø¦ÙˆÛŒÛŒ Ùˆ Ù†ÛŒØ² Ù†Ø³Ø®Ù‡ Ù¾ÛŒ Ø¯ÛŒ Ø§Ù ØªØ¹Ø¯Ø§Ø¯ÛŒ Ø§Ø² Ù…Ù‚Ø§Ù„Ù‡ Ù‡Ø§ÛŒ Ø¹Ù„Ù…ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù…ÙˆØ¶ÙˆØ¹ Ù‡Ø§ÛŒÙ…Ø®ØªÙ„Ù ØªØ§Ø±ÛŒØ® Ø´Ù†Ø§Ø³ÛŒ Ùˆ ØªØ­ÙˆÙ„Ø§Øª ØªØ§Ø±ÛŒØ® Ø¨Ø§Ø³ØªØ§Ù† Ø§Ø³Øª. ØªÙ‚Ø§Ø¶Ø§ Ù…ÛŒ Ø´ÙˆØ¯ Ú©Ù‡ Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø±Ø§ Ø¨Ù‡ Ø¯Ù‚Ù‘Øª Ù…Ù„Ø§Ø­Ø¸Ù‡ ÙØ±Ù…Ø§ÛŒÛŒØ¯. Ø¨Ø®Ø´ÛŒ Ø§Ø²Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ØŒ Ø´Ø§Ù…Ù„ Ù…Ù†Ø§Ø¨Ø¹ Ùˆ Ù…ÙˆØ§Ø¯ Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ú©ØªØ§Ø¨ØŒ Ø¨Ø± Ø±ÙˆÛŒ ÙˆØ¨Ú¯Ø§Ù‡ Ù‡Ø§ÛŒ Ø¯ÙØªØ± ØªØ§Ù„ÛŒÙ Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒ Ø¯Ø±Ø³ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Ùˆ Ù…ØªÙˆØ³Ø·Ù‡ Ù†Ø¸Ø±ÛŒÙˆ Ú¯Ø±ÙˆÙ‡ Ø¯Ø±Ø³ÛŒ ØªØ§Ø±ÛŒØ® Ø¢Ù† Ø¯ÙØªØ± Ù†ÛŒØ² Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø§ÛŒÙ† Ù†Ø±Ù… Ø§ÙØ²Ø§Ø± Ø±Ø§ Ù…ÛŒ ØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ø·Ø±ÛŒÙ‚ ÙˆØ¨Ú¯Ø§Ù‡ Ù¾Ø®Ø´ Ø§Ù†ØªØ´Ø§Ø±Ø§Øª Ù…Ø¯Ø±Ø³Ù‡ Ø¨Ù‡ØªÙ‡ÛŒÙ‡ Ú©Ù†ÛŒØ¯. en-madreseh.ir Ù†Ø´Ø§Ù†ÛŒwww.talif.sch.ir : Ø¢Ø¯Ø±Ø³ ÙˆØ¨Ú¯Ø§Ù‡ Ø¯ÙØªØ± ØªØ§Ù„ÛŒÙ Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒ Ø¯Ø±Ø³ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Ùˆ Ù…ØªÙˆØ³Ø·Ù‡ Ù†Ø¸Ø±ÛŒØ¢Ø¯Ø±Ø³ ÙˆØ¨Ú¯Ø§Ù‡ Ú¯Ø±ÙˆÙ‡ Ø¯Ø±Ø³ÛŒ ØªØ§Ø±ÛŒØ® Ø¯ÙØªØ± ØªØ§Ù„ÛŒÙ Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒ Ø¯Ø±Ø³ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Ùˆ Ù…ØªÙˆØ³Ø·Ù‡ Ù†Ø¸Ø±ÛŒ:www.history-dept.talif.sch.irÛ±Û° ÙÙ‡Ø±Ø³Øª Ù…Ù†Ø§Ø¨Ø¹ Ùˆ Ù…Ø¢Ø®Ø°ÛŒ Ú©Ù‡ Ø¯Ø± ØªØ¯ÙˆÛŒÙ† Ùˆ ØªØ§Ù„ÛŒÙ Ù…Ø­ØªÙˆØ§ Ùˆ ØªØµØ§ÙˆÛŒØ± Ú©ØªØ§Ø¨ Ø§Ø² Ø¢Ù†Ù‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ØŒ Ø¯Ø± Ø¨Ø³ØªÙ‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ù†Ø±Ù… Ø§ÙØ²Ø§Ø±Ø¨Ø± ÙØ±Ø§Ø² Ø¢Ø³Ù…Ø§Ù† Ùˆ Ú©ØªØ§Ø¨ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ù…Ø¹Ù„Ù… Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª.Û±Û± Ù‡Ù…Ú©Ø§Ø± Ú¯Ø±Ø§Ù…ÛŒ! Ú¯Ø±ÙˆÙ‡ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø±ÛŒØ²Ø§Ù† Ùˆ Ù…ÙˆÙ„ÙØ§Ù† Ú©ØªØ§Ø¨ Ø¯Ø±Ø³ÛŒ Ø¯Ø± Ø¯ÙØªØ± ØªØ§Ù„ÛŒÙ Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒ Ø¯Ø±Ø³ÛŒØŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø¯Ø±ÛŒØ§ÙØª Ø§Ù†ØªÙ‚Ø§Ø¯Ù‡Ø§ ÙˆÙ¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ù‡Ø§ÛŒ Ø´Ù…Ø§ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø§ÛŒÙ† Ú©ØªØ§Ø¨ Ù‡Ø³ØªÙ†Ø¯. Ù„Ø·ÙØ§Ù‹ Ø¯ÛŒØ¯Ú¯Ø§Ù‡ Ù‡Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ù¾ÛŒØ§Ù… Ù†Ú¯Ø§Ø± )Ø§ÛŒÙ…ÛŒÙ„( Ø²ÛŒØ± Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯:history-dept@talif.sch.irÚ¯Ø±ÙˆÙ‡ ØªØ§Ø±ÛŒØ® Ø¯ÙØªØ± ØªØ§Ù„ÛŒÙ Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒ Ø¯Ø±Ø³ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Ùˆ Ù…ØªÙˆØ³Ø·Ù‡ Ù†Ø¸Ø±ÛŒØ³Ø®Ù†ÛŒ Ø¨Ø§ Ø¯Ø§Ù†Ø´ Ø¢Ù…ÙˆØ²Ø§Ù† Ø¹Ø²ÛŒØ²Ø¯Ø§Ù†Ø´ Ø¢Ù…ÙˆØ² Ø¹Ø²ÛŒØ²! Ø¯Ø± Ø¯ÙˆØ±Ù‡ Ù‡Ø§ÛŒ Ø§Ø¨ØªØ¯Ø§ÛŒÛŒ Ùˆ Ø§ÙˆÙ‘Ù„ Ù…ØªÙˆØ³Ø·Ù‡ØŒ Ø¨Ø®Ø´ Ù‡Ø§ÛŒÛŒ Ø§Ø² Ú©ØªØ§Ø¨ Ù…Ø·Ø§Ù„Ø¹Ø§Øª Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ø´Ù…Ø§ Ø¯Ø± Ø¨Ø§Ø±Ù‡ ØªØ§Ø±ÛŒØ® Ø§ÛŒØ±Ø§Ù†Ùˆ Ø§Ø³Ù„Ø§Ù… Ø¨ÙˆØ¯. Ø¯Ø± Ø¯ÙˆØ±Ù‡ Ø¯ÙˆÙ… Ù…ØªÙˆØ³Ø·Ù‡ØŒ ØªØ§Ø±ÛŒØ® Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø±Ø³ÛŒ Ù…Ø³ØªÙ‚Ù„ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ø´ Ø¢Ù…ÙˆØ²Ø§Ù† Ø±Ø´ØªÙ‡ Ø§Ø¯Ø¨ÛŒØ§Øª Ùˆ Ø¹Ù„ÙˆÙ… Ø§Ù†Ø³Ø§Ù†ÛŒØŒ Ø¯Ø±Ù¾Ø§ÛŒÙ‡ Ù‡Ø§ÛŒ Ø¯Ù‡Ù…ØŒ ÛŒØ§Ø²Ø¯Ù‡Ù… Ùˆ Ø¯ÙˆØ§Ø²Ø¯Ù‡Ù… Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒ Ø´ÙˆØ¯.ØªÙˆØ¬Ù‡ Ø´Ù…Ø§ Ø¹Ø²ÛŒØ²Ø§Ù† Ø±Ø§ Ø¨Ù‡ Ú†Ù†Ø¯ Ù†Ú©ØªÙ‡ Ø¯Ø± Ø¨Ø§Ø±Ù‡ Ú©ØªØ§Ø¨ Ø¯Ø±Ø³ÛŒ ØªØ§Ø±ÛŒØ® Ø¯Ù‡Ù… Ø¬Ù„Ø¨ Ù…ÛŒ Ú©Ù†ÛŒÙ…:Û± Ø§ÛŒÙ† Ú©ØªØ§Ø¨ Ø¯Ø§Ø±Ø§ÛŒ Ø³Ù‡ ÙØµÙ„ Ø§Ø³Øª. Ø¯Ø± ÙØµÙ„ Ø§ÙˆÙ‘Ù„ )ØªØ§Ø±ÛŒØ® Ø´Ù†Ø§Ø³ÛŒ(ØŒ Ø´Ù…Ø§ Ø¯Ø± Ø¨Ø§Ø±Ù‡ Ù…ÙˆØ¶ÙˆØ¹ØŒ Ù…Ù†Ø§Ø¨Ø¹ Ùˆ Ø±ÙˆØ´ Ù¾Ú˜ÙˆÙ‡Ø´ Ø¯Ø± Ø¹Ù„Ù… ØªØ§Ø±ÛŒØ®Ù…Ø·Ø§Ù„Ø¹Ù‡ Ùˆ Ú©Ø§ÙˆØ´ Ø®ÙˆØ§Ù‡ÛŒØ¯ Ú©Ø±Ø¯. Ù…Ø­ØªÙˆØ§ÛŒ Ø§ÛŒÙ† ÙØµÙ„ Ø¨Ù‡ Ø´Ù…Ø§ Ú©Ù…Ú© Ù…ÛŒ Ú©Ù†Ø¯ Ú©Ù‡ Ù¾ÛŒ Ø¨Ø¨Ø±ÛŒØ¯ ØªØ§Ø±ÛŒØ® Ú†Ú¯ÙˆÙ†Ù‡ Ùˆ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú†Ù‡ Ù…Ù†Ø§Ø¨Ø¹Ùˆ Ù…Ø¢Ø®Ø°ÛŒØŒ Ù¾Ú˜ÙˆÙ‡Ø´ Ùˆ Ù†ÙˆØ´ØªÙ‡ Ù…ÛŒ Ø´ÙˆØ¯.'\nØ±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ ØªØ¯Ø±ÛŒØ³ Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ú©ØªØ§Ø¨ Ø¯Ø±Ø³ÛŒ ØªØ§Ø±ÛŒØ® Ø¯Ù‡Ù…\n'Ù¾Ø§ÛŒÙ‡ Ù‡Ø§ÛŒ Ø¯Ù‡Ù…ØŒ ÛŒØ§Ø²Ø¯Ù‡Ù… Ùˆ Ø¯ÙˆØ§Ø²Ø¯Ù‡Ù… Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒ Ø´ÙˆØ¯.ØªÙˆØ¬Ù‡ Ø´Ù…Ø§ Ø¹Ø²ÛŒØ²Ø§Ù† Ø±Ø§ Ø¨Ù‡ Ú†Ù†Ø¯ Ù†Ú©ØªÙ‡ Ø¯Ø± Ø¨Ø§Ø±Ù‡ Ú©ØªØ§Ø¨ Ø¯Ø±Ø³ÛŒ ØªØ§Ø±ÛŒØ® Ø¯Ù‡Ù… Ø¬Ù„Ø¨ Ù…ÛŒ Ú©Ù†ÛŒÙ…:Û± Ø§ÛŒÙ† Ú©ØªØ§Ø¨ Ø¯Ø§Ø±Ø§ÛŒ Ø³Ù‡ ÙØµÙ„ Ø§Ø³Øª. Ø¯Ø± ÙØµÙ„ Ø§ÙˆÙ‘Ù„ )ØªØ§Ø±ÛŒØ® Ø´Ù†Ø§Ø³ÛŒ(ØŒ Ø´Ù…Ø§ Ø¯Ø± Ø¨Ø§Ø±Ù‡ Ù…ÙˆØ¶ÙˆØ¹ØŒ Ù…Ù†Ø§Ø¨Ø¹ Ùˆ Ø±ÙˆØ´ Ù¾Ú˜ÙˆÙ‡Ø´ Ø¯Ø± Ø¹Ù„Ù… ØªØ§Ø±ÛŒØ®Ù…Ø·Ø§Ù„Ø¹Ù‡ Ùˆ Ú©Ø§ÙˆØ´ Ø®ÙˆØ§Ù‡ÛŒØ¯ Ú©Ø±Ø¯. Ù…Ø­ØªÙˆØ§ÛŒ Ø§ÛŒÙ† ÙØµÙ„ Ø¨Ù‡ Ø´Ù…Ø§ Ú©Ù…Ú© Ù…ÛŒ Ú©Ù†Ø¯ Ú©Ù‡ Ù¾ÛŒ Ø¨Ø¨Ø±ÛŒØ¯ ØªØ§Ø±ÛŒØ® Ú†Ú¯ÙˆÙ†Ù‡ Ùˆ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú†Ù‡ Ù…Ù†Ø§Ø¨Ø¹Ùˆ Ù…Ø¢Ø®Ø°ÛŒØŒ Ù¾Ú˜ÙˆÙ‡Ø´ Ùˆ Ù†ÙˆØ´ØªÙ‡ Ù…ÛŒ Ø´ÙˆØ¯.ÙØµÙ„ Ø¯ÙˆÙ… Ú©ØªØ§Ø¨ Ø¨Ù‡ ØªØ§Ø±ÛŒØ® Ø¬Ù‡Ø§Ù† Ùˆ ÙØµÙ„ Ø³ÙˆÙ… Ø¨Ù‡ ØªØ§Ø±ÛŒØ® Ø§ÛŒØ±Ø§Ù† Ø¯Ø± Ø¯ÙˆØ±Ø§Ù† Ø¨Ø§Ø³ØªØ§Ù† Ø§Ø®ØªØµØ§Øµ Ø¯Ø§Ø±Ø¯. Ø§Ù†ØªØ¸Ø§Ø± Ù…ÛŒ Ø±ÙˆØ¯ Ú©Ù‡ Ø´Ù…Ø§ Ø¨Ø®Ø´ÛŒØ§Ø² Ø¢Ù…ÙˆØ®ØªÙ‡ Ù‡Ø§ÛŒØªØ§Ù† Ø¯Ø± ÙØµÙ„ Ø§ÙˆÙ‘Ù„ Ø±Ø§ Ø¯Ø± ÙØµÙ„ Ù‡Ø§ÛŒ Ø¯ÙˆÙ… Ùˆ Ø³ÙˆÙ… Ø¨Ù‡ Ú©Ø§Ø± Ø¨Ù†Ø¯ÛŒØ¯ Ùˆ Ù…Ù‡Ø§Ø±Øª Ú©Ø§ÙˆØ´Ú¯Ø±ÛŒ Ø®ÙˆØ¯ Ø±Ø§ ØªÙ‚ÙˆÛŒØª Ú©Ù†ÛŒØ¯.Û² Ø¯Ø§Ù†Ø´ Ø¢Ù…ÙˆØ² Ø¹Ø²ÛŒØ²! Ø§ÛŒÙ† Ú©ØªØ§Ø¨ Ø¨Ø§ Ø§ÛŒÙ† Ù‡Ø¯Ù ØªØ§Ù„ÛŒÙ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø´Ù…Ø§ Ú©Ù„Ù…Ù‡ Ø¨Ù‡ Ú©Ù„Ù…Ù‡ Ùˆ Ø³Ø·Ø± Ø¨Ù‡ Ø³Ø·Ø± Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ø®Ø§Ø·Ø± Ø¨Ø³Ù¾Ø§Ø±ÛŒØ¯Ø›Ø¨Ù„Ú©Ù‡ ÛŒÚ©ÛŒ Ø§Ø² Ø§Ù‡Ø¯Ø§Ù Ø§ÛŒÙ† Ú©ØªØ§Ø¨ØŒ ØªÙ‚ÙˆÛŒØª Ø§Ù†Ú¯ÛŒØ²Ù‡ Ø¬Ø³Øª ÙˆØ¬ÙˆÛŒ Ø¹Ù„Ù…ÛŒ Ùˆ Ø§Ø±ØªÙ‚Ø§ÛŒ Ù…Ù‡Ø§Ø±Øª Ú©Ø§ÙˆØ´Ú¯Ø±ÛŒ Ø´Ù…Ø§ Ø¯Ø± Ù…ÙˆØ¶ÙˆØ¹ Ù‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒØ§Ø³Øª. Ù‡Ø¯Ù Ø¯ÛŒÚ¯Ø±ØŒ ÙØ±Ø§Ù‡Ù… Ø¢ÙˆØ±Ø¯Ù† ÙØ±ØµØª Ù‡Ø§ÛŒÛŒ Ø§Ø³Øª Ú©Ù‡ Ø´Ù…Ø§ Ø¨Ø§ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯Ù† ÙØ¹Ù‘Ø§Ù„ÛŒØª Ù‡Ø§ÛŒ ÙØ±Ø¯ÛŒ Ùˆ Ú¯Ø±ÙˆÙ‡ÛŒØŒ Ø¹Ù„Ù„ Ùˆ Ù†ØªØ§ÛŒØ¬Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ù‡Ù… ØªØ§Ø±ÛŒØ®ÛŒ Ùˆ Ù†Ù‚Ø´ Ùˆ ØªØ§Ø«ÛŒØ± Ø´Ø®ØµÛŒØª Ù‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø±Ø§ Ø¯Ø± ØªØ§Ø±ÛŒØ®ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¯Ø±Ú© Ú©Ù†ÛŒØ¯.Ù…Ø«Ù„Ø§Ù‹ Ø§Ù†ØªØ¸Ø§Ø± Ù†Ù…ÛŒ Ø±ÙˆØ¯ Ú©Ù‡ Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ø³Ø§Ù„ ØªØ­ØµÛŒÙ„ÛŒØŒ Ø­Ø¬Ù… Ø¹Ø¸ÛŒÙ…ÛŒ Ø§Ø² Ù†Ø§Ù… Ù‡Ø§ÛŒ Ø³Ù„Ø³Ù„Ù‡ Ù‡Ø§ØŒ ÙØ±Ù…Ø§Ù†Ø±ÙˆØ§ÛŒØ§Ù†ØŒ Ú©Ø´ÙˆØ±Ù‡Ø§ØŒ Ø´Ù‡Ø±Ù‡Ø§ Ùˆ Ø³Ø§Ù„ Ù‡Ø§Ø±Ø§ Ø­ÙØ¸ Ú©Ø±Ø¯Ù‡ Ø¨Ø§Ø´ÛŒØ¯ØŒ Ø§Ù…Ø§ Ø§Ù†ØªØ¸Ø§Ø± Ù…ÛŒ Ø±ÙˆØ¯ Ú©Ù‡ Ø¨Ø¯Ø§Ù†ÛŒØ¯ Ú†Ø±Ø§ Ø§Ø´ÛŒØ§ Ùˆ Ø¢Ø«Ø§Ø± Ø¨Ø§Ø³ØªØ§Ù†ÛŒØŒ Ø§Ø±Ø²Ø´Ù…Ù†Ø¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø®Øª Ú¯Ø°Ø´ØªÙ‡ Ù…ÙÛŒØ¯ Ù‡Ø³ØªÙ†Ø¯.Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§Ù†ØªØ¸Ø§Ø± Ù…ÛŒ Ø±ÙˆØ¯ Ø¨ØªÙˆØ§Ù†ÛŒØ¯ Ø¹Ù„Ù„ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ù‡Ù… ØªØ§Ø±ÛŒØ®ÛŒ Ø±Ø§ Ú©Ù‡ Ø¯Ø± Ú©ØªØ§Ø¨ Ù…ÙˆØ±Ø¯ Ø¨Ø­Ø« Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡ Ø§Ù†Ø¯ØŒ Ø´Ø±Ø­ Ø¯Ù‡ÛŒØ¯ Ùˆ ÛŒØ§Ø§ÛŒÙ†Ú©Ù‡ Ù†Ù‚Ø´ Ùˆ ØªØ§Ø«ÛŒØ± Ø´Ø®ØµÛŒØª Ù‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ Ø²Ø±ØªØ´ØªØŒ Ú©ÙˆØ±Ø´ Ùˆ Ø¯Ø§Ø±ÛŒÙˆØ´ Ù‡Ø®Ø§Ù…Ù†Ø´ÛŒ Ø±Ø§ Ø¯Ø± ØªØ§Ø±ÛŒØ® Ø§ÛŒØ±Ø§Ù† Ø¨Ø§Ø³ØªØ§Ù† Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†ÛŒØ¯.Ø¯Ø± Ú©ØªØ§Ø¨ Ø¢Ù…Ø¯Ù‡ Ø§Ø³ØªØŒ Ø§Ø±Ø²Ø´ Ùˆ Ø§Ù‡Ù…ÛŒØªÛŒ Ù‡Ù…Ú†ÙˆÙ† Ø³Ø§ÛŒØ± \" Ú©ÛŒ ØªÙˆØ¶ÛŒØ­ \" Ùˆ \" Ø¨Ø±Ø±Ø³ÛŒ Ø´ÙˆØ§Ù‡Ø¯ Ùˆ Ù…Ø¯Ø§Ø±Ú© \" Û³ Ù…Ø·Ø§Ù„Ø¨ÛŒ Ú©Ù‡ Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù†Ù…Ø·Ø§Ù„Ø¨ Ú©ØªØ§Ø¨ Ø¯Ø§Ø±Ù†Ø¯ Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† Ø¢Ù†Ù‡Ø§ØŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø¨Ø§Ø­Ø« Ø¯ÛŒÚ¯Ø± Ø±Ø§ Ù…Ø´Ú©Ù„ Ù…ÛŒ Ú©Ù†Ø¯Ø› Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†ØŒ Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ù‡Ø§ Ø±Ø§ Ù…Ø§Ù†Ù†Ø¯ Ø¯ÛŒÚ¯Ø± Ù‚Ø³Ù…Øª Ù‡Ø§ÛŒÙˆ \" ÛŒÚ© ØªÙˆØ¶ÛŒØ­ \" Ú©ØªØ§Ø¨ Ø¬Ø¯Ù‘ÛŒ Ø¨Ú¯ÛŒØ±ÛŒØ¯ Ùˆ ÙØ¹Ù‘Ø§Ù„ÛŒØª Ù‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯. Ø¯Ø± Ø¢Ø²Ù…ÙˆÙ† Ù‡Ø§ÛŒ Ú©ØªØ¨ÛŒ Ùˆ Ù¾Ø§ÛŒØ§Ù†ÛŒØŒ Ø§Ø² Ù‚Ø³Ù…Øª Ù‡Ø§ÛŒÙ¾Ø±Ø³Ø´ÛŒ Ø·Ø±Ø­ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯Ø› Ø§Ù…Ø§ Ø¯Ø¨ÛŒØ± Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ø¯Ø± Ø¢Ø²Ù…ÙˆÙ† Ù‡Ø§ÛŒ Ù¾Ø§ÛŒØ§Ù†ÛŒØŒ Ù…ØªÙ† ÛŒÚ©ÛŒ Ø§Ø² Ø¨Ø±Ø±Ø³ÛŒ ØŒ\" Ø¨Ø±Ø±Ø³ÛŒ Ø´ÙˆØ§Ù‡Ø¯ Ùˆ Ù…Ø¯Ø§Ø±Ú© \" Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§Ø² Ù…ØªÙ†Ø´ÙˆØ§Ù‡Ø¯ Ùˆ Ù…Ø¯Ø§Ø±Ú© Ø±Ø§ØŒ Ø¨Ø§ Ø§Ø¹Ù„Ø§Ù… Ù‚Ø¨Ù„ÛŒØŒ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø´Ù…Ø§ Ù‚Ø±Ø§Ø± Ø¯Ù‡Ø¯ Ùˆ Ù¾Ø±Ø³Ø´ Ù‡Ø§ÛŒ Ø§Ø³ØªÙ†Ø¨Ø§Ø·ÛŒ Ùˆ Ù…ÙÙ‡ÙˆÙ…ÛŒ Ø¯Ø± Ø¨Ø§Ø±Ù‡ Ø¢Ù† Ø·Ø±Ø­ Ú©Ù†Ø¯.Û´ Ø¯Ø± Ø§ÛŒÙ† Ú©ØªØ§Ø¨ØŒ Ø¨Ø®Ø´ÛŒ Ø§Ø² Ù…Ø­ØªÙˆØ§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ù‡ ÙˆØ³ÛŒÙ„Ù‡ Ù†Ù‚Ø´Ù‡ Ù‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒØŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø®Ø· Ø²Ù…Ø§Ù† Ùˆ Ø¯ÛŒÚ¯Ø± Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§Ùˆ Ø¬Ø¯ÙˆÙ„ Ù‡Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ú©Ù‡ Ù„Ø§Ø²Ù… Ø§Ø³Øª Ø¨Ø§ ØªÙˆØ¬Ù‡ Ú©Ø§Ù…Ù„ Ø¨Ù‡ Ø¢Ù†Ù‡Ø§ ÙØ¹Ù‘Ø§Ù„ÛŒØª Ù‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯. Ø§Ø² Ù…Ù†Ø¯Ø±Ø¬Ø§Øª Ù†Ù‚Ø´Ù‡ Ù‡Ø§ØŒÙ†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ùˆ Ø¬Ø¯ÙˆÙ„ Ù‡Ø§ Ù†ÛŒØ² Ø¯Ø± Ø¢Ø²Ù…ÙˆÙ† Ù‡Ø§ÛŒ Ú©ØªØ¨ÛŒ Ùˆ Ù¾Ø§ÛŒØ§Ù†ÛŒ Ù¾Ø±Ø³Ø´ÛŒ Ø·Ø±Ø­ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.Ûµ Ù‚Ø·Ø¹Ù‡ Ù‡Ø§ÛŒ ØµÙˆØªÛŒØŒ ØªØµÙˆÛŒØ±ÛŒ Ùˆ ÙˆÛŒØ¯Ø¦ÙˆÛŒÛŒ Ù…Ø®ØªÙ„ÙÛŒ Ø¨Ø§ Ù…ÙˆØ¶ÙˆØ¹ Ù‡Ø§ÛŒ Ú¯ÙˆÙ†Ø§Ú¯ÙˆÙ† ØªØ§Ø±ÛŒØ®ÛŒØŒ ØªÙ‡ÛŒÙ‡ Ùˆ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø´Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø¨ÛŒØ± Ù…Ø­ØªØ±Ù…Ø¨Ù‡ ØªÙ†Ø§Ø³Ø¨ Ù…ÙˆØ¶ÙˆØ¹ Ùˆ Ù…ÙˆÙ‚Ø¹ÛŒØª ØªØ¯Ø±ÛŒØ³ØŒ Ø¯Ø± Ú©Ù„Ø§Ø³ Ø¨Ù‡ Ù†Ù…Ø§ÛŒØ´ Ø®ÙˆØ§Ù‡Ø¯ Ú¯Ø°Ø§Ø´Øª. Ø§ÛŒÙ† Ù‚Ø·Ø¹Ù‡ Ù‡Ø§ Ø¨Ù‡ Ø´Ù…Ø§ Ø¯Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ù‡ØªØ± ØªØ§Ø±ÛŒØ® Ú©Ù…Ú©Ù…ÛŒ Ú©Ù†Ù†Ø¯ .Û¶ Ø¯Ø§Ù†Ø´ Ø¢Ù…ÙˆØ² Ú¯Ø±Ø§Ù…ÛŒ! Ú¯Ø±ÙˆÙ‡ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø±ÛŒØ²Ø§Ù† Ùˆ Ù…ÙˆÙ„ÙØ§Ù† Ú©ØªØ§Ø¨ Ø¯Ø±Ø³ÛŒ Ø¯Ø± Ø¯ÙØªØ± ØªØ§Ù„ÛŒÙ Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒ Ø¯Ø±Ø³ÛŒØŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø¯Ø±ÛŒØ§ÙØª Ø§Ù†ØªÙ‚Ø§Ø¯Ù‡Ø§ Ùˆ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ù‡Ø§ÛŒØ´Ù…Ø§ Ø¯Ø± Ø¨Ø§Ø±Ù‡ Ø§ÛŒÙ† Ú©ØªØ§Ø¨ Ù‡Ø³ØªÙ†Ø¯. Ø®ÙˆØ§Ù‡Ø´Ù…Ù†Ø¯ÛŒÙ… Ø¯ÛŒØ¯Ú¯Ø§Ù‡ Ù‡Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ù¾ÛŒØ§Ù… Ù†Ú¯Ø§Ø± )Ø§ÛŒÙ…ÛŒÙ„( Ø²ÛŒØ± Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯:history-dept@talif.sch.irÚ¯Ø±ÙˆÙ‡ ØªØ§Ø±ÛŒØ® Ø¯ÙØªØ± ØªØ§Ù„ÛŒÙ Ú©ØªØ§Ø¨ Ù‡Ø§ÛŒ Ø¯Ø±Ø³ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Ùˆ Ù…ØªÙˆØ³Ø·Ù‡ Ù†Ø¸Ø±ÛŒÙØµÙ„ Ø§ÙˆÙ‘Ù„ØªØ§Ø±ÛŒØ® Ø´Ù†Ø§Ø³ÛŒØ›Ú©Ø§ÙˆØ´ Ú¯Ø°Ø´ØªÙ‡ØªØ§Ø±ÛŒØ®ØŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ùˆ Ú©Ø§ÙˆØ´ Ú¯Ø°Ø´ØªÙ‡ Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø®Øª Ø­Ø§Ù„ Ø§Ø³Øª. Ù…ÙˆØ±Ù‘Ø®Ø§Ù† Ø¨Ø§ Ø·Ø±Ø­Ù¾Ø±Ø³Ø´ØŒ Ú¯Ø±Ø¯Ø¢ÙˆØ±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ùˆ Ø¬Ø³Øª ÙˆØ¬ÙˆÛŒ Ø´ÙˆØ§Ù‡Ø¯ Ùˆ Ù…Ø¯Ø§Ø±Ú©ØŒ Ø²Ù†Ø¯Ú¯ÛŒÙ…Ø±Ø¯Ù…Ø§Ù† Ùˆ Ø¬ÙˆØ§Ù…Ø¹ Ù¾ÛŒØ´ÛŒÙ† Ø±Ø§ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ù…ÛŒ Ú©Ù†Ù†Ø¯. Ù¾Ú˜ÙˆÙ‡Ø´ Ø¯Ø± Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§Ùˆ ØªØ­ÙˆÙ„Ø§Øª ØªØ§Ø±ÛŒØ®ÛŒØŒ Ø§Ù†Ø³Ø§Ù† Ù‡Ø§ Ùˆ Ø¬ÙˆØ§Ù…Ø¹ÛŒ Ø±Ø§ Ú©Ù‡ Ø§Ù†Ø¯ÛŒØ´Ù‡ Ùˆ ØªØ¬Ø±Ø¨Ù‡ Ø¢Ù†Ø§Ù† Ø¯Ø±Ø´Ú©Ù„ Ø¯Ø§Ø¯Ù† Ø¨Ù‡ Ø²Ù†Ø¯Ú¯ÛŒ Ù…Ø§ Ù†Ù‚Ø´ Ø¯Ø§Ø´ØªÙ‡ Ø±Ø§ Ø¨Ù‡ Ù…Ø§ Ù…ÛŒ Ø´Ù†Ø§Ø³Ø§Ù†Ø¯ Ùˆ Ù…ÙˆØ¬Ø¨ Ù…ÛŒ Ø´ÙˆØ¯Ø§Ø² Ø³Ø±Ø§Ù†Ø¬Ø§Ù… Ú©Ø§Ø±Ù‡Ø§ Ùˆ Ø³Ø±Ù†ÙˆØ´Øª Ø¢Ù†Ø§Ù† Ø¹Ø¨Ø±Øª Ø¨Ú¯ÛŒØ±ÛŒÙ…. Ø¹Ù„Ø§ÙˆÙ‡ Ø¨Ø± Ø¢Ù†ØŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ ÙˆØªØ­Ù„ÛŒÙ„ ØªØ§Ø±ÛŒØ® Ø¨Ù‡ Ù…Ø§ Ú©Ù…Ú© Ù…ÛŒ Ú©Ù†Ø¯ Ú©Ù‡ Ù‡ÙˆÛŒØª Ùˆ Ø¬Ø§ÛŒÚ¯Ø§Ù‡ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø§ÛŒØ±Ø§Ù†ÛŒÙˆ Ù…Ø³Ù„Ù…Ø§Ù† Ø¯Ø± Ø¬Ù‡Ø§Ù† Ø§Ù…Ø±ÙˆØ²ÛŒØŒ Ø¨Ù‡ØªØ± Ùˆ Ø¯Ù‚ÛŒÙ‚ ØªØ± Ø¯Ø±Ú© Ú©Ù†ÛŒÙ….Û²Ø¯Ø±Ø³ Û± ØªØ§Ø±ÛŒØ® Ùˆ ØªØ§Ø±ÛŒØ® Ù†Ú¯Ø§Ø±ÛŒØªØ§Ú©Ù†ÙˆÙ† Ù…Ø·Ø§Ù„Ø¨ Ù…Ø®ØªÙ„ÙÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ ØªØ§Ø±ÛŒØ® Ùˆ Ú¯Ø°Ø´ØªÙ‡ Ø®ÙˆØ§Ù†Ø¯Ù‡ ÛŒØ§ Ø´Ù†ÛŒØ¯Ù‡ Ø§ÛŒØ¯. Ø­ØªÙ…Ø§Ù‹ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ù‡Ø§ Ø¨Ù‡ Ø°Ù‡Ù† Ø´Ù…Ø§ Ø±Ø§Ù‡ ÛŒØ§ÙØªÙ‡ Ú©Ù‡ ØªØ§Ø±ÛŒØ® Ú†ÛŒØ³ØªØŸÙ…ÙˆØ±Ù‘Ø®Ø§Ù† Ú†Ú¯ÙˆÙ†Ù‡ Ùˆ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú†Ù‡ Ù…Ù†Ø§Ø¨Ø¹ÛŒ ØªØ§Ø±ÛŒØ® Ø±Ø§ Ù…ÛŒ Ù†ÙˆÛŒØ³Ù†Ø¯ØŸ Ø§ØµÙ„Ø§Ù‹ Ú†Ø±Ø§ Ø¨Ø§ÛŒØ¯ ØªØ§Ø±ÛŒØ® Ø®ÙˆØ§Ù†Ø¯ØŸ Ø¯Ø± Ø§ÛŒÙ† Ø¯Ø±Ø³ Ø´Ù…Ø§ Ø¨Ø§ Ú©Ø§ÙˆØ´ Ø¯Ø±Ø¨Ø§Ø±Ù‡Ú†ÛŒØ³ØªÛŒ Ø¹Ù„Ù… ØªØ§Ø±ÛŒØ®ØŒ Ø´ÛŒÙˆÙ‡ Ù‡Ø§ÛŒ ØªØ§Ø±ÛŒØ® Ù†Ú¯Ø§Ø±ÛŒ Ùˆ Ú©Ø§Ø±Ú©Ø±Ø¯Ù‡Ø§ÛŒ Ø¢Ù†ØŒ Ù¾Ø§Ø³Ø® Ø§ÛŒÙ† Ù¾Ø±Ø³Ø´ Ù‡Ø§ Ø±Ø§ Ø¬Ø³Øª Ùˆ Ø¬Ùˆ Ú©Ø±Ø¯Ù‡ØŒ Ø®ÙˆØ§Ù‡ÛŒØ¯ ØªÙˆØ§Ù†Ø³Øª Ø¯Ø±Ø¨Ø§Ø±Ù‡Ú©ÛŒ Ù…ÙˆØ¶ÙˆØ¹ ØªØ§Ø±ÛŒØ®ÛŒØŒ Ø·Ø±Ø­ Ø³Ø§Ø¯Ù‡ Ù¾Ú˜ÙˆÙ‡Ø´ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.ØªØ§Ø±ÛŒØ® Ú†ÛŒØ³ØªØŸØ¯Ø§Ø±Ø§ÛŒ Ù…Ø¹Ø§Ù†ÛŒ Ù…Ø®ØªÙ„ÙÛŒ Ø§Ø³Øª. Ú¯Ø§Ù‡ÛŒ Ù…Ù†Ø¸ÙˆØ± Ø§Ø² \" ØªØ§Ø±ÛŒØ® \" ÙˆØ§Ú˜Ù‡Ø¢Ù†ØŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø­ÙˆØ§Ø¯Ø« Ùˆ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒÛŒ Ø§Ø³Øª Ú©Ù‡ ÙØ±Ø¯ ÛŒØ§ Ø¬Ø§Ù…Ø¹Ù‡Ú©Ù‡ Ù…Ù†Ø¸ÙˆØ± Ø§Ø² \" ØªØ§Ø±ÛŒØ® Ø§ÛŒØ±Ø§Ù† \" Ø§Ø² Ø³Ø± Ú¯Ø°Ø±Ø§Ù†Ø¯Ù‡ Ø§Ø³ØªØ› Ù…Ø§Ù†Ù†Ø¯Ø¢Ù†ØŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø­ÙˆØ§Ø¯Ø«ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ø²Ù…Ø§Ù† Ú¯Ø°Ø´ØªÙ‡ Ø¨Ø± Ø³Ø±Ù…Ø±Ø¯Ù… Ø§ÛŒØ±Ø§Ù† Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª. Ú¯Ø§Ù‡ÛŒ ÙˆØ§Ú˜Ù‡ ØªØ§Ø±ÛŒØ® Ø¯Ø± Ù†ÙˆØ´ØªÙ‡ Ù‡Ø§ Ùˆ'\nÙ…ÙØ§Ù‡ÛŒÙ… Ùˆ Ø§Ù‡Ø¯Ø§Ù Ú©ØªØ§Ø¨ Ø¯Ø±Ø³ÛŒ ØªØ§Ø±ÛŒØ® Ø¯Ù‡Ù…\n'Ú©ÛŒ Ù…ÙˆØ¶ÙˆØ¹ ØªØ§Ø±ÛŒØ®ÛŒØŒ Ø·Ø±Ø­ Ø³Ø§Ø¯Ù‡ Ù¾Ú˜ÙˆÙ‡Ø´ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.ØªØ§Ø±ÛŒØ® Ú†ÛŒØ³ØªØŸØ¯Ø§Ø±Ø§ÛŒ Ù…Ø¹Ø§Ù†ÛŒ Ù…Ø®ØªÙ„ÙÛŒ Ø§Ø³Øª. Ú¯Ø§Ù‡ÛŒ Ù…Ù†Ø¸ÙˆØ± Ø§Ø² \" ØªØ§Ø±ÛŒØ® \" ÙˆØ§Ú˜Ù‡Ø¢Ù†ØŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø­ÙˆØ§Ø¯Ø« Ùˆ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒÛŒ Ø§Ø³Øª Ú©Ù‡ ÙØ±Ø¯ ÛŒØ§ Ø¬Ø§Ù…Ø¹Ù‡Ú©Ù‡ Ù…Ù†Ø¸ÙˆØ± Ø§Ø² \" ØªØ§Ø±ÛŒØ® Ø§ÛŒØ±Ø§Ù† \" Ø§Ø² Ø³Ø± Ú¯Ø°Ø±Ø§Ù†Ø¯Ù‡ Ø§Ø³ØªØ› Ù…Ø§Ù†Ù†Ø¯Ø¢Ù†ØŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø­ÙˆØ§Ø¯Ø«ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ø²Ù…Ø§Ù† Ú¯Ø°Ø´ØªÙ‡ Ø¨Ø± Ø³Ø±Ù…Ø±Ø¯Ù… Ø§ÛŒØ±Ø§Ù† Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª. Ú¯Ø§Ù‡ÛŒ ÙˆØ§Ú˜Ù‡ ØªØ§Ø±ÛŒØ® Ø¯Ø± Ù†ÙˆØ´ØªÙ‡ Ù‡Ø§ ÙˆØ²Ø¨Ø§Ù† Ø¹Ù„Ù…ÛŒ Ø¨Ù‡ Ú©Ø§Ø± Ù…ÛŒ Ø±ÙˆØ¯ Ùˆ Ù…Ù†Ø¸ÙˆØ± Ø§Ø² Ø¢Ù† Ø¹Ù„Ù… Ùˆ Ø±ÙˆØ´ Ø¹Ù„Ù…ÛŒØ§Ø³Øª Ú©Ù‡ Ø¨Ù‡ ÙˆØ³ÛŒÙ„Ù‡ Ø¢Ù†ØŒ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ú¯Ø°Ø´ØªÙ‡ Ø¨Ø±Ø§Ø³Ø§Ø³ Ø´ÙˆØ§Ù‡Ø¯Ùˆ Ù…Ø¯Ø§Ø±Ú©ØŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ùˆ ØªØ­Ù„ÛŒÙ„ Ù…ÛŒ Ø´ÙˆÙ†Ø¯Ø› Ù…Ø«Ù„Ø§Ù‹ ØªØ§Ø±ÛŒØ® Ø±ÙˆØ§Ø¨Ø·Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø§ÛŒØ±Ø§Ù† Ùˆ Ù‡Ù†Ø¯ Ø¯Ø± Ø¯ÙˆØ±Ø§Ù† Ø§Ø´Ú©Ø§Ù†ÛŒØ§Ù†.Ú¯Ø§Ù‡ÛŒ Ù†ÛŒØ² Ù…Ù†Ø¸ÙˆØ± Ø§Ø² ØªØ§Ø±ÛŒØ®ØŒ ØªØ¹ÛŒÛŒÙ† Ø±ÙˆØ² Ùˆ Ù…Ø§Ù‡ Ùˆ Ø³Ø§Ù„ Ø§Ø³ØªØ›ÛŒØ¹Ù†ÛŒ Ù‡Ù…Ø§Ù† ØªÙ‚ÙˆÛŒÙ… Ø³Ø§Ù„Ø§Ù†Ù‡ Ú©Ù‡ Ù‡Ù…Ù‡ Ø¨Ø§ Ø¢Ù† Ø¢Ø´Ù†Ø§ Ù‡Ø³ØªÙ†Ø¯.Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯ ÙˆØ§Ú˜Ù‡ ØªØ§Ø±ÛŒØ® Ø¯Ø± Ø¬Ù…Ù„Ù‡ Ù‡Ø§ÛŒ Ø²ÛŒØ± Ú†Ù‡ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¯Ø§Ø±Ø¯Ø›Ø§Ù„Ù  ØªØ§Ø±ÛŒØ® Ø±Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ø´ÛŒÙˆÙ‡ Ø¹Ù„Ù…ÛŒ Ùˆ Ø±ÙˆØ´Ù…Ù†Ø¯ Ù†ÙˆØ´Øª.Ø¨  Ù‡Ù†ÙˆØ² ØªØ§Ø±ÛŒØ® Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª Ø´ÙˆØ±Ø§ÛŒ Ø¯Ø§Ù†Ø´ Ø¢Ù…ÙˆØ²ÛŒ Ù…Ø¯Ø±Ø³Ù‡ØŒ ØªØ¹ÛŒÛŒÙ† Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.Ù¾  ØªØ§Ø±ÛŒØ® Ø¨Ø§Ø³ØªØ§Ù† Ù¾Ø± Ø§Ø² Ø¬Ù†Ú¯ Ùˆ Ú©Ø´ÙˆØ±Ú¯Ø´Ø§ÛŒÛŒ Ø§Ø³Øª.ÙØ¹Ù‘Ø§Ù„ÛŒØª Û±ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§ÛŒ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ : Ø§Ù„Ù  Ø¯ÙˆØ± Ø§Ø² Ø¯Ø³ØªØ±Ø³ Ø§Ù†Ø¯Ùˆ Ù‚Ø§Ø¨Ù„ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù†ÛŒØ³ØªÙ†Ø¯ Ùˆ Ù†Ù…ÛŒ ØªÙˆØ§Ù† Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø³ØªÙ‚ÛŒÙ…Ø¯Ø±Ú© Ú©Ø±Ø¯ØŒ Ø¨Ù„Ú©Ù‡ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø´ÙˆØ§Ù‡Ø¯ Ùˆ Ù…Ø¯Ø§Ø±Ú©Ø´Ù†Ø§Ø®ØªØ› Ø¨  ØªÚ©Ø±Ø§Ø±Ù†Ø§Ù¾Ø°ÛŒØ±Ù†Ø¯ Ùˆ Ù‚Ø§Ø¨Ù„ ØªØ¬Ø±Ø¨Ù‡ Ù†ÛŒØ³ØªÙ†Ø¯Ø› Ù¾  Ù…Ø¬Ø²Ø§Ùˆ Ù…Ø³ØªÙ‚Ù„ Ù†ÛŒØ³ØªÙ†Ø¯ Ùˆ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø±Ø§Ø¨Ø·Ù‡ Ø¹Ù„Øª Ùˆ Ù…Ø¹Ù„ÙˆÙ„ÛŒ Ø¯Ø§Ø±Ù†Ø¯.Ø§Ù†Ø³Ø§Ù† Ø¯Ø± ØªØ§Ø±ÛŒØ®ØŒ Ù†Ù‚Ø´ Ù…Ø­ÙˆØ±ÛŒ Ùˆ Ø§Ø³Ø§Ø³ÛŒ Ø¯Ø§Ø±Ø¯ Ùˆ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ ÙˆØªØ­ÙˆÙ„Ø§Øª ØªØ§Ø±ÛŒØ®ÛŒ Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ø±ÙØªØ§Ø± Ù…ØªÙ‚Ø§Ø¨Ù„ Ø§Ù†Ø³Ø§Ù† Ù‡Ø§ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø±Ùˆ Ø¨Ø§ Ø·Ø¨ÛŒØ¹ØªØŒ Ù¾Ø¯ÛŒØ¯ Ø¢Ù…Ø¯Ù‡ Ø§Ù†Ø¯. Ù†Ù‡Ø§Ø¯Ù‡Ø§ØŒ Ø¬ÙˆØ§Ù…Ø¹ØŒ Ú©Ø´ÙˆØ±Ù‡Ø§ Ùˆ Ø¯Ø±ÛŒÚ© Ú©Ù„Ø§Ù… ØªÙ…Ø¯Ù† Ù†ÛŒØ² Ù¾ÛŒØ§Ù…Ø¯ Ø±ÙØªØ§Ø± ÙØ±Ø¯ÛŒ Ùˆ Ø¬Ù…Ø¹ÛŒ Ø§Ù†Ø³Ø§Ù† Ù‡Ø§Ø³Øª.ØªØ§Ø±ÛŒØ®ØŒ Ø¹Ù„Ù…ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¨Ù‡ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø¬Ù†Ø¨Ù‡ Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø²Ù†Ø¯Ú¯ÛŒØ§Ù†Ø³Ø§Ù† Ù‡Ø§ Ùˆ Ø¬Ø§Ù…Ø¹Ù‡ Ù‡Ø§ Ø¯Ø± Ú¯Ø°Ø´ØªÙ‡ Ù…ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø¯ Ùˆ Ø¹Ù„Ù„ Ùˆ Ù†ØªØ§ÛŒØ¬Ø§ÙÚ©Ø§Ø± Ùˆ Ø§ÙŽØ¹Ù…Ø§Ù„ Ù¾ÛŒØ´ÛŒÙ†ÛŒØ§Ù† Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„ Ù…ÛŒ Ú©Ù†Ø¯.Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†ØŒ Ù‡Ø¯Ù Ø¹Ù„Ù… ØªØ§Ø±ÛŒØ®ØŒ Ø¢Ú¯Ø§Ù‡ÛŒ Ø§Ø² Ø²Ù†Ø¯Ú¯ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ø¯Ø±Ú¯Ø°Ø´ØªÙ‡ Ø§Ø³Øª Ú©Ù‡ Ø´Ø§Ù…Ù„ ØªÙ…Ø§Ù…ÛŒ Ø¬Ù†Ø¨Ù‡ Ù‡Ø§ÛŒ ÙÚ©Ø±ÛŒØŒ Ù…Ø°Ù‡Ø¨ÛŒØŒØ³ÛŒØ§Ø³ÛŒØŒ Ù†Ø¸Ø§Ù…ÛŒØŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒØŒ Ø¹Ù„Ù…ÛŒØŒ Ø­Ù‚ÙˆÙ‚ÛŒ Ùˆ Ù‡Ù†Ø±ÛŒ Ù…ÛŒ Ø´ÙˆØ¯.Ù†Ù…Ø§ÛŒØ´ ØªØµÙˆÛŒØ±Û³Ù¾ÛŒØ´ÛŒÙ†Ù‡ ØªØ§Ø±ÛŒØ® Ù†Ú¯Ø§Ø±ÛŒØ¹Ù„Ù… ØªØ§Ø±ÛŒØ® Ù¾ÛŒØ´ÛŒÙ†Ù‡ Ø¨Ø³ÛŒØ§Ø± Ú©Ù‡Ù†ÛŒ Ø¯Ø§Ø±Ø¯. Ù¾Ø³ Ø§Ø² Ø§Ø®ØªØ±Ø§Ø¹ Ø®Ø·Ø¯Ø± Ø­Ø¯ÙˆØ¯ Ûµ Ù‡Ø²Ø§Ø± Ø³Ø§Ù„ Ù¾ÛŒØ´ØŒ Ø¨Ù‡ ØªØ¯Ø±ÛŒØ¬ ØªÙˆØ¬Ù‡ Ø§Ù†Ø³Ø§Ù† Ø¨Ù‡ Ø«Ø¨ØªÙˆ Ù†Ú¯Ø§Ø±Ø´ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ø¹ØµØ± Ø®ÙˆÛŒØ´ Ø¬Ù„Ø¨ Ø´Ø¯. Ù‚Ø¯ÛŒÙ…ÛŒ ØªØ±ÛŒÙ†Ù…ØªÙ† ØªØ§Ø±ÛŒØ®ÛŒ Ú©Ù‡ ØªØ§Ú©Ù†ÙˆÙ† Ú©Ø´Ù Ø´Ø¯Ù‡ØŒ Ø³Ù†Ú¯ Ù†ÙˆØ´ØªÙ‡ Ø§ÛŒ Ø¨Ù‡ Ø®Ø·Ú©Ù‡Ù† Ù…ØµØ±ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¨ÛŒØ´ Ø§Ø² Û´ Ù‡Ø²Ø§Ø± Ø³Ø§Ù„ Ù‚Ø¯Ù…Øª Ø¯Ø§Ø±Ø¯. Ø¯Ø±Ø§ÛŒÙ† Ø³Ù†Ú¯ Ù†ÙˆØ´ØªÙ‡ Ù†Ø§Ù… ØªØ¹Ø¯Ø§Ø¯ÛŒ Ø§Ø² ÙØ±Ø§Ø¹Ù†Ù‡ Ùˆ Ø¨Ø±Ø®ÛŒ Ø­ÙˆØ§Ø¯Ø« Ø¯ÙˆØ±Ø§Ù†Ø¢Ù†Ø§Ù† Ø°Ú©Ø± Ø´Ø¯Ù‡ Ø§Ø³Øª.Ø§Ø² Ø±ÙˆØ²Ú¯Ø§Ø± Ø¨Ø§Ø³ØªØ§Ù†ÛŒ Ø§ÛŒØ±Ø§Ù†ØŒ Ø¨Ù‡ ÙˆÛŒÚ˜Ù‡ Ø¯ÙˆØ±Ù‡ Ù‡Ø®Ø§Ù…Ù†Ø´ÛŒØ§Ù† Ùˆ Ø³Ø§Ø³Ø§Ù†ÛŒØ§Ù†ØŒÙ†ÛŒØ² Ø³Ù†Ú¯ Ù†ÙˆØ´ØªÙ‡ Ù‡Ø§ÛŒÛŒ Ø¨Ø§Ù‚ÛŒ Ù…Ø§Ù†Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ù†Ø´Ø§Ù† Ø§Ø² ØªÙˆØ¬Ù‡Ø§ÛŒØ±Ø§Ù†ÛŒØ§Ù† Ø¨Ù‡ Ø«Ø¨Øª Ùˆ Ø¶Ø¨Ø· ÙˆÙ‚Ø§ÛŒØ¹ ØªØ§Ø±ÛŒØ®ÛŒ Ø¯Ø§Ø±Ø¯.Ø®Ø¯Ø§ÛŒ Ù†Ø§Ù…Ú© Ù‡Ø§( Ø¯Ø± Ø²Ù…Ø§Ù† ( \" Ø®Ø¯Ø§ÛŒ Ù†Ø§Ù…Ù‡ Ù‡Ø§ \" Ø¹Ù„Ø§ÙˆÙ‡ Ø¨Ø± Ø¢Ù†ØŒ ØªØ¯ÙˆÛŒÙ†Ø³Ø§Ø³Ø§Ù†ÛŒØ§Ù† Ø¯Ù„Ø§Ù„Øª Ø¨Ø± Ø¹Ù„Ø§Ù‚Ù‡ Ø§ÛŒØ±Ø§Ù†ÛŒØ§Ù† Ø¨Ù‡ Ø«Ø¨Øª Ùˆ Ù†Ú¯Ø§Ø±Ø´ Ø§Ø®Ø¨Ø§Ø± ÙˆØ±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ù‡Ù… Ø¯Ø§Ø±Ø¯.Ø³Ù†Ú¯ Ù¾Ø§Ù„Ø±Ù…ÙˆØŒ ØªÚ©Ù‡ Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø§Ø² Ú©ÛŒ Ø³ØªÙˆÙ† Ø³Ù†Ú¯ÛŒ ÛŒØ§Ø¯Ø¨ÙˆØ¯ Ø¨Ù‡ Ù†Ø§Ù… ï‚›Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù¾Ø§Ø¯Ø´Ø§Ù‡ÛŒ Ú©Ù‡Ù† Ù…ØµØ± Ø§Ø³Øª. Ù…ÙˆØ²Ù‡ Ø´Ù‡Ø± \" Ø³Ø§Ù„Ù†Ø§Ù…Ù‡ Ø´Ø§Ù‡ÛŒ \"Ù¾Ø§Ù„Ø±Ù…Ùˆ  Ø§ÛŒØªØ§Ù„ÛŒØ§Ø¨Ø±Ø±Ø³ÛŒ Ø´ÙˆØ§Ù‡Ø¯ Ùˆ Ù…Ø¯Ø§Ø±Ú©Ø³Ù†Ú¯ Ù†ÙˆØ´ØªÙ‡ Ø¨ÛŒØ³ØªÙˆÙ† Ú©Ù‡ Ø¨Ù‡ Ø¯Ø³ØªÙˆØ± Ø¯Ø§Ø±ÛŒÙˆØ´ ÛŒÚ©Ù…ØŒ Ù¾Ø§Ø¯Ø´Ø§Ù‡ Ù‡Ø®Ø§Ù…Ù†Ø´ÛŒ Ø¨Ø± Ø³ÛŒÙ†Ù‡ Ú©ÙˆÙ‡ÛŒ ÙˆØ§Ù‚Ø¹ Ø¯Ø± Ø´Ù‡Ø± Ø¨ÛŒØ³ØªÙˆÙ† Ø§Ø² ØªÙˆØ§Ø¨Ø¹ Ø´Ù‡Ø±Ø³ØªØ§Ù†Ù‡Ø±Ø³ÛŒÙ† Ø¯Ø± Ø§Ø³ØªØ§Ù† Ú©Ø±Ù…Ø§Ù†Ø´Ø§Ù‡ Ú©Ù†ÙˆÙ†ÛŒ Ú©Ù†Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³ØªØŒ ÛŒÚ©ÛŒ Ø§Ø² Ø¨Ø²Ø±Ú¯ ØªØ±ÛŒÙ† Ø³Ù†Ú¯ Ù†ÙˆØ´ØªÙ‡ Ù‡Ø§ÛŒ Ø¬Ù‡Ø§Ù† Ø¨Ù‡ Ø´Ù…Ø§Ø± Ù…ÛŒ Ø±ÙˆØ¯. Ø¯Ø§Ø±ÛŒÙˆØ´ Ø¯Ø± Ø§ÛŒÙ†Ø³Ù†Ú¯ Ù†ÙˆØ´ØªÙ‡ Ø¨Ù‡ Ø¨Ø±Ø®ÛŒ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ø®ÙˆØ¯ Ø§Ø²Ø¬Ù…Ù„Ù‡ Ø³Ø±Ú©ÙˆØ¨ Ú¯ÙŽÙˆÙ…Ø§ØªÙ‡ÙŽ Ù…ÙØº Ùˆ Ø¯ÛŒÚ¯Ø± Ø´ÙˆØ±Ø´ÛŒØ§Ù† Ø§Ø´Ø§Ø±Ù‡ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª. Ø¯Ø± Ø¨Ù†Ø¯Ù‡Ø§ÛŒ Û±Û² Ùˆ Û±Û³ Ø§Ø² Ø³ØªÙˆÙ†Ø¯Ø§Ø±ÛŒÙˆØ´ Ø´Ø§Ù‡ Ú¯ÙˆÛŒØ¯: Ø§ÛŒÙ† Ø´Ø§Ù‡ÛŒ Ú©Ù‡ Ú¯ÙŽÙˆÙ…Ø§ØªÙ‡ÙŽ Ù…Øº Ø§Ø² Ú©Ù…Ø¨ÙˆØ¬ÛŒÙ‡ ]Ù¾Ø³Ø± Ú©ÙˆØ±Ø´ Ø¨Ø²Ø±Ú¯[ Ø³ØªØ§Ù†Ø¯Ù‡ \" : Ø§ÙˆÙ‘Ù„ Ø³Ù†Ú¯ Ù†ÙˆØ´ØªÙ‡ Ø¨ÛŒØ³ØªÙˆÙ† Ø¢Ù…Ø¯Ù‡ Ø§Ø³ØªØ¯ÛŒØ±Ú¯Ø§Ù‡Ø§Ù† Ø¯Ø± ]Ø®Ø§Ù†Ø¯Ø§Ù†[ Ù…Ø§ Ø¨ÙˆØ¯. Ù¾Ø³ Ø§Ø² Ø¢Ù† Ú¯ÙŽÙˆÙ…Ø§ØªÙ‡ÙŽ Ù…Øº )Ø¢Ù† Ø±Ø§( Ø§Ø² Ú©Ù…Ø¨ÙˆØ¬ÛŒÙ‡ Ø³ØªØ§Ù†Ø¯.Ù‡Ù… Ù¾Ø§Ø±Ø³ØŒ Ù‡Ù… Ù…Ø§Ø¯ØŒ Ù‡Ù… Ø³Ø§ÛŒØ± Ú©Ø´ÙˆØ±Ù‡Ø§ Ø±Ø§ Ø§Ùˆ ØªØµØ±Ù Ú©Ø±Ø¯ Ùˆ Ø§Ø² Ø¢Ù†Ù Ø®ÙˆØ¯ Ù†Ù…ÙˆØ¯. Ø§Ùˆ Ø´Ø§Ù‡ Ø´Ø¯ ... Ø¢Ù† Ú¯Ø§Ù‡ Ù…Ù† Ø¨Ø§ Ú†Ù†Ø¯ Ù…Ø±Ø¯ØŒ Ø¢Ù† Ú¯ÙŽÙˆÙ…Ø§ØªÙ‡ÙŽ Ù…Øº Ùˆ.\" Ø¢Ù†Ù‡Ø§ÛŒÛŒ Ø±Ø§ Ú©Ù‡ Ø¨Ø±ØªØ±ÛŒÙ† Ù…Ø±Ø¯Ø§Ù†Ù Ø¯Ø³ØªÛŒØ§Ø± ]Ø§Ùˆ[ Ø¨ÙˆØ¯Ù†Ø¯ØŒ Ú©Ø´ØªÙ… ... Ø´Ø§Ù‡ÛŒ Ø±Ø§ Ø§Ø² Ø§Ùˆ Ø³ØªØ§Ù†Ø¯Ù…. Ø¨Ù‡ Ø®ÙˆØ§Ø³Øª Ø§Ù‡ÙŽÙˆØ±ÙŽÙ‡ Ù…Ø²Ø¯Ø§ Ù…Ù† Ø´Ø§Ù‡ Ø´Ø¯Ù…Û´Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…Ø·Ø§Ù„Ø¨ Ø³Ù†Ú¯ Ù†ÙˆØ´ØªÙ‡ Ø¨ÛŒØ³ØªÙˆÙ†ØŒ Ø¯Ù„ÛŒÙ„ Ø¨ÛŒØ§ÙˆØ±ÛŒØ¯ Ú©Ù‡ Ú†Ø±Ø§ Ø§ÛŒÙ† Ø³Ù†Ú¯ Ù†ÙˆØ´ØªÙ‡ ÛŒÚ© Ø³Ù†Ø¯ ØªØ§Ø±ÛŒØ®ÛŒ Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª.ÙØ¹Ù‘Ø§Ù„ÛŒØª Û²ØªØ§Ø±ÛŒØ® Ù†Ú¯Ø§Ø±ÛŒ Ø¯Ø± Ù…ÙÙ‡ÙˆÙ… Ú©Ø§Ù…Ù„ Ø¢Ù†ØŒ Ø§Ø² Ù‚Ø±Ù† Ù¾Ù†Ø¬Ù… Ù¾ÛŒØ´ Ø§Ø² Ù…ÛŒÙ„Ø§Ø¯ Ø¯Ø± ÛŒÙˆÙ†Ø§Ù† Ø¨Ø§Ø³ØªØ§Ù† ÙˆÙ„Ù‚Ø¨ Ú¯Ø±ÙØªØŒ Ø¢ØºØ§Ø² Ø´Ø¯. Ú©ØªØ§Ø¨ ØªØ§Ø±ÛŒØ® Ø§Ùˆ Ú©Ù‡ Ø¨ÛŒØ´ØªØ± \" Ù¾Ø¯Ø± ØªØ§Ø±ÛŒØ® \" Ø¨Ø§ Ø¸Ù‡ÙˆØ± Ù‡ÙØ±ÙˆØ¯ÙØª Û±ØŒ Ú©Ù‡Ù…Ø·Ø§Ù„Ø¨ Ø¢Ù† Ø´Ø±Ø­ Ø¬Ù†Ú¯ Ù‡Ø§ÛŒ ÛŒÙˆÙ†Ø§Ù† Ùˆ Ø§ÛŒØ±Ø§Ù† Ø§Ø³ØªØŒ Ú©Ù‡Ù† ØªØ±ÛŒÙ† Ùˆ Ú©Ø§Ù…Ù„ ØªØ±ÛŒÙ† Ú©ØªØ§Ø¨ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ù‡ Ø¬Ø§ Ù…Ø§Ù†Ø¯Ù‡ Ø§Ø² Ø¹ØµØ± Ø¨Ø§Ø³ØªØ§Ù† Ø§Ø³Øª. Ù‡Ø±ÙˆØ¯Øª Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ·Ø¨Ù‡ Ø§ÛŒØ±Ø§Ù†ØŒ Ø®ÙˆØ§Ø³ØªÙ‡ ÛŒØ§ Ù†Ø§Ø®ÙˆØ§Ø³ØªÙ‡ØŒ Ø¯Ú†Ø§Ø± Ù„ØºØ²Ø´ Ù‡Ø§ Ùˆ Ú¯Ø§Ù‡ ØºØ±Ø¶ ÙˆØ±Ø²ÛŒ Ù‡Ø§ÛŒÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.Ù¾Ø³ Ø§Ø² Ù‡Ø±ÙˆØ¯ØªØŒ Ø¯ÛŒÚ¯Ø± Ù†ÙˆÛŒØ³Ù†Ø¯Ú¯Ø§Ù† ÛŒÙˆÙ†Ø§Ù†ÛŒ Ùˆ Ø³Ù¾Ø³ Ø±ÙˆÙ…ÛŒØŒ Ø±Ø§Ù‡ÛŒ Ø±Ø§ Ú©Ù‡ Ø§Ùˆ Ø¢ØºØ§Ø² Ú©Ù†Ù†Ø¯Ù‡Ø¢Ù† Ø¨ÙˆØ¯ØŒ Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ø¯Ù†Ø¯ Ùˆ ÙÙ†Ù‘ ØªØ§Ø±ÛŒØ® Ù†ÙˆÛŒØ³ÛŒ Ø±Ø§ ØªÚ©Ø§Ù…Ù„ Ø¨Ø®Ø´ÛŒØ¯Ù†Ø¯. Ø§Ø² Ø§ÛŒÙ† Ù…ÙˆØ±Ù‘Ø®Ø§Ù†ØŒ Ø¢Ø«Ø§Ø±Ø¨Ø±Ø¬Ø³ØªÙ‡ ØªØ§Ø±ÛŒØ®ÛŒ Ø¨Ù‡ Ø¬Ø§ Ù…Ø§Ù†Ø¯Ù‡ Ø§Ø³Øª.Ø¨Ø±Ø®ÛŒ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø±Ø§Ù† Ù…Ø¹ØªÙ‚Ø¯Ù†Ø¯ Ú©Ù‡ Ø±ÙˆØ§Ø¬ Ùˆ Ø±ÙˆÙ†Ù‚ Ø§Ø¯Ø¨ÛŒØ§Øª Ùˆ ÙÙ„Ø³ÙÙ‡ØŒ ØªØ§Ø«ÛŒØ± Ù…Ù‡Ù…ÛŒ Ø¨Ø±Ø´Ú©ÙˆÙØ§ÛŒÛŒ Ùˆ Ú¯Ø³ØªØ±Ø´ Ø¹Ù„Ù… ØªØ§Ø±ÛŒØ® Ø¯Ø± ÛŒÙˆÙ†Ø§Ù† Ø¨Ø§Ø³ØªØ§Ù† Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª.Ø¯Ø± Ø¯ÙˆØ±Ø§Ù† Ø§Ø³Ù„Ø§Ù…ÛŒØŒ ØªØ§Ø±ÛŒØ® Ù†Ú¯Ø§Ø±ÛŒ Ø¯Ø± Ù…ÛŒØ§Ù† Ù…Ø³Ù„Ù…Ø§Ù†Ø§Ù† Ø±ÙˆØ§Ø¬ Ùˆ Ú¯Ø³ØªØ±Ø´ Ú†Ø´Ù…Ú¯ÛŒØ±'\nÙˆØ§Ú˜Ù‡ ØªØ§Ø±ÛŒØ® Ùˆ Ù…Ø¹Ø§Ù†ÛŒ Ø¢Ù†\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\npersianQA_pair\nDataset: persianQA_pair at 5a314e7\nSize: 9,008 training samples\nColumns: positive and anchor\nApproximate statistics based on the first 1000 samples:\npositive\nanchor\ntype\nstring\nstring\ndetails\nmin: 135 tokensmean: 266.58 tokensmax: 512 tokens\nmin: 5 tokensmean: 12.4 tokensmax: 34 tokens\nSamples:\npositive\nanchor\nØ´Ø±Ú©Øª ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Û€ Ø§ØµÙÙ‡Ø§Ù†ØŒ Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ† ÙˆØ§Ø­Ø¯ ØµÙ†Ø¹ØªÛŒ Ø®ØµÙˆØµÛŒ Ø¯Ø± Ø§ÛŒØ±Ø§Ù† Ùˆ Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ† Ù…Ø¬ØªÙ…Ø¹ ØªÙˆÙ„ÛŒØ¯ ÙÙˆÙ„Ø§Ø¯ Ø¯Ø± Ú©Ø´ÙˆØ± Ø§ÛŒØ±Ø§Ù† Ø§Ø³ØªØŒ Ú©Ù‡ Ø¯Ø± Ø´Ø±Ù‚ Ø´Ù‡Ø± Ù…Ø¨Ø§Ø±Ú©Ù‡ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯. ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Ù‡ Ù‡Ù…â€ŒØ§Ú©Ù†ÙˆÙ† Ù…Ø­Ø±Ú© Ø¨Ø³ÛŒØ§Ø±ÛŒ Ø§Ø² ØµÙ†Ø§ÛŒØ¹ Ø¨Ø§Ù„Ø§Ø¯Ø³ØªÛŒ Ùˆ Ù¾Ø§ÛŒÛŒÙ†â€ŒØ¯Ø³ØªÛŒ Ø§Ø³Øª. ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Ù‡ Ø¯Ø± Û±Û± Ø¯ÙˆØ±Ù‡ Ø¬Ø§ÛŒØ²Û€ Ù…Ù„ÛŒ ØªØ¹Ø§Ù„ÛŒ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ Ùˆ Û¶ Ø¯ÙˆØ±Ù‡ Ø¬Ø§ÛŒØ²Û€ Ø´Ø±Ú©Øª Ø¯Ø§Ù†Ø´ÛŒ Ø¯Ø± Ú©Ø´ÙˆØ± Ø±ØªØ¨Û€ Ù†Ø®Ø³Øª Ø±Ø§ Ø¨Ø¯Ø³Øª Ø¢ÙˆØ±Ø¯Ù‡â€ŒØ§Ø³Øª Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§ÛŒÙ† Ø´Ø±Ú©Øª Ø¯Ø± Ø³Ø§Ù„ Û±Û³Û¹Û± Ø¨Ø±Ø§ÛŒ Ù†Ø®Ø³ØªÛŒÙ†â€ŒØ¨Ø§Ø± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªÙ†Ù‡Ø§ Ø´Ø±Ú©Øª Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨Ø§ Ú©Ø³Ø¨ Ø§Ù…ØªÛŒØ§Ø² Û¶ÛµÛ´ ØªÙ†Ø¯ÛŒØ³ Ø²Ø±ÛŒÙ† Ø¬Ø§ÛŒØ²Û€ Ù…Ù„ÛŒ ØªØ¹Ø§Ù„ÛŒ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ Ø±Ø§ Ø§Ø² Ø¢Ù† Ø®ÙˆØ¯ Ú©Ù†Ø¯. Ø´Ø±Ú©Øª ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Û€ Ø§ØµÙÙ‡Ø§Ù† Ø¯Ø± Û²Û³ Ø¯ÛŒ Ù…Ø§Ù‡ Û±Û³Û·Û± Ø§Ø­Ø¯Ø§Ø« Ø´Ø¯ Ùˆ Ø§Ú©Ù†ÙˆÙ† Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ† ÙˆØ§Ø­Ø¯Ù‡Ø§ÛŒ ØµÙ†Ø¹ØªÛŒ Ùˆ Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ù…Ø¬ØªÙ…Ø¹ ØªÙˆÙ„ÛŒØ¯ ÙÙˆÙ„Ø§Ø¯ Ø¯Ø± Ø§ÛŒØ±Ø§Ù† Ø§Ø³Øª. Ø§ÛŒÙ† Ø´Ø±Ú©Øª Ø¯Ø± Ø²Ù…ÛŒÙ†ÛŒ Ø¨Ù‡ Ù…Ø³Ø§Ø­Øª Û³Ûµ Ú©ÛŒÙ„ÙˆÙ…ØªØ± Ù…Ø±Ø¨Ø¹ Ø¯Ø± Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ø´Ù‡Ø± Ù…Ø¨Ø§Ø±Ú©Ù‡ Ùˆ Ø¯Ø± Û·Ûµ Ú©ÛŒÙ„ÙˆÙ…ØªØ±ÛŒ Ø¬Ù†ÙˆØ¨ ØºØ±Ø¨ÛŒ Ø´Ù‡Ø± Ø§ØµÙÙ‡Ø§Ù† ÙˆØ§Ù‚Ø¹ Ø´Ø¯Ù‡â€ŒØ§Ø³Øª. Ù…ØµØ±Ù Ø¢Ø¨ Ø§ÛŒÙ† Ú©Ø§Ø±Ø®Ø§Ù†Ù‡ Ø¯Ø± Ú©Ù…ØªØ±ÛŒÙ† Ù…ÛŒØ²Ø§Ù† Ø®ÙˆØ¯ØŒ Û±Ù«ÛµÙª Ø§Ø² Ø¯Ø¨ÛŒ Ø²Ø§ÛŒÙ†Ø¯Ù‡â€ŒØ±ÙˆØ¯ Ø¨Ø±Ø§Ø¨Ø± Ø³Ø§Ù„Ø§Ù†Ù‡ Û²Û³ Ù…ÛŒÙ„ÛŒÙˆÙ† Ù…ØªØ± Ù…Ú©Ø¹Ø¨ Ø¯Ø± Ø³Ø§Ù„ Ø§Ø³Øª Ùˆ Ø®ÙˆØ¯ ÛŒÚ©ÛŒ Ø§Ø² Ø¹ÙˆØ§Ù…Ù„ Ú©Ù…â€ŒØ¢Ø¨ÛŒ Ø²Ø§ÛŒÙ†Ø¯Ù‡â€ŒØ±ÙˆØ¯ Ø´Ù†Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\nØ´Ø±Ú©Øª ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Ù‡ Ø¯Ø± Ú©Ø¬Ø§ ÙˆØ§Ù‚Ø¹ Ø´Ø¯Ù‡ Ø§Ø³Øª\nØ´Ø±Ú©Øª ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Û€ Ø§ØµÙÙ‡Ø§Ù†ØŒ Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ† ÙˆØ§Ø­Ø¯ ØµÙ†Ø¹ØªÛŒ Ø®ØµÙˆØµÛŒ Ø¯Ø± Ø§ÛŒØ±Ø§Ù† Ùˆ Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ† Ù…Ø¬ØªÙ…Ø¹ ØªÙˆÙ„ÛŒØ¯ ÙÙˆÙ„Ø§Ø¯ Ø¯Ø± Ú©Ø´ÙˆØ± Ø§ÛŒØ±Ø§Ù† Ø§Ø³ØªØŒ Ú©Ù‡ Ø¯Ø± Ø´Ø±Ù‚ Ø´Ù‡Ø± Ù…Ø¨Ø§Ø±Ú©Ù‡ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯. ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Ù‡ Ù‡Ù…â€ŒØ§Ú©Ù†ÙˆÙ† Ù…Ø­Ø±Ú© Ø¨Ø³ÛŒØ§Ø±ÛŒ Ø§Ø² ØµÙ†Ø§ÛŒØ¹ Ø¨Ø§Ù„Ø§Ø¯Ø³ØªÛŒ Ùˆ Ù¾Ø§ÛŒÛŒÙ†â€ŒØ¯Ø³ØªÛŒ Ø§Ø³Øª. ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Ù‡ Ø¯Ø± Û±Û± Ø¯ÙˆØ±Ù‡ Ø¬Ø§ÛŒØ²Û€ Ù…Ù„ÛŒ ØªØ¹Ø§Ù„ÛŒ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ Ùˆ Û¶ Ø¯ÙˆØ±Ù‡ Ø¬Ø§ÛŒØ²Û€ Ø´Ø±Ú©Øª Ø¯Ø§Ù†Ø´ÛŒ Ø¯Ø± Ú©Ø´ÙˆØ± Ø±ØªØ¨Û€ Ù†Ø®Ø³Øª Ø±Ø§ Ø¨Ø¯Ø³Øª Ø¢ÙˆØ±Ø¯Ù‡â€ŒØ§Ø³Øª Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§ÛŒÙ† Ø´Ø±Ú©Øª Ø¯Ø± Ø³Ø§Ù„ Û±Û³Û¹Û± Ø¨Ø±Ø§ÛŒ Ù†Ø®Ø³ØªÛŒÙ†â€ŒØ¨Ø§Ø± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªÙ†Ù‡Ø§ Ø´Ø±Ú©Øª Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨Ø§ Ú©Ø³Ø¨ Ø§Ù…ØªÛŒØ§Ø² Û¶ÛµÛ´ ØªÙ†Ø¯ÛŒØ³ Ø²Ø±ÛŒÙ† Ø¬Ø§ÛŒØ²Û€ Ù…Ù„ÛŒ ØªØ¹Ø§Ù„ÛŒ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ Ø±Ø§ Ø§Ø² Ø¢Ù† Ø®ÙˆØ¯ Ú©Ù†Ø¯. Ø´Ø±Ú©Øª ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Û€ Ø§ØµÙÙ‡Ø§Ù† Ø¯Ø± Û²Û³ Ø¯ÛŒ Ù…Ø§Ù‡ Û±Û³Û·Û± Ø§Ø­Ø¯Ø§Ø« Ø´Ø¯ Ùˆ Ø§Ú©Ù†ÙˆÙ† Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ† ÙˆØ§Ø­Ø¯Ù‡Ø§ÛŒ ØµÙ†Ø¹ØªÛŒ Ùˆ Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ù…Ø¬ØªÙ…Ø¹ ØªÙˆÙ„ÛŒØ¯ ÙÙˆÙ„Ø§Ø¯ Ø¯Ø± Ø§ÛŒØ±Ø§Ù† Ø§Ø³Øª. Ø§ÛŒÙ† Ø´Ø±Ú©Øª Ø¯Ø± Ø²Ù…ÛŒÙ†ÛŒ Ø¨Ù‡ Ù…Ø³Ø§Ø­Øª Û³Ûµ Ú©ÛŒÙ„ÙˆÙ…ØªØ± Ù…Ø±Ø¨Ø¹ Ø¯Ø± Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ø´Ù‡Ø± Ù…Ø¨Ø§Ø±Ú©Ù‡ Ùˆ Ø¯Ø± Û·Ûµ Ú©ÛŒÙ„ÙˆÙ…ØªØ±ÛŒ Ø¬Ù†ÙˆØ¨ ØºØ±Ø¨ÛŒ Ø´Ù‡Ø± Ø§ØµÙÙ‡Ø§Ù† ÙˆØ§Ù‚Ø¹ Ø´Ø¯Ù‡â€ŒØ§Ø³Øª. Ù…ØµØ±Ù Ø¢Ø¨ Ø§ÛŒÙ† Ú©Ø§Ø±Ø®Ø§Ù†Ù‡ Ø¯Ø± Ú©Ù…ØªØ±ÛŒÙ† Ù…ÛŒØ²Ø§Ù† Ø®ÙˆØ¯ØŒ Û±Ù«ÛµÙª Ø§Ø² Ø¯Ø¨ÛŒ Ø²Ø§ÛŒÙ†Ø¯Ù‡â€ŒØ±ÙˆØ¯ Ø¨Ø±Ø§Ø¨Ø± Ø³Ø§Ù„Ø§Ù†Ù‡ Û²Û³ Ù…ÛŒÙ„ÛŒÙˆÙ† Ù…ØªØ± Ù…Ú©Ø¹Ø¨ Ø¯Ø± Ø³Ø§Ù„ Ø§Ø³Øª Ùˆ Ø®ÙˆØ¯ ÛŒÚ©ÛŒ Ø§Ø² Ø¹ÙˆØ§Ù…Ù„ Ú©Ù…â€ŒØ¢Ø¨ÛŒ Ø²Ø§ÛŒÙ†Ø¯Ù‡â€ŒØ±ÙˆØ¯ Ø´Ù†Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\nÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Ù‡ Ú†Ù†Ø¯ Ø¨Ø§Ø± Ø¨Ø±Ù†Ø¯Ù‡ Ø¬Ø§ÛŒØ²Ù‡ Ø´Ø±Ú©Øª Ø¯Ø§Ù†Ø´ÛŒ Ø±Ø§ Ú©Ø³Ø¨ Ú©Ø±Ø¯Ù‡ Ø§Ø³ØªØŸ\nØ´Ø±Ú©Øª ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Û€ Ø§ØµÙÙ‡Ø§Ù†ØŒ Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ† ÙˆØ§Ø­Ø¯ ØµÙ†Ø¹ØªÛŒ Ø®ØµÙˆØµÛŒ Ø¯Ø± Ø§ÛŒØ±Ø§Ù† Ùˆ Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ† Ù…Ø¬ØªÙ…Ø¹ ØªÙˆÙ„ÛŒØ¯ ÙÙˆÙ„Ø§Ø¯ Ø¯Ø± Ú©Ø´ÙˆØ± Ø§ÛŒØ±Ø§Ù† Ø§Ø³ØªØŒ Ú©Ù‡ Ø¯Ø± Ø´Ø±Ù‚ Ø´Ù‡Ø± Ù…Ø¨Ø§Ø±Ú©Ù‡ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯. ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Ù‡ Ù‡Ù…â€ŒØ§Ú©Ù†ÙˆÙ† Ù…Ø­Ø±Ú© Ø¨Ø³ÛŒØ§Ø±ÛŒ Ø§Ø² ØµÙ†Ø§ÛŒØ¹ Ø¨Ø§Ù„Ø§Ø¯Ø³ØªÛŒ Ùˆ Ù¾Ø§ÛŒÛŒÙ†â€ŒØ¯Ø³ØªÛŒ Ø§Ø³Øª. ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Ù‡ Ø¯Ø± Û±Û± Ø¯ÙˆØ±Ù‡ Ø¬Ø§ÛŒØ²Û€ Ù…Ù„ÛŒ ØªØ¹Ø§Ù„ÛŒ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ Ùˆ Û¶ Ø¯ÙˆØ±Ù‡ Ø¬Ø§ÛŒØ²Û€ Ø´Ø±Ú©Øª Ø¯Ø§Ù†Ø´ÛŒ Ø¯Ø± Ú©Ø´ÙˆØ± Ø±ØªØ¨Û€ Ù†Ø®Ø³Øª Ø±Ø§ Ø¨Ø¯Ø³Øª Ø¢ÙˆØ±Ø¯Ù‡â€ŒØ§Ø³Øª Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§ÛŒÙ† Ø´Ø±Ú©Øª Ø¯Ø± Ø³Ø§Ù„ Û±Û³Û¹Û± Ø¨Ø±Ø§ÛŒ Ù†Ø®Ø³ØªÛŒÙ†â€ŒØ¨Ø§Ø± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ØªÙ†Ù‡Ø§ Ø´Ø±Ú©Øª Ø§ÛŒØ±Ø§Ù†ÛŒ Ø¨Ø§ Ú©Ø³Ø¨ Ø§Ù…ØªÛŒØ§Ø² Û¶ÛµÛ´ ØªÙ†Ø¯ÛŒØ³ Ø²Ø±ÛŒÙ† Ø¬Ø§ÛŒØ²Û€ Ù…Ù„ÛŒ ØªØ¹Ø§Ù„ÛŒ Ø³Ø§Ø²Ù…Ø§Ù†ÛŒ Ø±Ø§ Ø§Ø² Ø¢Ù† Ø®ÙˆØ¯ Ú©Ù†Ø¯. Ø´Ø±Ú©Øª ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Û€ Ø§ØµÙÙ‡Ø§Ù† Ø¯Ø± Û²Û³ Ø¯ÛŒ Ù…Ø§Ù‡ Û±Û³Û·Û± Ø§Ø­Ø¯Ø§Ø« Ø´Ø¯ Ùˆ Ø§Ú©Ù†ÙˆÙ† Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ† ÙˆØ§Ø­Ø¯Ù‡Ø§ÛŒ ØµÙ†Ø¹ØªÛŒ Ùˆ Ø¨Ø²Ø±Ú¯ØªØ±ÛŒÙ† Ù…Ø¬ØªÙ…Ø¹ ØªÙˆÙ„ÛŒØ¯ ÙÙˆÙ„Ø§Ø¯ Ø¯Ø± Ø§ÛŒØ±Ø§Ù† Ø§Ø³Øª. Ø§ÛŒÙ† Ø´Ø±Ú©Øª Ø¯Ø± Ø²Ù…ÛŒÙ†ÛŒ Ø¨Ù‡ Ù…Ø³Ø§Ø­Øª Û³Ûµ Ú©ÛŒÙ„ÙˆÙ…ØªØ± Ù…Ø±Ø¨Ø¹ Ø¯Ø± Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ø´Ù‡Ø± Ù…Ø¨Ø§Ø±Ú©Ù‡ Ùˆ Ø¯Ø± Û·Ûµ Ú©ÛŒÙ„ÙˆÙ…ØªØ±ÛŒ Ø¬Ù†ÙˆØ¨ ØºØ±Ø¨ÛŒ Ø´Ù‡Ø± Ø§ØµÙÙ‡Ø§Ù† ÙˆØ§Ù‚Ø¹ Ø´Ø¯Ù‡â€ŒØ§Ø³Øª. Ù…ØµØ±Ù Ø¢Ø¨ Ø§ÛŒÙ† Ú©Ø§Ø±Ø®Ø§Ù†Ù‡ Ø¯Ø± Ú©Ù…ØªØ±ÛŒÙ† Ù…ÛŒØ²Ø§Ù† Ø®ÙˆØ¯ØŒ Û±Ù«ÛµÙª Ø§Ø² Ø¯Ø¨ÛŒ Ø²Ø§ÛŒÙ†Ø¯Ù‡â€ŒØ±ÙˆØ¯ Ø¨Ø±Ø§Ø¨Ø± Ø³Ø§Ù„Ø§Ù†Ù‡ Û²Û³ Ù…ÛŒÙ„ÛŒÙˆÙ† Ù…ØªØ± Ù…Ú©Ø¹Ø¨ Ø¯Ø± Ø³Ø§Ù„ Ø§Ø³Øª Ùˆ Ø®ÙˆØ¯ ÛŒÚ©ÛŒ Ø§Ø² Ø¹ÙˆØ§Ù…Ù„ Ú©Ù…â€ŒØ¢Ø¨ÛŒ Ø²Ø§ÛŒÙ†Ø¯Ù‡â€ŒØ±ÙˆØ¯ Ø´Ù†Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\nØ´Ø±Ú©Øª ÙÙˆÙ„Ø§Ø¯ Ù…Ø¨Ø§Ø±Ú©Ù‡ Ø¯Ø± Ø³Ø§Ù„ Û±Û³Û¹Û± Ú†Ù‡ Ø¬Ø§ÛŒØ²Ù‡ Ø§ÛŒ Ø¨Ø±Ø¯ØŸ\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nTraining Hyperparameters\nNon-Default Hyperparameters\nper_device_train_batch_size: 24\nper_device_eval_batch_size: 24\ngradient_accumulation_steps: 2\ntorch_empty_cache_steps: 400\nweight_decay: 0.01\nlr_scheduler_type: cosine\nwarmup_ratio: 0.1\nseed: 2024\ndata_seed: 2024\nfp16: True\ngroup_by_length: True\nbatch_sampler: no_duplicates\nAll Hyperparameters\nClick to expand\noverwrite_output_dir: False\ndo_predict: False\neval_strategy: no\nprediction_loss_only: True\nper_device_train_batch_size: 24\nper_device_eval_batch_size: 24\nper_gpu_train_batch_size: None\nper_gpu_eval_batch_size: None\ngradient_accumulation_steps: 2\neval_accumulation_steps: None\ntorch_empty_cache_steps: 400\nlearning_rate: 5e-05\nweight_decay: 0.01\nadam_beta1: 0.9\nadam_beta2: 0.999\nadam_epsilon: 1e-08\nmax_grad_norm: 1\nnum_train_epochs: 3\nmax_steps: -1\nlr_scheduler_type: cosine\nlr_scheduler_kwargs: {}\nwarmup_ratio: 0.1\nwarmup_steps: 0\nlog_level: passive\nlog_level_replica: warning\nlog_on_each_node: True\nlogging_nan_inf_filter: True\nsave_safetensors: True\nsave_on_each_node: False\nsave_only_model: False\nrestore_callback_states_from_checkpoint: False\nno_cuda: False\nuse_cpu: False\nuse_mps_device: False\nseed: 2024\ndata_seed: 2024\njit_mode_eval: False\nuse_ipex: False\nbf16: False\nfp16: True\nfp16_opt_level: O1\nhalf_precision_backend: auto\nbf16_full_eval: False\nfp16_full_eval: False\ntf32: None\nlocal_rank: 0\nddp_backend: None\ntpu_num_cores: None\ntpu_metrics_debug: False\ndebug: []\ndataloader_drop_last: False\ndataloader_num_workers: 0\ndataloader_prefetch_factor: None\npast_index: -1\ndisable_tqdm: False\nremove_unused_columns: True\nlabel_names: None\nload_best_model_at_end: False\nignore_data_skip: False\nfsdp: []\nfsdp_min_num_params: 0\nfsdp_config: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}\nfsdp_transformer_layer_cls_to_wrap: None\naccelerator_config: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}\ndeepspeed: None\nlabel_smoothing_factor: 0.0\noptim: adamw_torch\noptim_args: None\nadafactor: False\ngroup_by_length: True\nlength_column_name: length\nddp_find_unused_parameters: None\nddp_bucket_cap_mb: None\nddp_broadcast_buffers: False\ndataloader_pin_memory: True\ndataloader_persistent_workers: False\nskip_memory_metrics: True\nuse_legacy_prediction_loop: False\npush_to_hub: False\nresume_from_checkpoint: None\nhub_model_id: None\nhub_strategy: every_save\nhub_private_repo: False\nhub_always_push: False\ngradient_checkpointing: False\ngradient_checkpointing_kwargs: None\ninclude_inputs_for_metrics: False\neval_do_concat_batches: True\nfp16_backend: auto\npush_to_hub_model_id: None\npush_to_hub_organization: None\nmp_parameters:\nauto_find_batch_size: False\nfull_determinism: False\ntorchdynamo: None\nray_scope: last\nddp_timeout: 1800\ntorch_compile: False\ntorch_compile_backend: None\ntorch_compile_mode: None\ndispatch_batches: None\nsplit_batches: None\ninclude_tokens_per_second: False\ninclude_num_input_tokens_seen: False\nneftune_noise_alpha: None\noptim_target_modules: None\nbatch_eval_metrics: False\neval_on_start: False\neval_use_gather_object: False\nbatch_sampler: no_duplicates\nmulti_dataset_batch_sampler: proportional\nTraining Logs\nClick to expand\nEpoch\nStep\nTraining Loss\n0.0101\n100\n1.2173\n0.0203\n200\n0.8005\n0.0304\n300\n0.6933\n0.0406\n400\n0.5501\n0.0507\n500\n0.5745\n0.0609\n600\n0.5772\n0.0710\n700\n0.5297\n0.0812\n800\n0.6225\n0.0913\n900\n0.5587\n0.1015\n1000\n0.5391\n0.1116\n1100\n0.5087\n0.1218\n1200\n0.5091\n0.1319\n1300\n0.5353\n0.1421\n1400\n0.4989\n0.1522\n1500\n0.5317\n0.1624\n1600\n0.519\n0.1725\n1700\n0.5118\n0.1827\n1800\n0.4773\n0.1928\n1900\n0.4411\n0.2030\n2000\n0.4618\n0.2131\n2100\n0.3866\n0.2233\n2200\n0.4524\n0.2334\n2300\n0.5271\n0.2436\n2400\n0.4519\n0.2537\n2500\n0.4865\n0.2639\n2600\n0.52\n0.2740\n2700\n0.54\n0.2841\n2800\n0.4525\n0.2943\n2900\n0.5002\n0.3044\n3000\n0.532\n0.3146\n3100\n0.4829\n0.3247\n3200\n0.4658\n0.3349\n3300\n0.5031\n0.3450\n3400\n0.4907\n0.3552\n3500\n0.5019\n0.3653\n3600\n0.4788\n0.3755\n3700\n0.4884\n0.3856\n3800\n0.4998\n0.3958\n3900\n0.4321\n0.4059\n4000\n0.4428\n0.4161\n4100\n0.4564\n0.4262\n4200\n0.4349\n0.4364\n4300\n0.4219\n0.4465\n4400\n0.4411\n0.4567\n4500\n0.4448\n0.4668\n4600\n0.4334\n0.4770\n4700\n0.4255\n0.4871\n4800\n0.4147\n0.4973\n4900\n0.4263\n0.5074\n5000\n0.4483\n0.5176\n5100\n0.4437\n0.5277\n5200\n0.4062\n0.5379\n5300\n0.3974\n0.5480\n5400\n0.3455\n0.5581\n5500\n0.3383\n0.5683\n5600\n0.4156\n0.5784\n5700\n0.4296\n0.5886\n5800\n0.4115\n0.5987\n5900\n0.3977\n0.6089\n6000\n0.3736\n0.6190\n6100\n0.4001\n0.6292\n6200\n0.3721\n0.6393\n6300\n0.4244\n0.6495\n6400\n0.3653\n0.6596\n6500\n0.394\n0.6698\n6600\n0.3749\n0.6799\n6700\n0.3964\n0.6901\n6800\n0.3958\n0.7002\n6900\n0.3585\n0.7104\n7000\n0.3609\n0.7205\n7100\n0.3645\n0.7307\n7200\n0.4257\n0.7408\n7300\n0.3894\n0.7510\n7400\n0.3714\n0.7611\n7500\n0.4011\n0.7713\n7600\n0.4147\n0.7814\n7700\n0.3923\n0.7916\n7800\n0.345\n0.8017\n7900\n0.387\n0.8119\n8000\n0.3609\n0.8220\n8100\n0.4609\n0.8321\n8200\n0.4027\n0.8423\n8300\n0.368\n0.8524\n8400\n0.3547\n0.8626\n8500\n0.3978\n0.8727\n8600\n0.3667\n0.8829\n8700\n0.3599\n0.8930\n8800\n0.3476\n0.9032\n8900\n0.3617\n0.9133\n9000\n0.4207\n0.9235\n9100\n0.4382\n0.9336\n9200\n0.377\n0.9438\n9300\n0.3602\n0.9539\n9400\n0.3025\n0.9641\n9500\n0.3186\n0.9742\n9600\n0.3121\n0.9844\n9700\n0.2976\n0.9945\n9800\n0.3133\n1.0047\n9900\n0.4134\n1.0148\n10000\n0.4225\n1.0250\n10100\n0.3739\n1.0351\n10200\n0.3789\n1.0453\n10300\n0.3096\n1.0554\n10400\n0.3306\n1.0656\n10500\n0.2934\n1.0757\n10600\n0.3379\n1.0859\n10700\n0.3441\n1.0960\n10800\n0.3407\n1.1061\n10900\n0.2935\n1.1163\n11000\n0.3357\n1.1264\n11100\n0.2743\n1.1366\n11200\n0.3177\n1.1467\n11300\n0.2951\n1.1569\n11400\n0.3293\n1.1670\n11500\n0.2638\n1.1772\n11600\n0.2723\n1.1873\n11700\n0.2616\n1.1975\n11800\n0.251\n1.2076\n11900\n0.1992\n1.2178\n12000\n0.213\n1.2279\n12100\n0.2288\n1.2381\n12200\n0.2777\n1.2482\n12300\n0.1971\n1.2584\n12400\n0.2549\n1.2685\n12500\n0.2604\n1.2787\n12600\n0.2657\n1.2888\n12700\n0.2064\n1.2990\n12800\n0.238\n1.3091\n12900\n0.2239\n1.3193\n13000\n0.2004\n1.3294\n13100\n0.2283\n1.3396\n13200\n0.1925\n1.3497\n13300\n0.2301\n1.3599\n13400\n0.2076\n1.3700\n13500\n0.2103\n1.3802\n13600\n0.1967\n1.3903\n13700\n0.2302\n1.4004\n13800\n0.1867\n1.4106\n13900\n0.1793\n1.4207\n14000\n0.1959\n1.4309\n14100\n0.1483\n1.4410\n14200\n0.1675\n1.4512\n14300\n0.1883\n1.4613\n14400\n0.1896\n1.4715\n14500\n0.1774\n1.4816\n14600\n0.1634\n1.4918\n14700\n0.1593\n1.5019\n14800\n0.1952\n1.5121\n14900\n0.1845\n1.5222\n15000\n0.1874\n1.5324\n15100\n0.1678\n1.5425\n15200\n0.1383\n1.5527\n15300\n0.1202\n1.5628\n15400\n0.1535\n1.5730\n15500\n0.1996\n1.5831\n15600\n0.1604\n1.5933\n15700\n0.1658\n1.6034\n15800\n0.1417\n1.6136\n15900\n0.1486\n1.6237\n16000\n0.1574\n1.6339\n16100\n0.1505\n1.6440\n16200\n0.1561\n1.6542\n16300\n0.1317\n1.6643\n16400\n0.1633\n1.6744\n16500\n0.1567\n1.6846\n16600\n0.1388\n1.6947\n16700\n0.1461\n1.7049\n16800\n0.142\n1.7150\n16900\n0.1229\n1.7252\n17000\n0.152\n1.7353\n17100\n0.1547\n1.7455\n17200\n0.1481\n1.7556\n17300\n0.1412\n1.7658\n17400\n0.1611\n1.7759\n17500\n0.1497\n1.7861\n17600\n0.1485\n1.7962\n17700\n0.1184\n1.8064\n17800\n0.1686\n1.8165\n17900\n0.1326\n1.8267\n18000\n0.1665\n1.8368\n18100\n0.1561\n1.8470\n18200\n0.1527\n1.8571\n18300\n0.1372\n1.8673\n18400\n0.1811\n1.8774\n18500\n0.12\n1.8876\n18600\n0.1366\n1.8977\n18700\n0.1432\n1.9079\n18800\n0.17\n1.9180\n18900\n0.1779\n1.9282\n19000\n0.1565\n1.9383\n19100\n0.1471\n1.9484\n19200\n0.1266\n1.9586\n19300\n0.1204\n1.9687\n19400\n0.0959\n1.9789\n19500\n0.1228\n1.9890\n19600\n0.1347\n1.9992\n19700\n0.0911\n2.0093\n19800\n0.2626\n2.0195\n19900\n0.1626\n2.0296\n20000\n0.1461\n2.0398\n20100\n0.1219\n2.0499\n20200\n0.1223\n2.0601\n20300\n0.1203\n2.0702\n20400\n0.1312\n2.0804\n20500\n0.1246\n2.0905\n20600\n0.1374\n2.1007\n20700\n0.1185\n2.1108\n20800\n0.1175\n2.1210\n20900\n0.1013\n2.1311\n21000\n0.1205\n2.1413\n21100\n0.1206\n2.1514\n21200\n0.1085\n2.1616\n21300\n0.1112\n2.1717\n21400\n0.1046\n2.1819\n21500\n0.0908\n2.1920\n21600\n0.0807\n2.2022\n21700\n0.0754\n2.2123\n21800\n0.0773\n2.2224\n21900\n0.0815\n2.2326\n22000\n0.1078\n2.2427\n22100\n0.0679\n2.2529\n22200\n0.0824\n2.2630\n22300\n0.0962\n2.2732\n22400\n0.1108\n2.2833\n22500\n0.0619\n2.2935\n22600\n0.0829\n2.3036\n22700\n0.0792\n2.3138\n22800\n0.0782\n2.3239\n22900\n0.0743\n2.3341\n23000\n0.0788\n2.3442\n23100\n0.0638\n2.3544\n23200\n0.0927\n2.3645\n23300\n0.0763\n2.3747\n23400\n0.0782\n2.3848\n23500\n0.0813\n2.3950\n23600\n0.0736\n2.4051\n23700\n0.0612\n2.4153\n23800\n0.0593\n2.4254\n23900\n0.0543\n2.4356\n24000\n0.046\n2.4457\n24100\n0.0472\n2.4559\n24200\n0.0648\n2.4660\n24300\n0.058\n2.4762\n24400\n0.0603\n2.4863\n24500\n0.0486\n2.4964\n24600\n0.0605\n2.5066\n24700\n0.0745\n2.5167\n24800\n0.0621\n2.5269\n24900\n0.0576\n2.5370\n25000\n0.0567\n2.5472\n25100\n0.0418\n2.5573\n25200\n0.0405\n2.5675\n25300\n0.0684\n2.5776\n25400\n0.0597\n2.5878\n25500\n0.0564\n2.5979\n25600\n0.0576\n2.6081\n25700\n0.0383\n2.6182\n25800\n0.0592\n2.6284\n25900\n0.0487\n2.6385\n26000\n0.0569\n2.6487\n26100\n0.0533\n2.6588\n26200\n0.0497\n2.6690\n26300\n0.0629\n2.6791\n26400\n0.0563\n2.6893\n26500\n0.0568\n2.6994\n26600\n0.045\n2.7096\n26700\n0.0562\n2.7197\n26800\n0.0571\n2.7299\n26900\n0.0599\n2.7400\n27000\n0.0596\n2.7502\n27100\n0.0552\n2.7603\n27200\n0.0545\n2.7704\n27300\n0.064\n2.7806\n27400\n0.0493\n2.7907\n27500\n0.0518\n2.8009\n27600\n0.0517\n2.8110\n27700\n0.0609\n2.8212\n27800\n0.0603\n2.8313\n27900\n0.0665\n2.8415\n28000\n0.062\n2.8516\n28100\n0.0581\n2.8618\n28200\n0.0822\n2.8719\n28300\n0.0666\n2.8821\n28400\n0.0545\n2.8922\n28500\n0.0671\n2.9024\n28600\n0.0444\n2.9125\n28700\n0.1006\n2.9227\n28800\n0.0825\n2.9328\n28900\n0.0649\n2.9430\n29000\n0.0684\n2.9531\n29100\n0.0444\n2.9633\n29200\n0.0492\n2.9734\n29300\n0.051\n2.9836\n29400\n0.0511\n2.9937\n29500\n0.0561\nFramework Versions\nPython: 3.12.2\nSentence Transformers: 3.0.1\nTransformers: 4.43.1\nPyTorch: 2.3.0\nAccelerate: 0.31.0\nDatasets: 2.19.2\nTokenizers: 0.19.1\nCitation\nBibTeX\nSentence Transformers and SoftmaxLoss\n@inproceedings{reimers-2019-sentence-bert,\ntitle = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\nauthor = \"Reimers, Nils and Gurevych, Iryna\",\nbooktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\nmonth = \"11\",\nyear = \"2019\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://arxiv.org/abs/1908.10084\",\n}\nContrastiveLoss\n@inproceedings{hadsell2006dimensionality,\nauthor={Hadsell, R. and Chopra, S. and LeCun, Y.},\nbooktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},\ntitle={Dimensionality Reduction by Learning an Invariant Mapping},\nyear={2006},\nvolume={2},\nnumber={},\npages={1735-1742},\ndoi={10.1109/CVPR.2006.100}\n}\nMultipleNegativesRankingLoss\n@misc{henderson2017efficient,\ntitle={Efficient Natural Language Response Suggestion for Smart Reply},\nauthor={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},\nyear={2017},\neprint={1705.00652},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "lmms-lab/LLaVA-Video-7B-Qwen2": "LLaVA-Video-7B-Qwen2\nTable of Contents\nModel Summary\nUse\nIntended use\nGeneration\nTraining\nModel\nHardware & Software\nCitation\nLLaVA-Video-7B-Qwen2\nTable of Contents\nModel Summary\nUse\nLimitations\nTraining\nLicense\nCitation\nModel Summary\nThe LLaVA-Video models are 7/72B parameter models trained on LLaVA-Video-178K and LLaVA-OneVision Dataset, based on Qwen2 language model with a context window of 32K tokens.\nThis model support at most 64 frames.\nProject Page: Project Page.\nPaper: For more details, please check our paper\nRepository: LLaVA-VL/LLaVA-NeXT\nPoint of Contact: Yuanhan Zhang\nLanguages: English, Chinese\nUse\nIntended use\nThe model was trained on LLaVA-Video-178K and LLaVA-OneVision Dataset, having the ability to interact with images, multi-image and videos, but specific to videos.\nFeel free to share your generations in the Community tab!\nGeneration\nWe provide the simple generation process for using our model. For more details, you could refer to Github.\n# pip install git+https://github.com/LLaVA-VL/LLaVA-NeXT.git\nfrom llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\nfrom llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom PIL import Image\nimport requests\nimport copy\nimport torch\nimport sys\nimport warnings\nfrom decord import VideoReader, cpu\nimport numpy as np\nwarnings.filterwarnings(\"ignore\")\ndef load_video(video_path, max_frames_num,fps=1,force_sample=False):\nif max_frames_num == 0:\nreturn np.zeros((1, 336, 336, 3))\nvr = VideoReader(video_path, ctx=cpu(0),num_threads=1)\ntotal_frame_num = len(vr)\nvideo_time = total_frame_num / vr.get_avg_fps()\nfps = round(vr.get_avg_fps()/fps)\nframe_idx = [i for i in range(0, len(vr), fps)]\nframe_time = [i/fps for i in frame_idx]\nif len(frame_idx) > max_frames_num or force_sample:\nsample_fps = max_frames_num\nuniform_sampled_frames = np.linspace(0, total_frame_num - 1, sample_fps, dtype=int)\nframe_idx = uniform_sampled_frames.tolist()\nframe_time = [i/vr.get_avg_fps() for i in frame_idx]\nframe_time = \",\".join([f\"{i:.2f}s\" for i in frame_time])\nspare_frames = vr.get_batch(frame_idx).asnumpy()\n# import pdb;pdb.set_trace()\nreturn spare_frames,frame_time,video_time\npretrained = \"lmms-lab/LLaVA-Video-7B-Qwen2\"\nmodel_name = \"llava_qwen\"\ndevice = \"cuda\"\ndevice_map = \"auto\"\ntokenizer, model, image_processor, max_length = load_pretrained_model(pretrained, None, model_name, torch_dtype=\"bfloat16\", device_map=device_map)  # Add any other thing you want to pass in llava_model_args\nmodel.eval()\nvideo_path = \"XXXX\"\nmax_frames_num = 64\nvideo,frame_time,video_time = load_video(video_path, max_frames_num, 1, force_sample=True)\nvideo = image_processor.preprocess(video, return_tensors=\"pt\")[\"pixel_values\"].cuda().half()\nvideo = [video]\nconv_template = \"qwen_1_5\"  # Make sure you use correct chat template for different models\ntime_instruciton = f\"The video lasts for {video_time:.2f} seconds, and {len(video[0])} frames are uniformly sampled from it. These frames are located at {frame_time}.Please answer the following questions related to this video.\"\nquestion = DEFAULT_IMAGE_TOKEN + f\"\\n{time_instruciton}\\nPlease describe this video in detail.\"\nconv = copy.deepcopy(conv_templates[conv_template])\nconv.append_message(conv.roles[0], question)\nconv.append_message(conv.roles[1], None)\nprompt_question = conv.get_prompt()\ninput_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\ncont = model.generate(\ninput_ids,\nimages=video,\nmodalities= [\"video\"],\ndo_sample=False,\ntemperature=0,\nmax_new_tokens=4096,\n)\ntext_outputs = tokenizer.batch_decode(cont, skip_special_tokens=True)[0].strip()\nprint(text_outputs)\nTraining\nModel\nArchitecture: SO400M + Qwen2\nInitialized Model: lmms-lab/llava-onevision-qwen2-7b-si\nData: A mixture of 1.6M single-image/multi-image/video data, 1 epoch, full model\nPrecision: bfloat16\nHardware & Software\nGPUs: 256 * Nvidia Tesla A100 (for whole model series training)\nOrchestration: Huggingface Trainer\nNeural networks: PyTorch\nCitation\n@misc{zhang2024videoinstructiontuningsynthetic,\ntitle={Video Instruction Tuning With Synthetic Data},\nauthor={Yuanhan Zhang and Jinming Wu and Wei Li and Bo Li and Zejun Ma and Ziwei Liu and Chunyuan Li},\nyear={2024},\neprint={2410.02713},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2410.02713},\n}",
    "ldzhangyx/instruct-MusicGen": "No model card",
    "onnx-community/maskformer-swin-tiny-ade": "Usage (Transformers.js)\nhttps://huggingface.co/facebook/maskformer-swin-tiny-ade with ONNX weights to be compatible with Transformers.js.\nUsage (Transformers.js)\nIf you haven't already, you can install the Transformers.js JavaScript library from NPM using:\nnpm i @huggingface/transformers\nExample: Scene segmentation with onnx-community/maskformer-swin-tiny-ade.\nimport { pipeline } from '@huggingface/transformers';\n// Create an image segmentation pipeline\nconst segmenter = await pipeline('image-segmentation', 'onnx-community/maskformer-swin-tiny-ade');\n// Segment an image\nconst url = 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg';\nconst output = await segmenter(url);\nconsole.log(output)\n// [\n//   {\n//     score: 0.9240802526473999,\n//     label: 'plant',\n//     mask: RawImage { ... }\n//   },\n//   {\n//     score: 0.967036783695221,\n//     label: 'house',\n//     mask: RawImage { ... }\n//   },\n//   ...\n//   }\n// ]\nYou can visualize the outputs with:\nfor (let i = 0; i < output.length; ++i) {\nconst { mask, label } = output[i];\nmask.save(`${label}-${i}.png`);\n}\nNote: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using ðŸ¤— Optimum and structuring your repo like this one (with ONNX weights located in a subfolder named onnx).",
    "Zuntan/iNiverseFluxV11-8step": "ãƒ¬ã‚·ãƒ”\nEasyForge ã® flux_tool ã§ã€iNiverse Mix Flux v1.1 ã‚’ Hyper Flux ã§ 8steps ç”Ÿæˆã«ã—ã¦ã€GGUF ã¨ nf4 ã«å¤‰æ›ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚Geforce RTX 3060 12GB ã§ã€é€Ÿåº¦é‡è¦–ã® nf4 ãªã‚‰ 30ç§’ã€é«˜å“è³ªã® Q8_0 ãªã‚‰ 45ç§’ã§ç”Ÿæˆã§ãã¾ã™ã€‚Flux Dev ç”¨ ã® LoRA ã‚‚ä½¿ãˆã¾ã™ã€‚\nãƒ¬ã‚·ãƒ”\n*.bat ã¯ EasyForge ã® flux_tool/ ã«ã‚ã‚Šã¾ã™ã€‚\nExtractLora-FluxD-fp16.bat ã‚’å®Ÿè¡Œã—ã¦ã€iNiverse ã‹ã‚‰ LoRA ã‚’ Dim 128 ã§æŠ½å‡º\nModelMergeLora.bat ã‚’å®Ÿè¡Œã—ã¦ã€flux1-dev-hyper8.safetensors ã«æŠ½å‡ºã—ãŸ LoRA ã‚’é‡ã¿ 1.0 ã§ãƒžãƒ¼ã‚¸\nConvertGguf.bat ã‚’å®Ÿè¡Œã—ã¦ã€ãƒžãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã® Q8_0, Q6_K, Q4_KS ã«å¤‰æ›\nForge ã® Checkpoint Merger ã§ nf4 ã¨ nf4-AIO(All-in-One, t5xxl_fp16, ae, clip_l è¾¼ã¿) ã«å¤‰æ›\niNiverseV11x05-8step*.* ã¯ 2. ã® LoRA é‡ã¿ã‚’ 0.5 ã«ã—ãŸã‚‚ã®ã§ã™ã€‚\niNiverseV11x05-FCAnimeV33x05-8step*.* ã¯ã€æ›´ã« Flat Color Anime v3.3 ã‚’é‡ã¿ 0.5 ã§ãƒžãƒ¼ã‚¸ã—ãŸã‚‚ã®ã§ã™ã€‚\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã€Œfca_style animeã€ã‚’è¿½åŠ ã—ã¦ã”åˆ©ç”¨ãã ã•ã„ã€‚",
    "omogr/xtts-ru-ipa": "license: cc-by-nc-sa-4.0\nXTTS model trained on IPA transcription for the Russian language\nColab\nlicense: cc-by-nc-sa-4.0",
    "kitsumed/yolov8m_seg-speech-bubble": "Informations\nModels\nPreviews\nInformations\nWith this image segmentation model, you can easily detect speech bubbles in comics or manga and obtain a mask to perform actions on them.\nNote that the mask segmentation is often wavy at the edges of the detected speech bubbles.\nModels\nThis repo contains the original .pt model and a .onnx model expored with the dynamic argument set on true.\nTry out my model with your own pictures now by using my huggingface space here.\nPreviews\nShow\nComics\nManga",
    "Plachta/Seed-VC": "README.md exists but content is empty.",
    "ljnlonoljpiljm/florence-2-large-nsfw-pretrain": "Model Card for Model ID\nModel Details\nModel Description\nModel Sources [optional]\nUses\nDirect Use\nDownstream Use [optional]\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nModel Examination [optional]\nEnvironmental Impact\nTechnical Specifications [optional]\nModel Architecture and Objective\nCompute Infrastructure\nCitation [optional]\nGlossary [optional]\nMore Information [optional]\nModel Card Authors [optional]\nModel Card Contact\nModel Card for Model ID\nModel Details\nModel Description\nThis is the model card of a ðŸ¤— transformers model that has been pushed on the Hub. This model card has been automatically generated.\nDeveloped by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nModel type: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\nFinetuned from model [optional]: [More Information Needed]\nModel Sources [optional]\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemo [optional]: [More Information Needed]\nUses\nDirect Use\n[More Information Needed]\nDownstream Use [optional]\n[More Information Needed]\nOut-of-Scope Use\n[More Information Needed]\nBias, Risks, and Limitations\n[More Information Needed]\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\nHow to Get Started with the Model\nUse the code below to get started with the model.\n[More Information Needed]\nTraining Details\nTraining Data\n[More Information Needed]\nTraining Procedure\nPreprocessing [optional]\n[More Information Needed]\nTraining Hyperparameters\nTraining regime: [More Information Needed]\nSpeeds, Sizes, Times [optional]\n[More Information Needed]\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\n[More Information Needed]\nFactors\n[More Information Needed]\nMetrics\n[More Information Needed]\nResults\n[More Information Needed]\nSummary\nModel Examination [optional]\n[More Information Needed]\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: [More Information Needed]\nHours used: [More Information Needed]\nCloud Provider: [More Information Needed]\nCompute Region: [More Information Needed]\nCarbon Emitted: [More Information Needed]\nTechnical Specifications [optional]\nModel Architecture and Objective\n[More Information Needed]\nCompute Infrastructure\n[More Information Needed]\nHardware\n[More Information Needed]\nSoftware\n[More Information Needed]\nCitation [optional]\nBibTeX:\n[More Information Needed]\nAPA:\n[More Information Needed]\nGlossary [optional]\n[More Information Needed]\nMore Information [optional]\n[More Information Needed]\nModel Card Authors [optional]\n[More Information Needed]\nModel Card Contact\n[More Information Needed]",
    "yangtao9009/PASD-SDXL": "README.md exists but content is empty.",
    "MaziyarPanahi/Yi-Coder-9B-Chat-GGUF": "MaziyarPanahi/Yi-Coder-9B-Chat-GGUF\nDescription\nAbout GGUF\nSpecial thanks\nMaziyarPanahi/Yi-Coder-9B-Chat-GGUF\nModel creator: 01-ai\nOriginal model: 01-ai/Yi-Coder-9B-Chat\nDescription\nMaziyarPanahi/Yi-Coder-9B-Chat-GGUF contains GGUF format model files for 01-ai/Yi-Coder-9B-Chat.\nAbout GGUF\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\nHere is an incomplete list of clients and libraries that are known to support GGUF:\nllama.cpp. The source project for GGUF. Offers a CLI and a server option.\nllama-cpp-python, a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\nLM Studio, an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\ntext-generation-webui, the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\nKoboldCpp, a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\nGPT4All, a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\nLoLLMS Web UI, a great web UI with many interesting and unique features, including a full model library for easy model selection.\nFaraday.dev, an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\ncandle, a Rust ML framework with a focus on performance, including GPU support, and ease of use.\nctransformers, a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\nSpecial thanks\nðŸ™ Special thanks to Georgi Gerganov and the whole team working on llama.cpp for making all of this possible.",
    "us4/fin-llama3.1-8b": "Model Card for Fin-LLaMA 3.1 8B\nModel Details\nModel Description\nFiles and Formats\nGGUF Formats and Usage\nModel Sources\nUses\nDirect Use\nDownstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nModel Examination\nEnvironmental Impact\nTechnical Specifications\nModel Architecture and Objective\nCompute Infrastructure\nCitation\nMore Information\nModel Card Contact\nModel Card for Fin-LLaMA 3.1 8B\nThis is the model card for Fin-LLaMA 3.1 8B, a fine-tuned version of LLaMA 3.1 trained specifically on financial news data. The model is built to generate coherent and relevant financial, economic, and business text responses. It also includes multiple quantized GGUF model formats for resource-efficient deployment.\nModel Details\nModel Description\nThe Fin-LLaMA 3.1 8B model was fine-tuned using the Unsloth library, employing LoRA adapters for efficient training, and is available in various quantized GGUF formats. The model is instruction-tuned to generate text in response to finance-related queries.\nDeveloped by: us4\nModel type: Transformer (LLaMA 3.1 architecture, 8B parameters)\nLanguages: English\nLicense: [More Information Needed]\nFine-tuned from model: LLaMA 3.1 8B\nFiles and Formats\nThe repository contains multiple files, including safetensors and GGUF formats for different quantization levels. Below is the list of key files and their details:\nadapter_config.json (778 Bytes): Configuration for the adapter model.\nadapter_model.safetensors (5.54 GB): Adapter model in safetensors format.\nconfig.json (978 Bytes): Model configuration file.\ngeneration_config.json (234 Bytes): Generation configuration file for text generation.\nmodel-00001-of-00004.safetensors (4.98 GB): Part 1 of the model in safetensors format.\nmodel-00002-of-00004.safetensors (5.00 GB): Part 2 of the model in safetensors format.\nmodel-00003-of-00004.safetensors (4.92 GB): Part 3 of the model in safetensors format.\nmodel-00004-of-00004.safetensors (1.17 GB): Part 4 of the model in safetensors format.\nmodel-q4_0.gguf (4.66 GB): Quantized GGUF format (Q4_0).\nmodel-q4_k_m.gguf (4.92 GB): Quantized GGUF format (Q4_K_M).\nmodel-q5_k_m.gguf (5.73 GB): Quantized GGUF format (Q5_K_M).\nmodel-q8_0.gguf (8.54 GB): Quantized GGUF format (Q8_0).\nmodel.safetensors.index.json (24 KB): Index file for the safetensors model.\nspecial_tokens_map.json (454 Bytes): Special tokens mapping file.\ntokenizer.json (9.09 MB): Tokenizer configuration for the model.\ntokenizer_config.json (55.4 KB): Additional tokenizer settings.\ntraining_args.bin (5.56 KB): Training arguments used for fine-tuning.\nGGUF Formats and Usage\nThe GGUF formats are optimized for memory-efficient inference, especially for edge devices or deployment in low-resource environments. Hereâ€™s a breakdown of the quantized GGUF formats available:\nQ4_0: 4-bit quantized model for high memory efficiency with some loss in precision.\nQ4_K_M: 4-bit quantized with optimized configurations for maintaining precision.\nQ5_K_M: 5-bit quantized model balancing memory efficiency and accuracy.\nQ8_0: 8-bit quantized model for higher precision with a larger memory footprint.\nGGUF files available in the repository:\nmodel-q4_0.gguf (4.66 GB)\nmodel-q4_k_m.gguf (4.92 GB)\nmodel-q5_k_m.gguf (5.73 GB)\nmodel-q8_0.gguf (8.54 GB)\nTo load and use these GGUF models for inference:\nfrom unsloth import FastLanguageModel\n#\nmodel, tokenizer = FastLanguageModel.from_pretrained(\nmodel_name=\"us4/fin-llama3.1-8b\",\nmax_seq_length=2048,\nload_in_4bit=True,  # Set to False for Q8_0 format\nquantization_method=\"q4_k_m\"  # Change to the required format (e.g., \"q5_k_m\" or \"q8_0\")\n)\nModel Sources\nRepository: Fin-LLaMA 3.1 8B on Hugging Face\nPaper: LLaMA: Open and Efficient Foundation Language Models\nUses\nThe Fin-LLaMA 3.1 8B model is designed for generating business, financial, and economic-related text.\nDirect Use\nThe model can be directly used for text generation tasks, such as generating financial news summaries, analysis, or responses to finance-related prompts.\nDownstream Use\nThe model can be further fine-tuned for specific financial tasks, such as question-answering systems, summarization of financial reports, or automation of business processes.\nOut-of-Scope Use\nThe model is not suited for use in domains outside of finance, such as medical or legal text generation, nor should it be used for tasks that require deep financial forecasting or critical decision-making without human oversight.\nBias, Risks, and Limitations\nThe model may inherit biases from the financial news data it was trained on. Since financial reporting can be region-specific and company-biased, users should exercise caution when applying the model in various international contexts.\nRecommendations\nUsers should carefully evaluate the generated text in critical business or financial settings. Ensure the generated content aligns with local regulations and company policies.\nTraining Details\nTraining Data\nThe model was fine-tuned on a dataset of financial news articles, consisting of titles and content from various financial media sources. The dataset has been pre-processed to remove extraneous information and ensure consistency across financial terms.\nTraining Procedure\nPreprocessing\nThe training data was tokenized using the LLaMA tokenizer, with prompts formatted to include both the title and content of financial news articles.\nTraining Hyperparameters\nTraining regime: Mixed precision (FP16), gradient accumulation steps: 8, max steps: 500.\nLearning Rate: 5e-5 for fine-tuning, 1e-5 for embeddings.\nBatch size: 8 per device.\nSpeeds, Sizes, Times\nThe model training took place over approximately 500 steps on an A100 GPU. Checkpoint files range from 4.98 GB to 8.54 GB depending on quantization.\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\nThe model was tested on unseen financial news articles from the same source domains as the training set.\nFactors\nEvaluation focused on the modelâ€™s ability to generate coherent financial summaries and responses.\nMetrics\nCommon text-generation metrics such as perplexity, accuracy in summarization, and human-in-the-loop evaluations were used.\nResults\nThe model demonstrated strong performance in generating high-quality financial text. It maintained coherence over long sequences and accurately represented financial data from the prompt.\nModel Examination\nNo interpretability techniques have yet been applied to this model, but explainability is under consideration for future versions.\nEnvironmental Impact\nTraining carbon emissions can be estimated using the Machine Learning Impact calculator.\nHardware Type: A100 GPU\nHours used: Approximately 72 hours for fine-tuning\nCloud Provider: AWS\nCompute Region: US-East\nCarbon Emitted: Estimated at 43 kg of CO2eq\nTechnical Specifications\nModel Architecture and Objective\nThe Fin-LLaMA 3.1 8B model is based on the LLaMA 3.1 architecture and uses LoRA adapters to efficiently fine-tune the model on financial data.\nCompute Infrastructure\nThe model was trained on A100 GPUs using PyTorch and the Hugging Face ðŸ¤— Transformers library.\nHardware\nGPU: A100 (80GB)\nStorage Requirements: Around 20GB for the fine-tuned checkpoints, depending on quantization format.\nSoftware\nLibrary: Hugging Face Transformers, Unsloth, PyTorch, PEFT\nVersion: Unsloth v1.0, PyTorch 2.0, Hugging Face Transformers 4.30.0\nCitation\nIf you use this model in your research or applications, please consider citing:\nBibTeX:\n@article{touvron2023llama,\ntitle={LLaMA: Open and Efficient Foundation Language Models},\nauthor={Touvron, Hugo and others},\njournal={arXiv preprint arXiv:2302.13971},\nyear={2023}\n}\n@misc{us4_fin_llama3_1,\ntitle={Fin-LLaMA 3.1 8B - Fine-tuned on Financial News},\nauthor={us4},\nyear={2024},\nhowpublished={\\url{https://huggingface.co/us4/fin-llama3.1-8b}},\n}\nMore Information\nFor any additional information, please refer to the repository or contact the authors via the Hugging Face Hub.\nModel Card Contact\n[More Information Needed]",
    "art0123/ComfyUI_art": "ComfyUI_art_v5 (Ð¡Ð±Ð¾Ñ€ÐºÐ°)\nâœ¨ Ð˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ:\nComfyUI Ð¸ Ð²ÑÐµ ÑƒÐ·Ð»Ñ‹ Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ½Ñ‹ Ðº Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¼ Ð²ÐµÑ€ÑÐ¸ÑÐ¼ Ð¿Ð¾ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸ÑŽ Ð½Ð° 28.08.2025.\nÐ’Ð¾Ð·Ð²Ñ€Ð°Ñ‚ Ð½Ð° python 3.11.9 Ð´Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐµÐ¹ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸ ÑÐ¾ Ð¼Ð½Ð¾Ð³Ð¸Ð¼Ð¸ ÑƒÐ·Ð»Ð°Ð¼Ð¸.\nÐ’ ÑÐ±Ð¾Ñ€ÐºÑƒ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹: Pytorch 2.8.0 + cuda128, Xformers 0.0.32.post2, Insightface, Onnxruntime, Facexlib, Dlib Ð¸ Bitsandbytes.\nÐ¢Ð°ÐºÐ¶Ðµ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð²ÑÐµ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð ÐµÐ°ÐºÑ‚Ð¾Ñ€Ð¾Ð¼.\nÐ Ñ‚Ð°ÐºÐ¶Ðµ nunchaku-1.0.0.dev20250823.\nÐ”Ð»Ñ Ð²Ð°ÑˆÐµÐ³Ð¾ ÑƒÐ´Ð¾Ð±ÑÑ‚Ð²Ð° Ð² ÑÐ±Ð¾Ñ€ÐºÑƒ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð½Ð°Ð±Ð¾Ñ€ ÑƒÐ·Ð»Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ñ€Ð°ÑÑˆÐ¸Ñ€ÑÐµÑ‚ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ ComfyUI: ComfyUI-Manager, Comfyui-Crystools, Rgthree-Comfy, Comfyui-Reactor-node, Comfy-WaveSpeed, ComfyUI-GGUF, ComfyUI-nunchaku, Res4lyf.\nâœ¨ÐšÐ°Ðº ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ?\nÐŸÑ€Ð¾ÑÑ‚Ð¾ Ñ€Ð°ÑÐ¿Ð°ÐºÑƒÐ¹Ñ‚Ðµ Ð°Ñ€Ñ…Ð¸Ð².\nÐ”Ð»Ñ Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ run_nvidia_gpu.bat.\nâœ¨ÐšÐ°Ðº Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ?\nÐ—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ Ð±Ð°Ñ‚Ð½Ð¸Ðº ComfyUI_update_ru.bat, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ ComfyUI Ð´Ð¾ Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð²ÐµÑ€ÑÐ¸Ð¸. Ð•ÑÐ»Ð¸ Ñƒ Ð²Ð°Ñ Ð²Ð¾Ð·Ð½Ð¸ÐºÐ½ÑƒÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÐµÐ¼ Ñ€ÑƒÑÑÐºÐ¾Ð³Ð¾ ÑˆÑ€Ð¸Ñ„Ñ‚Ð°, Ñ‚Ð¾ Ð´Ð»Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð²Ð¾ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚ÐµÑÑŒ Ñ„Ð°Ð¹Ð»Ð¾Ð¼ ComfyUI_update_eng.bat.\nÐ’ Ð¿Ð°Ð¿ÐºÐµ Install Ð½Ð°Ð¿Ð¸ÑÐ°Ð» Ð¸ Ð¿Ð¾Ð¼ÐµÑÑ‚Ð¸Ð» Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ Ð±Ð°Ñ‚Ð½Ð¸ÐºÐ¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¿Ð¾Ð¼Ð¾Ð³ÑƒÑ‚ Ð²Ð°Ð¼ ÐµÑÐ»Ð¸ Ñƒ Ð²Ð°Ñ Ñ‡Ñ‚Ð¾ Ñ‚Ð¾ ÑÐ»Ð¾Ð¼Ð°ÐµÑ‚ÑÑ Ð¸Ð»Ð¸ Ð²Ñ‹ Ð·Ð°Ñ…Ð¾Ñ‚Ð¸Ñ‚Ðµ Ð¾Ñ‚ÐºÐ°Ñ‚Ð¸Ñ‚ÑŒ Ð²ÐµÑ€ÑÐ¸Ð¸.\n//.........................................................................................\nComfyUI_art_v4 (Ð¡Ð±Ð¾Ñ€ÐºÐ°)\nÐ˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ:\nComfyUI Ð¸ Ð²ÑÐµ ÑƒÐ·Ð»Ñ‹ Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ½Ñ‹ Ðº Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¼ Ð²ÐµÑ€ÑÐ¸ÑÐ¼ Ð¿Ð¾ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸ÑŽ Ð½Ð° 02.06.2025.\nÐ’Ð¾Ð·Ð²Ñ€Ð°Ñ‚ Ð½Ð° python 3.11.9 Ð´Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐµÐ¹ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸ ÑÐ¾ Ð¼Ð½Ð¾Ð³Ð¸Ð¼Ð¸ ÑƒÐ·Ð»Ð°Ð¼Ð¸.\nÐ’ ÑÐ±Ð¾Ñ€ÐºÑƒ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹: Pytorch 2.7.0 + cuda128. Xformers 0.0.31.dev1030. Insightface, Onnxruntime, Facexlib, Dlib Ð¸ Bitsandbytes. Ð¢Ð°ÐºÐ¶Ðµ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð²ÑÐµ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð ÐµÐ°ÐºÑ‚Ð¾Ñ€Ð¾Ð¼.\nÐ”Ð»Ñ Ð²Ð°ÑˆÐµÐ³Ð¾ ÑƒÐ´Ð¾Ð±ÑÑ‚Ð²Ð° Ð² ÑÐ±Ð¾Ñ€ÐºÑƒ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð½Ð°Ð±Ð¾Ñ€ ÑƒÐ·Ð»Ð¾Ð², ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ñ€Ð°ÑÑˆÐ¸Ñ€ÑÐµÑ‚ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ ComfyUI: ComfyUI-Manager, Comfyui-Crystools, Rgthree-Comfy, Comfyui-Reactor-node, Comfy-WaveSpeed.\nÐšÐ°Ðº ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ?\nÐŸÑ€Ð¾ÑÑ‚Ð¾ Ñ€Ð°ÑÐ¿Ð°ÐºÑƒÐ¹Ñ‚Ðµ Ð°Ñ€Ñ…Ð¸Ð².\nÐ”Ð»Ñ Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ run_nvidia_gpu.bat.\nÐšÐ°Ðº Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ?\nÐ—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ Ð±Ð°Ñ‚Ð½Ð¸Ðº ComfyUI_update_ru.bat, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ ComfyUI Ð´Ð¾ Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð²ÐµÑ€ÑÐ¸Ð¸.\nÐ•ÑÐ»Ð¸ Ñƒ Ð²Ð°Ñ Ð²Ð¾Ð·Ð½Ð¸ÐºÐ½ÑƒÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÐµÐ¼ Ñ€ÑƒÑÑÐºÐ¾Ð³Ð¾ ÑˆÑ€Ð¸Ñ„Ñ‚Ð°, Ñ‚Ð¾ Ð´Ð»Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð²Ð¾ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚ÐµÑÑŒ Ñ„Ð°Ð¹Ð»Ð¾Ð¼ ComfyUI_update_eng.bat.\n//.........................................................................................\nComfyUI_art_v3 (Ð¡Ð±Ð¾Ñ€ÐºÐ°) - ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ! ðŸ”§\nâœ¨Ð˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ:\nComfyUI Ð¸ Ð²ÑÐµ ÑƒÐ·Ð»Ñ‹ Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ½Ñ‹ Ðº Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¼ Ð²ÐµÑ€ÑÐ¸ÑÐ¼ Ð¿Ð¾ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸ÑŽ Ð½Ð° 05.05.2025.\nÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½ Pytorch Ð´Ð¾ Ð²ÐµÑ€ÑÐ¸Ð¸ 2.7.0 + cuda128.\nÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½ Xformers Ð´Ð¾ Ð²ÐµÑ€ÑÐ¸Ð¸ 0.0.31.dev1030.\nâœ¨ Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð½Ð¾Ð²Ñ‹Ðµ ÑƒÐ·Ð»Ñ‹. ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº...\nAGSoft, cg-use-everywhere, ComfyUI_AdvancedRefluxControl, ComfyUI_bnb_nf4_fp4_Loaders, ComfyUI_Comfyroll_CustomNodes, comfyui_controlnet_aux, comfyui_custom_nodes_alekpet, comfyui_essentials, comfyui_faceanalysis, comfyui_instantid, comfyui_ipadapter_plus, comfyui_layerstyle, ComfyUI_LayerStyle_Advance, comfyui_patches_ll, comfyui_pulid_flux_ll, comfyui_segment_anything, comfyui_ttp_toolset, comfyui_ultimatesdupscale, comfyui_zenid, comfyui-art-venture, ComfyUI-AutoCropFaces, ComfyUI-Crystools, comfyui-custom-scripts, comfyui-detail-daemon, comfyui-enricos-nodes, comfyui-florence2, ComfyUI-Fluxtapoz, ComfyUI-FramePackWrapper, ComfyUI-GGUF, comfyui-ic-light, comfyui-impact-pack, comfyui-impact-subpack, comfyui-inpaint-cropandstitch, comfyui-inpaint-nodes, comfyui-inspire-pack, comfyui-kjnodes, comfyui-manager, comfyui-mxtoolkit, ComfyUI-PuLID-Flux-Enhanced, comfyui-reactor-node, comfyui-rmbg, ComfyUI-segment-anything-2, Comfyui-StableSR, comfyui-supir, ComfyUI-TiledDiffusion, comfyui-videohelpersuite, comfyui-wd14-tagger, efficiency-nodes-comfyui, facerestore_cf, pulid_comfyui, rgthree-comfy, teacache, was-node-suite-comfyui, wavespeed\n//.........................................................................................\nComfyUI_art_v2 (Ð¡Ð±Ð¾Ñ€ÐºÐ°)\nÐ§Ñ‚Ð¾ Ð½Ð¾Ð²Ð¾Ð³Ð¾:\nComfyUI Ð¸ Ð²ÑÐµ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ðµ ÑƒÐ·Ð»Ñ‹ Ð¿Ñ€Ð¸Ð²ÐµÐ´ÐµÐ½Ñ‹ Ðº Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¼ Ð²ÐµÑ€ÑÐ¸ÑÐ¼ Ð¿Ð¾ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸ÑŽ Ð½Ð° 08.02.2025.\nÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½ Pytorch Ð´Ð¾ Ð²ÐµÑ€ÑÐ¸Ð¸ 2.6.0 + cuda126\nÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½ Xformers Ð´Ð¾ Ð²ÐµÑ€ÑÐ¸Ð¸ 0.0.29.post2\nÐ”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð½Ð¾Ð²Ñ‹Ðµ ÑƒÐ·Ð»Ñ‹:\nComfy-WaveSpeed, ComfyUI-TeaCache, ComfyUI_Patches_ll, ComfyUI-SUPIR, comfyui_ttp_toolset, comfyui-detail-daemon, comfyui_zenid\n//.........................................................................................\nComfyUI_art_v1 (Ð¡Ð±Ð¾Ñ€ÐºÐ°)\nÐŸÑ€Ð¸Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽ Ð²ÑÐµÑ…, ÐºÑ‚Ð¾ Ð»ÑŽÐ±Ð¸Ñ‚ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÑÐ¼Ð¸ Ð¸ ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð°Ñ€Ñ‚Ñ‹! Ð¡ÐµÐ³Ð¾Ð´Ð½Ñ Ñ Ñ…Ð¾Ñ‡Ñƒ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ð²Ð°Ð¼ ÑÐ²Ð¾ÑŽ ÑÐ±Ð¾Ñ€ÐºÑƒ ComfyUI, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ ÑÑ‚Ð°Ð½ÐµÑ‚ Ð²Ð°ÑˆÐ¸Ð¼ Ð½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ð¼ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸ÐºÐ¾Ð¼ Ð² Ð¼Ð¸Ñ€Ðµ Ñ†Ð¸Ñ„Ñ€Ð¾Ð²Ð¾Ð³Ð¾ Ñ‚Ð²Ð¾Ñ€Ñ‡ÐµÑÑ‚Ð²Ð°.\nÐ§Ñ‚Ð¾ Ð²Ð½ÑƒÑ‚Ñ€Ð¸?\nÐ’ ÑÐ±Ð¾Ñ€ÐºÑƒ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ñ‹Ðµ Ð²ÐµÑ€ÑÐ¸Ð¸ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²: Pytorch, Xformers, Insightface, Onnxruntime, Facexlib, Dlib Ð¸ Bitsandbytes. Ð¢Ð°ÐºÐ¶Ðµ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð²ÑÐµ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð ÐµÐ°ÐºÑ‚Ð¾Ñ€Ð¾Ð¼.\nÐ”Ð»Ñ Ð²Ð°ÑˆÐµÐ³Ð¾ ÑƒÐ´Ð¾Ð±ÑÑ‚Ð²Ð° Ð² ÑÐ±Ð¾Ñ€ÐºÑƒ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ñ‹ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ðµ Ð¿Ð°ÐºÐµÑ‚Ñ‹ Ð½Ð¾Ð´, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ñ€Ð°ÑÑˆÐ¸Ñ€ÑÑŽÑ‚ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ ComfyUI:\nComfyUI-Manager, Comfyui-Crystools, Rgthree-Comfy, ComfyUI-Custom-Scripts, Comfyui-Kjnodes, ComfyUI Impact Pack, ComfyUI Impact Subpack, ComfyUI Inspire Pack, Cg-use-everywhere, ComfyUI-Easy-Use, Comfyroll_CustomNodes, ComfyUI_Custom_Nodes_AlekPet, Comfyui-Reactor-node, ComfyUI-GGUF, Comfyui_bnb_nf4_loaders, ComfyUI_IPAdapter_plus, ComfyUI_InstantID, ComfyUI_essentials, ComfyUI_FaceAnalysis, PuLID_ComfyUI, ComfyUI_PuLID_Flux_ll, ComfyUI-Florence2, ComfyUI-AutoCropFaces, Comfyui_controlnet_aux, ComfyUI_AdvancedRefluxControl, ComfyUI_UltimateSDUpscale, Comfyui-inpaint-nodes, ComfyUI-Inpaint-CropAndStitch, ComfyUI-Fluxtapoz, ComfyUI-mxToolkit, ComfyUI-RMBG, ComfyUI ArtVenture, ComfyUI-enricos-nodes, ComfyUI-TiledDiffusion.\nÐšÐ°Ðº ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ?\nÐŸÑ€Ð¾ÑÑ‚Ð¾ Ñ€Ð°ÑÐ¿Ð°ÐºÑƒÐ¹Ñ‚Ðµ Ð°Ñ€Ñ…Ð¸Ð².\nÐ”Ð»Ñ Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ run_nvidia_gpu.bat.\nÐšÐ°Ðº Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ.\nÐŸÐµÑ€ÐµÐ¹Ð´Ð¸Ñ‚Ðµ Ð² Ð¿Ð°Ð¿ÐºÑƒ Install.\nÐ—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ Ð±Ð°Ñ‚Ð½Ð¸Ðº update_ComfyUI.bat, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¾Ð±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ ComfyUI Ð´Ð¾ Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð²ÐµÑ€ÑÐ¸Ð¸.\nÐ’ Ð¿Ð°Ð¿ÐºÐµ Install Ñ‚Ð°ÐºÐ¶Ðµ Ð½Ð°Ð¹Ð´ÐµÑ‚Ðµ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð±Ð°Ñ‚Ð½Ð¸ÐºÐ¸ Ð´Ð»Ñ Ð²Ñ‹Ð±Ð¾Ñ€Ð¾Ñ‡Ð½Ð¾Ð¹ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð² Ð¸Ð»Ð¸ Ð¿ÐµÑ€ÐµÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸ ÑÐ»Ð¾Ð¼Ð°Ð½Ð½Ñ‹Ñ… ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð². Ð•ÑÐ»Ð¸ Ð²Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚Ðµ Ð´Ñ€ÑƒÐ³ÑƒÑŽ ÑÐ±Ð¾Ñ€ÐºÑƒ ComfyUI, Ð¿Ñ€Ð¾ÑÑ‚Ð¾ ÑÐºÐ¾Ð¿Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ð¿Ð°Ð¿ÐºÑƒ Install Ð² ÐºÐ¾Ñ€ÐµÐ½ÑŒ Ð²Ð°ÑˆÐµÐ¹ ComfyUI, Ð¸ Ð¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚ÐµÑÑŒ Ð½Ð° Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÐµ!\nÐ’ Ð·Ð°ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ\nÐ­Ñ‚Ð° ÑÐ±Ð¾Ñ€ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð° ÑÐ¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ð¾ Ð´Ð»Ñ Ñ‚ÐµÑ…, ÐºÑ‚Ð¾ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÑ‚ ÑÐ²Ð¾Ð¹ Ð¿ÑƒÑ‚ÑŒ Ð² Ð¼Ð¸Ñ€Ðµ Ð½ÐµÐ¹Ñ€Ð¾ÑÐµÑ‚ÐµÐ¹ Ð¸ Ñ†Ð¸Ñ„Ñ€Ð¾Ð²Ð¾Ð³Ð¾ Ð¸ÑÐºÑƒÑÑÑ‚Ð²Ð°. Ð¯ Ð¿Ð¾ÑÑ‚Ð°Ñ€Ð°Ð»ÑÑ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ Ð²ÑÑ‘ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾Ðµ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð²Ð°ÑˆÐµ Ð·Ð½Ð°ÐºÐ¾Ð¼ÑÑ‚Ð²Ð¾ Ñ ComfyUI Ð±Ñ‹Ð»Ð¾ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼ Ð¸ ÐºÐ¾Ð¼Ñ„Ð¾Ñ€Ñ‚Ð½Ñ‹Ð¼. Ð’ÑÑ‘, Ñ‡Ñ‚Ð¾ Ð²Ð°Ð¼ Ð½ÑƒÐ¶Ð½Ð¾, ÑƒÐ¶Ðµ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¾ Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¾ â€“ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ñ€Ð°ÑÐ¿Ð°ÐºÑƒÐ¹Ñ‚Ðµ Ð°Ñ€Ñ…Ð¸Ð², ÑÐ»ÐµÐ´ÑƒÐ¹Ñ‚Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸ÑÐ¼, Ð¸ Ð²Ñ‹ ÑÐ¼Ð¾Ð¶ÐµÑ‚Ðµ ÑÑ€Ð°Ð·Ñƒ Ð¿Ñ€Ð¸ÑÑ‚ÑƒÐ¿Ð¸Ñ‚ÑŒ Ðº ÑÐ¾Ð·Ð´Ð°Ð½Ð¸ÑŽ ÑÐ²Ð¾Ð¸Ñ… Ð¿ÐµÑ€Ð²Ñ‹Ñ… ÑˆÐµÐ´ÐµÐ²Ñ€Ð¾Ð².\nÐÐ°Ð´ÐµÑŽÑÑŒ, Ñ‡Ñ‚Ð¾ ComfyUI ÑÑ‚Ð°Ð½ÐµÑ‚ Ð²Ð°ÑˆÐ¸Ð¼ Ð½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ð¼ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸ÐºÐ¾Ð¼, Ð²Ð´Ð¾Ñ…Ð½Ð¾Ð²Ð¸Ñ‚ Ð½Ð° ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¸ Ð¿Ð¾Ð¼Ð¾Ð¶ÐµÑ‚ Ñ€Ð°ÑÐºÑ€Ñ‹Ñ‚ÑŒ Ð²Ð°Ñˆ Ñ‚Ð²Ð¾Ñ€Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð». ÐÐµ Ð±Ð¾Ð¹Ñ‚ÐµÑÑŒ Ð¿Ñ€Ð¾Ð±Ð¾Ð²Ð°Ñ‚ÑŒ Ð½Ð¾Ð²Ð¾Ðµ, Ð·Ð°Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¸ Ð´ÐµÐ»Ð¸Ñ‚ÑŒÑÑ ÑÐ²Ð¾Ð¸Ð¼Ð¸ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸! Ð£Ð´Ð°Ñ‡Ð¸ Ð² Ñ‚Ð²Ð¾Ñ€Ñ‡ÐµÑÑ‚Ð²Ðµ, Ð¸ Ð¿ÑƒÑÑ‚ÑŒ Ð²Ð°ÑˆÐ¸ Ð¸Ð´ÐµÐ¸ Ð¾Ð¶Ð¸Ð²Ð°ÑŽÑ‚ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÑÑ‚Ð¾Ð¹ ÑÐ±Ð¾Ñ€ÐºÐ¸! ðŸŽ¨âœ¨",
    "Qwen/Qwen2-VL-2B": "Qwen2-VL-2B\nIntroduction\nWhatâ€™s New in Qwen2-VL?\nRequirements\nCitation\nQwen2-VL-2B\nIntroduction\nWe're excited to unveil Qwen2-VL, the latest iteration of our Qwen-VL model, representing nearly a year of innovation.\nThis is the base pretrained model of Qwen2-VL-2B without instruction tuning.\nWhatâ€™s New in Qwen2-VL?\nKey Enhancements:\nSoTA understanding of images of various resolution & ratio: Qwen2-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\nUnderstanding videos of 20min+: Qwen2-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.\nAgent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions.\nMultilingual Support: to serve global users, besides English and Chinese, Qwen2-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc.\nModel Architecture Updates:\nNaive Dynamic Resolution: Unlike before, Qwen2-VL can handle arbitrary image resolutions, mapping them into a dynamic number of visual tokens, offering a more human-like visual processing experience.\nMultimodal Rotary Position Embedding (M-ROPE): Decomposes positional embedding into parts to capture 1D textual, 2D visual, and 3D video positional information, enhancing its multimodal processing capabilities.\nWe have three models with 2, 7 and 72 billion parameters.\nThis repo contains the pretrained 2B Qwen2-VL model.\nFor more information, visit our Blog and GitHub.\nRequirements\nThe code of Qwen2-VL has been in the latest Hugging Face transformers and we advise you to install the latest version with command pip install -U transformers, or you might encounter the following error:\nKeyError: 'qwen2_vl'\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{Qwen2-VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "ibm-granite/granite-guardian-hap-38m": "Granite-Guardian-HAP-38m\nModel Summary\nUsage\nIntended Use\nPrediction\nCookbook on Model Usage as a Guardrail\nCookbook on Model Usage for HAP Annotation on Documents in Bulk using Data Prep Kit\nPerformance Comparison with Other Models\nEthical Considerations and Limitations\nResources\nGranite-Guardian-HAP-38m\nModel Summary\nThis model is IBM's lightweight, 4-layer toxicity binary classifier for English. Its latency characteristics make it a suitable guardrail for any large language model. It can also be used for bulk processing of data where high throughput is needed. It has been trained on several benchmark datasets in English, specifically for detecting hateful, abusive, profane and other toxic content in plain text.\nDevelopers: IBM Research\nRelease Date: September 6th, 2024\nLicense: Apache 2.0.\nUsage\nIntended Use\nThis model offers very low inference latency and is capable of running on CPUs apart from GPUs and AIUs. It features 38 million parameters, reducing the number of hidden layers from 12 to 4, decreasing the hidden size from 768 to 576, and the intermediate size from 3072 to 768, compared to the original RoBERTa model architecture. The latency on CPU vs accuracy numbers for this model in comparision to others is shown in the chart below.\nPrediction\n# Example of how to use the model\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nmodel_name_or_path = 'ibm-granite/granite-guardian-hap-38m'\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n# Sample text\ntext = [\"This is the 1st test\", \"This is the 2nd test\"]\ninput = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\nwith torch.no_grad():\nlogits = model(**input).logits\nprediction = torch.argmax(logits, dim=1).detach().numpy().tolist() # Binary prediction where label 1 indicates toxicity.\nprobability = torch.softmax(logits, dim=1).detach().numpy()[:,1].tolist() #  Probability of toxicity.\nCookbook on Model Usage as a Guardrail\nThis recipe illustrates the use of the model either in a prompt, the output, or both. This is an example of a â€œguard railâ€ typically used in generative AI applications for safety.\nGuardrail Cookbook\nCookbook on Model Usage for HAP Annotation on Documents in Bulk using Data Prep Kit\nThis recipe illustrates the use of the model for HAP annotation of documents in bulk. The documents are read from a parquet file. It is then fed to the model sentence by sentence for a document and a HAP score for the document is decided. This is then stored back in the parquet file. Here is the sample cookbook for it Data Prep Kit Cookbook\nPerformance Comparison with Other Models\nThe model outperforms most popular models with significantly lower inference latency. If a better F1 score is required, please refer to IBM's 12-layer model here.\nEthical Considerations and Limitations\nThe use of model-based guardrails for Large Language Models (LLMs) involves risks and ethical considerations people must be aware of. This model operates on chunks of texts and provides a score indicating the presence of hate speech, abuse, or profanity. However, the efficacy of the model can be limited by several factors: the potential inability to capture nuanced meanings or the risk of false positives or negatives on text that is dissimilar to the training data. Previous research has demonstrated the risk of various biases in toxicity or hate speech detection. That is also relevant to this work. We urge the community to use this model with ethical intentions and in a responsible way.\nResources\nâ­ï¸ Learn about the latest updates with Granite: https://www.ibm.com/granite\nðŸ“„ Get started with tutorials, best practices, and prompt engineering advice: https://www.ibm.com/granite/docs/\nðŸ’¡ Learn about the latest Granite learning resources: https://ibm.biz/granite-learning-resources",
    "ai4bharat/indicconformer_stt_ta_hybrid_ctc_rnnt_large": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nIndicConformer\nLanguage\nInput\nOutput\nModel Architecture\nAI4Bharat NeMo:\nUsage\nInference using CTC decoder\nInference using RNNT decoder\nIndicConformer\nIndicConformer is a Hybrid CTC-RNNT conformer ASR(Automatic Speech Recognition) model.\nLanguage\nTamil\nInput\nThis model accepts 16000 KHz Mono-channel Audio (wav files) as input.\nOutput\nThis model provides transcribed speech as a string for a given audio sample.\nModel Architecture\nThis model is a conformer-Large model, consisting of 120M parameters, as the encoder, with a hybrid CTC-RNNT decoder. The model has 17 conformer blocks with\n512 as the model dimension.\nAI4Bharat NeMo:\nTo load, train, fine-tune or play with the model you will need to install AI4Bharat NeMo. We recommend you install it using the command shown below\ngit clone https://github.com/AI4Bharat/NeMo.git && cd NeMo && git checkout nemo-v2 && bash reinstall.sh\nUsage\nDownload and load the model from Huggingface.\nimport torch\nimport nemo.collections.asr as nemo_asr\nmodel = nemo_asr.models.ASRModel.from_pretrained(\"ai4bharat/indicconformer_stt_ta_hybrid_rnnt_large\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.freeze() # inference mode\nmodel = model.to(device) # transfer model to device\nGet an audio file ready by running the command shown below in your terminal. This will convert the audio to 16000 Hz and monochannel.\nffmpeg -i sample_audio.wav -ac 1 -ar 16000 sample_audio_infer_ready.wav\nInference using CTC decoder\nmodel.cur_decoder = \"ctc\"\nctc_text = model.transcribe(['sample_audio_infer_ready.wav'], batch_size=1,logprobs=False, language_id='ta')[0]\nprint(ctc_text)\nInference using RNNT decoder\nmodel.cur_decoder = \"rnnt\"\nrnnt_text = model.transcribe(['sample_audio_infer_ready.wav'], batch_size=1, language_id='ta')[0]\nprint(rnnt_text)",
    "jinaai/reader-lm-1.5b": "Intro\nModels\nGet Started\nOn Google Colab\nLocal\nAWS Sagemaker & Azure Marketplace\nTrained by Jina AI.\nA new version of this model has been released! ReaderLM-v2!\nBlog | Colab\nIntro\nJina Reader-LM is a series of models that convert HTML content to Markdown content, which is useful for content conversion tasks. The model is trained on a curated collection of HTML content and its corresponding Markdown content.\nModels\nName\nContext Length\nDownload\nreader-lm-0.5b\n256K\nðŸ¤— Hugging Face\nreader-lm-1.5b\n256K\nðŸ¤— Hugging Face\nGet Started\nOn Google Colab\nThe easiest way to experience reader-lm is by running our Colab notebook,\nwhere we demonstrate how to use reader-lm-1.5b to convert the HackerNews website into markdown. The notebook is optimized to run smoothly on Google Colabâ€™s free T4 GPU tier. You can also load reader-lm-0.5b or change the URL to any website and explore the output. Note that the input (i.e., the prompt) to the model is the raw HTMLâ€”no prefix instruction is required.\nLocal\nTo use this model, you need to install transformers:\npip install transformers<=4.43.4\nThen, you can use the model as follows:\n# pip install transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"jinaai/reader-lm-1.5b\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n# example html content\nhtml_content = \"<html><body><h1>Hello, world!</h1></body></html>\"\nmessages = [{\"role\": \"user\", \"content\": html_content}]\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\nprint(input_text)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=1024, temperature=0, do_sample=False, repetition_penalty=1.08)\nprint(tokenizer.decode(outputs[0]))\nAWS Sagemaker & Azure Marketplace\nAWS 0.5b\nAWS 1.5b\nAzure 0.5b\nAzure 1.5b",
    "google/gemma-7b-aps-it": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, youâ€™re required to review and agree to Googleâ€™s usage license. To do this, please ensure youâ€™re logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma Model Card\nModel Information\nDescription\nContext Length\nUsage\nInputs and outputs\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma Model Card\nModel Page: Gemma\nThis model card corresponds to the 7B finetuned version of the Gemma-APS model.\nYou can also visit the model card of the 2B finetuned model.\nResources and Technical Documentation:\nScalable and Domain-General Abstractive Proposition Segmentation\nGemma Technical Report\nResponsible Generative AI Toolkit\nGemma on Kaggle\nTerms of Use: Terms\nAuthors: Mohammad Javad Hosseini, Yang Gao, Tim BaumgÃ¤rtner, Alex Fabrikant, Reinald Kim Amplayo\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma-APS is a generative model and a research tool for abstractive proposition segmentation (APS for short), a.k.a. claim extraction.\nGiven a text passage, the model segments the content into the individual facts, statements, and ideas expressed in the text, and restates\nthem in full sentences with small changes to the original text.\nThis model can be used for research where there is a need to break down text content into meaningful components. Applications include\ngrounding, retrieval, fact-checking, and evaluation of generation tasks (such as summarization) where it can be useful to divide up\nindividual propositions (claims) so that they can be processed independently. For more information, check out the research paper.\nContext Length\nModels are trained on a context length of 8192 tokens.\nUsage\nBelow we share some code snippets on how to get quickly started with running the model. First make sure to pip install -U transformers nltk,\nthen copy the snippet from the section that is relevant for your usecase.\nFor ease-of-use, we define two helper functions for pre-processing input and post-processing output of the model:\nimport nltk\nimport re\nnltk.download('punkt')\nstart_marker = '<s>'\nend_marker = '</s>'\nseparator = '\\n'\ndef create_propositions_input(text: str) -> str:\ninput_sents = nltk.tokenize.sent_tokenize(text)\npropositions_input = ''\nfor sent in input_sents:\npropositions_input += f'{start_marker} ' + sent + f' {end_marker}{separator}'\npropositions_input = propositions_input.strip(f'{separator}')\nreturn propositions_input\ndef process_propositions_output(text):\npattern = re.compile(f'{re.escape(start_marker)}(.*?){re.escape(end_marker)}', re.DOTALL)\noutput_grouped_strs = re.findall(pattern, text)\npredicted_grouped_propositions = []\nfor grouped_str in output_grouped_strs:\ngrouped_str = grouped_str.strip(separator)\nprops = [x[2:] for x in grouped_str.split(separator)]\npredicted_grouped_propositions.append(props)\nreturn predicted_grouped_propositions\nUsage with the pipeline API\nfrom transformers import pipeline\nimport torch\ngenerator = pipeline('text-generation', 'google/gemma-7b-aps-it', device_map='auto', torch_dtype=torch.bfloat16)\npassage = 'Sarah Stage, 30, welcomed James Hunter into the world on Tuesday.\\nThe baby boy weighed eight pounds seven ounces and was 22 inches long.'\nmessages = [{'role': 'user', 'content': create_propositions_input(passage)}]\noutput = generator(messages, max_new_tokens=4096, return_full_text=False)\nresult = process_propositions_output(output[0]['generated_text'])\nprint(result)\nExample output\n[\n[\n\"Sarah Stage welcomed James Hunter into the world.\",\n\"Sarah Stage is 30 years old.\",\n\"James Hunter was welcomed on Tuesday.\"\n],\n[\n\"James Hunter weighed eight pounds seven ounces.\",\n\"James Hunter was 22 inches long.\"\n]\n]\nUsage with AutoModel and AutoTokenizer APIs\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nmodel_id = 'google/gemma-7b-aps-it'\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map='auto',\ntorch_dtype=torch.bfloat16,\n)\npassage = \"For more than 40 years, the lyrics of American Pie have been puzzled over. This week the handwritten lyrics sold for more than $1 million at auction. The verses contain hidden references to seminal events of the 50s and 60s. It includes nods to Buddy Holly, Charles Manson and Martin Luther King.\"\nmessages = [{'role': 'user', 'content': create_propositions_input(passage)}]\ninputs = tokenizer.apply_chat_template(messages, return_tensors='pt', add_generation_prompt=True, return_dict=True).to(model.device)\noutput = model.generate(**inputs, max_new_tokens=4096, do_sample=False)\ngenerated_text = tokenizer.batch_decode(output[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)[0]\nresult = process_propositions_output(generated_text)\nprint(result)\nExample output\n[\n[\n\"The lyrics of American Pie have been puzzled over for more than 40 years.\"\n],\n[\n\"The handwritten lyrics sold for more than $1 million.\",\n\"The handwritten lyrics sold at auction.\",\n\"The handwritten lyrics sold this week.\"\n],\n[\n\"The verses contain hidden references to seminal events of the 50s.\",\n\"The verses contain hidden references to seminal events of the 60s.\"\n],\n[\n\"The lyrics include nods to Buddy Holly.\",\n\"The lyrics include nods to Charles Manson.\",\n\"The lyrics include nods to Martin Luther King.\"\n]\n]\nInputs and outputs\nInput: A text passage.\nOutput: List of propositions for all the sentences in the text passage. The propositions for each sentence are grouped separately.\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThe training data contains synthetically generated examples, where each example has (input passage, propositions list) pairs, with the\npropositions list containing propositions for all the sentences in the input passage (one group of propositions for each sentence).\nThe input passages are generated by few-shot prompting Gemini Ultra.\nThe propositions list is generated by applying a teacher LLM on the input passage. The teacher LLM is a Gemini Pro model trained on\na filtered version of the ROSE dataset.\nSee the research paper for all the details.\nData Preprocessing\nWe filtered example passages that have >=4 tokens overlap with any of the few-shot examples used for prompting Gemini Ultra.\nWe used the ROSE dataset for training the teacher LLM (Gemini Pro). We filtered ROSE examples using an entailment model to remove\ncases that do not satisfy desired properties of propositions.\nImplementation Information\nDetails about the model internals.\nHardware\nSimilar to Gemma, Gemma-APS was trained on TPUv5e.\nTraining large language models requires significant computational power. TPUs, designed specifically for matrix operations common in machine learning, offer several advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive computations involved in training LLMs. They can speed up training considerably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory, allowing for the handling of large models and batch sizes during training. This can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable solution for handling the growing complexity of large foundation models. You can distribute training across multiple TPU devices for faster and more efficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more cost-effective solution for training large models compared to CPU-based infrastructure, especially when considering the time and resources saved due to faster training.\nThese advantages are aligned with Google's commitments to operate sustainably.\nSoftware\nTraining was done using JAX.\nJAX allows researchers to leverage the latest generation of hardware, including TPUs, for faster and more efficient training of large models.\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nEvaluation was done on one existing in-domain dataset (development set of the ROSE dataset filtered by an entailment model) and two out-of-domain datasets introduced in the paper. Evaluation was performed based on our new metrics for the abstractive proposition segmentation task.\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nThese models are only suitable for abstractive proposition segmentation for English text, not any other task or language. While we have tested the models on three evaluation datasets and have obtained positive results compared to strong baselines, the model might still have errors on some examples.\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nThese models are only suitable for abstractive proposition segmentation for English text, not any other task or language.\nWhile we have tested it on three evaluation datasets and have obtained positive results compared to strong baselines,\nthe models might still have errors on some examples.\nLimitations\nThese models have certain limitations that users should be aware of.\nTraining Data\nThe quality and diversity of the training data significantly influence the\nmodel's capabilities. Biases or gaps in the training data can lead to\nlimitations in the model's responses.\nThe scope of the training dataset determines the subject areas the model can\nhandle effectively.\nWe have tested our models on passages from different domains, where passages\ncontain a few sentences.\nThis model supports abstractive proposition segmentation in English, not any\nother language.\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. LLMs might struggle to grasp subtle\nnuances, sarcasm, or figurative language.\nFactual Accuracy\nLLMs generate responses based on information they learned from their\ntraining datasets, but they are not knowledge bases. They may generate\nincorrect or outdated factual statements.\nCommon Sense\nLLMs rely on statistical patterns in language. They might lack the ability\nto apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:\nBias and Fairness\nLLMs trained on large-scale, real-world text data can reflect socio-cultural\nbiases embedded in the training material. These models underwent careful\nscrutiny, input data pre-processing described and posterior evaluations\nreported in this card.\nMisinformation and Misuse\nLLMs can be misused to generate text that is false, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to share\ninnovation by making LLM technology accessible to developers and researchers\nacross the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content safety\nare essential. Developers are encouraged to exercise caution and implement\nappropriate content safety safeguards based on their specific product policies\nand application use cases.\nMisuse for malicious purposes: Technical limitations and developer and\nend-user education can help mitigate against malicious applications of LLMs.\nEducational resources and reporting mechanisms for users to flag misuse are\nprovided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal of PII\n(Personally Identifiable Information). Developers are encouraged to adhere to\nprivacy regulations with privacy-preserving techniques.\nBenefits\nThese models are useful for academics working on abstractive proposition segmentation (claim extraction) research or other problems (e.g., grounding, retrieval, fact-checking) that could benefit from this task.",
    "JusperLee/Apollo": "Apollo: Band-sequence Modeling for High-Quality Music Restoration in Compressed Audio\nðŸ“– Abstract\nðŸ”¥ News\nâš¡ï¸ Installation\nðŸ–¥ï¸ Usage\nðŸ—‚ï¸ Datasets\nðŸš€ Training\nðŸŽ¨ Evaluation\nðŸ“Š Results\nLicense\nAcknowledgements\nCitation\nContact\nKai Li1,2, Yi Luo2\n1Tsinghua University, Beijing, China\n2Tencent AI Lab, Shenzhen, China\nArXiv | Demo\nApollo: Band-sequence Modeling for High-Quality Music Restoration in Compressed Audio\nðŸ“– Abstract\nApollo is a novel music restoration method designed to address distortions and artefacts caused by audio codecs, especially at low bitrates. Operating in the frequency domain, Apollo uses a frequency band-split module, band-sequence modeling, and frequency band reconstruction to restore the audio quality of MP3-compressed music. It divides the spectrogram into sub-bands, extracts gain-shape representations, and models both sub-band and temporal information for high-quality audio recovery. Trained with a Generative Adversarial Network (GAN), Apollo outperforms existing SR-GAN models on the MUSDB18-HQ and MoisesDB datasets, excelling in complex multi-instrument and vocal scenarios, while maintaining efficiency.\nðŸ”¥ News\n[2024.09.10] Apollo is now available on ArXiv and Demo.\n[2024.09.106] Apollo checkpoints and pre-trained models are available for download.\nâš¡ï¸ Installation\nclone the repository\ngit clone https://github.com/JusperLee/Apollo.git && cd Apollo\nconda create --name look2hear --file look2hear.yml\nconda activate look2hear\nðŸ–¥ï¸ Usage\nðŸ—‚ï¸ Datasets\nApollo is trained on the MUSDB18-HQ and MoisesDB datasets. To download the datasets, run the following commands:\nwget https://zenodo.org/records/3338373/files/musdb18hq.zip?download=1\nwget https://ds-website-downloads.55c2710389d9da776875002a7d018e59.r2.cloudflarestorage.com/moisesdb.zip\nDuring data preprocessing, we drew inspiration from music separation techniques and implemented the following steps:\nSource Activity Detection (SAD):We used a Source Activity Detector (SAD) to remove silent regions from the audio tracks, retaining only the significant portions for training.\nData Augmentation:We performed real-time data augmentation by mixing tracks from different songs. For each mix, we randomly selected between 1 and 8 stems from the 11 available tracks, extracting 3-second clips from each selected stem. These clips were scaled in energy by a random factor within the range of [-10, 10] dB relative to their original levels. The selected clips were then summed together to create simulated mixed music.\nSimulating Dynamic Bitrate Compression:We simulated various bitrate scenarios by applying MP3 codecs with bitrates of [24000, 32000, 48000, 64000, 96000, 128000].\nRescaling:To ensure consistency across all samples, we rescaled both the target and the encoded audio based on their maximum absolute values.\nSaving as HDF5:After preprocessing, all data (including the source stems, mixed tracks, and compressed audio) was saved in HDF5 format, making it easy to load for training and evaluation purposes.\nðŸš€ Training\nTo train the Apollo model, run the following command:\npython train.py --conf_dir=configs/apollo.yml\nðŸŽ¨ Evaluation\nTo evaluate the Apollo model, run the following command:\npython inference.py --in_wav=assets/input.wav --out_wav=assets/output.wav\nðŸ“Š Results\nHere, you can include a brief overview of the performance metrics or results that Apollo achieves using different bitrates\nDifferent methods' SDR/SI-SNR/VISQOL scores for various types of music, as well as the number of model parameters and GPU inference time. For the GPU inference time test, a music signal with a sampling rate of 44.1 kHz and a length of 1 second was used.\nLicense\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\nAcknowledgements\nApollo is developed by the Look2Hear at Tsinghua University.\nCitation\nIf you use Apollo in your research or project, please cite the following paper:\n@article{li2024apollo,\ntitle={Apollo: Band-sequence Modeling for High-Quality Music Restoration in Compressed Audio},\nauthor={Li, Kai and Luo, Yi},\njournal={xxxxxx},\nyear={2024}\n}\nContact\nFor any questions or feedback regarding Apollo, feel free to reach out to us via email: tsinghua.kaili@gmail.com",
    "Politrees/PolGen": "README.md exists but content is empty.",
    "Ruben-F/bloodcelldiff": "Model Details\nModel Description\nModel Sources [optional]\nUses\nDirect Use\nDownstream Use [optional]\nOut-of-Scope Use\nBias, Risks, and Limitations\nBias\nRisks\nLimitations\nRecommendations\nThis model is a YOLOv8-based object detection model specifically trained for detecting and classifying blood cells in microscopy images. It aims to identify various types of blood cells, such as Red Blood Cells (RBC), Platelets (PLT), and several types of White Blood Cells (WBC), including Neutrophils, Lymphocytes, Monocytes, Eosinophils, and Basophils.\nModel Details\nModel Description\nThis model is built upon the YOLOv8 architecture and trained to detect and classify different types of blood cells from microscopy images. The model has been fine-tuned to recognize specific cell types, making it suitable for applications in medical diagnostics and research.\nDeveloped by:\nModel type: YOLOv8 (Object Detection)\nLicense: [Specify License, e.g., MIT]\nFinetuned from model: YOLOv8s\nModel Sources [optional]\nRepository: https://huggingface.co/Ruben-F/bloodcelldiff\nUses\nDirect Use\nThis model can be used directly for detecting and classifying blood cells in microscopy images. It can be integrated into medical imaging pipelines or used for annotating blood cell images.\nDownstream Use [optional]\nThis model can be fine-tuned for other related tasks or integrated into broader biomedical imaging systems where precise blood cell detection is required.\nOut-of-Scope Use\nThis model is not suitable for general object detection tasks outside of blood cell classification or for use with non-microscopy images where blood cells are not present.\nBias, Risks, and Limitations\nBias\nThe model is trained on specific blood cell types and may not generalize well to other types of cells or variations in microscopy image quality. Ensure that the dataset used for training is representative of the scenarios where the model will be applied.\nRisks\nMisclassifications may occur if the model encounters unfamiliar cell types or poorly captured images. Always validate the model's predictions with domain experts.\nLimitations\nThe model's performance is contingent on the quality and diversity of the training dataset. It may not perform optimally in cases of extreme image noise or unusual cell appearances.\nRecommendations\nUsers should be aware of the model's limitations and verify predictions in critical applications. Continuous monitoring and periodic re-evaluation with updated datasets are recommended."
}