{
    "Wan-AI/Wan2.1-T2V-14B": "Wan2.1\nVideo Demos\nðŸ”¥ Latest News!!\nðŸ“‘ Todo List\nQuickstart\nManual Evaluation\nComputational Efficiency on Different GPUs\nCommunity Contributions\nIntroduction of Wan2.1\nCitation\nLicense Agreement\nAcknowledgements\nContact Us\nWan2.1\nðŸ’œ Wan Â Â  ï½œ Â Â  ðŸ–¥ï¸ GitHub Â Â   | Â Â ðŸ¤— Hugging FaceÂ Â  | Â Â ðŸ¤– ModelScopeÂ Â  | Â Â  ðŸ“‘ Paper (Coming soon) Â Â  | Â Â  ðŸ“‘ Blog Â Â  | Â Â ðŸ’¬ WeChat GroupÂ Â  | Â Â  ðŸ“– Discord\nWan: Open and Advanced Large-Scale Video Generative Models\nIn this repository, we present Wan2.1, a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. Wan2.1 offers these key features:\nðŸ‘ SOTA Performance: Wan2.1 consistently outperforms existing open-source models and state-of-the-art commercial solutions across multiple benchmarks.\nðŸ‘ Supports Consumer-grade GPUs: The T2V-1.3B model requires only 8.19 GB VRAM, making it compatible with almost all consumer-grade GPUs. It can generate a 5-second 480P video on an RTX 4090 in about 4 minutes (without optimization techniques like quantization). Its performance is even comparable to some closed-source models.\nðŸ‘ Multiple Tasks: Wan2.1 excels in Text-to-Video, Image-to-Video, Video Editing, Text-to-Image, and Video-to-Audio, advancing the field of video generation.\nðŸ‘ Visual Text Generation: Wan2.1 is the first video model capable of generating both Chinese and English text, featuring robust text generation that enhances its practical applications.\nðŸ‘ Powerful Video VAE: Wan-VAE delivers exceptional efficiency and performance, encoding and decoding 1080P videos of any length while preserving temporal information, making it an ideal foundation for video and image generation.\nThis repository features our T2V-14B model, which establishes a new SOTA performance benchmark among both open-source and closed-source models. It demonstrates exceptional capabilities in generating high-quality visuals with significant motion dynamics. It is also the only video model capable of producing both Chinese and English text and supports video generation at both 480P and 720P resolutions.\nVideo Demos\nYour browser does not support the video tag.\nðŸ”¥ Latest News!!\nFeb 22, 2025: ðŸ‘‹ We've released the inference code and weights of Wan2.1.\nðŸ“‘ Todo List\nWan2.1 Text-to-Video\nMulti-GPU Inference code of the 14B and 1.3B models\nCheckpoints of the 14B and 1.3B models\nGradio demo\nDiffusers integration\nComfyUI integration\nWan2.1 Image-to-Video\nMulti-GPU Inference code of the 14B model\nCheckpoints of the 14B model\nGradio demo\nDiffusers integration\nComfyUI integration\nQuickstart\nInstallation\nClone the repo:\ngit clone https://github.com/Wan-Video/Wan2.1.git\ncd Wan2.1\nInstall dependencies:\n# Ensure torch >= 2.4.0\npip install -r requirements.txt\nModel Download\nModels\nDownload Link\nNotes\nT2V-14B\nðŸ¤— Huggingface      ðŸ¤– ModelScope\nSupports both 480P and 720P\nI2V-14B-720P\nðŸ¤— Huggingface    ðŸ¤– ModelScope\nSupports 720P\nI2V-14B-480P\nðŸ¤— Huggingface    ðŸ¤– ModelScope\nSupports 480P\nT2V-1.3B\nðŸ¤— Huggingface     ðŸ¤– ModelScope\nSupports 480P\nðŸ’¡Note: The 1.3B model is capable of generating videos at 720P resolution. However, due to limited training at this resolution, the results are generally less stable compared to 480P. For optimal performance, we recommend using 480P resolution.\nDownload models using ðŸ¤— huggingface-cli:\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download Wan-AI/Wan2.1-T2V-14B --local-dir ./Wan2.1-T2V-14B\nDownload models using ðŸ¤– modelscope-cli:\npip install modelscope\nmodelscope download Wan-AI/Wan2.1-T2V-14B --local_dir ./Wan2.1-T2V-14B\nRun Text-to-Video Generation\nThis repository supports two Text-to-Video models (1.3B and 14B) and two resolutions (480P and 720P). The parameters and configurations for these models are as follows:\nTask\nResolution\nModel\n480P\n720P\nt2v-14B\nâœ”ï¸\nâœ”ï¸\nWan2.1-T2V-14B\nt2v-1.3B\nâœ”ï¸\nâŒ\nWan2.1-T2V-1.3B\n(1) Without Prompt Extention\nTo facilitate implementation, we will start with a basic version of the inference process that skips the prompt extension step.\nSingle-GPU inference\npython generate.py  --task t2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-T2V-14B --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\nIf you encounter OOM (Out-of-Memory) issues, you can use the --offload_model True and --t5_cpu options to reduce GPU memory usage. For example, on an RTX 4090 GPU:\npython generate.py  --task t2v-1.3B --size 832*480 --ckpt_dir ./Wan2.1-T2V-1.3B --offload_model True --t5_cpu --sample_shift 8 --sample_guide_scale 6 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\nðŸ’¡Note: If you are using the T2V-1.3B model, we recommend setting the parameter --sample_guide_scale 6. The --sample_shift parameter can be adjusted within the range of 8 to 12 based on the performance.\nMulti-GPU inference using FSDP + xDiT USP\npip install \"xfuser>=0.4.1\"\ntorchrun --nproc_per_node=8 generate.py --task t2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-T2V-14B --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\n(2) Using Prompt Extention\nExtending the prompts can effectively enrich the details in the generated videos, further enhancing the video quality. Therefore, we recommend enabling prompt extension. We provide the following two methods for prompt extension:\nUse the Dashscope API for extension.\nApply for a dashscope.api_key in advance (EN | CN).\nConfigure the environment variable DASH_API_KEY to specify the Dashscope API key. For users of Alibaba Cloud's international site, you also need to set the environment variable DASH_API_URL to 'https://dashscope-intl.aliyuncs.com/api/v1'. For more detailed instructions, please refer to the dashscope document.\nUse the qwen-plus model for text-to-video tasks and qwen-vl-max for image-to-video tasks.\nYou can modify the model used for extension with the parameter --prompt_extend_model. For example:\nDASH_API_KEY=your_key python generate.py  --task t2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-T2V-14B --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage\" --use_prompt_extend --prompt_extend_method 'dashscope' --prompt_extend_target_lang 'ch'\nUsing a local model for extension.\nBy default, the Qwen model on HuggingFace is used for this extension. Users can choose based on the available GPU memory size.\nFor text-to-video tasks, you can use models like Qwen/Qwen2.5-14B-Instruct, Qwen/Qwen2.5-7B-Instruct and Qwen/Qwen2.5-3B-Instruct\nFor image-to-video tasks, you can use models like Qwen/Qwen2.5-VL-7B-Instruct and Qwen/Qwen2.5-VL-3B-Instruct.\nLarger models generally provide better extension results but require more GPU memory.\nYou can modify the model used for extension with the parameter --prompt_extend_model , allowing you to specify either a local model path or a Hugging Face model. For example:\npython generate.py  --task t2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-T2V-14B --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage\" --use_prompt_extend --prompt_extend_method 'local_qwen' --prompt_extend_target_lang 'ch'\n(3) Runing local gradio\ncd gradio\n# if one uses dashscopeâ€™s API for prompt extension\nDASH_API_KEY=your_key python t2v_14B_singleGPU.py --prompt_extend_method 'dashscope' --ckpt_dir ./Wan2.1-T2V-14B\n# if one uses a local model for prompt extension\npython t2v_14B_singleGPU.py --prompt_extend_method 'local_qwen' --ckpt_dir ./Wan2.1-T2V-14B\nManual Evaluation\nThrough manual evaluation, the results generated after prompt extension are superior to those from both closed-source and open-source models.\nComputational Efficiency on Different GPUs\nWe test the computational efficiency of different Wan2.1 models on different GPUs in the following table. The results are presented in the format: Total time (s) / peak GPU memory (GB).\nThe parameter settings for the tests presented in this table are as follows:\n(1) For the 1.3B model on 8 GPUs, set --ring_size 8 and --ulysses_size 1;\n(2) For the 14B model on 1 GPU, use --offload_model True;\n(3) For the 1.3B model on a single 4090 GPU, set --offload_model True --t5_cpu;\n(4) For all testings, no prompt extension was applied, meaning --use_prompt_extend was not enabled.\nCommunity Contributions\nDiffSynth-Studio provides more support for Wan, including video-to-video, FP8 quantization, VRAM optimization, LoRA training, and more. Please refer to their examples.\nIntroduction of Wan2.1\nWan2.1  is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. These include our novel spatio-temporal variational autoencoder (VAE), scalable training strategies, large-scale data construction, and automated evaluation metrics. Collectively, these contributions enhance the modelâ€™s performance and versatility.\n(1) 3D Variational Autoencoders\nWe propose a novel 3D causal VAE architecture, termed Wan-VAE specifically designed for video generation. By combining multiple strategies, we improve spatio-temporal compression, reduce memory usage, and ensure temporal causality. Wan-VAE demonstrates significant advantages in performance efficiency compared to other open-source VAEs. Furthermore, our Wan-VAE can encode and decode unlimited-length 1080P videos without losing historical temporal information, making it particularly well-suited for video generation tasks.\n(2) Video Diffusion DiT\nWan2.1 is designed using the Flow Matching framework within the paradigm of mainstream Diffusion Transformers. Our model's architecture uses the T5 Encoder to encode multilingual text input, with cross-attention in each transformer block embedding the text into the model structure. Additionally, we employ an MLP with a Linear layer and a SiLU layer to process the input time embeddings and predict six modulation parameters individually. This MLP is shared across all transformer blocks, with each block learning a distinct set of biases. Our experimental findings reveal a significant performance improvement with this approach at the same parameter scale.\nModel\nDimension\nInput Dimension\nOutput Dimension\nFeedforward Dimension\nFrequency Dimension\nNumber of Heads\nNumber of Layers\n1.3B\n1536\n16\n16\n8960\n256\n12\n30\n14B\n5120\n16\n16\n13824\n256\n40\n40\nData\nWe curated and deduplicated a candidate dataset comprising a vast amount of image and video data. During the data curation process, we designed a four-step data cleaning process, focusing on fundamental dimensions, visual quality and motion quality. Through the robust data processing pipeline, we can easily obtain high-quality, diverse, and large-scale training sets of images and videos.\nComparisons to SOTA\nWe compared Wan2.1 with leading open-source and closed-source models to evaluate the performace. Using our carefully designed set of 1,035 internal prompts, we tested across 14 major dimensions and 26 sub-dimensions. We then compute the total score by performing a weighted calculation on the scores of each dimension, utilizing weights derived from human preferences in the matching process. The detailed results are shown in the table below. These results demonstrate our model's superior performance compared to both open-source and closed-source models.\nCitation\nIf you find our work helpful, please cite us.\n@article{wan2.1,\ntitle   = {Wan: Open and Advanced Large-Scale Video Generative Models},\nauthor  = {Wan Team},\njournal = {},\nyear    = {2025}\n}\nLicense Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generate contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the license.\nAcknowledgements\nWe would like to thank the contributors to the SD3, Qwen, umt5-xxl, diffusers and HuggingFace repositories, for their open research.\nContact Us\nIf you would like to leave a message to our research or product teams, feel free to join our Discord or WeChat groups!",
    "Wan-AI/Wan2.1-I2V-14B-720P": "Wan2.1\nVideo Demos\nðŸ”¥ Latest News!!\nðŸ“‘ Todo List\nQuickstart\nManual Evaluation\nComputational Efficiency on Different GPUs\nIntroduction of Wan2.1\nCitation\nLicense Agreement\nAcknowledgements\nContact Us\nWan2.1\nðŸ’œ Wan Â Â  ï½œ Â Â  ðŸ–¥ï¸ GitHub Â Â   | Â Â ðŸ¤— Hugging FaceÂ Â  | Â Â ðŸ¤– ModelScopeÂ Â  | Â Â  ðŸ“‘ Paper (Coming soon) Â Â  | Â Â  ðŸ“‘ Blog Â Â  | Â Â ðŸ’¬ WeChat GroupÂ Â  | Â Â  ðŸ“– Discord\nWan: Open and Advanced Large-Scale Video Generative Models\nIn this repository, we present Wan2.1, a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. Wan2.1 offers these key features:\nðŸ‘ SOTA Performance: Wan2.1 consistently outperforms existing open-source models and state-of-the-art commercial solutions across multiple benchmarks.\nðŸ‘ Supports Consumer-grade GPUs: The T2V-1.3B model requires only 8.19 GB VRAM, making it compatible with almost all consumer-grade GPUs. It can generate a 5-second 480P video on an RTX 4090 in about 4 minutes (without optimization techniques like quantization). Its performance is even comparable to some closed-source models.\nðŸ‘ Multiple Tasks: Wan2.1 excels in Text-to-Video, Image-to-Video, Video Editing, Text-to-Image, and Video-to-Audio, advancing the field of video generation.\nðŸ‘ Visual Text Generation: Wan2.1 is the first video model capable of generating both Chinese and English text, featuring robust text generation that enhances its practical applications.\nðŸ‘ Powerful Video VAE: Wan-VAE delivers exceptional efficiency and performance, encoding and decoding 1080P videos of any length while preserving temporal information, making it an ideal foundation for video and image generation.\nThis repository contains our I2V-14B model, which is capable of generating 720P high-definition videos. After thousands of rounds of human evaluations, this model has outperformed both closed-source and open-source alternatives, achieving state-of-the-art performance.\nVideo Demos\nYour browser does not support the video tag.\nðŸ”¥ Latest News!!\nFeb 25, 2025: ðŸ‘‹ We've released the inference code and weights of Wan2.1.\nðŸ“‘ Todo List\nWan2.1 Text-to-Video\nMulti-GPU Inference code of the 14B and 1.3B models\nCheckpoints of the 14B and 1.3B models\nGradio demo\nDiffusers integration\nComfyUI integration\nWan2.1 Image-to-Video\nMulti-GPU Inference code of the 14B model\nCheckpoints of the 14B model\nGradio demo\nDiffusers integration\nComfyUI integration\nQuickstart\nInstallation\nClone the repo:\ngit clone https://github.com/Wan-Video/Wan2.1.git\ncd Wan2.1\nInstall dependencies:\n# Ensure torch >= 2.4.0\npip install -r requirements.txt\nModel Download\nModels\nDownload Link\nNotes\nT2V-14B\nðŸ¤— Huggingface      ðŸ¤– ModelScope\nSupports both 480P and 720P\nI2V-14B-720P\nðŸ¤— Huggingface    ðŸ¤– ModelScope\nSupports 720P\nI2V-14B-480P\nðŸ¤— Huggingface    ðŸ¤– ModelScope\nSupports 480P\nT2V-1.3B\nðŸ¤— Huggingface     ðŸ¤– ModelScope\nSupports 480P\nðŸ’¡Note: The 1.3B model is capable of generating videos at 720P resolution. However, due to limited training at this resolution, the results are generally less stable compared to 480P. For optimal performance, we recommend using 480P resolution.\nDownload models using ðŸ¤— huggingface-cli:\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download Wan-AI/Wan2.1-I2V-14B-720P --local-dir ./Wan2.1-I2V-14B-720P\nDownload models using ðŸ¤– modelscope-cli:\npip install modelscope\nmodelscope download Wan-AI/Wan2.1-I2V-14B-720P --local_dir ./Wan2.1-I2V-14B-720P\nRun Image-to-Video Generation\nSimilar to Text-to-Video, Image-to-Video is also divided into processes with and without the prompt extension step. The specific parameters and their corresponding settings are as follows:\nTask\nResolution\nModel\n480P\n720P\ni2v-14B\nâŒ\nâœ”ï¸\nWan2.1-I2V-14B-720P\ni2v-14B\nâœ”ï¸\nâŒ\nWan2.1-T2V-14B-480P\n(1) Without Prompt Extention\nSingle-GPU inference\npython generate.py --task i2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-I2V-14B-720P --image examples/i2v_input.JPG --prompt \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline's intricate details and the refreshing atmosphere of the seaside.\"\nðŸ’¡For the Image-to-Video task, the size parameter represents the area of the generated video, with the aspect ratio following that of the original input image.\nMulti-GPU inference using FSDP + xDiT USP\npip install \"xfuser>=0.4.1\"\ntorchrun --nproc_per_node=8 generate.py --task i2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-I2V-14B-720P --image examples/i2v_input.JPG --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline's intricate details and the refreshing atmosphere of the seaside.\"\n(2) Using Prompt Extention\nRun with local prompt extention using Qwen/Qwen2.5-VL-7B-Instruct:\npython generate.py --task i2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-I2V-14B-720P --image examples/i2v_input.JPG --use_prompt_extend --prompt_extend_model Qwen/Qwen2.5-VL-7B-Instruct --prompt \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline's intricate details and the refreshing atmosphere of the seaside.\"\nRun with remote prompt extention using dashscope:\nDASH_API_KEY=your_key python generate.py --task i2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-I2V-14B-720P --image examples/i2v_input.JPG --use_prompt_extend --prompt_extend_method 'dashscope' --prompt \"Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline's intricate details and the refreshing atmosphere of the seaside.\"\n(3) Runing local gradio\ncd gradio\n# if one only uses 480P model in gradio\nDASH_API_KEY=your_key python i2v_14B_singleGPU.py --prompt_extend_method 'dashscope' --ckpt_dir_480p ./Wan2.1-I2V-14B-480P\n# if one only uses 720P model in gradio\nDASH_API_KEY=your_key python i2v_14B_singleGPU.py --prompt_extend_method 'dashscope' --ckpt_dir_720p ./Wan2.1-I2V-14B-720P\n# if one uses both 480P and 720P models in gradio\nDASH_API_KEY=your_key python i2v_14B_singleGPU.py --prompt_extend_method 'dashscope' --ckpt_dir_480p ./Wan2.1-I2V-14B-480P --ckpt_dir_720p ./Wan2.1-I2V-14B-720P\nManual Evaluation\nWe conducted extensive manual evaluations to evaluate the performance of the Image-to-Video model, and the results are presented in the table below. The results clearly indicate that Wan2.1 outperforms both closed-source and open-source models.\nComputational Efficiency on Different GPUs\nWe test the computational efficiency of different Wan2.1 models on different GPUs in the following table. The results are presented in the format: Total time (s) / peak GPU memory (GB).\nThe parameter settings for the tests presented in this table are as follows:\n(1) For the 1.3B model on 8 GPUs, set --ring_size 8 and --ulysses_size 1;\n(2) For the 14B model on 1 GPU, use --offload_model True;\n(3) For the 1.3B model on a single 4090 GPU, set --offload_model True --t5_cpu;\n(4) For all testings, no prompt extension was applied, meaning --use_prompt_extend was not enabled.\nIntroduction of Wan2.1\nWan2.1  is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. These include our novel spatio-temporal variational autoencoder (VAE), scalable training strategies, large-scale data construction, and automated evaluation metrics. Collectively, these contributions enhance the modelâ€™s performance and versatility.\n(1) 3D Variational Autoencoders\nWe propose a novel 3D causal VAE architecture, termed Wan-VAE specifically designed for video generation. By combining multiple strategies, we improve spatio-temporal compression, reduce memory usage, and ensure temporal causality. Wan-VAE demonstrates significant advantages in performance efficiency compared to other open-source VAEs. Furthermore, our Wan-VAE can encode and decode unlimited-length 1080P videos without losing historical temporal information, making it particularly well-suited for video generation tasks.\n(2) Video Diffusion DiT\nWan2.1 is designed using the Flow Matching framework within the paradigm of mainstream Diffusion Transformers. Our model's architecture uses the T5 Encoder to encode multilingual text input, with cross-attention in each transformer block embedding the text into the model structure. Additionally, we employ an MLP with a Linear layer and a SiLU layer to process the input time embeddings and predict six modulation parameters individually. This MLP is shared across all transformer blocks, with each block learning a distinct set of biases. Our experimental findings reveal a significant performance improvement with this approach at the same parameter scale.\nModel\nDimension\nInput Dimension\nOutput Dimension\nFeedforward Dimension\nFrequency Dimension\nNumber of Heads\nNumber of Layers\n1.3B\n1536\n16\n16\n8960\n256\n12\n30\n14B\n5120\n16\n16\n13824\n256\n40\n40\nData\nWe curated and deduplicated a candidate dataset comprising a vast amount of image and video data. During the data curation process, we designed a four-step data cleaning process, focusing on fundamental dimensions, visual quality and motion quality. Through the robust data processing pipeline, we can easily obtain high-quality, diverse, and large-scale training sets of images and videos.\nComparisons to SOTA\nWe compared Wan2.1 with leading open-source and closed-source models to evaluate the performace. Using our carefully designed set of 1,035 internal prompts, we tested across 14 major dimensions and 26 sub-dimensions. We then compute the total score by performing a weighted calculation on the scores of each dimension, utilizing weights derived from human preferences in the matching process. The detailed results are shown in the table below. These results demonstrate our model's superior performance compared to both open-source and closed-source models.\nCitation\nIf you find our work helpful, please cite us.\n@article{wan2.1,\ntitle   = {Wan: Open and Advanced Large-Scale Video Generative Models},\nauthor  = {Wan Team},\njournal = {},\nyear    = {2025}\n}\nLicense Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generate contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the license.\nAcknowledgements\nWe would like to thank the contributors to the SD3, Qwen, umt5-xxl, diffusers and HuggingFace repositories, for their open research.\nContact Us\nIf you would like to leave a message to our research or product teams, feel free to join our Discord or WeChat groups!",
    "Qwen/QwQ-32B": "QwQ-32B\nIntroduction\nRequirements\nQuickstart\nUsage Guidelines\nEvaluation & Performance\nCitation\nQwQ-32B\nIntroduction\nQwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.\nThis repo contains the QwQ 32B model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training (Supervised Finetuning and Reinforcement Learning)\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 32.5B\nNumber of Paramaters (Non-Embedding): 31.0B\nNumber of Layers: 64\nNumber of Attention Heads (GQA): 40 for Q and 8 for KV\nContext Length: Full 131,072 tokens\nFor prompts exceeding 8,192 tokens in length, you must enable YaRN as outlined in this section.\nNote: For the best experience, please review the usage guidelines before deploying QwQ models.\nYou can try our demo or access QwQ models via QwenChat.\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nQwQ is based on Qwen2.5, whose code has been in the latest Hugging face transformers. We advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/QwQ-32B\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"How many r's are in the word \\\"strawberry\\\"\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\nUsage Guidelines\nTo achieve optimal performance, we recommend the following settings:\nEnforce Thoughtful Output: Ensure the model starts with \"<think>\\n\" to prevent generating empty thinking content, which can degrade output quality. If you use apply_chat_template and set add_generation_prompt=True, this is already automatically implemented, but it may cause the response to lack the <think> tag at the beginning. This is normal behavior.\nSampling Parameters:\nUse Temperature=0.6, TopP=0.95, MinP=0 instead of Greedy decoding to avoid endless repetitions.\nUse TopK between 20 and 40 to filter out rare token occurrences while maintaining the diversity of the generated output.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may result in occasional language mixing and a slight decrease in performance.\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. This feature is already implemented in apply_chat_template.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g.,\\\"answer\\\": \\\"C\\\".\" in the prompt.\nHandle Long Inputs: For inputs exceeding 8,192 tokens, enable YaRN to improve the model's ability to capture long-sequence information effectively.\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nFor deployment, we recommend using vLLM. Please refer to our Documentation for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nEvaluation & Performance\nDetailed evaluation results are reported in this ðŸ“‘ blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwq32b,\ntitle = {QwQ-32B: Embracing the Power of Reinforcement Learning},\nurl = {https://qwenlm.github.io/blog/qwq-32b/},\nauthor = {Qwen Team},\nmonth = {March},\nyear = {2025}\n}\n@article{qwen2.5,\ntitle={Qwen2.5 Technical Report},\nauthor={An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},\njournal={arXiv preprint arXiv:2412.15115},\nyear={2024}\n}",
    "google/gemma-3-1b-it-qat-q4_0-gguf": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, youâ€™re required to review and agree to Googleâ€™s usage license. To do this, please ensure youâ€™re logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 3 model card\nModel Information\nDescription\nInputs and outputs\nUsage\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma 3 model card\nModel Page: Gemma\nThis repository corresponds to the 1B instruction-tuned version of the Gemma 3 model in GGUF format using Quantization Aware Training (QAT).\nThe GGUF corresponds to Q4_0 quantization.\nThanks to QAT, the model is able to preserve similar quality as bfloat16 while significantly reducing the memory requirements\nto load the model.\nYou can find the half-precision version here.\nResources and Technical Documentation:\nGemma 3 Technical Report\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google DeepMind\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be summarized\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens\neach\nTotal input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n32K tokens for the 1B size\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output context of 8192 tokens\nUsage\nBelow, there are some code snippets on how to get quickly started with running the model.\nllama.cpp (text-only)\n./llama-cli -hf google/gemma-3-1b-it-qat-q4_0-gguf -p \"Write a poem about the Kraken.\"\nollama (text only)\nUsing GGUFs with Ollama via Hugging Face does not support image inputs at the moment. Please check the docs on running gated repositories.\nollama run hf.co/google/gemma-3-1b-it-qat-q4_0-gguf\nCitation\n@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model is\nexposed to a broad range of linguistic styles, topics, and vocabulary. The\ntraining dataset includes content in over 140 languages.\nCode: Exposing the model to code helps it to learn the syntax and\npatterns of programming languages, which improves its ability to generate\ncode and understand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nImages: A wide range of images enables the model to perform image\nanalysis and visual data extraction tasks.\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\nwas applied at multiple stages in the data preparation process to ensure\nthe exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models\nsafe and reliable, automated techniques were used to filter out certain\npersonal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in\nline with our policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using Tensor Processing Unit (TPU) hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive\ncomputations involved in training VLMs. They can speed up training\nconsiderably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory,\nallowing for the handling of large models and batch sizes during training.\nThis can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable\nsolution for handling the growing complexity of large foundation models.\nYou can distribute training across multiple TPU devices for faster and more\nefficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more\ncost-effective solution for training large models compared to CPU-based\ninfrastructure, especially when considering the time and resources saved\ndue to faster training.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nReasoning and factuality\nBenchmark\nMetric\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nHellaSwag\n10-shot\n62.3\n77.2\n84.2\n85.6\nBoolQ\n0-shot\n63.2\n72.3\n78.8\n82.4\nPIQA\n0-shot\n73.8\n79.6\n81.8\n83.3\nSocialIQA\n0-shot\n48.9\n51.9\n53.4\n54.9\nTriviaQA\n5-shot\n39.8\n65.8\n78.2\n85.5\nNatural Questions\n5-shot\n9.48\n20.0\n31.4\n36.1\nARC-c\n25-shot\n38.4\n56.2\n68.9\n70.6\nARC-e\n0-shot\n73.0\n82.4\n88.3\n89.0\nWinoGrande\n5-shot\n58.2\n64.7\n74.3\n78.8\nBIG-Bench Hard\nfew-shot\n28.4\n50.9\n72.6\n77.7\nDROP\n1-shot\n42.4\n60.1\n72.2\n77.2\nSTEM and code\nBenchmark\nMetric\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMMLU\n5-shot\n59.6\n74.5\n78.6\nMMLU (Pro COT)\n5-shot\n29.2\n45.3\n52.2\nAGIEval\n3-5-shot\n42.1\n57.4\n66.2\nMATH\n4-shot\n24.2\n43.3\n50.0\nGSM8K\n8-shot\n38.4\n71.0\n82.6\nGPQA\n5-shot\n15.0\n25.4\n24.3\nMBPP\n3-shot\n46.0\n60.4\n65.6\nHumanEval\n0-shot\n36.0\n45.7\n48.8\nMultilingual\nBenchmark\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMGSM\n2.04\n34.7\n64.3\n74.3\nGlobal-MMLU-Lite\n24.9\n57.0\n69.4\n75.7\nWMT24++ (ChrF)\n36.7\n48.4\n53.9\n55.7\nFloRes\n29.5\n39.2\n46.0\n48.8\nXQuAD (all)\n43.9\n68.0\n74.5\n76.8\nECLeKTic\n4.69\n11.0\n17.2\n24.4\nIndicGenBench\n41.4\n57.2\n61.7\n63.4\nMultimodal\nBenchmark\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nCOCOcap\n102\n111\n116\nDocVQA (val)\n72.8\n82.3\n85.6\nInfoVQA (val)\n44.1\n54.8\n59.4\nMMMU (pt)\n39.2\n50.3\n56.1\nTextVQA (val)\n58.9\n66.5\n68.6\nRealWorldQA\n45.5\n52.2\n53.9\nReMI\n27.3\n38.5\n44.8\nAI2D\n63.2\n75.2\n79.0\nChartQA\n63.6\n74.7\n76.3\nVQAv2\n63.9\n71.2\n72.9\nBLINK\n38.0\n35.9\n39.6\nOKVQA\n51.0\n58.7\n60.2\nTallyQA\n42.5\n51.8\n54.3\nSpatialSense VQA\n50.9\n60.0\n59.4\nCountBenchQA\n26.1\n17.8\n68.0\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild Safety: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent Safety: Evaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\nRepresentational Harms: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\nEvaluation Results\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text\nformats such as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces\nfor customer service, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus,\nresearch papers, or reports.\nImage Data Extraction: These models can be used to extract,\ninterpret, and summarize visual data for text communications.\nResearch and Education\nNatural Language Processing (NLP) and VLM Research: These\nmodels can serve as a foundation for researchers to experiment with VLM\nand NLP techniques, develop algorithms, and contribute to the\nadvancement of the field.\nLanguage Learning Tools: Support interactive language learning\nexperiences, aiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large\nbodies of text by generating summaries or answering questions about\nspecific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly\ninfluence the model's capabilities. Biases or gaps in the training data\ncan lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas\nthe model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear\nprompts and instructions. Open-ended or highly complex tasks might be\nchallenging.\nA model's performance can be influenced by the amount of context\nprovided (longer context generally leads to better outputs, up to a\ncertain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle\nto grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned\nfrom their training datasets, but they are not knowledge bases. They\nmay generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might\nlack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\nBias and Fairness\nVLMs trained on large-scale, real-world text and image data can\nreflect socio-cultural biases embedded in the training material. These\nmodels underwent careful scrutiny, input data pre-processing described\nand posterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading,\nor harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to\nshare innovation by making VLM technology accessible to developers and\nresearchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous\nmonitoring (using evaluation metrics, human review) and the exploration of\nde-biasing techniques during model training, fine-tuning, and other use\ncases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer\nand end-user education can help mitigate against malicious applications of\nVLMs. Educational resources and reporting mechanisms for users to flag\nmisuse are provided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal\nof certain personal information and other sensitive data. Developers are\nencouraged to adhere to privacy regulations with privacy-preserving\ntechniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "mlabonne/gemma-3-27b-it-abliterated": "ðŸ’Ž Gemma 3 27B IT Abliterated\nâš¡ï¸ Quantization\nâœ‚ï¸ Layerwise abliteration\nðŸ’Ž Gemma 3 27B IT Abliterated\nGemma 3 1B Abliterated â€¢ Gemma 3 4B Abliterated â€¢ Gemma 3 12B Abliterated\nThis is an uncensored version of google/gemma-3-27b-it created with a new abliteration technique.\nSee this article to know more about abliteration.\nI was playing with model weights and noticed that Gemma 3 was much more resilient to abliteration than other models like Qwen 2.5.\nI experimented with a few recipes to remove refusals while preserving most of the model capabilities.\nNote that this is fairly experimental, so it might not turn out as well as expected.\nI recommend using these generation parameters: temperature=1.0, top_k=64, top_p=0.95.\nâš¡ï¸ Quantization\nGGUF: https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF\nâœ‚ï¸ Layerwise abliteration\nIn the original technique, a refusal direction is computed by comparing the residual streams between target (harmful) and baseline (harmless) samples.\nHere, the model was abliterated by computing a refusal direction based on hidden states (inspired by Sumandora's repo) for each layer, independently.\nThis is combined with a refusal weight of 1.5 to upscale the importance of this refusal direction in each layer.\nThis created a very high acceptance rate (>90%) and still produced coherent outputs.",
    "google/gemma-3-27b-it-qat-q4_0-gguf": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, youâ€™re required to review and agree to Googleâ€™s usage license. To do this, please ensure youâ€™re logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 3 model card\nModel Information\nDescription\nInputs and outputs\nUsage\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma 3 model card\nModel Page: Gemma\nThis repository corresponds to the 27 instruction-tuned version of the Gemma 3 model in GGUF format using Quantization Aware Training (QAT).\nThe GGUF corresponds to Q4_0 quantization.\nThanks to QAT, the model is able to preserve similar quality as bfloat16 while significantly reducing the memory requirements\nto load the model.\nYou can find the half-precision version here.\nResources and Technical Documentation:\nGemma 3 Technical Report\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google DeepMind\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3 models are multimodal, handling text and image input and generating text\noutput, with open weights for both pre-trained variants and instruction-tuned\nvariants. Gemma 3 has a large, 128K context window, multilingual support in over\n140 languages, and is available in more sizes than previous versions. Gemma 3\nmodels are well-suited for a variety of text generation and image understanding\ntasks, including question answering, summarization, and reasoning. Their\nrelatively small size makes it possible to deploy them in environments with\nlimited resources such as laptops, desktops or your own cloud infrastructure,\ndemocratizing access to state of the art AI models and helping foster innovation\nfor everyone.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be summarized\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens\neach\nTotal input context of 128K tokens for the 4B, 12B, and 27B sizes, and\n32K tokens for the 1B size\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output context of 8192 tokens\nUsage\nBelow, there are some code snippets on how to get quickly started with running the model.\nllama.cpp (text-only)\n./llama-cli -hf google/gemma-3-27b-it-qat-q4_0-gguf -p \"Write a poem about the Kraken.\"\nllama.cpp (image input)\nwget https://github.com/bebechien/gemma/blob/main/surprise.png?raw=true -O ~/Downloads/surprise.png\n./llama-gemma3-cli -hf google/gemma-3-27b-it-qat-q4_0-gguf -p \"Describe this image.\" --image ~/Downloads/surprise.png\nollama (text only)\nUsing GGUFs with Ollama via Hugging Face does not support image inputs at the moment. Please check the docs on running gated repositories.\nollama run hf.co/google/gemma-3-27b-it-qat-q4_0-gguf\nCitation\n@article{gemma_2025,\ntitle={Gemma 3},\nurl={https://goo.gle/Gemma3Report},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2025}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 14 trillion tokens, the 12B model was\ntrained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and\n1B with 2 trillion tokens. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model is\nexposed to a broad range of linguistic styles, topics, and vocabulary. The\ntraining dataset includes content in over 140 languages.\nCode: Exposing the model to code helps it to learn the syntax and\npatterns of programming languages, which improves its ability to generate\ncode and understand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nImages: A wide range of images enables the model to perform image\nanalysis and visual data extraction tasks.\nThe combination of these diverse data sources is crucial for training a powerful\nmultimodal model that can handle a wide variety of different tasks and data\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering\nwas applied at multiple stages in the data preparation process to ensure\nthe exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models\nsafe and reliable, automated techniques were used to filter out certain\npersonal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in\nline with our policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using Tensor Processing Unit (TPU) hardware (TPUv4p,\nTPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant\ncomputational power. TPUs, designed specifically for matrix operations common in\nmachine learning, offer several advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive\ncomputations involved in training VLMs. They can speed up training\nconsiderably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory,\nallowing for the handling of large models and batch sizes during training.\nThis can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable\nsolution for handling the growing complexity of large foundation models.\nYou can distribute training across multiple TPU devices for faster and more\nefficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more\ncost-effective solution for training large models compared to CPU-based\ninfrastructure, especially when considering the time and resources saved\ndue to faster training.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nReasoning and factuality\nBenchmark\nMetric\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nHellaSwag\n10-shot\n62.3\n77.2\n84.2\n85.6\nBoolQ\n0-shot\n63.2\n72.3\n78.8\n82.4\nPIQA\n0-shot\n73.8\n79.6\n81.8\n83.3\nSocialIQA\n0-shot\n48.9\n51.9\n53.4\n54.9\nTriviaQA\n5-shot\n39.8\n65.8\n78.2\n85.5\nNatural Questions\n5-shot\n9.48\n20.0\n31.4\n36.1\nARC-c\n25-shot\n38.4\n56.2\n68.9\n70.6\nARC-e\n0-shot\n73.0\n82.4\n88.3\n89.0\nWinoGrande\n5-shot\n58.2\n64.7\n74.3\n78.8\nBIG-Bench Hard\nfew-shot\n28.4\n50.9\n72.6\n77.7\nDROP\n1-shot\n42.4\n60.1\n72.2\n77.2\nSTEM and code\nBenchmark\nMetric\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMMLU\n5-shot\n59.6\n74.5\n78.6\nMMLU (Pro COT)\n5-shot\n29.2\n45.3\n52.2\nAGIEval\n3-5-shot\n42.1\n57.4\n66.2\nMATH\n4-shot\n24.2\n43.3\n50.0\nGSM8K\n8-shot\n38.4\n71.0\n82.6\nGPQA\n5-shot\n15.0\n25.4\n24.3\nMBPP\n3-shot\n46.0\n60.4\n65.6\nHumanEval\n0-shot\n36.0\n45.7\n48.8\nMultilingual\nBenchmark\nGemma 3 PT 1B\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nMGSM\n2.04\n34.7\n64.3\n74.3\nGlobal-MMLU-Lite\n24.9\n57.0\n69.4\n75.7\nWMT24++ (ChrF)\n36.7\n48.4\n53.9\n55.7\nFloRes\n29.5\n39.2\n46.0\n48.8\nXQuAD (all)\n43.9\n68.0\n74.5\n76.8\nECLeKTic\n4.69\n11.0\n17.2\n24.4\nIndicGenBench\n41.4\n57.2\n61.7\n63.4\nMultimodal\nBenchmark\nGemma 3 PT 4B\nGemma 3 PT 12B\nGemma 3 PT 27B\nCOCOcap\n102\n111\n116\nDocVQA (val)\n72.8\n82.3\n85.6\nInfoVQA (val)\n44.1\n54.8\n59.4\nMMMU (pt)\n39.2\n50.3\n56.1\nTextVQA (val)\n58.9\n66.5\n68.6\nRealWorldQA\n45.5\n52.2\n53.9\nReMI\n27.3\n38.5\n44.8\nAI2D\n63.2\n75.2\n79.0\nChartQA\n63.6\n74.7\n76.3\nVQAv2\n63.9\n71.2\n72.9\nBLINK\n38.0\n35.9\n39.6\nOKVQA\n51.0\n58.7\n60.2\nTallyQA\n42.5\n51.8\n54.3\nSpatialSense VQA\n50.9\n60.0\n59.4\nCountBenchQA\n26.1\n17.8\n68.0\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild Safety: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent Safety: Evaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\nRepresentational Harms: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making.\nAssurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\nEvaluation Results\nFor all areas of safety testing, we saw major improvements in the categories of\nchild safety, content safety, and representational harms relative to previous\nGemma models. All testing was conducted without safety filters to evaluate the\nmodel capabilities and behaviors. For both text-to-text and image-to-text, and\nacross all model sizes, the model produced minimal policy violations, and showed\nsignificant improvements over previous Gemma models' performance with respect\nto ungrounded inferences. A limitation of our evaluations was they included only\nEnglish language prompts.\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen vision-language models (VLMs) models have a wide range of applications\nacross various industries and domains. The following list of potential uses is\nnot comprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text\nformats such as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces\nfor customer service, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus,\nresearch papers, or reports.\nImage Data Extraction: These models can be used to extract,\ninterpret, and summarize visual data for text communications.\nResearch and Education\nNatural Language Processing (NLP) and VLM Research: These\nmodels can serve as a foundation for researchers to experiment with VLM\nand NLP techniques, develop algorithms, and contribute to the\nadvancement of the field.\nLanguage Learning Tools: Support interactive language learning\nexperiences, aiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large\nbodies of text by generating summaries or answering questions about\nspecific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly\ninfluence the model's capabilities. Biases or gaps in the training data\ncan lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas\nthe model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear\nprompts and instructions. Open-ended or highly complex tasks might be\nchallenging.\nA model's performance can be influenced by the amount of context\nprovided (longer context generally leads to better outputs, up to a\ncertain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle\nto grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned\nfrom their training datasets, but they are not knowledge bases. They\nmay generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might\nlack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\nBias and Fairness\nVLMs trained on large-scale, real-world text and image data can\nreflect socio-cultural biases embedded in the training material. These\nmodels underwent careful scrutiny, input data pre-processing described\nand posterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading,\nor harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to\nshare innovation by making VLM technology accessible to developers and\nresearchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous\nmonitoring (using evaluation metrics, human review) and the exploration of\nde-biasing techniques during model training, fine-tuning, and other use\ncases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer\nand end-user education can help mitigate against malicious applications of\nVLMs. Educational resources and reporting mechanisms for users to flag\nmisuse are provided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal\nof certain personal information and other sensitive data. Developers are\nencouraged to adhere to privacy regulations with privacy-preserving\ntechniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nvision-language model implementations designed from the ground up for\nresponsible AI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "Qwen/Qwen2.5-VL-32B-Instruct": "Qwen2.5-VL-32B-Instruct\nLatest Updates:\nIntroduction\nEvaluation\nVision\nText\nRequirements\nQuickstart\nUsing ðŸ¤—  Transformers to Chat\nðŸ¤– ModelScope\nMore Usage Tips\nProcessing Long Texts\nCitation\nQwen2.5-VL-32B-Instruct\nLatest Updates:\nIn addition to the original formula, we have further enhanced Qwen2.5-VL-32B's mathematical and problem-solving abilities through reinforcement learning. This has also significantly improved the model's subjective user experience, with response styles adjusted to better align with human preferences. Particularly for objective queries such as mathematics, logical reasoning, and knowledge-based Q&A, the level of detail in responses and the clarity of formatting have been noticeably enhanced.\nIntroduction\nIn the past five months since Qwen2-VLâ€™s release, numerous developers have built new models on the Qwen2-VL vision-language models, providing us with valuable feedback. During this period, we focused on building more useful vision-language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5-VL.\nKey Enhancements:\nUnderstand things visually: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\nBeing agentic: Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\nUnderstanding long videos and capturing events: Qwen2.5-VL can comprehend videos of over 1 hour, and this time it has a new ability of cpaturing event by pinpointing the relevant video segments.\nCapable of visual localization in different formats: Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\nGenerating structured outputs: for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\nModel Architecture Updates:\nDynamic Resolution and Frame Rate Training for Video Understanding:\nWe extend dynamic resolution to the temporal dimension by adopting dynamic FPS sampling, enabling the model to comprehend videos at various sampling rates. Accordingly, we update mRoPE in the time dimension with IDs and absolute time alignment, enabling the model to learn temporal sequence and speed, and ultimately acquire the ability to pinpoint specific moments.\nStreamlined and Efficient Vision Encoder\nWe enhance both training and inference speeds by strategically implementing window attention into the ViT. The ViT architecture is further optimized with SwiGLU and RMSNorm, aligning it with the structure of the Qwen2.5 LLM.\nWe have four models with 3, 7, 32 and 72 billion parameters. This repo contains the instruction-tuned 32B Qwen2.5-VL model. For more information, visit our Blog and GitHub.\nEvaluation\nVision\nDataset\nQwen2.5-VL-72B(ðŸ¤—ðŸ¤–)\nQwen2-VL-72B(ðŸ¤—ðŸ¤–)\nQwen2.5-VL-32B(ðŸ¤—ðŸ¤–)\nMMMU\n70.2\n64.5\n70\nMMMU Pro\n51.1\n46.2\n49.5\nMMStar\n70.8\n68.3\n69.5\nMathVista\n74.8\n70.5\n74.7\nMathVision\n38.1\n25.9\n40.0\nOCRBenchV2\n61.5/63.7\n47.8/46.1\n57.2/59.1\nCC-OCR\n79.8\n68.7\n77.1\nDocVQA\n96.4\n96.5\n94.8\nInfoVQA\n87.3\n84.5\n83.4\nLVBench\n47.3\n-\n49.00\nCharadesSTA\n50.9\n-\n54.2\nVideoMME\n73.3/79.1\n71.2/77.8\n70.5/77.9\nMMBench-Video\n2.02\n1.7\n1.93\nAITZ\n83.2\n-\n83.1\nAndroid Control\n67.4/93.7\n66.4/84.4\n69.6/93.3\nScreenSpot\n87.1\n-\n88.5\nScreenSpot Pro\n43.6\n-\n39.4\nAndroidWorld\n35\n-\n22.0\nOSWorld\n8.83\n-\n5.92\nText\nMODEL\nMMLU\nMMLU-PRO\nMATH\nGPQA-diamond\nMBPP\nHuman Eval\nQwen2.5-VL-32B\n78.4\n68.8\n82.2\n46.0\n84.0\n91.5\nMistral-Small-3.1-24B\n80.6\n66.8\n69.3\n46.0\n74.7\n88.4\nGemma3-27B-IT\n76.9\n67.5\n89\n42.4\n74.4\n87.8\nGPT-4o-Mini\n82.0\n61.7\n70.2\n39.4\n84.8\n87.2\nClaude-3.5-Haiku\n77.6\n65.0\n69.2\n41.6\n85.6\n88.1\nRequirements\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers accelerate\nor you might encounter the following error:\nKeyError: 'qwen2_5_vl'\nQuickstart\nBelow, we provide simple examples to show how to use Qwen2.5-VL with ðŸ¤– ModelScope and ðŸ¤— Transformers.\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers accelerate\nor you might encounter the following error:\nKeyError: 'qwen2_5_vl'\nWe offer a toolkit to help you handle various types of visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:\n# It's highly recommanded to use `[decord]` feature for faster video loading.\npip install qwen-vl-utils[decord]==0.0.8\nIf you are not using Linux, you might not be able to install decord from PyPI. In that case, you can use pip install qwen-vl-utils which will fall back to using torchvision for video processing. However, you can still install decord from source to get decord used when loading video.\nUsing ðŸ¤—  Transformers to Chat\nHere we show a code snippet to show you how to use the chat model with transformers and qwen_vl_utils:\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n# default: Load the model on the available device(s)\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen2.5-VL-32B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-VL-32B-Instruct\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-32B-Instruct\")\n# The default range for the number of visual tokens per image in the model is 4-16384.\n# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-32B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nMulti image inference\n# Messages containing multiple images and a text query\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n{\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n{\"type\": \"text\", \"text\": \"Identify the similarities between these images.\"},\n],\n}\n]\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nVideo inference\n# Messages containing a images list as a video and a text query\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"video\",\n\"video\": [\n\"file:///path/to/frame1.jpg\",\n\"file:///path/to/frame2.jpg\",\n\"file:///path/to/frame3.jpg\",\n\"file:///path/to/frame4.jpg\",\n],\n},\n{\"type\": \"text\", \"text\": \"Describe this video.\"},\n],\n}\n]\n# Messages containing a local video path and a text query\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"video\",\n\"video\": \"file:///path/to/video1.mp4\",\n\"max_pixels\": 360 * 420,\n\"fps\": 1.0,\n},\n{\"type\": \"text\", \"text\": \"Describe this video.\"},\n],\n}\n]\n# Messages containing a video url and a text query\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"video\",\n\"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n},\n{\"type\": \"text\", \"text\": \"Describe this video.\"},\n],\n}\n]\n#In Qwen 2.5 VL, frame rate information is also input into the model to align with absolute time.\n# Preparation for inference\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\nfps=fps,\npadding=True,\nreturn_tensors=\"pt\",\n**video_kwargs,\n)\ninputs = inputs.to(\"cuda\")\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. change the backend by FORCE_QWENVL_VIDEO_READER=torchvision or FORCE_QWENVL_VIDEO_READER=decord if you prefer not to use the default one.\nBackend\nHTTP\nHTTPS\ntorchvision >= 0.19.0\nâœ…\nâœ…\ntorchvision < 0.19.0\nâŒ\nâŒ\ndecord\nâœ…\nâŒ\nBatch inference\n# Sample messages for batch inference\nmessages1 = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n{\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n{\"type\": \"text\", \"text\": \"What are the common elements in these pictures?\"},\n],\n}\n]\nmessages2 = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n# Combine messages for batch processing\nmessages = [messages1, messages2]\n# Preparation for batch inference\ntexts = [\nprocessor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\nfor msg in messages\n]\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=texts,\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n# Batch Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_texts = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_texts)\nðŸ¤– ModelScope\nWe strongly advise users especially those in mainland China to use ModelScope. snapshot_download can help you solve issues concerning downloading checkpoints.\nMore Usage Tips\nFor input images, we support local files, base64, and URLs. For videos, we currently only support local files.\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\n## Local file path\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n## Image URL\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n## Base64 encoded image\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\nImage Resolution for performance boost\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage.\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n\"Qwen/Qwen2.5-VL-32B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n)\nBesides, We provide two methods for fine-grained control over the image size input to the model:\nDefine min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels.\nSpecify exact dimensions: Directly set resized_height and resized_width. These values will be rounded to the nearest multiple of 28.\n# min_pixels and max_pixels\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"file:///path/to/your/image.jpg\",\n\"resized_height\": 280,\n\"resized_width\": 420,\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# resized_height and resized_width\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"file:///path/to/your/image.jpg\",\n\"min_pixels\": 50176,\n\"max_pixels\": 50176,\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\nProcessing Long Texts\nThe current config.json is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n{\n...,\n\"type\": \"yarn\",\n\"mrope_section\": [\n16,\n24,\n24\n],\n\"factor\": 4,\n\"original_max_position_embeddings\": 32768\n}\nHowever, it should be noted that this method has a significant impact on the performance of temporal and spatial localization tasks, and is therefore not recommended for use.\nAt the same time, for long video inputs, since MRoPE itself is more economical with ids, the max_position_embeddings can be directly modified to a larger value, such as 64k.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}",
    "Qwen/Qwen3-14B": "Qwen3-14B\nQwen3 Highlights\nModel Overview\nQuickstart\nSwitching Between Thinking and Non-Thinking Mode\nenable_thinking=True\nenable_thinking=False\nAdvanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\nAgentic Use\nProcessing Long Texts\nBest Practices\nCitation\nQwen3-14B\nQwen3 Highlights\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\nUniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.\nSignificantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\nSuperior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\nExpertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\nSupport of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.\nModel Overview\nQwen3-14B has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 14.8B\nNumber of Paramaters (Non-Embedding): 13.2B\nNumber of Layers: 40\nNumber of Attention Heads (GQA): 40 for Q and 8 for KV\nContext Length: 32,768 natively and 131,072 tokens with YaRN.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nQuickstart\nThe code of Qwen3 has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-14B\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-14B --reasoning-parser qwen3\nvLLM:vllm serve Qwen/Qwen3-14B --enable-reasoning --reasoning-parser deepseek_r1\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nSwitching Between Thinking and Non-Thinking Mode\nThe enable_thinking switch is also available in APIs created by SGLang and vLLM.\nPlease refer to our documentation for SGLang and vLLM users.\nenable_thinking=True\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting enable_thinking=True or leaving it as the default value in tokenizer.apply_chat_template, the model will engage its thinking mode.\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True  # True is the default value for enable_thinking\n)\nIn this mode, the model will generate think content wrapped in a <think>...</think> block, followed by the final response.\nFor thinking mode, use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 (the default setting in generation_config.json). DO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the Best Practices section.\nenable_thinking=False\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\nIn this mode, the model will not generate any think content and will not include a <think>...</think> block.\nFor non-thinking mode, we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0. For more detailed guidance, please refer to the Best Practices section.\nAdvanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when enable_thinking=True. Specifically, you can add /think and /no_think to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\nHere is an example of a multi-turn conversation:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nclass QwenChatbot:\ndef __init__(self, model_name=\"Qwen/Qwen3-14B\"):\nself.tokenizer = AutoTokenizer.from_pretrained(model_name)\nself.model = AutoModelForCausalLM.from_pretrained(model_name)\nself.history = []\ndef generate_response(self, user_input):\nmessages = self.history + [{\"role\": \"user\", \"content\": user_input}]\ntext = self.tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ninputs = self.tokenizer(text, return_tensors=\"pt\")\nresponse_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\nresponse = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n# Update history\nself.history.append({\"role\": \"user\", \"content\": user_input})\nself.history.append({\"role\": \"assistant\", \"content\": response})\nreturn response\n# Example Usage\nif __name__ == \"__main__\":\nchatbot = QwenChatbot()\n# First input (without /think or /no_think tags, thinking mode is enabled by default)\nuser_input_1 = \"How many r's in strawberries?\"\nprint(f\"User: {user_input_1}\")\nresponse_1 = chatbot.generate_response(user_input_1)\nprint(f\"Bot: {response_1}\")\nprint(\"----------------------\")\n# Second input with /no_think\nuser_input_2 = \"Then, how many r's in blueberries? /no_think\"\nprint(f\"User: {user_input_2}\")\nresponse_2 = chatbot.generate_response(user_input_2)\nprint(f\"Bot: {response_2}\")\nprint(\"----------------------\")\n# Third input with /think\nuser_input_3 = \"Really? /think\"\nprint(f\"User: {user_input_3}\")\nresponse_3 = chatbot.generate_response(user_input_3)\nprint(f\"Bot: {response_3}\")\nFor API compatibility, when enable_thinking=True, regardless of whether the user uses /think or /no_think, the model will always output a block wrapped in <think>...</think>. However, the content inside this block may be empty if thinking is disabled.\nWhen enable_thinking=False, the soft switches are not valid. Regardless of any /think or /no_think tags input by the user, the model will not generate think content and will not include a <think>...</think> block.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-14B',\n# Use the endpoint provided by Alibaba Model Studio:\n# 'model_type': 'qwen_dashscope',\n# 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n# Other parameters:\n# 'generate_cfg': {\n#         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n#         # Do not add: When the response has been separated by reasoning_content and content.\n#         'thought_in_content': True,\n#     },\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nProcessing Long Texts\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model's performance on context lengths of up to 131,072 tokens using the YaRN method.\nYaRN is currently supported by several inference frameworks, e.g., transformers and llama.cpp for local use, vllm and sglang for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\nModifying the model files:\nIn the config.json file, add the rope_scaling fields:\n{\n...,\n\"rope_scaling\": {\n\"rope_type\": \"yarn\",\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768\n}\n}\nFor llama.cpp, you need to regenerate the GGUF file after the modification.\nPassing command line arguments:\nFor vllm, you can use\nvllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' --max-model-len 131072\nFor sglang, you can use\npython -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}'\nFor llama-server from llama.cpp, you can use\nllama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\nIf you encounter the following warning\nUnrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\nplease upgrade transformers>=4.51.0.\nAll the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nIt is also recommended to modify the factor as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set factor as 2.0.\nThe default max_position_embeddings in config.json is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\nThe endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nFor thinking mode (enable_thinking=True), use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0. DO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions.\nFor non-thinking mode (enable_thinking=False), we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "unsloth/Qwen3-14B-GGUF": "To Switch Between Thinking and Non-Thinking\nQwen3-14B\nQwen3 Highlights\nModel Overview\nQuickstart\nSwitching Between Thinking and Non-Thinking Mode\nenable_thinking=True\nenable_thinking=False\nAdvanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\nAgentic Use\nProcessing Long Texts\nBest Practices\nCitation\nSee our collection for all versions of Qwen3 including GGUF, 4-bit & 16-bit formats.\nLearn to run Qwen3 correctly - Read our Guide.\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nâœ¨ Run & Fine-tune Qwen3 with Unsloth!\nFine-tune Qwen3 (14B) for free using our Google Colab notebook here!\nRead our Blog about Qwen3 support: unsloth.ai/blog/qwen3\nView the rest of our notebooks in our docs here.\nRun & export your fine-tuned model to Ollama, llama.cpp or HF.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nQwen3 (14B)\nâ–¶ï¸ Start on Colab\n3x faster\n70% less\nGRPO with Qwen3 (8B)\nâ–¶ï¸ Start on Colab\n3x faster\n80% less\nLlama-3.2 (3B)\nâ–¶ï¸ Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\nâ–¶ï¸ Start on Colab\n2x faster\n60% less\nQwen2.5 (7B)\nâ–¶ï¸ Start on Colab\n2x faster\n60% less\nPhi-4 (14B)\nâ–¶ï¸ Start on Colab\n2x faster\n50% less\nTo Switch Between Thinking and Non-Thinking\nIf you are using llama.cpp, Ollama, Open WebUI etc., you can add /think and /no_think to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\nHere is an example of multi-turn conversation:\n> Who are you /no_think\n<think>\n</think>\nI am Qwen, a large-scale language model developed by Alibaba Cloud. [...]\n> How many 'r's are in 'strawberries'? /think\n<think>\nOkay, let's see. The user is asking how many times the letter 'r' appears in the word \"strawberries\". [...]\n</think>\nThe word strawberries contains 3 instances of the letter r. [...]\nQwen3-14B\nQwen3 Highlights\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\nUniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.\nSignificantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\nSuperior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\nExpertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\nSupport of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.\nModel Overview\nQwen3-14B has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 14.8B\nNumber of Paramaters (Non-Embedding): 13.2B\nNumber of Layers: 40\nNumber of Attention Heads (GQA): 40 for Q and 8 for KV\nContext Length: 32,768 natively and 131,072 tokens with YaRN.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nQuickstart\nThe code of Qwen3 has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-14B\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\nFor deployment, you can use vllm>=0.8.5 or sglang>=0.4.5.post2 to create an OpenAI-compatible API endpoint:\nvLLM:vllm serve Qwen/Qwen3-14B --enable-reasoning --reasoning-parser deepseek_r1\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-14B --reasoning-parser deepseek-r1\nSwitching Between Thinking and Non-Thinking Mode\nThe enable_thinking switch is also available in APIs created by vLLM and SGLang.\nPlease refer to our documentation for more details.\nenable_thinking=True\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting enable_thinking=True or leaving it as the default value in tokenizer.apply_chat_template, the model will engage its thinking mode.\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True  # True is the default value for enable_thinking\n)\nIn this mode, the model will generate think content wrapped in a <think>...</think> block, followed by the final response.\nFor thinking mode, use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 (the default setting in generation_config.json). DO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the Best Practices section.\nenable_thinking=False\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\nIn this mode, the model will not generate any think content and will not include a <think>...</think> block.\nFor non-thinking mode, we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0. For more detailed guidance, please refer to the Best Practices section.\nAdvanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when enable_thinking=True. Specifically, you can add /think and /no_think to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\nHere is an example of a multi-turn conversation:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nclass QwenChatbot:\ndef __init__(self, model_name=\"Qwen/Qwen3-14B\"):\nself.tokenizer = AutoTokenizer.from_pretrained(model_name)\nself.model = AutoModelForCausalLM.from_pretrained(model_name)\nself.history = []\ndef generate_response(self, user_input):\nmessages = self.history + [{\"role\": \"user\", \"content\": user_input}]\ntext = self.tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ninputs = self.tokenizer(text, return_tensors=\"pt\")\nresponse_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\nresponse = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n# Update history\nself.history.append({\"role\": \"user\", \"content\": user_input})\nself.history.append({\"role\": \"assistant\", \"content\": response})\nreturn response\n# Example Usage\nif __name__ == \"__main__\":\nchatbot = QwenChatbot()\n# First input (without /think or /no_think tags, thinking mode is enabled by default)\nuser_input_1 = \"How many r's in strawberries?\"\nprint(f\"User: {user_input_1}\")\nresponse_1 = chatbot.generate_response(user_input_1)\nprint(f\"Bot: {response_1}\")\nprint(\"----------------------\")\n# Second input with /no_think\nuser_input_2 = \"Then, how many r's in blueberries? /no_think\"\nprint(f\"User: {user_input_2}\")\nresponse_2 = chatbot.generate_response(user_input_2)\nprint(f\"Bot: {response_2}\")\nprint(\"----------------------\")\n# Third input with /think\nuser_input_3 = \"Really? /think\"\nprint(f\"User: {user_input_3}\")\nresponse_3 = chatbot.generate_response(user_input_3)\nprint(f\"Bot: {response_3}\")\nNote\nFor API compatibility, when enable_thinking=True, regardless of whether the user uses /think or /no_think, the model will always output a block wrapped in <think>...</think>. However, the content inside this block may be empty if thinking is disabled.\nWhen enable_thinking=False, the soft switches are not valid. Regardless of any /think or /no_think tags input by the user, the model will not generate think content and will not include a <think>...</think> block.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-14B',\n# Use the endpoint provided by Alibaba Model Studio:\n# 'model_type': 'qwen_dashscope',\n# 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n# Other parameters:\n# 'generate_cfg': {\n#         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n#         # Do not add: When the response has been separated by reasoning_content and content.\n#         'thought_in_content': True,\n#     },\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nProcessing Long Texts\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model's performance on context lengths of up to 131,072 tokens using the YaRN method.\nYaRN is currently supported by several inference frameworks, e.g., transformers and llama.cpp for local use, vllm and sglang for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\nModifying the model files:\nIn the config.json file, add the rope_scaling fields:\n{\n...,\n\"rope_scaling\": {\n\"type\": \"yarn\",\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768\n}\n}\nFor llama.cpp, you need to regenerate the GGUF file after the modification.\nPassing command line arguments:\nFor vllm, you can use\nvllm serve ... --rope-scaling '{\"type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' --max-model-len 131072\nFor sglang, you can use\npython -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}'\nFor llama-server from llama.cpp, you can use\nllama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\nIf you encounter the following warning\nUnrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\nplease upgrade transformers>=4.51.0.\nAll the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nIt is also recommended to modify the factor as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set factor as 2.0.\nThe default max_position_embeddings in config.json is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\nThe endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nFor thinking mode (enable_thinking=True), use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0. DO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions.\nFor non-thinking mode (enable_thinking=False), we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3,\ntitle  = {Qwen3},\nurl    = {https://qwenlm.github.io/blog/qwen3/},\nauthor = {Qwen Team},\nmonth  = {April},\nyear   = {2025}\n}",
    "unsloth/Qwen3-1.7B-GGUF": "To Switch Between Thinking and Non-Thinking\nQwen3-1.7B\nQwen3 Highlights\nModel Overview\nQuickstart\nSwitching Between Thinking and Non-Thinking Mode\nenable_thinking=True\nenable_thinking=False\nAdvanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\nAgentic Use\nBest Practices\nCitation\nSee our collection for all versions of Qwen3 including GGUF, 4-bit & 16-bit formats.\nLearn to run Qwen3 correctly - Read our Guide.\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nâœ¨ Run & Fine-tune Qwen3 with Unsloth!\nFine-tune Qwen3 (14B) for free using our Google Colab notebook here!\nRead our Blog about Qwen3 support: unsloth.ai/blog/qwen3\nView the rest of our notebooks in our docs here.\nRun & export your fine-tuned model to Ollama, llama.cpp or HF.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nQwen3 (14B)\nâ–¶ï¸ Start on Colab\n3x faster\n70% less\nGRPO with Qwen3 (8B)\nâ–¶ï¸ Start on Colab\n3x faster\n80% less\nLlama-3.2 (3B)\nâ–¶ï¸ Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\nâ–¶ï¸ Start on Colab\n2x faster\n60% less\nQwen2.5 (7B)\nâ–¶ï¸ Start on Colab\n2x faster\n60% less\nPhi-4 (14B)\nâ–¶ï¸ Start on Colab\n2x faster\n50% less\nTo Switch Between Thinking and Non-Thinking\nIf you are using llama.cpp, Ollama, Open WebUI etc., you can add /think and /no_think to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\nHere is an example of multi-turn conversation:\n> Who are you /no_think\n<think>\n</think>\nI am Qwen, a large-scale language model developed by Alibaba Cloud. [...]\n> How many 'r's are in 'strawberries'? /think\n<think>\nOkay, let's see. The user is asking how many times the letter 'r' appears in the word \"strawberries\". [...]\n</think>\nThe word strawberries contains 3 instances of the letter r. [...]\nQwen3-1.7B\nQwen3 Highlights\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\nUniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.\nSignificantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\nSuperior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\nExpertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\nSupport of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.\nModel Overview\nQwen3-1.7B has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 1.7B\nNumber of Paramaters (Non-Embedding): 1.4B\nNumber of Layers: 28\nNumber of Attention Heads (GQA): 16 for Q and 8 for KV\nContext Length: 32,768\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nQuickstart\nThe code of Qwen3 has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-1.7B\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\nFor deployment, you can use vllm>=0.8.5 or sglang>=0.4.5.post2 to create an OpenAI-compatible API endpoint:\nvLLM:vllm serve Qwen/Qwen3-1.7B --enable-reasoning --reasoning-parser deepseek_r1\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-1.7B --reasoning-parser deepseek-r1\nSwitching Between Thinking and Non-Thinking Mode\nThe enable_thinking switch is also available in APIs created by vLLM and SGLang.\nPlease refer to our documentation for more details.\nenable_thinking=True\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting enable_thinking=True or leaving it as the default value in tokenizer.apply_chat_template, the model will engage its thinking mode.\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True  # True is the default value for enable_thinking\n)\nIn this mode, the model will generate think content wrapped in a <think>...</think> block, followed by the final response.\nFor thinking mode, use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 (the default setting in generation_config.json). DO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the Best Practices section.\nenable_thinking=False\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\nIn this mode, the model will not generate any think content and will not include a <think>...</think> block.\nFor non-thinking mode, we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0. For more detailed guidance, please refer to the Best Practices section.\nAdvanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when enable_thinking=True. Specifically, you can add /think and /no_think to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\nHere is an example of a multi-turn conversation:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nclass QwenChatbot:\ndef __init__(self, model_name=\"Qwen/Qwen3-1.7B\"):\nself.tokenizer = AutoTokenizer.from_pretrained(model_name)\nself.model = AutoModelForCausalLM.from_pretrained(model_name)\nself.history = []\ndef generate_response(self, user_input):\nmessages = self.history + [{\"role\": \"user\", \"content\": user_input}]\ntext = self.tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ninputs = self.tokenizer(text, return_tensors=\"pt\")\nresponse_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\nresponse = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n# Update history\nself.history.append({\"role\": \"user\", \"content\": user_input})\nself.history.append({\"role\": \"assistant\", \"content\": response})\nreturn response\n# Example Usage\nif __name__ == \"__main__\":\nchatbot = QwenChatbot()\n# First input (without /think or /no_think tags, thinking mode is enabled by default)\nuser_input_1 = \"How many r's in strawberries?\"\nprint(f\"User: {user_input_1}\")\nresponse_1 = chatbot.generate_response(user_input_1)\nprint(f\"Bot: {response_1}\")\nprint(\"----------------------\")\n# Second input with /no_think\nuser_input_2 = \"Then, how many r's in blueberries? /no_think\"\nprint(f\"User: {user_input_2}\")\nresponse_2 = chatbot.generate_response(user_input_2)\nprint(f\"Bot: {response_2}\")\nprint(\"----------------------\")\n# Third input with /think\nuser_input_3 = \"Really? /think\"\nprint(f\"User: {user_input_3}\")\nresponse_3 = chatbot.generate_response(user_input_3)\nprint(f\"Bot: {response_3}\")\nNote\nFor API compatibility, when enable_thinking=True, regardless of whether the user uses /think or /no_think, the model will always output a block wrapped in <think>...</think>. However, the content inside this block may be empty if thinking is disabled.\nWhen enable_thinking=False, the soft switches are not valid. Regardless of any /think or /no_think tags input by the user, the model will not generate think content and will not include a <think>...</think> block.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-1.7B',\n# Use the endpoint provided by Alibaba Model Studio:\n# 'model_type': 'qwen_dashscope',\n# 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n# Other parameters:\n# 'generate_cfg': {\n#         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n#         # Do not add: When the response has been separated by reasoning_content and content.\n#         'thought_in_content': True,\n#     },\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nFor thinking mode (enable_thinking=True), use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0. DO NOT use greedy decoding, as it can lead to performance degradation and endless repetitions.\nFor non-thinking mode (enable_thinking=False), we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3,\ntitle  = {Qwen3},\nurl    = {https://qwenlm.github.io/blog/qwen3/},\nauthor = {Qwen Team},\nmonth  = {April},\nyear   = {2025}\n}",
    "KangLiao/Puffin": "Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation\nPaper\nAbstract\nLinks\nModel Details\nDirect Use\nSample Usage\nCitation\nLicense\nThinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation\nPaper\nThis model was presented in the paper Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation.\nAbstract\nCamera-centric understanding and generation are two cornerstones of spatial intelligence, yet they are typically studied in isolation. We present Puffin, a unified camera-centric multimodal model that extends spatial awareness along the camera dimension. Puffin integrates language regression and diffusion-based generation to interpret and create scenes from arbitrary viewpoints. To bridge the modality gap between cameras and vision-language, we introduce a novel paradigm that treats camera as language, enabling thinking with camera. This guides the model to align spatially grounded visual cues with photographic terminology while reasoning across geometric context. Puffin is trained on Puffin-4M, a large-scale dataset of 4 million vision-language-camera triplets. We incorporate both global camera parameters and pixel-wise camera maps, yielding flexible and reliable spatial generation. Experiments demonstrate Puffin superior performance over specialized models for camera-centric generation and understanding. With instruction tuning, Puffin generalizes to diverse cross-view tasks such as spatial imagination, world exploration, and photography guidance.\nLinks\nProject Page: https://kangliao929.github.io/projects/puffin\nGitHub Repository: https://github.com/KangLiao929/Puffin\nHugging Face Space: https://huggingface.co/spaces/KangLiao/Puffin\nHugging Face Dataset: https://huggingface.co/datasets/KangLiao/Puffin-4M\nModel Details\nPuffin is a unified camera-centric multimodal model that extends spatial awareness along the camera dimension. It learns the camera-centric understanding and generation tasks in a unified multimodal framework. To bridge the modality gap between cameras and vision-language, we introduce a novel paradigm that treats camera as language, enabling thinking with camera. This guides the model to align spatially grounded visual cues with photographic terminology while reasoning across geometric context.\nDeveloped by\nKang Liao, Size Wu, Zhonghua Wu, Linyi Jin, Chao Wang, Yikai Wang, Fei Wang, Wei Li, Chen Change Loy\nAffiliation\nS-Lab, Nanyang Technological University\nFirst released\narXiv pre-print, 2025\nModel type\nUnified multimodal models (diffusion / autoregressive modelling with camera-centric understanding and generation)\nModality\nImage â†’ Text+Camera; Text+Camera â†’ Image; Image+Camera â†’ Image; Image+Camera â†’ Text\nDirect Use\nCamera-centric understanding and generation from a single image or a pair of text and camera, supports the thinking mode.\nWorld exploration: performs the cross-view generation from a given initial view and target camera configuration.\nSpatial imagination: imagines the scene description based on an initial view and target camera configuration.\n3D virtual object insertion in AR/VR: assists the virtual 3D object insertion into in-the-wild images by calibrating camera parameters\nSample Usage\nThis section demonstrates how to generate images with camera control using Puffin-Base, based on the examples provided in the GitHub repository.\nFirst, download the model checkpoints from ðŸ¤— KangLiao/Puffin and organize them in a checkpoints directory, for example:\nPuffin/\nâ”œâ”€â”€ checkpoints\nâ”œâ”€â”€ Puffin-Align.pth # provided for customized SFT\nâ”œâ”€â”€ Puffin-Base.pth\nâ”œâ”€â”€ Puffin-Thinking.pth\nâ”œâ”€â”€ Puffin-Instruct.pth\nYou can use huggingface-cli to download the checkpoints:\n# pip install -U \"huggingface_hub[cli]\"\nhuggingface-cli download KangLiao/Puffin  --local-dir checkpoints --repo-type model\nTo run the camera-controllable image generation:\nexport PYTHONPATH=./:$PYTHONPATH\npython scripts/demo/generation.py configs/pipelines/stage_2_base.py \\\n--checkpoint checkpoints/Puffin-Base.pth --output generation_result.jpg \\\n--prompt \"A streetlamp casts light on an outdoor mural with intricate floral designs and text, set against a building wall.\" \\\n-r -0.3939 -p 0.0277 -f 0.7595\nThis command generates an image based on the provided text prompt and camera parameters (roll: -r, pitch: -p, vertical field-of-view: -f, all in radians). The output image will be saved as generation_result.jpg.\nTo enable the thinking mode for image generation, please simply change the settings and append the --thinking flag:\npython scripts/demo/generation.py configs/pipelines/stage_3_thinking.py \\\n--checkpoint checkpoints/Puffin-Thinking.pth --output generation_result_thinking.jpg \\\n--prompt \"A streetlamp casts light on an outdoor mural with intricate floral designs and text, set against a building wall.\" \\\n-r -0.3939 -p 0.0277 -f 0.7595 \\\n--thinking\nCitation\nIf you find Puffin useful for your research or applications, please cite our paper using the following BibTeX:\n@article{liao2025puffin,\ntitle={Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation},\nauthor={Liao, Kang and Wu, Size and Wu, Zhonghua and Jin, Linyi and Wang, Chao and Wang, Yikai and Wang, Fei and Li, Wei and Loy, Chen Change},\njournal={arXiv preprint arXiv:2510.08673},\nyear={2025}\n}\nLicense\nThis project is licensed under NTU S-Lab License 1.0.",
    "Sao10K/Lmao_life_updates": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\n24/10/25\ni'm alive\ntime passed by quick, for real\na year ago i was at a completely different place in life.\nnow i don't know what I want to do\nfeel kinda aimless after just finishing my national service\nmy datasets outdated and all, probably time to rework things\nsao out (hopefully with more)\nSup.\nNoticed the lack of model uploads? me too, man.\nWhen life is good, well, you tend to enjoy it without thinking of the future much\nand when things go to shit, life hits you even harder.\nThe past month and a half has been bad to me lol\nrelationship issues as per usual, you know how things go. then, bam, double whammy.\nlike two weeks ago, a routine day shift turns to shit, next patient was my own dad, imagine that. a major case too, had to standby the hospital and all.\nNerves had rattled me, struggled with the basics I had mastered. Thought I could handle it emotionally, be calm and treat him, turns out nope. not even.\nThankfully I had colleagues and seniors with me on call.\nHe's stable now, thank god.\nI guess having understanding bosses help too, when I couldnt focus on anything for a while. had my break to focus and all.\nanyway ill release models soon, or something",
    "pfnet/plamo-2-translate": "PLaMo Translation Model\nUsage\nmain/base model\nevaluation model\nBias, Risks, and Limitations\nAcknowledgement\nAI policies for Preferred Networks, Inc. group\nPLaMo Translation Model\nPLaMoç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã¯Preferred Networksã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸç¿»è¨³å‘ã‘ç‰¹åŒ–åž‹å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\nè©³ã—ãã¯ãƒ–ãƒ­ã‚°è¨˜äº‹ãŠã‚ˆã³ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\nPLaMo Translation Model is a specialized large-scale language model developed by Preferred Networks for translation tasks.\nFor details, please refer to the blog post and press release.\nList of models:\nplamo-2-translate ... Post-trained model for translation\nplamo-2-translate-base ... Base model for translation\nplamo-2-translate-eval ... Pair-wise evaluation model\nPLaMo Translation Model is released under PLaMo community license. Please check the following license and agree to this before downloading.\n(EN) https://plamo.preferredai.jp/info/plamo-community-license-en\n(JA) https://plamo.preferredai.jp/info/plamo-community-license-ja\nNOTE: This model has NOT been instruction-tuned for chat dialog or other downstream tasks.\nFor commercial users\nPlease check the PLaMo community license and contact us via the following form to use commercial purpose.\n(EN/JA) https://forms.gle/mTL8tBLrMYXKNZD56\nUsage\nmain/base model\nimport vllm\n# max_model_len/max_num_batched_tokens can be increased when running on a GPU with substantial memory.\n# NOTE: Switch to \"pfnet/plamo-2-translate-base\" to try the base model.\nllm = vllm.LLM(model=\"pfnet/plamo-2-translate\", trust_remote_code=True, max_model_len=2000, max_num_batched_tokens=2000)\nprompt = r'''<|plamo:op|>dataset\ntranslation\n<|plamo:op|>input lang=English\nWrite the text to be translated here.\n<|plamo:op|>output lang=Japanese\n'''\nresponses = llm.generate([prompt] * 1, sampling_params=vllm.SamplingParams(temperature=0, max_tokens=1024, stop=[\"<|plamo:op|>\"]))\n# NOTE: This outputs \"ã“ã“ã«ç¿»è¨³ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\".\nprint(responses[0].outputs[0].text)\nevaluation model\nimport vllm\n# max_model_len/max_num_batched_tokens can be increased when running on a GPU with substantial memory.\nllm = vllm.LLM(model=\"pfnet/plamo-2-translate-eval\", trust_remote_code=True, max_model_len=2000, max_num_batched_tokens=2000)\nprompt = r'''<|plamo:op|>dataset\ntranslation evaluation\n<|plamo:op|>input lang=English\nThis is an apple.\n<|plamo:op|>output id=A lang=Japanese\nã“ã‚Œã¯ã‚Šã‚“ã”ã§ã™ã€‚\n<|plamo:op|>output id=B lang=Japanese\nã“ã‚Œã¯ãƒªãƒ³ã‚´ã§ã™ã€‚\n<|plamo:op|>best\nid='''\nresponses = llm.generate([prompt] * 1, sampling_params=vllm.SamplingParams(temperature=0, max_tokens=1, stop=[\"<|plamo:op|>\"]))\n# NOTE: This outputs \"A\".\nprint(responses[0].outputs[0].text)\nBias, Risks, and Limitations\nPLaMo Translation Model is a new technology that carries risks with use. Testing conducted to date has been in English and Japanese, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, PLaMo Translation Modelâ€™s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of PLaMo Translation Model, developers should perform safety testing and tuning tailored to their specific applications of the model.\nAcknowledgement\nThis model is trained under the project, â€œResearch and Development Project of the Enhanced Infrastructures for Post 5G Information and Communication Systemâ€ (JPNP 20017), subsidized by the New Energy and Industrial Technology Development Organization (NEDO).\nAI policies for Preferred Networks, Inc. group\n(EN) https://www.preferred.jp/en/company/aipolicy/\n(JA) https://www.preferred.jp/ja/company/aipolicy/",
    "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B": "DeepSeek-R1-0528\n1. Introduction\n2. Evaluation Results\nDeepSeek-R1-0528\nDeepSeek-R1-0528-Qwen3-8B\n3. Chat Website & API Platform\n4. How to Run Locally\nSystem Prompt\nTemperature\nPrompts for File Uploading and Web Search\n5. License\n6. Citation\n7. Contact\nDeepSeek-R1-0528\nPaper LinkðŸ‘ï¸\n1. Introduction\nThe DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.\nCompared to the previous version, the upgraded model shows significant improvements in handling complex reasoning tasks. For instance, in the AIME 2025 test, the modelâ€™s accuracy has increased from 70% in the previous version to 87.5% in the current version. This advancement stems from enhanced thinking depth during the reasoning process: in the AIME test set, the previous model used an average of 12K tokens per question, whereas the new version averages 23K tokens per question.\nBeyond its improved reasoning capabilities, this version also offers a reduced hallucination rate, enhanced support for function calling, and better experience for vibe coding.\n2. Evaluation Results\nDeepSeek-R1-0528\nFor all our models, the maximum generation length is set to 64K tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 16 responses per query to estimate pass@1.\nCategory\nBenchmark (Metric)\nDeepSeek R1\nDeepSeek R1 0528\nGeneral\nMMLU-Redux (EM)\n92.9\n93.4\nMMLU-Pro (EM)\n84.0\n85.0\nGPQA-Diamond (Pass@1)\n71.5\n81.0\nSimpleQA (Correct)\n30.1\n27.8\nFRAMES (Acc.)\n82.5\n83.0\nHumanity's Last Exam (Pass@1)\n8.5\n17.7\nCode\nLiveCodeBench (2408-2505) (Pass@1)\n63.5\n73.3\nCodeforces-Div1 (Rating)\n1530\n1930\nSWE Verified (Resolved)\n49.2\n57.6\nAider-Polyglot (Acc.)\n53.3\n71.6\nMath\nAIME 2024 (Pass@1)\n79.8\n91.4\nAIME 2025 (Pass@1)\n70.0\n87.5\nHMMT 2025 (Pass@1)\n41.7\n79.4\nCNMO 2024 (Pass@1)\n78.8\n86.9\nTools\nBFCL_v3_MultiTurn (Acc)\n-\n37.0\nTau-Bench   (Pass@1)\n-\n53.5(Airline)/63.9(Retail)\nNote: We use Agentless framework to evaluate model performance on SWE-Verified. We only evaluate text-only prompts in HLE testsets.  GPT-4.1 is employed to act user role in Tau-bench evaluation.\nDeepSeek-R1-0528-Qwen3-8B\nMeanwhile, we distilled the chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B. This model achieves state-of-the-art (SOTA) performance among open-source models on the AIME 2024, surpassing Qwen3 8B by +10.0% and matching the performance of Qwen3-235B-thinking. We believe that the chain-of-thought from DeepSeek-R1-0528 will hold significant importance for both academic research on reasoning models and industrial development focused on small-scale models.\nAIME 24\nAIME 25\nHMMT Feb 25\nGPQA Diamond\nLiveCodeBench (2408-2505)\nQwen3-235B-A22B\n85.7\n81.5\n62.5\n71.1\n66.5\nQwen3-32B\n81.4\n72.9\n-\n68.4\n-\nQwen3-8B\n76.0\n67.3\n-\n62.0\n-\nPhi-4-Reasoning-Plus-14B\n81.3\n78.0\n53.6\n69.3\n-\nGemini-2.5-Flash-Thinking-0520\n82.3\n72.0\n64.2\n82.8\n62.3\no3-mini (medium)\n79.6\n76.7\n53.3\n76.8\n65.9\nDeepSeek-R1-0528-Qwen3-8B\n86.0\n76.3\n61.5\n61.1\n60.5\n3. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek's official website: chat.deepseek.com, and switch on the button \"DeepThink\"\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com\n4. How to Run Locally\nPlease visit DeepSeek-R1 repository for more information about running DeepSeek-R1-0528 locally.\nCompared to previous versions of DeepSeek-R1, the usage recommendations for DeepSeek-R1-0528 have the following changes:\nSystem prompt is supported now.\nIt is not required to add \"<think>\\n\" at the beginning of the output to force the model into thinking pattern.\nThe model architecture of DeepSeek-R1-0528-Qwen3-8B is identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528. This model can be run in the same manner as Qwen3-8B, but it is essential to ensure that all configuration files are sourced from our repository rather than the original Qwen3 project.\nSystem Prompt\nIn the official DeepSeek web/app, we use the same system prompt with a specific date.\nè¯¥åŠ©æ‰‹ä¸ºDeepSeek-R1ï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ ã€‚\nä»Šå¤©æ˜¯{current date}ã€‚\nFor example,\nè¯¥åŠ©æ‰‹ä¸ºDeepSeek-R1ï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ ã€‚\nä»Šå¤©æ˜¯2025å¹´5æœˆ28æ—¥ï¼Œæ˜ŸæœŸä¸€ã€‚\nTemperature\nIn our web and application environments, the temperature parameter $T_{model}$ is set to 0.6.\nPrompts for File Uploading and Web Search\nFor file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments.\nfile_template = \\\n\"\"\"[file name]: {file_name}\n[file content begin]\n{file_content}\n[file content end]\n{question}\"\"\"\nFor Web Search, {search_results}, {cur_date}, and {question} are arguments.\nFor Chinese query, we use the prompt:\nsearch_answer_zh_template = \\\n'''# ä»¥ä¸‹å†…å®¹æ˜¯åŸºäºŽç”¨æˆ·å‘é€çš„æ¶ˆæ¯çš„æœç´¢ç»“æžœ:\n{search_results}\nåœ¨æˆ‘ç»™ä½ çš„æœç´¢ç»“æžœä¸­ï¼Œæ¯ä¸ªç»“æžœéƒ½æ˜¯[webpage X begin]...[webpage X end]æ ¼å¼çš„ï¼ŒXä»£è¡¨æ¯ç¯‡æ–‡ç« çš„æ•°å­—ç´¢å¼•ã€‚è¯·åœ¨é€‚å½“çš„æƒ…å†µä¸‹åœ¨å¥å­æœ«å°¾å¼•ç”¨ä¸Šä¸‹æ–‡ã€‚è¯·æŒ‰ç…§å¼•ç”¨ç¼–å·[citation:X]çš„æ ¼å¼åœ¨ç­”æ¡ˆä¸­å¯¹åº”éƒ¨åˆ†å¼•ç”¨ä¸Šä¸‹æ–‡ã€‚å¦‚æžœä¸€å¥è¯æºè‡ªå¤šä¸ªä¸Šä¸‹æ–‡ï¼Œè¯·åˆ—å‡ºæ‰€æœ‰ç›¸å…³çš„å¼•ç”¨ç¼–å·ï¼Œä¾‹å¦‚[citation:3][citation:5]ï¼Œåˆ‡è®°ä¸è¦å°†å¼•ç”¨é›†ä¸­åœ¨æœ€åŽè¿”å›žå¼•ç”¨ç¼–å·ï¼Œè€Œæ˜¯åœ¨ç­”æ¡ˆå¯¹åº”éƒ¨åˆ†åˆ—å‡ºã€‚\nåœ¨å›žç­”æ—¶ï¼Œè¯·æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š\n- ä»Šå¤©æ˜¯{cur_date}ã€‚\n- å¹¶éžæœç´¢ç»“æžœçš„æ‰€æœ‰å†…å®¹éƒ½ä¸Žç”¨æˆ·çš„é—®é¢˜å¯†åˆ‡ç›¸å…³ï¼Œä½ éœ€è¦ç»“åˆé—®é¢˜ï¼Œå¯¹æœç´¢ç»“æžœè¿›è¡Œç”„åˆ«ã€ç­›é€‰ã€‚\n- å¯¹äºŽåˆ—ä¸¾ç±»çš„é—®é¢˜ï¼ˆå¦‚åˆ—ä¸¾æ‰€æœ‰èˆªç­ä¿¡æ¯ï¼‰ï¼Œå°½é‡å°†ç­”æ¡ˆæŽ§åˆ¶åœ¨10ä¸ªè¦ç‚¹ä»¥å†…ï¼Œå¹¶å‘Šè¯‰ç”¨æˆ·å¯ä»¥æŸ¥çœ‹æœç´¢æ¥æºã€èŽ·å¾—å®Œæ•´ä¿¡æ¯ã€‚ä¼˜å…ˆæä¾›ä¿¡æ¯å®Œæ•´ã€æœ€ç›¸å…³çš„åˆ—ä¸¾é¡¹ï¼›å¦‚éžå¿…è¦ï¼Œä¸è¦ä¸»åŠ¨å‘Šè¯‰ç”¨æˆ·æœç´¢ç»“æžœæœªæä¾›çš„å†…å®¹ã€‚\n- å¯¹äºŽåˆ›ä½œç±»çš„é—®é¢˜ï¼ˆå¦‚å†™è®ºæ–‡ï¼‰ï¼Œè¯·åŠ¡å¿…åœ¨æ­£æ–‡çš„æ®µè½ä¸­å¼•ç”¨å¯¹åº”çš„å‚è€ƒç¼–å·ï¼Œä¾‹å¦‚[citation:3][citation:5]ï¼Œä¸èƒ½åªåœ¨æ–‡ç« æœ«å°¾å¼•ç”¨ã€‚ä½ éœ€è¦è§£è¯»å¹¶æ¦‚æ‹¬ç”¨æˆ·çš„é¢˜ç›®è¦æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„æ ¼å¼ï¼Œå……åˆ†åˆ©ç”¨æœç´¢ç»“æžœå¹¶æŠ½å–é‡è¦ä¿¡æ¯ï¼Œç”Ÿæˆç¬¦åˆç”¨æˆ·è¦æ±‚ã€æžå…·æ€æƒ³æ·±åº¦ã€å¯Œæœ‰åˆ›é€ åŠ›ä¸Žä¸“ä¸šæ€§çš„ç­”æ¡ˆã€‚ä½ çš„åˆ›ä½œç¯‡å¹…éœ€è¦å°½å¯èƒ½å»¶é•¿ï¼Œå¯¹äºŽæ¯ä¸€ä¸ªè¦ç‚¹çš„è®ºè¿°è¦æŽ¨æµ‹ç”¨æˆ·çš„æ„å›¾ï¼Œç»™å‡ºå°½å¯èƒ½å¤šè§’åº¦çš„å›žç­”è¦ç‚¹ï¼Œä¸”åŠ¡å¿…ä¿¡æ¯é‡å¤§ã€è®ºè¿°è¯¦å°½ã€‚\n- å¦‚æžœå›žç­”å¾ˆé•¿ï¼Œè¯·å°½é‡ç»“æž„åŒ–ã€åˆ†æ®µè½æ€»ç»“ã€‚å¦‚æžœéœ€è¦åˆ†ç‚¹ä½œç­”ï¼Œå°½é‡æŽ§åˆ¶åœ¨5ä¸ªç‚¹ä»¥å†…ï¼Œå¹¶åˆå¹¶ç›¸å…³çš„å†…å®¹ã€‚\n- å¯¹äºŽå®¢è§‚ç±»çš„é—®ç­”ï¼Œå¦‚æžœé—®é¢˜çš„ç­”æ¡ˆéžå¸¸ç®€çŸ­ï¼Œå¯ä»¥é€‚å½“è¡¥å……ä¸€åˆ°ä¸¤å¥ç›¸å…³ä¿¡æ¯ï¼Œä»¥ä¸°å¯Œå†…å®¹ã€‚\n- ä½ éœ€è¦æ ¹æ®ç”¨æˆ·è¦æ±‚å’Œå›žç­”å†…å®¹é€‰æ‹©åˆé€‚ã€ç¾Žè§‚çš„å›žç­”æ ¼å¼ï¼Œç¡®ä¿å¯è¯»æ€§å¼ºã€‚\n- ä½ çš„å›žç­”åº”è¯¥ç»¼åˆå¤šä¸ªç›¸å…³ç½‘é¡µæ¥å›žç­”ï¼Œä¸èƒ½é‡å¤å¼•ç”¨ä¸€ä¸ªç½‘é¡µã€‚\n- é™¤éžç”¨æˆ·è¦æ±‚ï¼Œå¦åˆ™ä½ å›žç­”çš„è¯­è¨€éœ€è¦å’Œç”¨æˆ·æé—®çš„è¯­è¨€ä¿æŒä¸€è‡´ã€‚\n# ç”¨æˆ·æ¶ˆæ¯ä¸ºï¼š\n{question}'''\nFor English query, we use the prompt:\nsearch_answer_en_template = \\\n'''# The following contents are the search results related to the user's message:\n{search_results}\nIn the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.\nWhen responding, please keep the following points in mind:\n- Today is {cur_date}.\n- Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.\n- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.\n- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.\n- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.\n- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.\n- Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.\n- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.\n- Unless the user requests otherwise, your response should be in the same language as the user's question.\n# The user's message is:\n{question}'''\n5. License\nThis code repository is licensed under MIT License. The use of DeepSeek-R1 models is also subject to MIT License. DeepSeek-R1 series (including Base and Chat) supports commercial use and distillation.\n6. Citation\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\ntitle={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},\nauthor={DeepSeek-AI},\nyear={2025},\neprint={2501.12948},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.12948},\n}\n7. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "lerobot/smolvla_base": "SmolVLA: A vision-language-action model for affordable and efficient robotics\nSmolVLA: A vision-language-action model for affordable and efficient robotics\nResources and technical documentation:\nSmolVLA Paper\nSmolVLA Blogpost\nCode\nTrain using Google Colab Notebook\nSmolVLA HF Documentation\nDesigned by Hugging Face.\nThis model has 450M parameters in total.\nYou can use inside the LeRobot library.\nBefore proceeding to the next steps, you need to properly install the environment by following Installation Guide on the docs.\nInstall smolvla extra dependencies:\npip install -e \".[smolvla]\"\nExample of finetuning the smolvla pretrained model (smolvla_base):\npython lerobot/scripts/train.py \\\n--policy.path=lerobot/smolvla_base \\\n--dataset.repo_id=lerobot/svla_so101_pickplace \\\n--batch_size=64 \\\n--steps=20000 \\\n--output_dir=outputs/train/my_smolvla \\\n--job_name=my_smolvla_training \\\n--policy.device=cuda \\\n--wandb.enable=true\nExample of finetuning the smolvla neural network with pretrained VLM and action expert\nintialized from scratch:\npython lerobot/scripts/train.py \\\n--dataset.repo_id=lerobot/svla_so101_pickplace \\\n--batch_size=64 \\\n--steps=200000 \\\n--output_dir=outputs/train/my_smolvla \\\n--job_name=my_smolvla_training \\\n--policy.device=cuda \\\n--wandb.enable=true",
    "Qwen/Qwen3-Embedding-4B": "Qwen3-Embedding-4B\nHighlights\nModel Overview\nQwen3 Embedding Series Model list\nUsage\nSentence Transformers Usage\nTransformers Usage\nvLLM Usage\nText Embeddings Inference (TEI) Usage\nEvaluation\nMTEB (Multilingual)\nMTEB (Eng v2)\nC-MTEB (MTEB Chinese)\nCitation\nQwen3-Embedding-4B\nHighlights\nThe Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings and reranking models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.\nExceptional Versatility: The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks No.1 in the MTEB multilingual leaderboard (as of June 5, 2025, score 70.58), while the reranking model excels in various text retrieval scenarios.\nComprehensive Flexibility: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for both embedding and reranking models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, the embedding model allows for flexible vector definitions across all dimensions, and both embedding and reranking models support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.\nMultilingual Capability: The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities.\nModel Overview\nQwen3-Embedding-4B has the following features:\nModel Type: Text Embedding\nSupported Languages: 100+ Languages\nNumber of Paramaters: 4B\nContext Length: 32k\nEmbedding Dimension: Up to 2560, supports user-defined output dimensions ranging from 32 to 2560\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub.\nQwen3 Embedding Series Model list\nModel Type\nModels\nSize\nLayers\nSequence Length\nEmbedding Dimension\nMRL Support\nInstruction Aware\nText Embedding\nQwen3-Embedding-0.6B\n0.6B\n28\n32K\n1024\nYes\nYes\nText Embedding\nQwen3-Embedding-4B\n4B\n36\n32K\n2560\nYes\nYes\nText Embedding\nQwen3-Embedding-8B\n8B\n36\n32K\n4096\nYes\nYes\nText Reranking\nQwen3-Reranker-0.6B\n0.6B\n28\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-4B\n4B\n36\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-8B\n8B\n36\n32K\n-\n-\nYes\nNote:\nMRL Support indicates whether the embedding model supports custom dimensions for the final embedding.\nInstruction Aware notes whether the embedding or reranking model supports customizing the input instruction according to different tasks.\nOur evaluation indicates that, for most downstream tasks, using instructions (instruct) typically yields an improvement of 1% to 5% compared to not using them. Therefore, we recommend that developers create tailored instructions specific to their tasks and scenarios. In multilingual contexts, we also advise users to write their instructions in English, as most instructions utilized during the model training process were originally written in English.\nUsage\nWith Transformers versions earlier than 4.51.0, you may encounter the following error:\nKeyError: 'qwen3'\nSentence Transformers Usage\n# Requires transformers>=4.51.0\n# Requires sentence-transformers>=2.7.0\nfrom sentence_transformers import SentenceTransformer\n# Load the model\nmodel = SentenceTransformer(\"Qwen/Qwen3-Embedding-4B\")\n# We recommend enabling flash_attention_2 for better acceleration and memory saving,\n# together with setting `padding_side` to \"left\":\n# model = SentenceTransformer(\n#     \"Qwen/Qwen3-Embedding-4B\",\n#     model_kwargs={\"attn_implementation\": \"flash_attention_2\", \"device_map\": \"auto\"},\n#     tokenizer_kwargs={\"padding_side\": \"left\"},\n# )\n# The queries and documents to embed\nqueries = [\n\"What is the capital of China?\",\n\"Explain gravity\",\n]\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n]\n# Encode the queries and documents. Note that queries benefit from using a prompt\n# Here we use the prompt called \"query\" stored under `model.prompts`, but you can\n# also pass your own prompt via the `prompt` argument\nquery_embeddings = model.encode(queries, prompt_name=\"query\")\ndocument_embeddings = model.encode(documents)\n# Compute the (cosine) similarity between the query and document embeddings\nsimilarity = model.similarity(query_embeddings, document_embeddings)\nprint(similarity)\n# tensor([[0.7534, 0.1147],\n#         [0.0320, 0.6258]])\nTransformers Usage\n# Requires transformers>=4.51.0\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ndef last_token_pool(last_hidden_states: Tensor,\nattention_mask: Tensor) -> Tensor:\nleft_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\nif left_padding:\nreturn last_hidden_states[:, -1]\nelse:\nsequence_lengths = attention_mask.sum(dim=1) - 1\nbatch_size = last_hidden_states.shape[0]\nreturn last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery:{query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'What is the capital of China?'),\nget_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-4B', padding_side='left')\nmodel = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-4B')\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-4B', attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16).cuda()\nmax_length = 8192\n# Tokenize the input texts\nbatch_dict = tokenizer(\ninput_texts,\npadding=True,\ntruncation=True,\nmax_length=max_length,\nreturn_tensors=\"pt\",\n)\nbatch_dict.to(model.device)\noutputs = model(**batch_dict)\nembeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.7534257769584656, 0.1146894246339798], [0.03198453038930893, 0.6258305311203003]]\nvLLM Usage\n# Requires vllm>=0.8.5\nimport torch\nimport vllm\nfrom vllm import LLM\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery:{query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'What is the capital of China?'),\nget_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"The capital of China is Beijing.\",\n\"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\nmodel = LLM(model=\"Qwen/Qwen3-Embedding-4B\", task=\"embed\")\noutputs = model.embed(input_texts)\nembeddings = torch.tensor([o.outputs.embedding for o in outputs])\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.7525103688240051, 0.1143278032541275], [0.030893627554178238, 0.6239761114120483]]\nðŸ“Œ Tip: We recommend that developers customize the instruct according to their specific scenarios, tasks, and languages. Our tests have shown that in most retrieval scenarios, not using an instruct on the query side can lead to a drop in retrieval performance by approximately 1% to 5%.\nText Embeddings Inference (TEI) Usage\nYou can either run / deploy TEI on NVIDIA GPUs as:\ndocker run --gpus all -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7.2 --model-id Qwen/Qwen3-Embedding-4B --dtype float16\nOr on CPU devices as:\ndocker run -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-1.7.2 --model-id Qwen/Qwen3-Embedding-4B --dtype float16\nAnd then, generate the embeddings sending a HTTP POST request as:\ncurl http://localhost:8080/embed \\\n-X POST \\\n-d '{\"inputs\": [\"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: What is the capital of China?\", \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: Explain gravity\"]}' \\\n-H \"Content-Type: application/json\"\nEvaluation\nMTEB (Multilingual)\nModel\nSize\nMean (Task)\nMean (Type)\nBitxt Mining\nClass.\nClust.\nInst. Retri.\nMulti. Class.\nPair. Class.\nRerank\nRetri.\nSTS\nNV-Embed-v2\n7B\n56.29\n49.58\n57.84\n57.29\n40.80\n1.04\n18.63\n78.94\n63.82\n56.72\n71.10\nGritLM-7B\n7B\n60.92\n53.74\n70.53\n61.83\n49.75\n3.45\n22.77\n79.94\n63.78\n58.31\n73.33\nBGE-M3\n0.6B\n59.56\n52.18\n79.11\n60.35\n40.88\n-3.11\n20.1\n80.76\n62.79\n54.60\n74.12\nmultilingual-e5-large-instruct\n0.6B\n63.22\n55.08\n80.13\n64.94\n50.75\n-0.40\n22.91\n80.86\n62.61\n57.12\n76.81\ngte-Qwen2-1.5B-instruct\n1.5B\n59.45\n52.69\n62.51\n58.32\n52.05\n0.74\n24.02\n81.58\n62.58\n60.78\n71.61\ngte-Qwen2-7b-Instruct\n7B\n62.51\n55.93\n73.92\n61.55\n52.77\n4.94\n25.48\n85.13\n65.55\n60.08\n73.98\ntext-embedding-3-large\n-\n58.93\n51.41\n62.17\n60.27\n46.89\n-2.68\n22.03\n79.17\n63.89\n59.27\n71.68\nCohere-embed-multilingual-v3.0\n-\n61.12\n53.23\n70.50\n62.95\n46.89\n-1.89\n22.74\n79.88\n64.07\n59.16\n74.80\ngemini-embedding-exp-03-07\n-\n68.37\n59.59\n79.28\n71.82\n54.59\n5.18\n29.16\n83.63\n65.58\n67.71\n79.40\nQwen3-Embedding-0.6B\n0.6B\n64.33\n56.00\n72.22\n66.83\n52.33\n5.09\n24.59\n80.83\n61.41\n64.64\n76.17\nQwen3-Embedding-4B\n4B\n69.45\n60.86\n79.36\n72.33\n57.15\n11.56\n26.77\n85.05\n65.08\n69.60\n80.86\nQwen3-Embedding-8B\n8B\n70.58\n61.69\n80.89\n74.00\n57.65\n10.06\n28.66\n86.40\n65.63\n70.88\n81.08\nNote: For compared models, the scores are retrieved from MTEB online leaderboard on May 24th, 2025.\nMTEB (Eng v2)\nMTEB English / Models\nParam.\nMean(Task)\nMean(Type)\nClass.\nClust.\nPair Class.\nRerank.\nRetri.\nSTS\nSumm.\nmultilingual-e5-large-instruct\n0.6B\n65.53\n61.21\n75.54\n49.89\n86.24\n48.74\n53.47\n84.72\n29.89\nNV-Embed-v2\n7.8B\n69.81\n65.00\n87.19\n47.66\n88.69\n49.61\n62.84\n83.82\n35.21\nGritLM-7B\n7.2B\n67.07\n63.22\n81.25\n50.82\n87.29\n49.59\n54.95\n83.03\n35.65\ngte-Qwen2-1.5B-instruct\n1.5B\n67.20\n63.26\n85.84\n53.54\n87.52\n49.25\n50.25\n82.51\n33.94\nstella_en_1.5B_v5\n1.5B\n69.43\n65.32\n89.38\n57.06\n88.02\n50.19\n52.42\n83.27\n36.91\ngte-Qwen2-7B-instruct\n7.6B\n70.72\n65.77\n88.52\n58.97\n85.9\n50.47\n58.09\n82.69\n35.74\ngemini-embedding-exp-03-07\n-\n73.3\n67.67\n90.05\n59.39\n87.7\n48.59\n64.35\n85.29\n38.28\nQwen3-Embedding-0.6B\n0.6B\n70.70\n64.88\n85.76\n54.05\n84.37\n48.18\n61.83\n86.57\n33.43\nQwen3-Embedding-4B\n4B\n74.60\n68.10\n89.84\n57.51\n87.01\n50.76\n68.46\n88.72\n34.39\nQwen3-Embedding-8B\n8B\n75.22\n68.71\n90.43\n58.57\n87.52\n51.56\n69.44\n88.58\n34.83\nC-MTEB (MTEB Chinese)\nC-MTEB\nParam.\nMean(Task)\nMean(Type)\nClass.\nClust.\nPair Class.\nRerank.\nRetr.\nSTS\nmultilingual-e5-large-instruct\n0.6B\n58.08\n58.24\n69.80\n48.23\n64.52\n57.45\n63.65\n45.81\nbge-multilingual-gemma2\n9B\n67.64\n68.52\n75.31\n59.30\n86.67\n68.28\n73.73\n55.19\ngte-Qwen2-1.5B-instruct\n1.5B\n67.12\n67.79\n72.53\n54.61\n79.5\n68.21\n71.86\n60.05\ngte-Qwen2-7B-instruct\n7.6B\n71.62\n72.19\n75.77\n66.06\n81.16\n69.24\n75.70\n65.20\nritrieve_zh_v1\n0.3B\n72.71\n73.85\n76.88\n66.5\n85.98\n72.86\n76.97\n63.92\nQwen3-Embedding-0.6B\n0.6B\n66.33\n67.45\n71.40\n68.74\n76.42\n62.58\n71.03\n54.52\nQwen3-Embedding-4B\n4B\n72.27\n73.51\n75.46\n77.89\n83.34\n66.05\n77.03\n61.26\nQwen3-Embedding-8B\n8B\n73.84\n75.00\n76.97\n80.08\n84.23\n66.99\n78.21\n63.53\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{qwen3embedding,\ntitle={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\nauthor={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\njournal={arXiv preprint arXiv:2506.05176},\nyear={2025}\n}",
    "Qwen/Qwen3-Embedding-0.6B-GGUF": "Qwen3-Embedding-0.6B-GGUF\nHighlights\nModel Overview\nQwen3 Embedding Series Model list\nUsage\nllama.cpp\nEvaluation\nMTEB (Multilingual)\nMTEB (Eng v2)\nC-MTEB (MTEB Chinese)\nCitation\nQwen3-Embedding-0.6B-GGUF\nHighlights\nThe Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings and reranking models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.\nExceptional Versatility: The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks No.1 in the MTEB multilingual leaderboard (as of June 5, 2025, score 70.58), while the reranking model excels in various text retrieval scenarios.\nComprehensive Flexibility: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for both embedding and reranking models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, the embedding model allows for flexible vector definitions across all dimensions, and both embedding and reranking models support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.\nMultilingual Capability: The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities.\nModel Overview\nQwen3-Embedding-0.6B-GGUF has the following features:\nModel Type: Text Embedding\nSupported Languages: 100+ Languages\nNumber of Paramaters: 0.6B\nContext Length: 32k\nEmbedding Dimension: Up to 1024, supports user-defined output dimensions ranging from 32 to 1024\nQuantization: q8_0, f16\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub.\nQwen3 Embedding Series Model list\nModel Type\nModels\nSize\nLayers\nSequence Length\nEmbedding Dimension\nMRL Support\nInstruction Aware\nText Embedding\nQwen3-Embedding-0.6B\n0.6B\n28\n32K\n1024\nYes\nYes\nText Embedding\nQwen3-Embedding-4B\n4B\n36\n32K\n2560\nYes\nYes\nText Embedding\nQwen3-Embedding-8B\n8B\n36\n32K\n4096\nYes\nYes\nText Reranking\nQwen3-Reranker-0.6B\n0.6B\n28\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-4B\n4B\n36\n32K\n-\n-\nYes\nText Reranking\nQwen3-Reranker-8B\n8B\n36\n32K\n-\n-\nYes\nNote:\nMRL Support indicates whether the embedding model supports custom dimensions for the final embedding.\nInstruction Aware notes whether the embedding or reranking model supports customizing the input instruction according to different tasks.\nOur evaluation indicates that, for most downstream tasks, using instructions (instruct) typically yields an improvement of 1% to 5% compared to not using them. Therefore, we recommend that developers create tailored instructions specific to their tasks and scenarios. In multilingual contexts, we also advise users to write their instructions in English, as most instructions utilized during the model training process were originally written in English.\nUsage\nðŸ“Œ Tip: We recommend that developers customize the instruct according to their specific scenarios, tasks, and languages. Our tests have shown that in most retrieval scenarios, not using an instruct on the query side can lead to a drop in retrieval performance by approximately 1% to 5%.\nllama.cpp\nCheck out our llama.cpp documentation for more usage guide.\nWe advise you to clone llama.cpp and install it following the official guide. We follow the latest version of llama.cpp.\nIn the following demonstration, we assume that you are running commands under the repository llama.cpp.\nYou can run Qwen3 Embedding with one command:\n./build/bin/llama-embedding -m model.gguf  -p \"<your context here>\"  --pooling last --verbose-prompt\nOr launch a server:\n./build/bin/llama-server -m model.gguf --embedding --pooling last -ub 8192 --verbose-prompt\nEvaluation\nMTEB (Multilingual)\nModel\nSize\nMean (Task)\nMean (Type)\nBitxt Mining\nClass.\nClust.\nInst. Retri.\nMulti. Class.\nPair. Class.\nRerank\nRetri.\nSTS\nNV-Embed-v2\n7B\n56.29\n49.58\n57.84\n57.29\n40.80\n1.04\n18.63\n78.94\n63.82\n56.72\n71.10\nGritLM-7B\n7B\n60.92\n53.74\n70.53\n61.83\n49.75\n3.45\n22.77\n79.94\n63.78\n58.31\n73.33\nBGE-M3\n0.6B\n59.56\n52.18\n79.11\n60.35\n40.88\n-3.11\n20.1\n80.76\n62.79\n54.60\n74.12\nmultilingual-e5-large-instruct\n0.6B\n63.22\n55.08\n80.13\n64.94\n50.75\n-0.40\n22.91\n80.86\n62.61\n57.12\n76.81\ngte-Qwen2-1.5B-instruct\n1.5B\n59.45\n52.69\n62.51\n58.32\n52.05\n0.74\n24.02\n81.58\n62.58\n60.78\n71.61\ngte-Qwen2-7b-Instruct\n7B\n62.51\n55.93\n73.92\n61.55\n52.77\n4.94\n25.48\n85.13\n65.55\n60.08\n73.98\ntext-embedding-3-large\n-\n58.93\n51.41\n62.17\n60.27\n46.89\n-2.68\n22.03\n79.17\n63.89\n59.27\n71.68\nCohere-embed-multilingual-v3.0\n-\n61.12\n53.23\n70.50\n62.95\n46.89\n-1.89\n22.74\n79.88\n64.07\n59.16\n74.80\nGemini Embedding\n-\n68.37\n59.59\n79.28\n71.82\n54.59\n5.18\n29.16\n83.63\n65.58\n67.71\n79.40\nQwen3-Embedding-0.6B\n0.6B\n64.33\n56.00\n72.22\n66.83\n52.33\n5.09\n24.59\n80.83\n61.41\n64.64\n76.17\nQwen3-Embedding-4B\n4B\n69.45\n60.86\n79.36\n72.33\n57.15\n11.56\n26.77\n85.05\n65.08\n69.60\n80.86\nQwen3-Embedding-8B\n8B\n70.58\n61.69\n80.89\n74.00\n57.65\n10.06\n28.66\n86.40\n65.63\n70.88\n81.08\nNote: For compared models, the scores are retrieved from MTEB online leaderboard on May 24th, 2025.\nMTEB (Eng v2)\nMTEB English / Models\nParam.\nMean(Task)\nMean(Type)\nClass.\nClust.\nPair Class.\nRerank.\nRetri.\nSTS\nSumm.\nmultilingual-e5-large-instruct\n0.6B\n65.53\n61.21\n75.54\n49.89\n86.24\n48.74\n53.47\n84.72\n29.89\nNV-Embed-v2\n7.8B\n69.81\n65.00\n87.19\n47.66\n88.69\n49.61\n62.84\n83.82\n35.21\nGritLM-7B\n7.2B\n67.07\n63.22\n81.25\n50.82\n87.29\n49.59\n54.95\n83.03\n35.65\ngte-Qwen2-1.5B-instruct\n1.5B\n67.20\n63.26\n85.84\n53.54\n87.52\n49.25\n50.25\n82.51\n33.94\nstella_en_1.5B_v5\n1.5B\n69.43\n65.32\n89.38\n57.06\n88.02\n50.19\n52.42\n83.27\n36.91\ngte-Qwen2-7B-instruct\n7.6B\n70.72\n65.77\n88.52\n58.97\n85.9\n50.47\n58.09\n82.69\n35.74\ngemini-embedding-exp-03-07\n-\n73.3\n67.67\n90.05\n59.39\n87.7\n48.59\n64.35\n85.29\n38.28\nQwen3-Embedding-0.6B\n0.6B\n70.70\n64.88\n85.76\n54.05\n84.37\n48.18\n61.83\n86.57\n33.43\nQwen3-Embedding-4B\n4B\n74.60\n68.10\n89.84\n57.51\n87.01\n50.76\n68.46\n88.72\n34.39\nQwen3-Embedding-8B\n8B\n75.22\n68.71\n90.43\n58.57\n87.52\n51.56\n69.44\n88.58\n34.83\nC-MTEB (MTEB Chinese)\nC-MTEB\nParam.\nMean(Task)\nMean(Type)\nClass.\nClust.\nPair Class.\nRerank.\nRetr.\nSTS\nmultilingual-e5-large-instruct\n0.6B\n58.08\n58.24\n69.80\n48.23\n64.52\n57.45\n63.65\n45.81\nbge-multilingual-gemma2\n9B\n67.64\n75.31\n59.30\n86.67\n68.28\n73.73\n55.19\n-\ngte-Qwen2-1.5B-instruct\n1.5B\n67.12\n67.79\n72.53\n54.61\n79.5\n68.21\n71.86\n60.05\ngte-Qwen2-7B-instruct\n7.6B\n71.62\n72.19\n75.77\n66.06\n81.16\n69.24\n75.70\n65.20\nritrieve_zh_v1\n0.3B\n72.71\n73.85\n76.88\n66.5\n85.98\n72.86\n76.97\n63.92\nQwen3-Embedding-0.6B\n0.6B\n66.33\n67.45\n71.40\n68.74\n76.42\n62.58\n71.03\n54.52\nQwen3-Embedding-4B\n4B\n72.27\n73.51\n75.46\n77.89\n83.34\n66.05\n77.03\n61.26\nQwen3-Embedding-8B\n8B\n73.84\n75.00\n76.97\n80.08\n84.23\n66.99\n78.21\n63.53\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{qwen3embedding,\ntitle={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\nauthor={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\njournal={arXiv preprint arXiv:2506.05176},\nyear={2025}\n}",
    "google/gemma-3n-E2B-it": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, youâ€™re required to review and agree to Googleâ€™s usage license. To do this, please ensure youâ€™re logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 3n model card\nModel Information\nDescription\nInputs and outputs\nUsage\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nThis repository corresponds to the launch version of Gemma 3n E2B IT (Instruct), to be used with Hugging Face transformers,\nsupporting text, audio, and vision (image and video) inputs.\nGemma 3n models have multiple architecture innovations:\nThey are available in two sizes based on effective parameters. While the raw parameter count of this model is 6B, the architecture design allows the model to be run with a memory footprint comparable to a traditional 2B model by offloading low-utilization matrices from the accelerator.\nThey use a MatFormer architecture that allows nesting sub-models within the E4B model. We provide one sub-model (this model repository), or you can access a spectrum of custom-sized models using the Mix-and-Match method.\nLearn more about these techniques in the technical blog post\nand the Gemma documentation.\nGemma 3n model card\nModel Page: Gemma 3n\nResources and Technical Documentation:\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on HuggingFace\nGemma on Vertex Model Garden\nTerms of Use: TermsAuthors: Google DeepMind\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nGemma 3n models are designed for efficient execution on low-resource devices.\nThey are capable of multimodal input, handling text, image, video, and audio\ninput, and generating text outputs, with open weights for pre-trained and\ninstruction-tuned variants. These models were trained with data in over 140\nspoken languages.\nGemma 3n models use selective parameter activation technology to reduce resource\nrequirements. This technique allows the models to operate at an effective size\nof 2B and 4B parameters, which is lower than the total number of parameters they\ncontain. For more information on Gemma 3n's efficient parameter management\ntechnology, see the\nGemma 3n\npage.\nInputs and outputs\nInput:\nText string, such as a question, a prompt, or a document to be\nsummarized\nImages, normalized to 256x256, 512x512, or 768x768 resolution\nand encoded to 256 tokens each\nAudio data encoded to 6.25 tokens per second from a single channel\nTotal input context of 32K tokens\nOutput:\nGenerated text in response to the input, such as an answer to a\nquestion, analysis of image content, or a summary of a document\nTotal output length up to 32K tokens, subtracting the request\ninput tokens\nUsage\nBelow, there are some code snippets on how to get quickly started with running\nthe model. First, install the Transformers library. Gemma 3n is supported\nstarting from transformers 4.53.0.\n$ pip install -U transformers\nThen, copy the snippet from the section that is relevant for your use case.\nRunning with the pipeline API\nYou can initialize the model and processor for inference with pipeline as\nfollows.\nfrom transformers import pipeline\nimport torch\npipe = pipeline(\n\"image-text-to-text\",\nmodel=\"google/gemma-3n-e2b-it\",\ndevice=\"cuda\",\ntorch_dtype=torch.bfloat16,\n)\nWith instruction-tuned models, you need to use chat templates to process our\ninputs first. Then, you can pass it to the pipeline.\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n{\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n]\n}\n]\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n# Okay, let's take a look!\n# Based on the image, the animal on the candy is a **turtle**.\n# You can see the shell shape and the head and legs.\nRunning the model on a single GPU\nfrom transformers import AutoProcessor, Gemma3nForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/gemma-3n-e2b-it\"\nmodel = Gemma3nForConditionalGeneration.from_pretrained(model_id, device=\"cuda\", torch_dtype=torch.bfloat16,).eval()\nprocessor = AutoProcessor.from_pretrained(model_id)\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n{\"type\": \"text\", \"text\": \"Describe this image in detail.\"}\n]\n}\n]\ninputs = processor.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_dict=True,\nreturn_tensors=\"pt\",\n).to(model.device, dtype=torch.bfloat16)\ninput_len = inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n# **Overall Impression:** The image is a close-up shot of a vibrant garden scene,\n# focusing on a cluster of pink cosmos flowers and a busy bumblebee.\n# It has a slightly soft, natural feel, likely captured in daylight.\nCitation\n@article{gemma_3n_2025,\ntitle={Gemma 3n},\nurl={https://ai.google.dev/gemma/docs/gemma-3n},\npublisher={Google DeepMind},\nauthor={Gemma Team},\nyear={2025}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset that includes a wide variety of sources\ntotalling approximately 11 trillion tokens. The knowledge cutoff date for the\ntraining data was June 2024. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model\nis exposed to a broad range of linguistic styles, topics, and vocabulary.\nThe training dataset includes content in over 140 languages.\nCode: Exposing the model to code helps it to learn the syntax and\npatterns of programming languages, which improves its ability to generate\ncode and understand code-related questions.\nMathematics: Training on mathematical text helps the model learn\nlogical reasoning, symbolic representation, and to address mathematical queries.\nImages: A wide range of images enables the model to perform image\nanalysis and visual data extraction tasks.\nAudio: A diverse set of sound samples enables the model to recognize\nspeech, transcribe text from recordings, and identify information in audio data.\nThe combination of these diverse data sources is crucial for training a\npowerful multimodal model that can handle a wide variety of different tasks and\ndata formats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material)\nfiltering was applied at multiple stages in the data preparation process to\nensure the exclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models\nsafe and reliable, automated techniques were used to filter out certain\npersonal information and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in\nline with\nour policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using Tensor Processing Unit\n(TPU) hardware (TPUv4p, TPUv5p\nand TPUv5e). Training generative models requires significant computational\npower. TPUs, designed specifically for matrix operations common in machine\nlearning, offer several advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive\ncomputations involved in training generative models. They can speed up\ntraining considerably compared to CPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory,\nallowing for the handling of large models and batch sizes during training.\nThis can lead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable\nsolution for handling the growing complexity of large foundation models.\nYou can distribute training across multiple TPU devices for faster and more\nefficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more\ncost-effective solution for training large models compared to CPU-based\ninfrastructure, especially when considering the time and resources saved\ndue to faster training.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and\nML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models. ML\nPathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like these ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models:\n\"the 'single controller' programming model of Jax and Pathways allows a single\nPython process to orchestrate the entire training run, dramatically simplifying\nthe development workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated at full precision (float32) against a large\ncollection of different datasets and metrics to cover different aspects of\ncontent generation. Evaluation results marked with IT are for\ninstruction-tuned models. Evaluation results marked with PT are for\npre-trained models.\nReasoning and factuality\nBenchmark\nMetric\nn-shot\nE2B PT\nE4B PT\nHellaSwag\nAccuracy\n10-shot\n72.2\n78.6\nBoolQ\nAccuracy\n0-shot\n76.4\n81.6\nPIQA\nAccuracy\n0-shot\n78.9\n81.0\nSocialIQA\nAccuracy\n0-shot\n48.8\n50.0\nTriviaQA\nAccuracy\n5-shot\n60.8\n70.2\nNatural Questions\nAccuracy\n5-shot\n15.5\n20.9\nARC-c\nAccuracy\n25-shot\n51.7\n61.6\nARC-e\nAccuracy\n0-shot\n75.8\n81.6\nWinoGrande\nAccuracy\n5-shot\n66.8\n71.7\nBIG-Bench Hard\nAccuracy\nfew-shot\n44.3\n52.9\nDROP\nToken F1 score\n1-shot\n53.9\n60.8\nMultilingual\nBenchmark\nMetric\nn-shot\nE2B IT\nE4B IT\nMGSM\nAccuracy\n0-shot\n53.1\n60.7\nWMT24++ (ChrF)\nCharacter-level F-score\n0-shot\n42.7\n50.1\nInclude\nAccuracy\n0-shot\n38.6\n57.2\nMMLU (ProX)\nAccuracy\n0-shot\n8.1\n19.9\nOpenAI MMLU\nAccuracy\n0-shot\n22.3\n35.6\nGlobal-MMLU\nAccuracy\n0-shot\n55.1\n60.3\nECLeKTic\nECLeKTic score\n0-shot\n2.5\n1.9\nSTEM and code\nBenchmark\nMetric\nn-shot\nE2B IT\nE4B IT\nGPQA Diamond\nRelaxedAccuracy/accuracy\n0-shot\n24.8\n23.7\nLiveCodeBench v5\npass@1\n0-shot\n18.6\n25.7\nCodegolf v2.2\npass@1\n0-shot\n11.0\n16.8\nAIME 2025\nAccuracy\n0-shot\n6.7\n11.6\nAdditional benchmarks\nBenchmark\nMetric\nn-shot\nE2B IT\nE4B IT\nMMLU\nAccuracy\n0-shot\n60.1\n64.9\nMBPP\npass@1\n3-shot\n56.6\n63.6\nHumanEval\npass@1\n0-shot\n66.5\n75.0\nLiveCodeBench\npass@1\n0-shot\n13.2\n13.2\nHiddenMath\nAccuracy\n0-shot\n27.7\n37.7\nGlobal-MMLU-Lite\nAccuracy\n0-shot\n59.0\n64.5\nMMLU (Pro)\nAccuracy\n0-shot\n40.5\n50.6\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild Safety: Evaluation of text-to-text and image to text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent Safety: Evaluation of text-to-text and image to text prompts\ncovering safety policies including, harassment, violence and gore, and hate\nspeech.\nRepresentational Harms: Evaluation of text-to-text and image to text\nprompts covering safety policies including bias, stereotyping, and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance\nevaluations\" which are our 'arms-length' internal evaluations for responsibility\ngovernance decision making. They are conducted separately from the model\ndevelopment team, to inform decision making about release. High level findings\nare fed back to the model team, but prompt sets are held-out to prevent\noverfitting and preserve the results' ability to inform decision making. Notable\nassurance evaluation results are reported to our Responsibility & Safety Council\nas part of release review.\nEvaluation Results\nFor all areas of safety testing, we saw safe levels of performance across the\ncategories of child safety, content safety, and representational harms relative\nto previous Gemma models. All testing was conducted without safety filters to\nevaluate the model capabilities and behaviors. For text-to-text,  image-to-text,\nand audio-to-text, and across all model sizes, the model produced minimal policy\nviolations, and showed significant improvements over previous Gemma models'\nperformance with respect to high severity violations. A limitation of our\nevaluations was they included primarily English language prompts.\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen generative models have a wide range of applications across various\nindustries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: Generate creative text formats such as\npoems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational\ninterfaces for customer service, virtual assistants, or interactive\napplications.\nText Summarization: Generate concise summaries of a text\ncorpus, research papers, or reports.\nImage Data Extraction: Extract, interpret, and summarize\nvisual data for text communications.\nAudio Data Extraction: Transcribe spoken language, translate speech\nto text in other languages, and analyze sound-based data.\nResearch and Education\nNatural Language Processing (NLP) and generative model\nResearch: These models can serve as a foundation for researchers to\nexperiment with generative models and NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\nLanguage Learning Tools: Support interactive language\nlearning experiences, aiding in grammar correction or providing writing\npractice.\nKnowledge Exploration: Assist researchers in exploring large\nbodies of data by generating summaries or answering questions about\nspecific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly\ninfluence the model's capabilities. Biases or gaps in the training data\ncan lead to limitations in the model's responses.\nThe scope of the training dataset determines the subject areas\nthe model can handle effectively.\nContext and Task Complexity\nModels are better at tasks that can be framed with clear\nprompts and instructions. Open-ended or highly complex tasks might be\nchallenging.\nA model's performance can be influenced by the amount of context\nprovided (longer context generally leads to better outputs, up to a\ncertain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. Models might struggle\nto grasp subtle nuances, sarcasm, or figurative language.\nFactual Accuracy\nModels generate responses based on information they learned\nfrom their training datasets, but they are not knowledge bases. They\nmay generate incorrect or outdated factual statements.\nCommon Sense\nModels rely on statistical patterns in language. They might\nlack the ability to apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of generative models raises several ethical concerns. In\ncreating an open model, we have carefully considered the following:\nBias and Fairness\nGenerative models trained on large-scale, real-world text and image data\ncan reflect socio-cultural biases embedded in the training material.\nThese models underwent careful scrutiny, input data pre-processing\ndescribed and posterior evaluations reported in this card.\nMisinformation and Misuse\nGenerative models can be misused to generate text that is\nfalse, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to\nshare innovation by making generative model technology accessible to\ndevelopers and researchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer\nand end-user education can help mitigate against malicious applications of\ngenerative models. Educational resources and reporting mechanisms for users\nto flag misuse are provided. Prohibited uses of Gemma models are outlined\nin the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal of\ncertain personal information and other sensitive data. Developers are\nencouraged to adhere to privacy regulations with privacy-preserving\ntechniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\ngenerative model implementations designed from the ground up for responsible AI\ndevelopment compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "mistralai/Mistral-Small-3.2-24B-Instruct-2506": "Mistral-Small-3.2-24B-Instruct-2506\nKey Features\nBenchmark Results\nText\nVision\nUsage\nvLLM (recommended)\nTransformers\nMistral-Small-3.2-24B-Instruct-2506\nMistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.\nSmall-3.2 improves in the following categories:\nInstruction following: Small-3.2 is better at following precise instructions\nRepetition errors: Small-3.2 produces less infinite generations or repetitive answers\nFunction calling: Small-3.2's function calling template is more robust (see here and examples)\nIn all other categories Small-3.2 should match or slightly improve compared to Mistral-Small-3.1-24B-Instruct-2503.\nKey Features\nsame as Mistral-Small-3.1-24B-Instruct-2503\nBenchmark Results\nWe compare Mistral-Small-3.2-24B to Mistral-Small-3.1-24B-Instruct-2503.\nFor more comparison against other models of similar size, please check Mistral-Small-3.1's Benchmarks'\nText\nInstruction Following / Chat / Tone\nModel\nWildbench v2\nArena Hard v2\nIF (Internal; accuracy)\nSmall 3.1 24B Instruct\n55.6%\n19.56%\n82.75%\nSmall 3.2 24B Instruct\n65.33%\n43.1%\n84.78%\nInfinite Generations\nSmall 3.2 reduces infinite generations by 2x on challenging, long and repetitive prompts.\nModel\nInfinite Generations (Internal; Lower is better)\nSmall 3.1 24B Instruct\n2.11%\nSmall 3.2 24B Instruct\n1.29%\nSTEM\nModel\nMMLU\nMMLU Pro (5-shot CoT)\nMATH\nGPQA Main (5-shot CoT)\nGPQA Diamond (5-shot CoT )\nMBPP Plus - Pass@5\nHumanEval Plus - Pass@5\nSimpleQA (TotalAcc)\nSmall 3.1 24B Instruct\n80.62%\n66.76%\n69.30%\n44.42%\n45.96%\n74.63%\n88.99%\n10.43%\nSmall 3.2 24B Instruct\n80.50%\n69.06%\n69.42%\n44.22%\n46.13%\n78.33%\n92.90%\n12.10%\nVision\nModel\nMMMU\nMathvista\nChartQA\nDocVQA\nAI2D\nSmall 3.1 24B Instruct\n64.00%\n68.91%\n86.24%\n94.08%\n93.72%\nSmall 3.2 24B Instruct\n62.50%\n67.09%\n87.4%\n94.86%\n92.91%\nUsage\nThe model can be used with the following frameworks;\nvllm (recommended): See here\ntransformers: See here\nNote 1: We recommend using a relatively low temperature, such as temperature=0.15.\nNote 2: Make sure to add a system prompt to the model to best tailor it to your needs. If you want to use the model as a general assistant, we recommend to use the one provided in the SYSTEM_PROMPT.txt file.\nvLLM (recommended)\nWe recommend using this model with vLLM.\nInstallation\nMake sure to install vLLM >= 0.9.1:\npip install vllm --upgrade\nDoing so should automatically install mistral_common >= 1.6.2.\nTo check:\npython -c \"import mistral_common; print(mistral_common.__version__)\"\nYou can also make use of a ready-to-go docker image or on the docker hub.\nServe\nWe recommend that you use Mistral-Small-3.2-24B-Instruct-2506 in a server/client setting.\nSpin up a server:\nvllm serve mistralai/Mistral-Small-3.2-24B-Instruct-2506 \\\n--tokenizer_mode mistral --config_format mistral \\\n--load_format mistral --tool-call-parser mistral \\\n--enable-auto-tool-choice --limit-mm-per-prompt '{\"image\":10}' \\\n--tensor-parallel-size 2\nNote: Running Mistral-Small-3.2-24B-Instruct-2506 on GPU requires ~55 GB of GPU RAM in bf16 or fp16.\nTo ping the client you can use a simple Python snippet. See the following examples.\nVision reasoning\nLeverage the vision capabilities of Mistral-Small-3.2-24B-Instruct-2506 to make the best choice given a scenario, go catch them all !\nPython snippet\nfrom datetime import datetime, timedelta\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.15\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\ntoday = datetime.today().strftime(\"%Y-%m-%d\")\nyesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\nmodel_name = repo_id.split(\"/\")[-1]\nreturn system_prompt.format(name=model_name, today=today, yesterday=yesterday)\nmodel_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438\"\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.\",\n},\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n],\n},\n]\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=messages,\ntemperature=TEMP,\nmax_tokens=MAX_TOK,\n)\nprint(response.choices[0].message.content)\n# In this situation, you are playing a PokÃ©mon game where your Pikachu (Level 42) is facing a wild Pidgey (Level 17). Here are the possible actions you can take and an analysis of each:\n# 1. **FIGHT**:\n#    - **Pros**: Pikachu is significantly higher level than the wild Pidgey, which suggests that it should be able to defeat Pidgey easily. This could be a good opportunity to gain experience points and possibly items or money.\n#    - **Cons**: There is always a small risk of Pikachu fainting, especially if Pidgey has a powerful move or a status effect that could hinder Pikachu. However, given the large level difference, this risk is minimal.\n# 2. **BAG**:\n#    - **Pros**: You might have items in your bag that could help in this battle, such as Potions, PokÃ© Balls, or Berries. Using an item could help you capture the Pidgey or heal your Pikachu if needed.\n#    - **Cons**: Using items might not be necessary given the level difference. It could be more efficient to just fight and defeat the Pidgey quickly.\n# 3. **POKÃ‰MON**:\n#    - **Pros**: You might have another PokÃ©mon in your party that is better suited for this battle or that you want to gain experience. Switching PokÃ©mon could also be a strategic move if you want to train a lower-level PokÃ©mon.\n#    - **Cons**: Switching PokÃ©mon might not be necessary since Pikachu is at a significant advantage. It could also waste time and potentially give Pidgey a turn to attack.\n# 4. **RUN**:\n#    - **Pros**: Running away could save time and conserve your PokÃ©mon's health and resources. If you are in a hurry or do not need the experience or items, running away is a safe option.\n#    - **Cons**: Running away means you miss out on the experience points and potential items or money that you could gain from defeating the Pidgey. It also means you do not get the chance to capture the Pidgey if you wanted to.\n# ### Recommendation:\n# Given the significant level advantage, the best action is likely to **FIGHT**. This will allow you to quickly defeat the Pidgey, gain experience points, and potentially earn items or money. If you are concerned about Pikachu's health, you could use an item from your **BAG** to heal it before or during the battle. Running away or switching PokÃ©mon does not seem necessary in this situation.\nFunction calling\nMistral-Small-3.2-24B-Instruct-2506 is excellent at function / tool calling tasks via vLLM. E.g.:\nPython snippet - easy\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.15\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nreturn system_prompt\nmodel_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://huggingface.co/datasets/patrickvonplaten/random_img/resolve/main/europe.png\"\ntools = [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_population\",\n\"description\": \"Get the up-to-date population of a given country.\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"country\": {\n\"type\": \"string\",\n\"description\": \"The country to find the population of.\",\n},\n\"unit\": {\n\"type\": \"string\",\n\"description\": \"The unit for the population.\",\n\"enum\": [\"millions\", \"thousands\"],\n},\n},\n\"required\": [\"country\", \"unit\"],\n},\n},\n},\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"rewrite\",\n\"description\": \"Rewrite a given text for improved clarity\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"text\": {\n\"type\": \"string\",\n\"description\": \"The input text to rewrite\",\n}\n},\n},\n},\n},\n]\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": \"Could you please make the below article more concise?\\n\\nOpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\",\n},\n{\n\"role\": \"assistant\",\n\"content\": \"\",\n\"tool_calls\": [\n{\n\"id\": \"bbc5b7ede\",\n\"type\": \"function\",\n\"function\": {\n\"name\": \"rewrite\",\n\"arguments\": '{\"text\": \"OpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\"}',\n},\n}\n],\n},\n{\n\"role\": \"tool\",\n\"content\": '{\"action\":\"rewrite\",\"outcome\":\"OpenAI is a FOR-profit company.\"}',\n\"tool_call_id\": \"bbc5b7ede\",\n\"name\": \"rewrite\",\n},\n{\n\"role\": \"assistant\",\n\"content\": \"---\\n\\nOpenAI is a FOR-profit company.\",\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"Can you tell me what is the biggest country depicted on the map?\",\n},\n{\n\"type\": \"image_url\",\n\"image_url\": {\n\"url\": image_url,\n},\n},\n],\n}\n]\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=messages,\ntemperature=TEMP,\nmax_tokens=MAX_TOK,\ntools=tools,\ntool_choice=\"auto\",\n)\nassistant_message = response.choices[0].message.content\nprint(assistant_message)\n# The biggest country depicted on the map is Russia.\nmessages.extend([\n{\"role\": \"assistant\", \"content\": assistant_message},\n{\"role\": \"user\", \"content\": \"What is the population of that country in millions?\"},\n])\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=messages,\ntemperature=TEMP,\nmax_tokens=MAX_TOK,\ntools=tools,\ntool_choice=\"auto\",\n)\nprint(response.choices[0].message.tool_calls)\n# [ChatCompletionMessageToolCall(id='3e92V6Vfo', function=Function(arguments='{\"country\": \"Russia\", \"unit\": \"millions\"}', name='get_current_population'), type='function')]\nPython snippet - complex\nimport json\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.15\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nreturn system_prompt\nmodel_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://math-coaching.com/img/fiche/46/expressions-mathematiques.jpg\"\ndef my_calculator(expression: str) -> str:\nreturn str(eval(expression))\ntools = [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"my_calculator\",\n\"description\": \"A calculator that can evaluate a mathematical expression.\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"expression\": {\n\"type\": \"string\",\n\"description\": \"The mathematical expression to evaluate.\",\n},\n},\n\"required\": [\"expression\"],\n},\n},\n},\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"rewrite\",\n\"description\": \"Rewrite a given text for improved clarity\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"text\": {\n\"type\": \"string\",\n\"description\": \"The input text to rewrite\",\n}\n},\n},\n},\n},\n]\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"Can you calculate the results for all the equations displayed in the image? Only compute the ones that involve numbers.\",\n},\n{\n\"type\": \"image_url\",\n\"image_url\": {\n\"url\": image_url,\n},\n},\n],\n},\n]\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=messages,\ntemperature=TEMP,\nmax_tokens=MAX_TOK,\ntools=tools,\ntool_choice=\"auto\",\n)\ntool_calls = response.choices[0].message.tool_calls\nprint(tool_calls)\n# [ChatCompletionMessageToolCall(id='CyQBSAtGh', function=Function(arguments='{\"expression\": \"6 + 2 * 3\"}', name='my_calculator'), type='function'), ChatCompletionMessageToolCall(id='KQqRCqvzc', function=Function(arguments='{\"expression\": \"19 - (8 + 2) + 1\"}', name='my_calculator'), type='function')]\nresults = []\nfor tool_call in tool_calls:\nfunction_name = tool_call.function.name\nfunction_args = tool_call.function.arguments\nif function_name == \"my_calculator\":\nresult = my_calculator(**json.loads(function_args))\nresults.append(result)\nmessages.append({\"role\": \"assistant\", \"tool_calls\": tool_calls})\nfor tool_call, result in zip(tool_calls, results):\nmessages.append(\n{\n\"role\": \"tool\",\n\"tool_call_id\": tool_call.id,\n\"name\": tool_call.function.name,\n\"content\": result,\n}\n)\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=messages,\ntemperature=TEMP,\nmax_tokens=MAX_TOK,\n)\nprint(response.choices[0].message.content)\n# Here are the results for the equations that involve numbers:\n# 1. \\( 6 + 2 \\times 3 = 12 \\)\n# 3. \\( 19 - (8 + 2) + 1 = 10 \\)\n# For the other equations, you need to substitute the variables with specific values to compute the results.\nInstruction following\nMistral-Small-3.2-24B-Instruct-2506 will follow your instructions down to the last letter !\nPython snippet\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.15\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nreturn system_prompt\nmodel_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": \"Write me a sentence where every word starts with the next letter in the alphabet - start with 'a' and end with 'z'.\",\n},\n]\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=messages,\ntemperature=TEMP,\nmax_tokens=MAX_TOK,\n)\nassistant_message = response.choices[0].message.content\nprint(assistant_message)\n# Here's a sentence where each word starts with the next letter of the alphabet, starting from 'a' and ending with 'z':\n# \"Always brave cats dance elegantly, fluffy giraffes happily ignore jungle kites, lovingly munching nuts, observing playful quails racing swiftly, tiny unicorns vaulting while xylophones yodel zealously.\"\n# This sentence follows the sequence from A to Z without skipping any letters.\nTransformers\nYou can also use Mistral-Small-3.2-24B-Instruct-2506 with Transformers !\nTo make the best use of our model with Transformers make sure to have installed mistral-common >= 1.6.2 to use our tokenizer.\npip install mistral-common --upgrade\nThen load our tokenizer along with the model and generate:\nPython snippet\nfrom datetime import datetime, timedelta\nimport torch\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom huggingface_hub import hf_hub_download\nfrom transformers import Mistral3ForConditionalGeneration\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\ntoday = datetime.today().strftime(\"%Y-%m-%d\")\nyesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\nmodel_name = repo_id.split(\"/\")[-1]\nreturn system_prompt.format(name=model_name, today=today, yesterday=yesterday)\nmodel_id = \"mistralai/Mistral-Small-3.2-24B-Instruct-2506\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\ntokenizer = MistralTokenizer.from_hf_hub(model_id)\nmodel = Mistral3ForConditionalGeneration.from_pretrained(\nmodel_id, torch_dtype=torch.bfloat16\n)\nimage_url = \"https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438\"\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.\",\n},\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n],\n},\n]\ntokenized = tokenizer.encode_chat_completion(ChatCompletionRequest(messages=messages))\ninput_ids = torch.tensor([tokenized.tokens])\nattention_mask = torch.ones_like(input_ids)\npixel_values = torch.tensor(tokenized.images[0], dtype=torch.bfloat16).unsqueeze(0)\nimage_sizes = torch.tensor([pixel_values.shape[-2:]])\noutput = model.generate(\ninput_ids=input_ids,\nattention_mask=attention_mask,\npixel_values=pixel_values,\nimage_sizes=image_sizes,\nmax_new_tokens=1000,\n)[0]\ndecoded_output = tokenizer.decode(output[len(tokenized.tokens) :])\nprint(decoded_output)\n# In this situation, you are playing a PokÃ©mon game where your Pikachu (Level 42) is facing a wild Pidgey (Level 17). Here are the possible actions you can take and an analysis of each:\n# 1. **FIGHT**:\n#    - **Pros**: Pikachu is significantly higher level than the wild Pidgey, which suggests that it should be able to defeat Pidgey easily. This could be a good opportunity to gain experience points and possibly items or money.\n#    - **Cons**: There is always a small risk of Pikachu fainting, especially if Pidgey has a powerful move or a status effect that could hinder Pikachu. However, given the large level difference, this risk is minimal.\n# 2. **BAG**:\n#    - **Pros**: You might have items in your bag that could help in this battle, such as Potions, PokÃ© Balls, or Berries. Using an item could help you capture Pidgey or heal Pikachu if needed.\n#    - **Cons**: Using items might not be necessary given the level difference. It could be more efficient to just fight and defeat Pidgey quickly.\n# 3. **POKÃ‰MON**:\n#    - **Pros**: You might have another PokÃ©mon in your party that is better suited for this battle or that you want to gain experience. Switching PokÃ©mon could also be strategic if you want to train a lower-level PokÃ©mon.\n#    - **Cons**: Switching PokÃ©mon might not be necessary since Pikachu is at a significant advantage. It could also waste time and potentially give Pidgey a turn to attack.\n# 4. **RUN**:\n#    - **Pros**: Running away could be a quick way to avoid the battle altogether. This might be useful if you are trying to conserve resources or if you are in a hurry to get to another location.\n#    - **Cons**: Running away means you miss out on the experience points, items, or money that you could gain from defeating Pidgey. It also might not be the most efficient use of your time if you are trying to train your PokÃ©mon.\n# ### Recommendation:\n# Given the significant level advantage, the best action to take is likely **FIGHT**. This will allow you to quickly defeat Pidgey and gain experience points for Pikachu. If you are concerned about Pikachu's health, you could use the **BAG** to heal Pikachu before or during the battle. Running away or switching PokÃ©mon does not seem necessary in this situation.",
    "numz/SeedVR2_comfyUI": "ComfyUI-SeedVR2_VideoUpscaler\nðŸ“‹ Quick Access\nðŸ†™ Note and futur releases\nðŸš€ Updates\nðŸŽ¯ Features\nðŸ”§ Requirements\nðŸ“¦ Installation\nðŸ“– Usage\nðŸ“Š Benchmarks\nâš ï¸ Limitations\nðŸ¤ Contributing\nHow to contribute:\nDevelopment Setup:\nCode Style:\nReporting Issues:\nðŸ™ Credits\nðŸ“œ License\nComfyUI-SeedVR2_VideoUpscaler\nOfficial release of SeedVR2 for ComfyUI that enables Upscale Video/Images generation.\nðŸ“‹ Quick Access\nðŸ†™ Note and futur releases\nðŸš€ Updates\nðŸŽ¯ Features\nðŸ”§ Requirements\nðŸ“¦ Installation\nðŸ“– Usage\nðŸ“Š Benchmarks\nâš ï¸ Limitations\nðŸ¤ Contributing\nðŸ™ Credits\nðŸ“„ License\nðŸ†™ Note and futur releases\nImprove FP8 integration, we are loosing some FP8 advantages during the process.\nTile-VAE integration if it works for video, I have test to do or if some dev want help, you are welcome.\n7B FP8 model seems to have quality issues, use 7BFP16 instead (If FP8 don't give OOM then FP16 will works) I have to review this.\nðŸš€ Updates\n2025.06.30\nðŸš€ Speed Up the process and less VRAM used (see new benchmark).\nðŸ› ï¸ Fixed leak memory on 3B models.\nâŒ Can now interrupt process if needed.\nâœ… refactored the code for better sharing with the community, feel free to propose pull requests.\nðŸ› ï¸ Removed flash attention dependency\n2025.06.24\nðŸš€ Speed up the process until x4 (see new benchmark)\n2025.06.22\nðŸ’ª FP8 compatibility !\nðŸš€ Speed Up all Process\nðŸš€ less VRAM consumption (Stay high, batch_size=1 for RTX4090 max, I'm trying to fix that)\nðŸ› ï¸ Better benchmark coming soon\n2025.06.20\nðŸ› ï¸ Initial push\nðŸŽ¯ Features\nHigh-quality Upscaling\nSuitable for any video length once the right settings are found\nModel Will Be Download Automatically from Models\nðŸ”§ Requirements\nA Huge VRAM capabilities is better, from my test, even the 3B version need a lot of VRAM at least 18GB.\nLast ComfyUI version with python 3.12.9 (may be works with older versions but I haven't test it)\nðŸ“¦ Installation\nClone this repository into your ComfyUI custom nodes directory:\ncd ComfyUI/custom_nodes\ngit clone https://github.com/numz/ComfyUI-SeedVR2_VideoUpscaler.git\nInstall the required dependencies:\nload venv and :\npip install -r ComfyUI-SeedVR2_VideoUpscaler/requirements.txt\ninstall flash_attn/triton, 6% faster on process, not a mandatory.\npip install flash_attn\npip install triton\nor\npython_embeded\\python.exe -m pip install -r flash_attn\ncheck here from https://github.com/loscrossos/lib_flashattention/releases and https://github.com/woct0rdho/triton-windows\nModels\nWill be automtically download into :\nmodels/SEEDVR2\nor can be found here (MODELS)\nðŸ“– Usage\nIn ComfyUI, locate the SeedVR2 Video Upscaler node in the node menu.\nâš ï¸ THINGS TO KNOW !!\ntemporal consistency : at least a batch_size of 5 is required to activate temporal consistency. SEEDVR2 need at least 5 frames to calculate it. A higher batch_size give better performances/results but need more than 24GB VRAM.\nVRAM usage : The input video resolution impacts VRAM consumption during the process. The larger the input video, the more VRAM will consume during the process. So, if you experience OOMs with a batch_size of at least 5, try reducing the input video resolution until it resolves.\nOf course, the output resolution also has an impact, so if your hardware doesn't allow it, reduce the output resolution.\nConfigure the node parameters:\nmodel: Select your 3B or 7B model\nseed: a seed but it generate another seed from this one\nnew_resolution: New desired short edge in px, will keep ratio on other edge\nbatch_size: VERY IMPORTANT!, this model consume a lot of VRAM, All your VRAM, even for the 3B model, so for GPU under 24GB VRAM keep this value Low, good value is \"1\" without temporal consistency, \"5\" for temporal consistency, but higher is this value better is the result.\npreserve_vram: for VRAM < 24GB, If true, It will unload unused models during process, longer but works, otherwise probably OOM with\nðŸ“Š Benchmarks\n7B models on NVIDIA H100 93GB VRAM (values in parentheses are from the previous benchmark):\nnb frames\nResolution\nBatch Size\nexecution time fp8 (s)\nFPS fp8\nexecution time fp16 (s)\nFPS fp16\nperf progress since start\n15\n512Ã—768 â†’ 1080Ã—1620\n5\n23.75 (26.71)\n0.63 (0.56)\n24.23 (27.75)\n0.61 (0.54) (0.10)\nx6.1\n27\n512Ã—768 â†’ 1080Ã—1620\n9\n27.75 (33.97)\n0.97 (0.79)\n28.48 (35.08)\n0.94 (0.77) (0.15)\nx6.2\n39\n512Ã—768 â†’ 1080Ã—1620\n13\n32.02 (41.01)\n1.21 (0.95)\n32.62 (42.08)\n1.19 (0.93) (0.19)\nx6.2\n51\n512Ã—768 â†’ 1080Ã—1620\n17\n36.39 (48.12)\n1.40 (1.06)\n37.30 (49.44)\n1.36 (1.03) (0.21)\nx6.4\n63\n512Ã—768 â†’ 1080Ã—1620\n21\n40.80 (55.40)\n1.54 (1.14)\n41.32 (56.70)\n1.52 (1.11) (0.23)\nx6.6\n75\n512Ã—768 â†’ 1080Ã—1620\n25\n45.37 (62.60)\n1.65 (1.20)\n45.79 (63.80)\n1.63 (1.18) (0.24)\nx6.8\n123\n512Ã—768 â†’ 1080Ã—1620\n41\n62.44 (91.38)\n1.96 (1.35)\n62.28 (92.90)\n1.97 (1.32) (0.28)\nx7.0\n243\n512Ã—768 â†’ 1080Ã—1620\n81\n106.13 (164.25)\n2.28 (1.48)\n104.68 (166.09)\n2.32 (1.46) (0.31)\nx7.4\n363\n512Ã—768 â†’ 1080Ã—1620\n121\n151.01 (238.18)\n2.40 (1.52)\n148.67 (239.80)\n2.44 (1.51) (0.33)\nx7.4\n453\n512Ã—768 â†’ 1080Ã—1620\n151\n186.98 (296.52)\n2.42 (1.53)\n184.11 (298.65)\n2.46 (1.52) (0.33)\nx7.4\n633\n512Ã—768 â†’ 1080Ã—1620\n211\n253.77 (406.65)\n2.49 (1.56)\n249.43 (409.44)\n2.53 (1.55) (0.34)\nx7.4\n903\n512Ã—768 â†’ 1080Ã—1620\n301\nOOM (OOM)\n(OOM)\nOOM (OOM)\n(OOM) (OOM)\n149\n854x480 â†’ 1920x1080\n149\n450.22\n0.41\n3B FP8 models on NVIDIA H100 93GB VRAM (values in parentheses are from the previous benchmark):\nnb frames\nResolution\nBatch Size\nexecution time fp8 (s)\nFPS fp8\nexecution time fp16 (s)\nFPS fp16\n149\n854x480 â†’ 1920x1080\n149\n361.22\n0.41\nNVIDIA RTX4090 24GB VRAM\nModel\nnb frames\nResolution\nBatch Size\nexecution time (seconds)\nFPS\nNote\n3B fp8\n5\n512x768 â†’ 1080x1620\n1\n14.66 (22.52)\n0.34 (0.22)\n3B fp16\n5\n512x768 â†’ 1080x1620\n1\n17.02 (27.84)\n0.29 (0.18)\n7B fp8\n5\n512x768 â†’ 1080x1620\n1\n46.23 (75.51)\n0.11 (0.07)\npreserve_memory=on\n7B fp16\n5\n512x768 â†’ 1080x1620\n1\n43.58 (78.93)\n0.11 (0.06)\npreserve_memory=on\n3B fp8\n10\n512x768 â†’ 1080x1620\n5\n39.75\n0.25\npreserve_memory=on\n3B fp8\n100\n512x768 â†’ 1080x1620\n5\n322.77\n0.31\npreserve_memory=on\n3B fp8\n1000\n512x768 â†’ 1080x1620\n5\n3624.08\n0.28\npreserve_memory=on\n3B fp8\n20\n512x768 â†’ 1080x1620\n1\n40.71 (65.40)\n0.49 (0.31)\n3B fp16\n20\n512x768 â†’ 1080x1620\n1\n44.76 (91.12)\n0.45 (0.22)\n3B fp8\n20\n512x768 â†’ 1280x1920\n1\n61.14 (89.10)\n0.33 (0.22)\n3B fp8\n20\n512x768 â†’ 1480x2220\n1\n79.66 (136.08)\n0.25 (0.15)\n3B fp8\n20\n512x768 â†’ 1620x2430\n1\n125.79 (191.28)\n0.16 (0.10)\npreserve_memory=off (preserve_memory=on)\n3B fp8\n149\n854x480 â†’ 1920x1080\n5\n782.76\n0.19\npreserve_memory=on\nâš ï¸ Limitations\nUse a lot of VRAM, it will take all!!\nProcessing speed depends on GPU capabilities\nðŸ¤ Contributing\nContributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.\nPlease make sure to update tests as appropriate.\nHow to contribute:\nFork the repository\nCreate your feature branch (git checkout -b feature/AmazingFeature)\nCommit your changes (git commit -m 'Add some AmazingFeature')\nPush to the branch (git push origin feature/AmazingFeature)\nOpen a Pull Request\nDevelopment Setup:\nClone the repository\nInstall dependencies\nMake your changes\nTest your changes\nSubmit a pull request\nCode Style:\nFollow the existing code style\nAdd comments for complex logic\nUpdate documentation if needed\nEnsure all tests pass\nReporting Issues:\nWhen reporting issues, please include:\nYour system specifications\nComfyUI version\nPython version\nError messages\nSteps to reproduce the issue\nðŸ™ Credits\nOriginal SeedVR2 implementation\nðŸ“œ License\nThe code in this repository is released under the MIT license as found in the LICENSE file.",
    "neta-art/Neta-Lumina": "Introduction\nKey Features\nModel Versions\nneta-lumina-v1.0\nneta-lumina-beta-0624-raw (archived)\nneta-lumina-beta-0624-aes-experimental (archived)\nHowâ€¯ to â€¯Use\nComfyUI\nEnvironment Requirements\nDownloads & Installation\nRecommended Settings\nPrompt Book\nCommunity\nRoadmap\nModel\nEcosystem\nLicenseâ€¯&â€¯Disclaimer\nParticipantsâ€¯&â€¯Contributors\nCommunity Contributors\nAppendixâ€¯&â€¯Resources\nä¸­æ–‡ç‰ˆæ¨¡åž‹è¯´æ˜Ž\nNeta Lumina Tech Report\nðŸ“½ï¸ Flash Preview\nYour browser does not support the video tag.\nIntroduction\nNetaâ€¯Lumina is a highâ€‘quality animeâ€‘style imageâ€‘generation model developed by Neta.art Lab.Building on the openâ€‘source Luminaâ€‘Imageâ€‘2.0 released by the Alphaâ€‘VLLM team at Shanghai AI Laboratory, we fineâ€‘tuned the model with a vast corpus of highâ€‘quality anime images and multilingual tag data. The preliminary result is a compelling model with powerful comprehension and interpretation abilities (thanks to Gemma text encoder), ideal for illustration, posters, storyboards, character design, and more.\nKey Features\nOptimized for diverse creative scenarios such as Furry, Guofeng (traditionalâ€‘Chinese aesthetics), pets, etc.\nWide coverage of characters and styles, from popular to niche concepts. (Still support danbooru tags!)\nAccurate naturalâ€‘language understanding with excellent adherence to complex prompts.\nNative multilingual support, with Chinese, English, and Japanese recommended first.\nModel Versions\nFor models in alpha tests, requst access at https://huggingface.co/neta-art/NetaLumina_Alpha if you are interested. We will keep updating.\nneta-lumina-v1.0\nOfficial Release: overall best performance\nneta-lumina-beta-0624-raw (archived)\nPrimary Goal: General knowledge and animeâ€‘style optimization\nData Set: >13 million animeâ€‘style images\n>46,000 A100 Hours\nHigher upper limit, suitable for pro users. Check Netaâ€¯Lumina Prompt Book for better results.\nneta-lumina-beta-0624-aes-experimental (archived)\nFirst beta release candidate\nPrimary Goal: Enhanced aesthetics, pose accuracy, and scene detail\nData Set: Hundreds of thousands of handpicked highâ€‘quality anime images (fineâ€‘tuned on an older version of raw model)\nUser-friendly, suitable for most people.\nHowâ€¯ to â€¯Use\nTry it at Hugging Face playground\nComfyUI\nNetaâ€¯Lumina is built on the Lumina2 Diffusion Transformer (DiT) framework, please follow these steps precisely.\nEnvironment Requirements\nCurrently Netaâ€¯Lumina runs only on ComfyUI:\nLatest ComfyUI installation\nâ‰¥â€¯8â€¯GB VRAM\nDownloads & Installation\nOriginal (component) release\nNeta Lumina-Beta\nDownload link: https://huggingface.co/neta-art/Neta-Lumina/blob/main/Unet/neta-lumina-v1.0.safetensors\nSave path: ComfyUI/models/unet/\nText Encoder (Gemma-2B)\nDownload link:https://huggingface.co/neta-art/Neta-Lumina/blob/main/Text%20Encoder/gemma_2_2b_fp16.safetensors\nSave path: ComfyUI/models/text_encoders/\nVAE Model (16-Channel FLUX VAE)\nDownload link: https://huggingface.co/neta-art/Neta-Lumina/blob/main/VAE/ae.safetensors\nSave path: ComfyUI/models/vae/\nWorkflow: load lumina_workflow.json in ComfyUI.\nUNETLoader â€“ loads the .pth\nVAELoader â€“ loads ae.safetensors\nCLIPLoader â€“ loads gemma_2_2b_fp16.safetensors\nTextâ€¯Encoder â€“ connects positive /negative prompts to K Sampler\nSimple merged releaseDownload neta-lumina-v1.0-all-in-one.safetensors,md5sum = dca54fef3c64e942c1a62a741c4f9d8a,you may use ComfyUIâ€™s simple checkpoint loader workflow.\nRecommended Settings\nSampler: res_multistep/ euler_ancestral\nScheduler: linear_quadratic\nSteps: 30\nCFG (guidance): 4â€¯â€“â€¯5.5\nEmptySD3LatentImage resolution: 1024â€¯Ã—â€¯1024,â€¯768â€¯Ã—â€¯1532,â€¯968â€¯Ã—â€¯1322, or >= 1024\nPrompt Book\nDetailed prompt guidelines: Netaâ€¯Lumina Prompt Book\nCommunity\nDiscord: https://discord.com/invite/TTTGccjbEa\nQQ group: 1039442542\nRoadmap\nModel\nContinous baseâ€‘model training to raise reasoning capability.\nAestheticâ€‘dataset iteration to improve anatomy, background richness, and overall appealness.\nSmarter, more versatile tagging tools to lower the creative barrier.\nEcosystem\nLoRA training tutorials and components\nExperienced users may already fineâ€‘tune via Luminaâ€‘Imageâ€‘2.0â€™s open code.\nDevelopment of advanced control / styleâ€‘consistency features (e.g., Ominiâ€¯Control). Call for Collaboration!\nLicenseâ€¯&â€¯Disclaimer\nNetaâ€¯Lumina is released under Apache License 2.0\nParticipantsâ€¯&â€¯Contributors\nSpecial thanks to the Alphaâ€‘VLLM team for openâ€‘sourcing Luminaâ€‘Imageâ€‘2.0\nModel development: Neta.art Lab (Civitai)\nCore Trainer:  li_li Civitai ãƒ» Huggingâ€¯Face\nPartners\nnebulae: Civitai ãƒ» Huggingâ€¯Face\nç”Ÿå§œ: Hugging Face\nå­™ä¸€\nnarugo1992 & deepghs: open datasets, processing tools, and models\nNaifu trainer at Mikubill\nCommunity Contributors\nEvaluators & developers: äºŒå°å§, spawner, Rnglg2\nOther contributors: æ²‰è¿·æ‘¸é±¼, poi, AshenWitch, ååˆ†æ— å¥ˆ, GHOSTLX, wenaka, iiiiii, å¹´ç³•ç‰¹å·¥é˜Ÿ, æ©åŒ¹å¸Œ, å¥¶å†», mumu, yizyin, smile, Yang, å¤ç¥ž, çµä¹‹è¯, LyloGummy, é›ªæ—¶\nAppendixâ€¯&â€¯Resources\nTeaCache: https://github.com/spawner1145/CUI-Lumina2-TeaCache\nAdvanced samplers & TeaCache guide (by spawner): https://docs.qq.com/doc/DZEFKb1ZrZVZiUmxw?nlc=1\nNetaâ€¯Lumina ComfyUI Manual (in Chinese): https://docs.qq.com/doc/DZEVQZFdtaERPdXVh",
    "zai-org/GLM-4.1V-9B-Thinking": "GLM-4.1V-9B-Thinking\nModel Introduction\nBenchmark Performance\nQuick Inference\nGLM-4.1V-9B-Thinking\nðŸ“– View the GLM-4.1V-9B-Thinking paper.\nðŸ“ Using GLM-4.1V-9B-Thinking API at Zhipu Foundation Model Open Platform\nModel Introduction\nVision-Language Models (VLMs) have become foundational components of intelligent systems. As real-world AI tasks grow\nincreasingly complex, VLMs must evolve beyond basic multimodal perception to enhance their reasoning capabilities in\ncomplex tasks. This involves improving accuracy, comprehensiveness, and intelligence, enabling applications such as\ncomplex problem solving, long-context understanding, and multimodal agents.\nBased on the GLM-4-9B-0414 foundation model, we present the new open-source VLM model\nGLM-4.1V-9B-Thinking, designed to explore the upper limits of reasoning in vision-language models. By introducing\na \"thinking paradigm\" and leveraging reinforcement learning, the model significantly enhances its capabilities. It\nachieves state-of-the-art performance among 10B-parameter VLMs, matching or even surpassing the 72B-parameter\nQwen-2.5-VL-72B on 18 benchmark tasks. We are also open-sourcing the base model GLM-4.1V-9B-Base to\nsupport further research into the boundaries of VLM capabilities.\nCompared to the previous generation models CogVLM2 and the GLM-4V series, GLM-4.1V-Thinking offers the\nfollowing improvements:\nThe first reasoning-focused model in the series, achieving world-leading performance not only in mathematics but also\nacross various sub-domains.\nSupports 64k context length.\nHandles arbitrary aspect ratios and up to 4K image resolution.\nProvides an open-source version supporting both Chinese and English bilingual usage.\nBenchmark Performance\nBy incorporating the Chain-of-Thought reasoning paradigm, GLM-4.1V-9B-Thinking significantly improves answer accuracy,\nrichness, and interpretability. It comprehensively surpasses traditional non-reasoning visual models.\nOut of 28 benchmark tasks, it achieved the best performance among 10B-level models on 23 tasks,\nand even outperformed the 72B-parameter Qwen-2.5-VL-72B on 18 tasks.\nQuick Inference\nThis is a simple example of running single-image inference using the transformers library.First, install the transformers library from source:\npip install transformers>=4.57.1\nThen, run the following code:\nfrom transformers import AutoProcessor, Glm4vForConditionalGeneration\nimport torch\nMODEL_PATH = \"zai-org/GLM-4.1V-9B-Thinking\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"url\": \"https://upload.wikimedia.org/wikipedia/commons/f/fa/Grayscale_8bits_palette_sample_image.png\"\n},\n{\n\"type\": \"text\",\n\"text\": \"describe this image\"\n}\n],\n}\n]\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH, use_fast=True)\nmodel = Glm4vForConditionalGeneration.from_pretrained(\npretrained_model_name_or_path=MODEL_PATH,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\noutput_text = processor.decode(generated_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)\nprint(output_text)\nFor video reasoning, web demo deployment, and more code, please check\nour GitHub.",
    "NeoQuasar/Kronos-base": "Kronos: A Foundation Model for the Language of Financial Markets\nIntroduction\nLive Demo\nModel Zoo\nGetting Started: Making Forecasts\nInstallation\n1. Load the Tokenizer and Model\n2. Instantiate the Predictor\n3. Prepare Input Data\n4. Generate Forecasts\n5. Example and Visualization\nðŸ”§ Finetuning on Your Own Data (A-Share Market Example)\nCitation\nLicense\nKronos: A Foundation Model for the Language of Financial Markets\nKronos is the first open-source foundation model for financial candlesticks (K-lines), trained on data from over 45 global exchanges. It is designed to handle the unique, high-noise characteristics of financial data.\nIntroduction\nKronos is a family of decoder-only foundation models, pre-trained specifically for the \"language\" of financial marketsâ€”K-line sequences. It leverages a novel two-stage framework:\nA specialized tokenizer first quantizes continuous, multi-dimensional K-line data (OHLCV) into hierarchical discrete tokens.\nA large, autoregressive Transformer is then pre-trained on these tokens, enabling it to serve as a unified model for diverse quantitative tasks.\nThe success of large-scale pre-training paradigm, exemplified by Large Language Models (LLMs), has inspired the development of Time Series Foundation Models (TSFMs). Kronos addresses existing limitations by introducing a specialized tokenizer that discretizes continuous market information into token sequences, preserving both price dynamics and trade activity patterns. We pre-train Kronos using an autoregressive objective on a massive, multi-market corpus of over 12 billion K-line records from 45 global exchanges, enabling it to learn nuanced temporal and cross-asset representations. Kronos excels in a zero-shot setting across a diverse set of financial tasks, including price series forecasting, volatility forecasting, and synthetic data generation.\nLive Demo\nWe have set up a live demo to visualize Kronos's forecasting results. The webpage showcases a forecast for the BTC/USDT trading pair over the next 24 hours.\nðŸ‘‰ Access the Live Demo Here\nModel Zoo\nWe release a family of pre-trained models with varying capacities to suit different computational and application needs. All models are readily accessible from the Hugging Face Hub.\nModel\nTokenizer\nContext length\nParam\nHugging Face Model Card\nKronos-mini\nKronos-Tokenizer-2k\n2048\n4.1M\nâœ… NeoQuasar/Kronos-mini\nKronos-small\nKronos-Tokenizer-base\n512\n24.7M\nâœ… NeoQuasar/Kronos-small\nKronos-base\nKronos-Tokenizer-base\n512\n102.3M\nâœ… NeoQuasar/Kronos-base\nKronos-large\nKronos-Tokenizer-base\n512\n499.2M\nâŒ Not yet publicly available\nGetting Started: Making Forecasts\nForecasting with Kronos is straightforward using the KronosPredictor class. It handles data preprocessing, normalization, prediction, and inverse normalization, allowing you to get from raw data to forecasts in just a few lines of code.\nImportant Note: The max_context for Kronos-small and Kronos-base is 512. This is the maximum sequence length the model can process. For optimal performance, it is recommended that your input data length (i.e., lookback) does not exceed this limit. The KronosPredictor will automatically handle truncation for longer contexts.\nHere is a step-by-step guide to making your first forecast.\nInstallation\nInstall Python 3.10+, and then install the dependencies from the GitHub repository's requirements.txt:\npip install -r requirements.txt\n1. Load the Tokenizer and Model\nFirst, load a pre-trained Kronos model and its corresponding tokenizer from the Hugging Face Hub.\nfrom model import Kronos, KronosTokenizer, KronosPredictor\n# Load from Hugging Face Hub\ntokenizer = KronosTokenizer.from_pretrained(\"NeoQuasar/Kronos-Tokenizer-base\")\nmodel = Kronos.from_pretrained(\"NeoQuasar/Kronos-small\")\n2. Instantiate the Predictor\nCreate an instance of KronosPredictor, passing the model, tokenizer, and desired device.\n# Initialize the predictor\npredictor = KronosPredictor(model, tokenizer, device=\"cuda:0\", max_context=512)\n3. Prepare Input Data\nThe predict method requires three main inputs:\ndf: A pandas DataFrame containing the historical K-line data. It must include columns ['open', 'high', 'low', 'close']. volume and amount are optional.\nx_timestamp: A pandas Series of timestamps corresponding to the historical data in df.\ny_timestamp: A pandas Series of timestamps for the future periods you want to predict.\nimport pandas as pd\n# Load your data (example data can be found in the GitHub repo)\ndf = pd.read_csv(\"./data/XSHG_5min_600977.csv\")\ndf['timestamps'] = pd.to_datetime(df['timestamps'])\n# Define context window and prediction length\nlookback = 400\npred_len = 120\n# Prepare inputs for the predictor\nx_df = df.loc[:lookback-1, ['open', 'high', 'low', 'close', 'volume', 'amount']]\nx_timestamp = df.loc[:lookback-1, 'timestamps']\ny_timestamp = df.loc[lookback:lookback+pred_len-1, 'timestamps']\n4. Generate Forecasts\nCall the predict method to generate forecasts. You can control the sampling process with parameters like T, top_p, and sample_count for probabilistic forecasting.\n# Generate predictions\npred_df = predictor.predict(\ndf=x_df,\nx_timestamp=x_timestamp,\ny_timestamp=y_timestamp,\npred_len=pred_len,\nT=1.0,          # Temperature for sampling\ntop_p=0.9,      # Nucleus sampling probability\nsample_count=1  # Number of forecast paths to generate and average\n)\nprint(\"Forecasted Data Head:\")\nprint(pred_df.head())\nThe predict method returns a pandas DataFrame containing the forecasted values for open, high, low, close, volume, and amount, indexed by the y_timestamp you provided.\n5. Example and Visualization\nFor a complete, runnable script that includes data loading, prediction, and plotting, please see examples/prediction_example.py in the GitHub repository.\nRunning this script will generate a plot comparing the ground truth data against the model's forecast, similar to the one shown below:\nAdditionally, a script that makes predictions without Volume and Amount data can be found in examples/prediction_wo_vol_example.py.\nðŸ”§ Finetuning on Your Own Data (A-Share Market Example)\nRefer to the README of GitHub repository.\nCitation\nIf you use Kronos in your research, we would appreciate a citation to our paper:\n@misc{shi2025kronos,\ntitle={Kronos: A Foundation Model for the Language of Financial Markets},\nauthor={Yu Shi and Zongliang Fu and Shuo Chen and Bohan Zhao and Wei Xu and Changshui Zhang and Jian Li},\nyear={2025},\neprint={2508.02739},\narchivePrefix={arXiv},\nprimaryClass={q-fin.ST},\nurl={https://arxiv.org/abs/2508.02739},\n}\nLicense\nThis project is licensed under the MIT License.",
    "zeroentropy/zerank-1-small": "Releasing zeroentropy/zerank-1-small\nHow to Use\nEvaluations\nReleasing zeroentropy/zerank-1-small\nIn search enginers, rerankers are crucial for improving the accuracy of your retrieval system.\nThis 1.7B reranker is the smaller version of our flagship model zeroentropy/zerank-1. Though the model is over 2x smaller, it maintains nearly the same standard of performance, continuing to outperform other popular rerankers, and displaying massive accuracy gains over traditional vector search.\nWe release this model under the open-source Apache 2.0 license, in order to support the open-source community and push the frontier of what's possible with open-source models.\nHow to Use\nfrom sentence_transformers import CrossEncoder\nmodel = CrossEncoder(\"zeroentropy/zerank-1-small\", trust_remote_code=True)\nquery_documents = [\n(\"What is 2+2?\", \"4\"),\n(\"What is 2+2?\", \"The answer is definitely 1 million\"),\n]\nscores = model.predict(query_documents)\nprint(scores)\nThe model can also be inferenced using ZeroEntropy's /models/rerank endpoint.\nEvaluations\nNDCG@10 scores between zerank-1-small and competing closed-source proprietary rerankers. Since we are evaluating rerankers, OpenAI's text-embedding-3-small is used as an initial retriever for the Top 100 candidate documents.\nTask\nEmbedding\ncohere-rerank-v3.5\nSalesforce/Llama-rank-v1\nzerank-1-small\nzerank-1\nCode\n0.678\n0.724\n0.694\n0.730\n0.754\nConversational\n0.250\n0.571\n0.484\n0.556\n0.596\nFinance\n0.839\n0.824\n0.828\n0.861\n0.894\nLegal\n0.703\n0.804\n0.767\n0.817\n0.821\nMedical\n0.619\n0.750\n0.719\n0.773\n0.796\nSTEM\n0.401\n0.510\n0.595\n0.680\n0.694\nComparing BM25 and Hybrid Search without and with zerank-1-small:",
    "mistralai/Voxtral-Mini-3B-2507": "Voxtral Mini 1.0 (3B) - 2507\nKey Features\nBenchmark Results\nAudio\nText\nUsage\nvLLM (recommended)\nAudio Instruct\nTransformers ðŸ¤—\nVoxtral Mini 1.0 (3B) - 2507\nVoxtral Mini is an enhancement of Ministral 3B, incorporating state-of-the-art audio input capabilities while retaining best-in-class text performance. It excels at speech transcription, translation and audio understanding.\nLearn more about Voxtral in our blog post here and our research paper.\nKey Features\nVoxtral builds upon Ministral-3B with powerful audio understanding capabilities.\nDedicated transcription mode: Voxtral can operate in a pure speech transcription mode to maximize performance. By default, Voxtral automatically predicts the source audio language and transcribes the text accordingly\nLong-form context: With a 32k token context length, Voxtral handles audios up to 30 minutes for transcription, or 40 minutes for understanding\nBuilt-in Q&A and summarization: Supports asking questions directly through audio. Analyze audio and generate structured summaries without the need for separate ASR and language models\nNatively multilingual: Automatic language detection and state-of-the-art performance in the worldâ€™s most widely used languages (English, Spanish, French, Portuguese, Hindi, German, Dutch, Italian)\nFunction-calling straight from voice: Enables direct triggering of backend functions, workflows, or API calls based on spoken user intents\nHighly capable at text: Retains the text understanding capabilities of its language model backbone, Ministral-3B\nBenchmark Results\nAudio\nAverage word error rate (WER) over the FLEURS, Mozilla Common Voice and Multilingual LibriSpeech benchmarks:\nText\nUsage\nThe model can be used with the following frameworks;\nvllm (recommended): See here\nTransformers ðŸ¤—: See here\nNotes:\ntemperature=0.2 and top_p=0.95 for chat completion (e.g. Audio Understanding) and temperature=0.0 for transcription\nMultiple audios per message and multiple user turns with audio are supported\nSystem prompts are not yet supported\nvLLM (recommended)\nWe recommend using this model with vLLM.\nInstallation\nMake sure to install vllm >= 0.10.0, we recommend using uv:\nuv pip install -U \"vllm[audio]\" --system\nDoing so should automatically install mistral_common >= 1.8.1.\nTo check:\npython -c \"import mistral_common; print(mistral_common.__version__)\"\nOffline\nYou can test that your vLLM setup works as expected by cloning the vLLM repo:\ngit clone https://github.com/vllm-project/vllm && cd vllm\nand then running:\npython examples/offline_inference/audio_language.py --num-audios 2 --model-type voxtral\nServe\nWe recommend that you use Voxtral-Small-24B-2507 in a server/client setting.\nSpin up a server:\nvllm serve mistralai/Voxtral-Mini-3B-2507 --tokenizer_mode mistral --config_format mistral --load_format mistral\nNote: Running Voxtral-Mini-3B-2507 on GPU requires ~9.5 GB of GPU RAM in bf16 or fp16.\nTo ping the client you can use a simple Python snippet. See the following examples.\nAudio Instruct\nLeverage the audio capabilities of Voxtral-Mini-3B-2507 to chat.\nMake sure that your client has mistral-common with audio installed:\npip install --upgrade mistral_common\\[audio\\]\nPython snippet\nfrom mistral_common.protocol.instruct.messages import TextChunk, AudioChunk, UserMessage, AssistantMessage, RawAudio\nfrom mistral_common.audio import Audio\nfrom huggingface_hub import hf_hub_download\nfrom openai import OpenAI\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://<your-server-host>:8000/v1\"\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\nobama_file = hf_hub_download(\"patrickvonplaten/audio_samples\", \"obama.mp3\", repo_type=\"dataset\")\nbcn_file = hf_hub_download(\"patrickvonplaten/audio_samples\", \"bcn_weather.mp3\", repo_type=\"dataset\")\ndef file_to_chunk(file: str) -> AudioChunk:\naudio = Audio.from_file(file, strict=False)\nreturn AudioChunk.from_audio(audio)\ntext_chunk = TextChunk(text=\"Which speaker is more inspiring? Why? How are they different from each other?\")\nuser_msg = UserMessage(content=[file_to_chunk(obama_file), file_to_chunk(bcn_file), text_chunk]).to_openai()\nprint(30 * \"=\" + \"USER 1\" + 30 * \"=\")\nprint(text_chunk.text)\nprint(\"\\n\\n\")\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=[user_msg],\ntemperature=0.2,\ntop_p=0.95,\n)\ncontent = response.choices[0].message.content\nprint(30 * \"=\" + \"BOT 1\" + 30 * \"=\")\nprint(content)\nprint(\"\\n\\n\")\n# The speaker who is more inspiring is the one who delivered the farewell address, as they express\n# gratitude, optimism, and a strong commitment to the nation and its citizens. They emphasize the importance of\n# self-government and active citizenship, encouraging everyone to participate in the democratic process. In contrast,\n# the other speaker provides a factual update on the weather in Barcelona, which is less inspiring as it\n# lacks the emotional and motivational content of the farewell address.\n# **Differences:**\n# - The farewell address speaker focuses on the values and responsibilities of citizenship, encouraging active participation in democracy.\n# - The weather update speaker provides factual information about the temperature in Barcelona, without any emotional or motivational content.\nmessages = [\nuser_msg,\nAssistantMessage(content=content).to_openai(),\nUserMessage(content=\"Ok, now please summarize the content of the first audio.\").to_openai()\n]\nprint(30 * \"=\" + \"USER 2\" + 30 * \"=\")\nprint(messages[-1][\"content\"])\nprint(\"\\n\\n\")\nresponse = client.chat.completions.create(\nmodel=model,\nmessages=messages,\ntemperature=0.2,\ntop_p=0.95,\n)\ncontent = response.choices[0].message.content\nprint(30 * \"=\" + \"BOT 2\" + 30 * \"=\")\nprint(content)\nTranscription\nVoxtral-Mini-3B-2507 has powerful transcription capabilities!\nMake sure that your client has mistral-common with audio installed:\npip install --upgrade mistral_common\\[audio\\]\nPython snippet\nfrom mistral_common.protocol.transcription.request import TranscriptionRequest\nfrom mistral_common.protocol.instruct.messages import RawAudio\nfrom mistral_common.audio import Audio\nfrom huggingface_hub import hf_hub_download\nfrom openai import OpenAI\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://<your-server-host>:8000/v1\"\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\nobama_file = hf_hub_download(\"patrickvonplaten/audio_samples\", \"obama.mp3\", repo_type=\"dataset\")\naudio = Audio.from_file(obama_file, strict=False)\naudio = RawAudio.from_audio(audio)\nreq = TranscriptionRequest(model=model, audio=audio, language=\"en\", temperature=0.0).to_openai(exclude=(\"top_p\", \"seed\"))\nresponse = client.audio.transcriptions.create(**req)\nprint(response)\nTransformers ðŸ¤—\nStarting with transformers >= 4.54.0 and above, you can run Voxtral natively!\nInstall Transformers:\npip install -U transformers\nMake sure to have mistral-common >= 1.8.1 installed with audio dependencies:\npip install --upgrade \"mistral-common[audio]\"\nAudio Instruct\nâž¡ï¸ multi-audio + text instruction\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Mini-3B-2507\"\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/mary_had_lamb.mp3\",\n},\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n},\n{\"type\": \"text\", \"text\": \"What sport and what nursery rhyme are referenced?\"},\n],\n}\n]\ninputs = processor.apply_chat_template(conversation)\ninputs = inputs.to(device, dtype=torch.bfloat16)\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\nprint(\"\\nGenerated response:\")\nprint(\"=\" * 80)\nprint(decoded_outputs[0])\nprint(\"=\" * 80)\nâž¡ï¸ multi-turn\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Mini-3B-2507\"\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n},\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\",\n},\n{\"type\": \"text\", \"text\": \"Describe briefly what you can hear.\"},\n],\n},\n{\n\"role\": \"assistant\",\n\"content\": \"The audio begins with the speaker delivering a farewell address in Chicago, reflecting on his eight years as president and expressing gratitude to the American people. The audio then transitions to a weather report, stating that it was 35 degrees in Barcelona the previous day, but the temperature would drop to minus 20 degrees the following day.\",\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n},\n{\"type\": \"text\", \"text\": \"Ok, now compare this new audio with the previous one.\"},\n],\n},\n]\ninputs = processor.apply_chat_template(conversation)\ninputs = inputs.to(device, dtype=torch.bfloat16)\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\nprint(\"\\nGenerated response:\")\nprint(\"=\" * 80)\nprint(decoded_outputs[0])\nprint(\"=\" * 80)\nâž¡ï¸ text only\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Mini-3B-2507\"\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"Why should AI models be open-sourced?\",\n},\n],\n}\n]\ninputs = processor.apply_chat_template(conversation)\ninputs = inputs.to(device, dtype=torch.bfloat16)\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\nprint(\"\\nGenerated response:\")\nprint(\"=\" * 80)\nprint(decoded_outputs[0])\nprint(\"=\" * 80)\nâž¡ï¸ audio only\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Mini-3B-2507\"\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n},\n],\n}\n]\ninputs = processor.apply_chat_template(conversation)\ninputs = inputs.to(device, dtype=torch.bfloat16)\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\nprint(\"\\nGenerated response:\")\nprint(\"=\" * 80)\nprint(decoded_outputs[0])\nprint(\"=\" * 80)\nâž¡ï¸ batched inference\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Mini-3B-2507\"\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\nconversations = [\n[\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n},\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\",\n},\n{\n\"type\": \"text\",\n\"text\": \"Who's speaking in the speach and what city's weather is being discussed?\",\n},\n],\n}\n],\n[\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"audio\",\n\"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n},\n{\"type\": \"text\", \"text\": \"What can you tell me about this audio?\"},\n],\n}\n],\n]\ninputs = processor.apply_chat_template(conversations)\ninputs = inputs.to(device, dtype=torch.bfloat16)\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\nprint(\"\\nGenerated responses:\")\nprint(\"=\" * 80)\nfor decoded_output in decoded_outputs:\nprint(decoded_output)\nprint(\"=\" * 80)\nTranscription\nâž¡ï¸ transcribe\nfrom transformers import VoxtralForConditionalGeneration, AutoProcessor\nimport torch\ndevice = \"cuda\"\nrepo_id = \"mistralai/Voxtral-Mini-3B-2507\"\nprocessor = AutoProcessor.from_pretrained(repo_id)\nmodel = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\ninputs = processor.apply_transcription_request(language=\"en\", audio=\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\", model_id=repo_id)\ninputs = inputs.to(device, dtype=torch.bfloat16)\noutputs = model.generate(**inputs, max_new_tokens=500)\ndecoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\nprint(\"\\nGenerated responses:\")\nprint(\"=\" * 80)\nfor decoded_output in decoded_outputs:\nprint(decoded_output)\nprint(\"=\" * 80)",
    "bosonai/higgs-audio-v2-generation-3B-base": "Higgs Audio V2: Redefining Expressiveness in Audio Generation\nTechnical Details\nAudio Tokenizer\nModel Architecture -- Dual FFN\nEvaluation\nGet Started\nLicense\nHiggs Audio V2: Redefining Expressiveness in Audio Generation\nCheck our open-source repository https://github.com/boson-ai/higgs-audio for more details!\nWe are open-sourcing Higgs Audio v2, a powerful audio foundation model pretrained on over 10 million hours of audio data and a diverse set of text data.\nDespite having no post-training or fine-tuning, Higgs Audio v2 excels in expressive audio generation, thanks to its deep language and acoustic understanding.\nOn EmergentTTS-Eval, the model achieves win rates of 75.7% and 55.7% over \"gpt-4o-mini-tts\" on the \"Emotions\" and \"Questions\" categories, respectively. It also obtains state-of-the-art performance on traditional TTS benchmarks like Seed-TTS Eval and Emotional Speech Dataset (ESD). Moreover, the model demonstrates capabilities rarely seen in previous systems, including automatic prosody adaptation during narration, zero-shot generation of natural multi-speaker dialogues in multiple languages, melodic humming with the cloned voice, and simultaneous generation of speech and background music.\nHere's the demo video that shows some of its emergent capabilities (remember to unmute):\nYour browser does not support the video tag.\nHere's another demo video that show-cases the model's multilingual capability and how it enabled live translation (remember to unmute):\nYour browser does not support the video tag.\nTechnical Details\nHiggs Audio v2 adopts the \"generation variant\" depicted in the architecture figure above. Its strong performance is driven by three key technical innovations:\nWe developed an automated annotation pipeline that leverages multiple ASR models, sound event classification models, and our in-house audio understanding model. Using this pipeline, we cleaned and annotated 10 million hours audio data, which we refer to as AudioVerse. The in-house understanding model is finetuned on top of Higgs Audio v1 Understanding, which adopts the \"understanding variant\" shown in the architecture figure.\nWe trained a unified audio tokenizer from scratch that captures both semantic and acoustic features.\nWe proposed the DualFFN architecture, which enhances the LLMâ€™s ability to model acoustics tokens with minimal computational overhead.\nAudio Tokenizer\nWe introduce a new discretized audio tokenizer that runs at just 25 frames per second while keepingâ€”or even improvingâ€”audio quality compared to tokenizers with twice the bitrate.\nOur model is the first to train on 24 kHz data covering speech, music, and sound events in one unified system.\nIt also uses a simple non-diffusion encoder/decoder for fast, batch inference. It achieves state-of-the-art performance in semantic and acoustic evaluations.\nCheck https://huggingface.co/bosonai/higgs-audio-v2-tokenizer for more information about the tokenizer.\nModel Architecture -- Dual FFN\nHiggs Audio v2 is built on top of Llama-3.2-3B. To enhance the modelâ€™s ability to process audio tokens,\nwe incorporate the \"DualFFN\" architecture as an audio adapter.\nDualFFN acts as an audio-specific expert, boosting the LLM's performance with minimal computational overhead.\nOur implementation preserves 91% of the original LLMâ€™s training speed with the inclusion of DualFFN, which has 2.2B parameters.\nThus, the total number of parameter for Higgs Audio v2 is 3.6B (LLM) + 2.2B (Audio Dual FFN), and it has the same training / inference FLOPs as Llama-3.2-3B.\nAblation study shows that the model equipped with DualFFN consistently outperforms its counterpart in terms of word error rate (WER) and speaker similarity.\nSee our architecture blog for more information.\nEvaluation\nHere's the performance of Higgs Audio v2 on four benchmarks,  Seed-TTS Eval, Emotional Speech Dataset (ESD), EmergentTTS-Eval, and Multi-speaker Eval:\nSeed-TTS Eval & ESD\nWe prompt Higgs Audio v2 with the reference text, reference audio, and target text for zero-shot TTS. We use the standard evaluation metrics from Seed-TTS Eval and ESD.\nSeedTTS-Eval\nESD\nWER â†“\nSIM â†‘\nWER â†“\nSIM (emo2vec) â†‘\nCosyvoice2\n2.28\n65.49\n2.71\n80.48\nQwen2.5-omniâ€ \n2.33\n64.10\n-\n-\nElevenLabs Multilingual V2\n1.43\n50.00\n1.66\n65.87\nHiggs Audio v1\n2.18\n66.27\n1.49\n82.84\nHiggs Audio v2 (base)\n2.44\n67.70\n1.78\n86.13\nEmergentTTS-Eval (\"Emotions\" and \"Questions\")\nFollowing the EmergentTTS-Eval Paper, we report the win-rate over \"gpt-4o-mini-tts\" with the \"alloy\" voice. Results of Higgs Audio v2 is obtained with the voice of \"belinda\". The judge model is Gemini 2.5 Pro.\nModel\nEmotions (%) â†‘\nQuestions (%) â†‘\nHiggs Audio v2 (base)\n75.71%\n55.71%\ngpt-4o-audio-previewâ€ \n61.64%\n47.85%\nHume.AI\n61.60%\n43.21%\nBASELINE: gpt-4o-mini-tts\n50.00%\n50.00%\nQwen 2.5 Omniâ€ \n41.60%\n51.78%\nminimax/speech-02-hd\n40.86%\n47.32%\nElevenLabs Multilingual v2\n30.35%\n39.46%\nDeepGram Aura-2\n29.28%\n48.21%\nSesame csm-1B\n15.96%\n31.78%\n'â€ ' means using the strong-prompting method described in the paper.\nMulti-speaker Eval\nWe also designed a multi-speaker evaluation benchmark to evaluate the capability of Higgs Audio v2 for multi-speaker dialog generation. The benchmark contains three subsets\ntwo-speaker-conversation: 1000 synthetic dialogues involving two speakers. We fix two reference audio clips to evaluate the model's ability in double voice cloning for utterances ranging from 4 to 10 dialogues between two randomly chosen persona.\nsmall talk (no ref): 250 synthetic dialogues curated in the same way as above, but are characterized by short utterances and a limited number of turns (4â€“6), we do not fix reference audios in this case and this set is designed to evaluate the model's ability to automatically assign appropriate voices to speakers.\nsmall talk (ref): 250 synthetic dialogues similar to above, but contains even shorter utterances as this set is meant to include reference clips in it's context, similar to two-speaker-conversation.\nWe report the word-error-rate (WER) and the geometric mean between intra-speaker similarity and inter-speaker dis-similarity on these three subsets. Other than Higgs Audio v2, we also evaluated MoonCast and nari-labs/Dia-1.6B-0626, two of the most popular open-source models capable of multi-speaker dialog generation.\nResults are summarized in the following table. We are not able to run nari-labs/Dia-1.6B-0626 on our \"two-speaker-conversation\" subset due to its strict limitation on the length of the utterances and output audio.\ntwo-speaker-conversation\nsmall talk\nsmall talk (no ref)\nWER â†“\nMean Sim & Dis-sim â†‘\nWER â†“\nMean Sim & Dis-sim â†‘\nWER â†“\nMean Sim & Dis-sim â†‘\nMoonCast\n38.77\n46.02\n8.33\n63.68\n24.65\n53.94\nnari-labs/Dia-1.6B-0626\n-\n-\n17.62\n63.15\n19.46\n61.14\nHiggs Audio v2 (base)\n18.88\n51.95\n11.89\n67.92\n14.65\n55.28\nGet Started\nYou need to first install the higgs-audio:\ngit clone https://github.com/boson-ai/higgs-audio.git\ncd higgs-audio\npython3 -m venv higgs_audio_env\nsource higgs_audio_env/bin/activate\npip install -r requirements.txt\npip install -e .\nAfterwards, try to run the following python code snippet to convert text to speech.\nfrom boson_multimodal.serve.serve_engine import HiggsAudioServeEngine, HiggsAudioResponse\nfrom boson_multimodal.data_types import ChatMLSample, Message, AudioContent\nimport torch\nimport torchaudio\nimport time\nimport click\nMODEL_PATH = \"bosonai/higgs-audio-v2-generation-3B-base\"\nAUDIO_TOKENIZER_PATH = \"bosonai/higgs-audio-v2-tokenizer\"\nsystem_prompt = (\n\"Generate audio following instruction.\\n\\n<|scene_desc_start|>\\nAudio is recorded from a quiet room.\\n<|scene_desc_end|>\"\n)\nmessages = [\nMessage(\nrole=\"system\",\ncontent=system_prompt,\n),\nMessage(\nrole=\"user\",\ncontent=\"The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.\",\n),\n]\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nserve_engine = HiggsAudioServeEngine(MODEL_PATH, AUDIO_TOKENIZER_PATH, device=device)\noutput: HiggsAudioResponse = serve_engine.generate(\nchat_ml_sample=ChatMLSample(messages=messages),\nmax_new_tokens=1024,\ntemperature=0.3,\ntop_p=0.95,\ntop_k=50,\nstop_strings=[\"<|end_of_text|>\", \"<|eot_id|>\"],\n)\ntorchaudio.save(f\"output.wav\", torch.from_numpy(output.audio)[None, :], output.sampling_rate)\nYou can also check https://github.com/boson-ai/higgs-audio/tree/main/examples for more example scripts.\nLicense\nSee LICENSE",
    "Pointcept/Concerto": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nModel weights for Concerto",
    "onnx-community/chatterbox-ONNX": "Key Details\nTips\nUsage\nAcknowledgements\nBuilt-in PerTh Watermarking for Responsible AI\nDisclaimer\nChatterbox TTS\nChatterbox Resemble AI's production-grade open source TTS model. Chatterbox supports English out of the box. Licensed under MIT, Chatterbox has been benchmarked against leading closed-source systems like ElevenLabs, and is consistently preferred in side-by-side evaluations.\nWhether you're working on memes, videos, games, or AI agents, Chatterbox brings your content to life. It's also the first open source TTS model to support emotion exaggeration control, a powerful feature that makes your voices stand out.\nChatterbox is provided in an exported ONNX format, enabling fast and portable inference with ONNX Runtime across platforms.\nKey Details\nSoTA zeroshot English TTS\n0.5B Llama backbone\nUnique exaggeration/intensity control\nUltra-stable with alignment-informed inference\nTrained on 0.5M hours of cleaned data\nWatermarked outputs (optional)\nEasy voice conversion script using onnxruntime\nOutperforms ElevenLabs\nTips\nGeneral Use (TTS and Voice Agents):\nThe default settings (exaggeration=0.5, cfg=0.5) work well for most prompts.\nExpressive or Dramatic Speech:\nTry increase exaggeration to around 0.7 or higher.\nHigher exaggeration tends to speed up speech;\nUsage\nLink to GitHub ONNX Export and Inference script\n# !pip install --upgrade onnxruntime==1.22.1 huggingface_hub==0.34.4 transformers==4.46.3 numpy==2.2.6 tqdm==4.67.1 librosa==0.11.0 soundfile==0.13.1 resemble-perth==1.0.1\nimport onnxruntime\nfrom huggingface_hub import hf_hub_download\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom tqdm import tqdm\nimport librosa\nimport soundfile as sf\nS3GEN_SR = 24000\nSTART_SPEECH_TOKEN = 6561\nSTOP_SPEECH_TOKEN = 6562\nclass RepetitionPenaltyLogitsProcessor:\ndef __init__(self, penalty: float):\nif not isinstance(penalty, float) or not (penalty > 0):\nraise ValueError(f\"`penalty` must be a strictly positive float, but is {penalty}\")\nself.penalty = penalty\ndef __call__(self, input_ids: np.ndarray, scores: np.ndarray) -> np.ndarray:\nscore = np.take_along_axis(scores, input_ids, axis=1)\nscore = np.where(score < 0, score * self.penalty, score / self.penalty)\nscores_processed = scores.copy()\nnp.put_along_axis(scores_processed, input_ids, score, axis=1)\nreturn scores_processed\ndef run_inference(\ntext=\"The Lord of the Rings is the greatest work of literature.\",\ntarget_voice_path=None,\nmax_new_tokens = 256,\nexaggeration=0.5,\noutput_dir=\"converted\",\noutput_file_name=\"output.wav\",\napply_watermark=True,\n):\nmodel_id = \"onnx-community/chatterbox-onnx\"\nif not target_voice_path:\ntarget_voice_path = hf_hub_download(repo_id=model_id, filename=\"default_voice.wav\", local_dir=output_dir)\n## Load model\nspeech_encoder_path = hf_hub_download(repo_id=model_id, filename=\"speech_encoder.onnx\", local_dir=output_dir, subfolder='onnx')\nhf_hub_download(repo_id=model_id, filename=\"speech_encoder.onnx_data\", local_dir=output_dir, subfolder='onnx')\nembed_tokens_path = hf_hub_download(repo_id=model_id, filename=\"embed_tokens.onnx\", local_dir=output_dir, subfolder='onnx')\nhf_hub_download(repo_id=model_id, filename=\"embed_tokens.onnx_data\", local_dir=output_dir, subfolder='onnx')\nconditional_decoder_path = hf_hub_download(repo_id=model_id, filename=\"conditional_decoder.onnx\", local_dir=output_dir, subfolder='onnx')\nhf_hub_download(repo_id=model_id, filename=\"conditional_decoder.onnx_data\", local_dir=output_dir, subfolder='onnx')\nlanguage_model_path = hf_hub_download(repo_id=model_id, filename=\"language_model.onnx\", local_dir=output_dir, subfolder='onnx')\nhf_hub_download(repo_id=model_id, filename=\"language_model.onnx_data\", local_dir=output_dir, subfolder='onnx')\n# # Start inferense sessions\nspeech_encoder_session = onnxruntime.InferenceSession(speech_encoder_path)\nembed_tokens_session = onnxruntime.InferenceSession(embed_tokens_path)\nllama_with_past_session = onnxruntime.InferenceSession(language_model_path)\ncond_decoder_session = onnxruntime.InferenceSession(conditional_decoder_path)\ndef execute_text_to_audio_inference(text):\nprint(\"Start inference script...\")\naudio_values, _ = librosa.load(target_voice_path, sr=S3GEN_SR)\naudio_values = audio_values[np.newaxis, :].astype(np.float32)\n## Prepare input\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ninput_ids = tokenizer(text, return_tensors=\"np\")[\"input_ids\"].astype(np.int64)\nposition_ids = np.where(\ninput_ids >= START_SPEECH_TOKEN,\n0,\nnp.arange(input_ids.shape[1])[np.newaxis, :] - 1\n)\nort_embed_tokens_inputs = {\n\"input_ids\": input_ids,\n\"position_ids\": position_ids,\n\"exaggeration\": np.array([exaggeration], dtype=np.float32)\n}\n## Instantiate the logits processors.\nrepetition_penalty = 1.2\nrepetition_penalty_processor = RepetitionPenaltyLogitsProcessor(penalty=repetition_penalty)\nnum_hidden_layers = 30\nnum_key_value_heads = 16\nhead_dim = 64\ngenerate_tokens = np.array([[START_SPEECH_TOKEN]], dtype=np.long)\n# ---- Generation Loop using kv_cache ----\nfor i in tqdm(range(max_new_tokens), desc=\"Sampling\", dynamic_ncols=True):\ninputs_embeds = embed_tokens_session.run(None, ort_embed_tokens_inputs)[0]\nif i == 0:\nort_speech_encoder_input = {\n\"audio_values\": audio_values,\n}\ncond_emb, prompt_token, ref_x_vector, prompt_feat = speech_encoder_session.run(None, ort_speech_encoder_input)\ninputs_embeds = np.concatenate((cond_emb, inputs_embeds), axis=1)\n## Prepare llm inputs\nbatch_size, seq_len, _ = inputs_embeds.shape\npast_key_values = {\nf\"past_key_values.{layer}.{kv}\": np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\nfor layer in range(num_hidden_layers)\nfor kv in (\"key\", \"value\")\n}\nattention_mask = np.ones((batch_size, seq_len), dtype=np.int64)\nlogits, *present_key_values = llama_with_past_session.run(None, dict(\ninputs_embeds=inputs_embeds,\nattention_mask=attention_mask,\n**past_key_values,\n))\nlogits = logits[:, -1, :]\nnext_token_logits = repetition_penalty_processor(generate_tokens, logits)\nnext_token = np.argmax(next_token_logits, axis=-1, keepdims=True).astype(np.int64)\ngenerate_tokens = np.concatenate((generate_tokens, next_token), axis=-1)\nif (next_token.flatten() == STOP_SPEECH_TOKEN).all():\nbreak\n# Get embedding for the new token.\nposition_ids = np.full(\n(input_ids.shape[0], 1),\ni + 1,\ndtype=np.int64,\n)\nort_embed_tokens_inputs[\"input_ids\"] = next_token\nort_embed_tokens_inputs[\"position_ids\"] = position_ids\n## Update values for next generation loop\nattention_mask = np.concatenate([attention_mask, np.ones((batch_size, 1), dtype=np.int64)], axis=1)\nfor j, key in enumerate(past_key_values):\npast_key_values[key] = present_key_values[j]\nspeech_tokens = generate_tokens[:, 1:-1]\nspeech_tokens = np.concatenate([prompt_token, speech_tokens], axis=1)\nreturn speech_tokens, ref_x_vector, prompt_feat\nspeech_tokens, speaker_embeddings, speaker_features = execute_text_to_audio_inference(text)\ncond_incoder_input = {\n\"speech_tokens\": speech_tokens,\n\"speaker_embeddings\": speaker_embeddings,\n\"speaker_features\": speaker_features,\n}\nwav = cond_decoder_session.run(None, cond_incoder_input)[0]\nwav = np.squeeze(wav, axis=0)\n# Optional: Apply watermark\nif apply_watermark:\nimport perth\nwatermarker = perth.PerthImplicitWatermarker()\nwav = watermarker.apply_watermark(wav, sample_rate=S3GEN_SR)\nsf.write(output_file_name, wav, S3GEN_SR)\nprint(f\"{output_file_name} was successfully saved\")\nif __name__ == \"__main__\":\nrun_inference(\ntext=\"Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy's Nexus in an epic late-game pentakill.\",\nexaggeration=0.5,\noutput_file_name=\"output.wav\",\napply_watermark=False,\n)\nAcknowledgements\nXenova\nVladislav Bronzov\nResemble AI\nBuilt-in PerTh Watermarking for Responsible AI\nEvery audio file generated by Chatterbox includes Resemble AI's Perth (Perceptual Threshold) Watermarker - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.\nDisclaimer\nDon't use this model to do bad things. Prompts are sourced from freely available data on the internet.",
    "google/medgemma-27b-it": "Access MedGemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access MedGemma on Hugging Face, you're required to review and agree to Health AI Developer Foundation's terms of use. To do this, please ensure you're logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nMedGemma model card\nModel information\nDescription\nHow to use\nExamples\nModel architecture overview\nTechnical specifications\nCitation\nInputs and outputs\nPerformance and validation\nKey performance metrics\nEthics and safety evaluation\nData card\nDataset overview\nData ownership and documentation\nData citation\nDe-identification/anonymization:\nImplementation information\nSoftware\nUse and limitations\nIntended use\nBenefits\nLimitations\nMedGemma model card\nModel documentation: MedGemma\nResources:\nModel on Google Cloud Model Garden: MedGemma\nModels on Hugging Face: Collection\nGitHub repository (supporting code, Colab notebooks, discussions, and\nissues): MedGemma\nQuick start notebook: GitHub\nFine-tuning notebook: GitHub\nConcept applications built using MedGemma: Collection\nSupport: See Contact\nLicense: The use of MedGemma is governed by the Health AI Developer\nFoundations terms of\nuse.\nAuthor: Google\nModel information\nThis section describes the MedGemma model and how to use it.\nDescription\nMedGemma is a collection of Gemma 3\nvariants that are trained for performance on medical text and image\ncomprehension. Developers can use MedGemma to accelerate building\nhealthcare-based AI applications. MedGemma currently comes in three variants: a\n4B multimodal version and 27B text-only and multimodal versions.\nBoth MedGemma multimodal versions utilize a\nSigLIP image encoder that has been\nspecifically pre-trained on a variety of de-identified medical data, including\nchest X-rays, dermatology images, ophthalmology images, and histopathology\nslides. Their LLM components are trained on a diverse set of medical data,\nincluding medical text, medical question-answer pairs, FHIR-based electronic\nhealth record data (27B multimodal only), radiology images, histopathology\npatches, ophthalmology images, and dermatology images.\nMedGemma 4B is available in both pre-trained (suffix: -pt) and\ninstruction-tuned (suffix -it) versions. The instruction-tuned version is a\nbetter starting point for most applications. The pre-trained version is\navailable for those who want to experiment more deeply with the models.\nMedGemma 27B multimodal has pre-training on medical image, medical record and\nmedical record comprehension tasks. MedGemma 27B text-only has been trained\nexclusively on medical text. Both models have been optimized for inference-time\ncomputation on medical reasoning. This means it has slightly higher performance\non some text benchmarks than MedGemma 27B multimodal. Users who want to work\nwith a single model for both medical text, medical record and medical image\ntasks are better suited for MedGemma 27B multimodal. Those that only need text\nuse-cases may be better served with the text-only variant. Both MedGemma 27B\nvariants are only available in instruction-tuned versions.\nMedGemma variants have been evaluated on a range of clinically relevant\nbenchmarks to illustrate their baseline performance. These evaluations are based\non both open benchmark datasets and curated datasets. Developers can fine-tune\nMedGemma variants for improved performance. Consult the Intended\nuse section below for more details.\nMedGemma is optimized for medical applications that involve a text generation\ncomponent. For medical image-based applications that do not involve text\ngeneration, such as data-efficient classification, zero-shot classification, or\ncontent-based or semantic image retrieval, the MedSigLIP image\nencoder\nis recommended. MedSigLIP is based on the same image encoder that powers\nMedGemma.\nPlease consult the MedGemma Technical Report\nfor more details.\nHow to use\nBelow are some example code snippets to help you quickly get started running the\nmodel locally on GPU. If you want to use the model at scale, we recommend that\nyou create a production version using Model\nGarden.\nFirst, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0.\n$ pip install -U transformers\nRun model with the pipeline API\nfrom transformers import pipeline\nfrom PIL import Image\nimport requests\nimport torch\npipe = pipeline(\n\"image-text-to-text\",\nmodel=\"google/medgemma-27b-it\",\ntorch_dtype=torch.bfloat16,\ndevice=\"cuda\",\n)\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful medical assistant.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [{\"type\": \"text\", \"text\": \"How do you differentiate bacterial from viral pneumonia?\"}]\n}\n]\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\n# Image attribution: Stillwaterising, CC0, via Wikimedia Commons\nimage_url = \"[https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png](https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png)\"\nimage = Image.open(requests.get(image_url, headers={\"User-Agent\": \"example\"}, stream=True).raw)\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are an expert radiologist.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Describe this X-ray\"},\n{\"type\": \"image\", \"image\": image}\n]\n}\n]\noutput = pipe(text=messages, max_new_tokens=200)\nprint(output[0][\"generated_text\"][-1][\"content\"])\nRun the model directly\n# pip install accelerate\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nfrom PIL import Image\nimport requests\nimport torch\nmodel_id = \"google/medgemma-27b-it\"\nmodel = AutoModelForImageTextToText.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful medical assistant.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [{\"type\": \"text\", \"text\": \"How do you differentiate bacterial from viral pneumonia?\"}]\n}\n]\ninputs = processor.apply_chat_template(\nmessages, add_generation_prompt=True, tokenize=True,\nreturn_dict=True, return_tensors=\"pt\"\n).to(model.device, dtype=torch.bfloat16)\ninput_len = inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**inputs, max_new_tokens=200, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\n# Image attribution: Stillwaterising, CC0, via Wikimedia Commons\nimage_url = \"[https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png](https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png)\"\nimage = Image.open(requests.get(image_url, headers={\"User-Agent\": \"example\"}, stream=True).raw)\nmessages = [\n{\n\"role\": \"system\",\n\"content\": [{\"type\": \"text\", \"text\": \"You are an expert radiologist.\"}]\n},\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"Describe this X-ray\"},\n{\"type\": \"image\", \"image\": image}\n]\n}\n]\ninputs = processor.apply_chat_template(\nmessages, add_generation_prompt=True, tokenize=True,\nreturn_dict=True, return_tensors=\"pt\"\n).to(model.device, dtype=torch.bfloat16)\ninput_len = inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**inputs, max_new_tokens=200, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nExamples\nSee the following Colab notebooks for examples of how to use MedGemma:\nTo give the model a quick try, running it locally with weights from Hugging\nFace, see Quick start notebook in\nColab.\nNote that you will need to use Colab Enterprise to obtain adequate GPU\nresources to run either 27B model without quantization.\nFor an example of fine-tuning the 4B model, see the Fine-tuning notebook in\nColab.\nThe 27B models can be fine tuned in a similar manner but will require more\ntime and compute resources than the 4B model.\nModel architecture overview\nThe MedGemma model is built based on Gemma 3 and\nuses the same decoder-only transformer architecture as Gemma 3. To read more\nabout the architecture, consult the Gemma 3 model\ncard.\nTechnical specifications\nModel type: Decoder-only Transformer architecture, see the Gemma 3\nTechnical\nReport\nInput modalities: 4B and 27B multimodal: Text, vision; 27B text: Text only\nOutput modality: Text only (all models)\nAttention mechanism: Grouped-query attention (GQA)\nContext length: Supports long context, at least 128K tokens\nKey publication: https://arxiv.org/abs/2507.05201\nModel created: July 9, 2025\nModel version: 1.0.0\nCitation\nWhen using this model, please cite: Sellergren et al. \"MedGemma Technical\nReport.\" arXiv preprint arXiv:2507.05201 (2025).\n@article{sellergren2025medgemma,\ntitle={MedGemma Technical Report},\nauthor={Sellergren, Andrew and Kazemzadeh, Sahar and Jaroensri, Tiam and Kiraly, Atilla and Traverse, Madeleine and Kohlberger, Timo and Xu, Shawn and Jamil, Fayaz and Hughes, CÃ­an and Lau, Charles and others},\njournal={arXiv preprint arXiv:2507.05201},\nyear={2025}\n}\nInputs and outputs\nInput:\nText string, such as a question or prompt\nImages, normalized to 896 x 896 resolution and encoded to 256 tokens each\nTotal input length of 128K tokens\nOutput:\nGenerated text in response to the input, such as an answer to a question,\nanalysis of image content, or a summary of a document\nTotal output length of 8192 tokens\nPerformance and validation\nMedGemma was evaluated across a range of different multimodal classification,\nreport generation, visual question answering, and text-based tasks.\nKey performance metrics\nImaging evaluations\nThe multimodal performance of MedGemma 4B and 27B multimodal was evaluated\nacross a range of benchmarks, focusing on radiology, dermatology,\nhistopathology, ophthalmology, and multimodal clinical reasoning.\nMedGemma 4B outperforms the base Gemma 3 4B model across all tested multimodal\nhealth benchmarks.\nTask and metric\nGemma 3 4B\nMedGemma 4B\nGemma 3 27B\nMedGemma 27B multimodal\nMedical image classification\nMIMIC CXR** - macro F1 for top 5 conditions\n81.2\n88.9\n71.7\n90.0\nCheXpert CXR - macro F1 for top 5 conditions\n32.6\n48.1\n26.2\n49.9\nCXR14 - macro F1 for 3 conditions\n32.0\n50.1\n31.4\n45.3\nPathMCQA* (histopathology, internal**)  - Accuracy\n37.1\n69.8\n42.2\n71.6\nUS-DermMCQA* - Accuracy\n52.5\n71.8\n66.9\n71.7\nEyePACS* (fundus, internal) - Accuracy\n14.4\n64.9\n20.3\n75.3\nVisual question answering\nSLAKE (radiology) - Tokenized F1\n40.2\n72.3\n42.5\n70.0\nVQA-RAD*** (radiology) - Tokenized F1\n33.6\n49.9\n42.7\n46.7\nKnowledge and reasoning\nMedXpertQA (text + multimodal questions) - Accuracy\n16.4\n18.8\n22.0\n26.8\n*Internal datasets. US-DermMCQA is described in Liu (2020, Nature\nmedicine), presented as a\n4-way MCQ per example for skin condition classification. PathMCQA is based on\nmultiple datasets, presented as 3-9 way MCQ per example for identification,\ngrading, and subtype for breast, cervical, and prostate cancer. EyePACS is a\ndataset of fundus images with classification labels based on 5-level diabetic\nretinopathy severity (None, Mild, Moderate, Severe, Proliferative). More details\nin the MedGemma Technical Report.\n**Based on radiologist adjudicated labels, described in Yang (2024,\narXiv) Section A.1.1.\n***Based on \"balanced split,\" described in Yang (2024,\narXiv).\nChest X-ray report generation\nMedGemma chest X-ray (CXR) report generation performance was evaluated on\nMIMIC-CXR using the RadGraph\nF1 metric. We compare the MedGemma\npre-trained checkpoint with our previous best model for CXR report generation,\nPaliGemma 2.\nMetric\nMedGemma 4B (pre-trained)\nMedGemma 4B (tuned for CXR)\nMedGemma 27B multimodal (pre-trained)*\nPaliGemma 2 3B (tuned for CXR)\nPaliGemma 2 10B (tuned for CXR)\nChest X-ray report generation\nMIMIC CXR - RadGraph F1\n29.5\n30.3\n27.0\n28.8\n29.5\n*Not released\nThe instruction-tuned versions of MedGemma 4B and MedGemma 27B achieve lower\nscores (21.9 and 21.3, respectively) due to the differences in reporting style\ncompared to the MIMIC ground truth reports. Further fine-tuning on MIMIC reports\nenables users to achieve improved performance, as shown by the improved\nperformance of the MedGemma 4B model that was tuned for CXR.\nText evaluations\nMedGemma 4B and text-only MedGemma 27B were evaluated across a range of\ntext-only benchmarks for medical knowledge and reasoning.\nThe MedGemma models outperform their respective base Gemma models across all\ntested text-only health benchmarks.\nMetric\nGemma 3 4B\nMedGemma 4B\nGemma 3 27B\nMedGemma 27B text-only\nMedGemma 27B multimodal\nMedQA (4-op)\n50.7\n64.4\n74.9\n89.8 (best-of-5) 87.7 (0-shot)\n87.0 (best-of-5) 85.3 (0-shot)\nMedMCQA\n45.4\n55.7\n62.6\n74.2\n70.2\nPubMedQA\n68.4\n73.4\n73.4\n76.8\n77.2\nMMLU Med\n67.2\n70.0\n83.3\n87.0\n86.2\nMedXpertQA (text only)\n11.6\n14.2\n15.7\n25.7\n23.7\nAfriMed-QA (25 question test set)\n48.0\n52.0\n72.0\n84.0\n72.0\nFor all MedGemma 27B results, test-time\nscaling is used to improve performance.\nMedical record evaluations\nAll models were evaluated on a question answer dataset from synthetic FHIR data\nto answer questions about patient records. MedGemma 27B multimodal's\nFHIR-specific training gives it significant improvement over other MedGemma and\nGemma models.\nMetric\nGemma 3 4B\nMedGemma 4B\nGemma 3 27B\nMedGemma 27B text-only\nMedGemma 27B multimodal\nEHRQA\n70.9\n67.6\n84.2\n86.3\n90.5\nEthics and safety evaluation\nEvaluation approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nChild safety: Evaluation of text-to-text and image-to-text prompts\ncovering child safety policies, including child sexual abuse and\nexploitation.\nContent safety: Evaluation of text-to-text and image-to-text prompts\ncovering safety policies, including harassment, violence and gore, and hate\nspeech.\nRepresentational harms: Evaluation of text-to-text and image-to-text\nprompts covering safety policies, including bias, stereotyping, and harmful\nassociations or inaccuracies.\nGeneral medical harms: Evaluation of text-to-text and image-to-text\nprompts covering safety policies, including information quality and harmful\nassociations or inaccuracies.\nIn addition to development level evaluations, we conduct \"assurance evaluations\"\nwhich are our \"arms-length\" internal evaluations for responsibility governance\ndecision making. They are conducted separately from the model development team,\nto inform decision making about release. High-level findings are fed back to the\nmodel team, but prompt sets are held out to prevent overfitting and preserve the\nresults' ability to inform decision making. Notable assurance evaluation results\nare reported to our Responsibility & Safety Council as part of release review.\nEvaluation results\nFor all areas of safety testing, we saw safe levels of performance across the\ncategories of child safety, content safety, and representational harms. All\ntesting was conducted without safety filters to evaluate the model capabilities\nand behaviors. For text-to-text, image-to-text, and audio-to-text, and across\nboth MedGemma model sizes, the model produced minimal policy violations. A\nlimitation of our evaluations was that they included primarily English language\nprompts.\nData card\nDataset overview\nTraining\nThe base Gemma models are pre-trained on a large corpus of text and code data.\nMedGemma multimodal variants utilize a\nSigLIP image encoder that has been\nspecifically pre-trained on a variety of de-identified medical data, including\nradiology images, histopathology images, ophthalmology images, and dermatology\nimages. Their LLM component is trained on a diverse set of medical data,\nincluding medical text, medical question-answer pairs, FHIR-based electronic\nhealth record data (27B multimodal only), radiology images, histopathology\npatches, ophthalmology images, and dermatology images.\nEvaluation\nMedGemma models have been evaluated on a comprehensive set of clinically\nrelevant benchmarks, including over 22 datasets across 6 different tasks and 4\nmedical image modalities. These benchmarks include both open and internal\ndatasets.\nSource\nMedGemma utilizes a combination of public and private datasets.\nThis model was trained on diverse public datasets including MIMIC-CXR (chest\nX-rays and reports), ChestImaGenome: Set of bounding boxes linking image\nfindings with anatomical regions for MIMIC-CXR (MedGemma 27B multimodal only),\nSLAKE (multimodal medical images and questions), PAD-UFES-20 (skin lesion images\nand data), SCIN (dermatology images), TCGA (cancer genomics data), CAMELYON\n(lymph node histopathology images), PMC-OA (biomedical literature with images),\nand Mendeley Digital Knee X-Ray (knee X-rays).\nAdditionally, multiple diverse proprietary datasets were licensed and\nincorporated (described next).\nData ownership and documentation\nMIMIC-CXR: MIT Laboratory\nfor Computational Physiology and Beth Israel Deaconess Medical Center\n(BIDMC).\nSlake-VQA: The Hong Kong Polytechnic\nUniversity (PolyU), with collaborators including West China Hospital of\nSichuan University and Sichuan Academy of Medical Sciences / Sichuan\nProvincial People's Hospital.\nPAD-UFES-20: Federal\nUniversity of EspÃ­rito Santo (UFES), Brazil, through its Dermatological and\nSurgical Assistance Program (PAD).\nSCIN: A collaboration\nbetween Google Health and Stanford Medicine.\nTCGA (The Cancer Genome Atlas): A joint\neffort of National Cancer Institute and National Human Genome Research\nInstitute. Data from TCGA are available via the Genomic Data Commons (GDC)\nCAMELYON: The data was\ncollected from Radboud University Medical Center and University Medical\nCenter Utrecht in the Netherlands.\nPMC-OA (PubMed Central Open Access\nSubset):\nMaintained by the National Library of Medicine (NLM) and National Center for\nBiotechnology Information (NCBI), which are part of the NIH.\nMedQA: This dataset was created by a\nteam of researchers led by Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung\nWeng, Hanyi Fang, and Peter Szolovits\nMendeley Digital Knee\nX-Ray: This dataset is\nfrom Rani Channamma University, and is hosted on Mendeley Data.\nAfriMed-QA: This data was developed and led by\nmultiple collaborating organizations and researchers include key\ncontributors: Intron Health, SisonkeBiotik, BioRAMP, Georgia Institute of\nTechnology, and MasakhaneNLP.\nVQA-RAD: This dataset was\ncreated by a research team led by Jason J. Lau, Soumya Gayen, Asma Ben\nAbacha, and Dina Demner-Fushman and their affiliated institutions (the US\nNational Library of Medicine and National Institutes of Health)\nChest ImaGenome: IBM\nResearch.\nMedExpQA:\nThis dataset was created by researchers at the HiTZ Center (Basque Center\nfor Language Technology and Artificial Intelligence).\nMedXpertQA: This\ndataset was developed by researchers at Tsinghua University (Beijing, China)\nand Shanghai Artificial Intelligence Laboratory (Shanghai, China).\nHealthSearchQA:\nThis dataset consists of consisting of 3,173 commonly searched consumer\nquestions\nIn addition to the public datasets listed above, MedGemma was also trained on\nde-identified, licensed datasets or datasets collected internally at Google from\nconsented participants.\nRadiology dataset 1: De-identified dataset of different CT studies\nacross body parts from a US-based radiology outpatient diagnostic center\nnetwork.\nOphthalmology dataset 1 (EyePACS): De-identified dataset of fundus\nimages from diabetic retinopathy screening.\nDermatology dataset 1: De-identified dataset of teledermatology skin\ncondition images (both clinical and dermatoscopic) from Colombia.\nDermatology dataset 2: De-identified dataset of skin cancer images (both\nclinical and dermatoscopic) from Australia.\nDermatology dataset 3: De-identified dataset of non-diseased skin images\nfrom an internal data collection effort.\nPathology dataset 1: De-identified dataset of histopathology H&E whole\nslide images created in collaboration with an academic research hospital and\nbiobank in Europe. Comprises de-identified colon, prostate, and lymph nodes.\nPathology dataset 2: De-identified dataset of lung histopathology H&E\nand IHC whole slide images created by a commercial biobank in the United\nStates.\nPathology dataset 3: De-identified dataset of prostate and lymph node\nH&E and IHC histopathology whole slide images created by a contract\nresearch organization in the United States.\nPathology dataset 4: De-identified dataset of histopathology whole slide\nimages created in collaboration with a large, tertiary teaching hospital in\nthe United States. Comprises a diverse set of tissue and stain types,\npredominantly H&E.\nEHR dataset 1: Question/answer dataset drawn from synthetic FHIR records\ncreated by Synthea. The test\nset includes 19 unique patients with 200 questions per patient divided into\n10 different categories.\nData citation\nMIMIC-CXR: Johnson, A., Pollard, T., Mark, R., Berkowitz, S., & Horng,\nS. (2024). MIMIC-CXR Database (version 2.1.0). PhysioNet.\nhttps://physionet.org/content/mimic-cxr/2.1.0/\nand Johnson, Alistair E. W., Tom J. Pollard, Seth J. Berkowitz, Nathaniel\nR. Greenbaum, Matthew P. Lungren, Chih-Ying Deng, Roger G. Mark, and Steven\nHorng. 2019. \"MIMIC-CXR, a de-Identified Publicly Available Database of\nChest Radiographs with Free-Text Reports.\" Scientific Data 6 (1): 1â€“8.\nSLAKE: Liu, Bo, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu.\n2021.SLAKE: A Semantically-Labeled Knowledge-Enhanced Dataset for Medical\nVisual Question Answering.\"\nhttp://arxiv.org/abs/2102.09542.\nPAD-UEFS-20: Pacheco, Andre GC, et al. \"PAD-UFES-20: A skin lesion\ndataset composed of patient data and clinical images collected from\nsmartphones.\" Data in brief 32 (2020): 106221.\nSCIN: Ward, Abbi, Jimmy Li, Julie Wang, Sriram Lakshminarasimhan, Ashley\nCarrick, Bilson Campana, Jay Hartford, et al. 2024. \"Creating an Empirical\nDermatology Dataset Through Crowdsourcing With Web Search Advertisements.\"\nJAMA Network Open 7 (11): e2446615â€“e2446615.\nTCGA: The results shown here are in whole or part based upon data\ngenerated by the TCGA Research Network:\nhttps://www.cancer.gov/tcga.\nCAMELYON16: Ehteshami Bejnordi, Babak, Mitko Veta, Paul Johannes van\nDiest, Bram van Ginneken, Nico Karssemeijer, Geert Litjens, Jeroen A. W. M.\nvan der Laak, et al. 2017. \"Diagnostic Assessment of Deep Learning\nAlgorithms for Detection of Lymph Node Metastases in Women With Breast\nCancer.\" JAMA 318 (22): 2199â€“2210.\nMendeley Digital Knee X-Ray: Gornale, Shivanand; Patravali, Pooja\n(2020), \"Digital Knee X-ray Images\", Mendeley Data, V1, doi:\n10.17632/t9ndx37v5h.1\nVQA-RAD: Lau, Jason J., Soumya Gayen, Asma Ben Abacha, and Dina\nDemner-Fushman. 2018. \"A Dataset of Clinically Generated Visual Questions\nand Answers about Radiology Images.\" Scientific Data 5 (1): 1â€“10.\nChest ImaGenome: Wu, J., Agu, N., Lourentzou, I., Sharma, A., Paguio,\nJ., Yao, J. S., Dee, E. C., Mitchell, W., Kashyap, S., Giovannini, A., Celi,\nL. A., Syeda-Mahmood, T., & Moradi, M. (2021). Chest ImaGenome Dataset\n(version 1.0.0). PhysioNet. RRID:SCR_007345.\nhttps://doi.org/10.13026/wv01-y230\nMedQA: Jin, Di, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang,\nand Peter Szolovits. 2020. \"What Disease Does This Patient Have? A\nLarge-Scale Open Domain Question Answering Dataset from Medical Exams.\"\nhttp://arxiv.org/abs/2009.13081.\nAfrimedQA: Olatunji, Tobi, Charles Nimo, Abraham Owodunni, Tassallah\nAbdullahi, Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, et al. 2024.\n\"AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering\nBenchmark Dataset.\"\nhttp://arxiv.org/abs/2411.15640.\nMedExpQA: Alonso, I., Oronoz, M., & Agerri, R. (2024). MedExpQA:\nMultilingual Benchmarking of Large Language Models for Medical Question\nAnswering. arXiv preprint arXiv:2404.05590. Retrieved from\nhttps://arxiv.org/abs/2404.05590\nMedXpertQA: Zuo, Yuxin, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu,\nErmo Hua, Kaiyan Zhang, Ning Ding, and Bowen Zhou. 2025. \"MedXpertQA:\nBenchmarking Expert-Level Medical Reasoning and Understanding.\"\nhttp://arxiv.org/abs/2501.18362.\nDe-identification/anonymization:\nGoogle and its partners utilize datasets that have been rigorously anonymized or\nde-identified to ensure the protection of individual research participants and\npatient privacy.\nImplementation information\nDetails about the model internals.\nSoftware\nTraining was done using JAX.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nUse and limitations\nIntended use\nMedGemma is an open multimodal generative AI model intended to be used as a\nstarting point that enables more efficient development of downstream healthcare\napplications involving medical text and images. MedGemma is intended for\ndevelopers in the life sciences and healthcare space. Developers are responsible\nfor training, adapting and making meaningful changes to MedGemma to accomplish\ntheir specific intended use. MedGemma models can be fine-tuned by developers\nusing their own proprietary data for their specific tasks or solutions.\nMedGemma is based on Gemma 3 and has been further trained on medical images and\ntext. MedGemma enables further development in any medical context (image and\ntextual), however the model was pre-trained using chest X-ray, pathology,\ndermatology, and fundus images. Examples of tasks within MedGemma's training\ninclude visual question answering pertaining to medical images, such as\nradiographs, or providing answers to textual medical questions. Full details of\nall the tasks MedGemma has been evaluated can be found in the MedGemma\nTechnical Report.\nBenefits\nProvides strong baseline medical image and text comprehension for models of\nits size.\nThis strong performance makes it efficient to adapt for downstream\nhealthcare-based use cases, compared to models of similar size without\nmedical data pre-training.\nThis adaptation may involve prompt engineering, grounding, agentic\norchestration or fine-tuning depending on the use case, baseline validation\nrequirements, and desired performance characteristics.\nLimitations\nMedGemma is not intended to be used without appropriate validation, adaptation\nand/or making meaningful modification by developers for their specific use case.\nThe outputs generated by MedGemma are not intended to directly inform clinical\ndiagnosis, patient management decisions, treatment recommendations, or any other\ndirect clinical practice applications. Performance benchmarks highlight baseline\ncapabilities on relevant benchmarks, but even for image and text domains that\nconstitute a substantial portion of training data, inaccurate model output is\npossible. All outputs from MedGemma should be considered preliminary and require\nindependent verification, clinical correlation, and further investigation\nthrough established research and development methodologies.\nMedGemma's multimodal capabilities have been primarily evaluated on single-image\ntasks. MedGemma has not been evaluated in use cases that involve comprehension\nof multiple images.\nMedGemma has not been evaluated or optimized for multi-turn applications.\nMedGemma's training may make it more sensitive to the specific prompt used than\nGemma 3.\nWhen adapting MedGemma developer should consider the following:\nBias in validation data: As with any research, developers should ensure\nthat any downstream application is validated to understand performance using\ndata that is appropriately representative of the intended use setting for\nthe specific application (e.g., age, sex, gender, condition, imaging device,\netc).\nData contamination concerns: When evaluating the generalization\ncapabilities of a large model like MedGemma in a medical context, there is a\nrisk of data contamination, where the model might have inadvertently seen\nrelated medical information during its pre-training, potentially\noverestimating its true ability to generalize to novel medical concepts.\nDevelopers should validate MedGemma on datasets not publicly available or\notherwise made available to non-institutional researchers to mitigate this\nrisk.",
    "nvidia/audio-flamingo-3": "Model Overview\nDescription:\nResults:\nModel Architecture:\nLicense / Terms of Use\nDeployment Geography\nUse Case\nRelease Date\nReferences:\nModel Architecture:\nInput:\nOutput:\nSoftware Integration:\nModel Version:\nTraining and Testing Datasets:\nTraining Dataset:\nTesting Dataset:\nInference:\nEthical Considerations:\nAcknowledgements\nModel Overview\nAudio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio-Language Models\nDescription:\nAudio Flamingo 3 (AF3) is a fully open, state-of-the-art Large Audio-Language Model (LALM) that advances reasoning and understanding across speech, sounds, and music. AF3 builds on previous work with innovations in:\nUnified audio representation learning (speech, sound, music)\nFlexible, on-demand chain-of-thought reasoning\nLong-context audio comprehension (up to 10 minutes)\nMulti-turn, multi-audio conversational dialogue (AF3-Chat)\nVoice-to-voice interaction (AF3-Chat)\nExtensive evaluations confirm AF3â€™s effectiveness, setting new benchmarks on over 20 public audio understanding and reasoning tasks.\nThis model is for non-commercial research purposes only.\nResults:\nModel Architecture:\nAudio Flamingo 3 uses AF-Whisper unified audio encoder, MLP-based audio adaptor, Decoder-only LLM backbone (Qwen2.5-7B), and Streaming TTS module (AF3-Chat). Audio Flamingo 3 can take up to 10 minutes of audio inputs.\nLicense / Terms of Use\nThe model is released under the NVIDIA OneWay Noncommercial License. Portions of the dataset generation are also subject to the Qwen Research License and OpenAIâ€™s Terms of Use.\nDeployment Geography\nGlobal.\nUse Case\nIntended for researchers and developers to explore:\nAudio question answering and reasoning\nLong-context audio comprehension\nInteractive sound/music design assistants\nMulti-turn (voice) chat\nRelease Date\nGithub (07/10/2025) via https://github.com/NVIDIA/audio-flamingo\nHuggingFace (07/10/2025) via https://huggingface.co/nvidia/audio-flamingo-3\nReferences:\nAudio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio-Language Models\nProject Page\nDemo Website\nHugging Face\nModel Architecture:\nArchitecture Type: TransformerNetwork Architecture: Audio Flamingo 3\nAF3 uses:\nAF-Whisper unified audio encoder\nMLP-based audio adaptor\nDecoder-only LLM backbone (Qwen2.5-7B)\nStreaming TTS module (AF3-Chat)\n**This model was developed based on NVILA and Qwen-2.5-7B\nInput:\nInput Type: Audio, Text\nInput Format: WAV/MP3/FLAC, UTF-8 text\nInput Parameters: Audio is Two-Dimensional (2D) and Text is One-Dimensional (1D)\nOther Properties Related to Input:\nMax Audio Length: 10 Minutes\nMax Text Length: 16000 tokens\nOutput:\nOutput Type: Text (and optional speech)\nText Format: UTF-8 string\nOutput Parameters: One-Dimensional (1D)\nOther Properties Related to Output:\nMax Text Length: 1024 tokens\nSpeech Format: streaming TTS (text-to-speech) waveform\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems (A100/H100). By leveraging NVIDIAâ€™s hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nSoftware Integration:\nRuntime Engine: PyTorch / HuggingFace Transformers\nSupported Hardware:\nNVIDIA Ampere (A100)\nNVIDIA Hopper (H100)\nSupported OS:\nLinux\nModel Version:\nv3.0\nTraining and Testing Datasets:\nTraining Dataset:\nAF3 is trained entirely on open-source audio data, organized into four novel, large-scale collections. For each dataset, we mention whether the dataset annotations are collected by Human or they are Automated i.e. generated using AI models.\nThe data collection method noted below applies for all datasets used for training and testing:\nData Collection Method: Human\nLabeling Collection Method: Please see below:\nGeneral Sound:\nWavCaps (Automated)\nMACS (Human)\nSoundDescs (Human)\nClotho-v2 (Human)\nWavText5K (Human)\nClotho-AQA (Human)\nOpen-AQA  (Automated)\nCompA-R  (Automated)\nSalmonn AQA  (Automated)\nAudio Entailment(Automated)\nCompA  (Automated)\nAudioSet  (Human)\nYouTube-8M  (Human)\nFSD50k  (Human)\nCochlScene  (Human)\nNonSpeech7K  (Human)\nChime-Home  (Human)\nSonyc-UST  (Human)\nMusic:\nLP-MusicCaps  (Automated)\nMusicQA  (Automated)\nMusicAVQA  (Human)\nMusicBench  (Automated)\nMu-LLAMA  (Automated)\nNSynth  (Human)\nFMA  (Human)\nMusDB-HQ  (Human)\nMusic4All  (Human)\nMillion Song Dataset  (Human)\nSpeech:\nMSP-Podcast  (Human)\nJL-Corpus  (Human)\nMELD  (Human)\nTess  (Human)\nOMGEmotion  (Human)\nEmov-DB  (Human)\nLibriSpeech  (Human)\nSPGISpeech  (Human)\nTEDLIUM  (Human)\nGigaSpeech  (Human)\nCommon Voice 15  (Human)\nVoxPopuli  (Human)\nVoxCeleb2  (Human)\nSwitchboard  (Human)\nAMI  (Human)\nVoice:\nVoiceAssistant-400K  (Automated)\nMixed:\nAudioSkills-XL (ours) (Automated)\nLongAudio-XL (ours) (Automated)\nAF-Think (ours) (Automated)\nAF-Chat (ours) (Automated)\nTesting Dataset:\nAudio Flamingo 3 is evaluated on the test split of the following datasets.\nData Collection Method: Human (for all datasets noted below)\nLabeling Method: See below\nClothoAQA  (Human)\nMusicAVQA  (Human)\nClotho-v2  (Human)\nCochlScene  (Human)\nNonSpeech7K  (Human)\nNSynth  (Human)\nAudioCaps  (Human)\nUS8K  (Human)\nGTZAN  (Human)\nMMAU  (Human)\nMMAR  (Human)\nAudio Entailment(Automated)\nCompA-R-test  (Automated)\nMuchoMusic  (Automated)\nOpen-AQA(Automated)\nMusicInstruct  (Automated)\nMusicQA  (Automated)\nCMM Hallucination  (Human)\nIEMOCAP  (Human)\nVoiceBench  (Human)\nOpenAudioBench (Human)\nSEED  (Human)\nLibriSpeech  (Human)\nSPGISpeech  (Human)\nTEDLIUM  (Human)\nGigaSpeech  (Human)\nCommon Voice 15  (Human)\nVoxPopuli  (Human)\nLongAudioBench (ours)  (Automated)\nAF-Chat-test (ours)  (Human)\nInference:\nEngine: HuggingFace TransformersTest Hardware: NVIDIA A100 80 GB\nEthical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nAcknowledgements\nBuilt with Qwen, NVILA and the open audio-ML community."
}