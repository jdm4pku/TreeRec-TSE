{
    "Qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ": "Qwen2.5-Coder-0.5B-Instruct-AWQ\nIntroduction\nRequirements\nQuickstart\nEvaluation & Performance\nCitation\nQwen2.5-Coder-0.5B-Instruct-AWQ\nIntroduction\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\nSignificantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\nA more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\nThis repo contains the AWQ-quantized 4-bit instruction-tuned 0.5B Qwen2.5-Coder model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\nNumber of Parameters: 0.49B\nNumber of Paramaters (Non-Embedding): 0.36B\nNumber of Layers: 24\nNumber of Attention Heads (GQA): 14 for Q and 2 for KV\nContext Length: Full 32,768 tokens\nQuantization: AWQ 4-bit\nFor more details, please refer to our blog, GitHub, Documentation, Arxiv.\nRequirements\nThe code of Qwen2.5-Coder has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nAlso check out our AWQ documentation for more usage guide.\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"write a quick sort algorithm.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{hui2024qwen2,\ntitle={Qwen2. 5-Coder Technical Report},\nauthor={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\njournal={arXiv preprint arXiv:2409.12186},\nyear={2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "zatochu/EasyFluffXL": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nPSA: Avoid using Beta scheduler for the Vpred checkpoints on Forge/ReForge if it produces broken outputs. May be hardware specific and not effect everyone. Possibly related to a very recent commit.\nPrompting Tip: While the model can be prompted with tags, natural language, and a mix of the two, the merging process has resulted in overlap of tags that mean one thing and the same words that can mean something else. If you find a certain character tag weak or otherwise \"off\", place a comma at the end of it. (Example: \"ikari shinji,\" instead of just \"ikari shinji\") A quirk of fine-tuned tag-based models is that the commas that seperate the tags in the training captions matter when prompting the resulting model. Commas aren't always necessary, especially if you're able to give supporting words that act as reinforcment for a specific tag, but sometimes a comma is needed for a full character likness. This can extend to artist tags and more niche concept tags.\n[November 18th, 2024]\nUpon further inspection, the original V-Prediction 0.5 version of NoobAI-XL officially added natural language captions to their dataset and I've observed that the raw model now responds to natural language prompts better than 1.0. Due to this, I'm holding off on any further merging and letting the upstream V-Prediction model become more trained as that will offer much more versatility for future merging with the goal of better prompt adherance with natural language prompting.\n[November 15th, 2024]\nEasyFluffXL-V added, a re-merged V-Prediction checkpoint that should allow higher CFG without burning and collapsing. 4-6 CFG range seems ideal now instead of 3-4. Ancestral samlpers are still quite sensitive to higher CFG, so be careful. I advise avoiding VpredIter and Vpred checkpoints but they will be left up for posterity.\n[November 14th, 2024]\nUploaded Redo version of Pre. Acts similar but should have less artifacting.\n[November 12th, 2024]\nPending.\n[November 11th, 2024]\nEasyFluffXLVpred/Iter is a more experimental checkpoint that utliizes NoobAI-XL's 0.5 V-Prediction checkpoint to convert the model to Vpred, which brings some advantages ranging from better colors and better prompt following. It also merges in Itercomp for the Iter version.\nEpsilon-prediction checkpoints are a WIP as it's hard to bring parity in performance between the two.\nLoRas trained on Noob 1.0 should still work adequately on this merge. If training a LoRA, I still reccomend training directly on NoobAI-XL 1.0.\nGenerally safe settings are ~4 CFG (higher than 4 can become noticably too saturated without rescale), Euler A, and Normal scheduler.\nI reccomend using an up-to-date ComfyUI. As of https://github.com/comfyanonymous/ComfyUI/commit/8b275ce5be29ff7d847c3c4c2f3fea1faa68e07b ComfyUI will also automatically detect that the model is V-Prediction/ZSNR and set it for you.\nIf for some reason the autodetection doesn't work or you are on an older ComfyUI, use the ModelSamplingDiscrete node to set V-Prediction and ZSNR.\nAlternatively your option is an up-to-date Forge/ReForge as they automatically detect the vpred and tznsr key I've added to the state_dict of both models and inference as V-Prediction/ZSNR. Forge currently doesn't have a RescaleCFG extension built in, but ReForge does. However I have included a backport of RescaleCFG for Forge in this repo that may work unless the code has changed since creating it. Extract it into extensions-builtin.\nAnother option is A1111's dev branch, as it also has auto-detection of V-Prediction SDXL checkpoints from my understanding.\n[Old]\nExperimental merge of NoobAI-XL 1.0, introducing natural language prompting. No Lightning or other low-step bake-ins. Too early to tell, but I do notice it performs measurably worse at multi-subject outputs that exceed more than 2-3 characters. Some tag associations get lost in meaning with natural language being present now. Despite that, I find the current state of the merge \"fun\". As there is most likely room for improvement, what I will initially upload will be with the caveat that it's an early version and in no way final. Euler A + Beta scheduler seems reasonably safe. Due to the merging process and addition of natural language prompting, some lower tag-count characters and concepts may require some upweighting and/or supporting tags. Some NSFW interactions require really delicate prompting to get what you want.\nThis model is give or take capable of the same content as NoobAI-XL 1.0, which consists of most if not all of Danbooru and E621 posts, given the respective cutoff dates. Meaning this checkpoint can and will produce NSFW content!",
    "shreyasmeher/ConflLlama": "ConflLlama: Domain-Specific LLM for Conflict Event Classification\nTraining Logs\nAcknowledgments\nConflLlama: Domain-Specific LLM for Conflict Event Classification\nConflLlama is a large language model fine-tuned to classify conflict events from text descriptions. This repository contains the GGUF quantized models (q4_k_m, q8_0, and BF16) based on Llama-3.1 8B, which have been adapted for the specialized domain of political violence research.\nThis model was developed as part of the research paper:\nMeher, S., & Brandt, P. T. (2025). ConflLlama: Domain-specific adaptation of large language models for conflict event classification. Research & Politics, July-September 2025. https://doi.org/10.1177/20531680251356282\nKey Contributions\nThe ConflLlama project demonstrates how efficient fine-tuning of large language models can significantly advance the automated classification of political events. The key contributions are:\nState-of-the-Art Performance: Achieves a macro-averaged AUC of 0.791 and a weighted F1-score of 0.753, representing a 37.6% improvement over the base model.\nEfficient Domain Adaptation: Utilizes Quantized Low-Rank Adaptation (QLORA) to fine-tune the Llama-3.1 8B model, making it accessible for researchers with consumer-grade hardware.\nEnhanced Classification: Delivers accuracy gains of up to 1463% in challenging and rare event categories like \"Unarmed Assault\".\nRobust Multi-Label Classification: Effectively handles complex events with multiple concurrent attack types, achieving a Subset Accuracy of 0.724.\nModel Performance\nConflLlama variants substantially outperform the base Llama-3.1 model in zero-shot classification. The fine-tuned models show significant gains across all major metrics, demonstrating the effectiveness of domain-specific adaptation.\nModel\nAccuracy\nMacro F1\nWeighted F1\nAUC\nConflLlama-Q8\n0.765\n0.582\n0.758\n0.791\nConflLlama-Q4\n0.729\n0.286\n0.718\n0.749\nBase Llama-3.1\n0.346\n0.012\n0.369\n0.575\nThe most significant improvements were observed in historically difficult-to-classify categories:\nUnarmed Assault: 1464% improvement (F1-score from 0.035 to 0.553).\nHostage Taking (Barricade): 692% improvement (F1-score from 0.045 to 0.353).\nHijacking: 527% improvement (F1-score from 0.100 to 0.629).\nArmed Assault: 84% improvement (F1-score from 0.374 to 0.687).\nBombing/Explosion: 65% improvement (F1-score from 0.549 to 0.908).\nModel Architecture and Training\nBase Model: unsloth/llama-3-8b-bnb-4bit\nFramework: QLoRA (Quantized Low-Rank Adaptation).\nHardware: NVIDIA A100-SXM4-40GB GPU on the Delta Supercomputer at NCSA.\nOptimizations: 4-bit quantization, gradient checkpointing, and other memory-saving techniques were used to ensure the model could be trained and run on consumer-grade hardware (under 6 GB of VRAM).\nLoRA Configuration:\nRank (r): 8\nAlpha (lora_alpha): 16\nTarget Modules: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj\nTraining Data\nDataset: Global Terrorism Database (GTD). The GTD contains systematic data on over 200,000 terrorist incidents.\nTime Period: The training dataset consists of 171,514 events that occurred before January 1, 2017. The test set includes 38,192 events from 2017 onwards.\nPreprocessing: The pipeline filters data by date, cleans text summaries, and combines primary, secondary, and tertiary attack types into a single multi-label field.\nIntended Use\nThis model is designed for academic and research purposes within the fields of political science, conflict studies, and security analysis.\nClassification of terrorist events based on narrative descriptions.\nResearch into patterns of political violence and terrorism.\nAutomated coding of event data for large-scale analysis.\nLimitations\nTemporal Scope: The model is trained on events prior to 2017 and may not fully capture novel or evolving attack patterns that have emerged since.\nTask-Specific Focus: The model is specialized for attack type classification and is not designed for identifying perpetrators, locations, or targets.\nData Dependency: Performance is dependent on the quality and detail of the input event descriptions.\nSemantic Ambiguity: The model may occasionally struggle to distinguish between semantically close categories, such as 'Armed Assault' and 'Assassination,' when tactical details overlap.\nEthical Considerations\nThe model is trained on sensitive data related to real-world terrorism and should be used responsibly for research purposes only.\nIt is intended for research and analysis, not for operational security decisions or prognostications.\nOutputs should be interpreted with an understanding of the data's context and the model's limitations. Over-classification can lead to resource misallocation in real-world scenarios.\nTraining Logs\nThe training logs show a successful training run with healthy convergence patterns:\nLoss & Learning Rate:\nLoss decreases from 1.95 to ~0.90, with rapid initial improvement. The final training loss reached 0.8843.\nLearning rate uses warmup/decay schedule, peaking at ~1.5x10^-4.\nTraining Stability:\nStable gradient norms (0.4-0.6 range).\nConsistent GPU memory usage (~5800MB allocated, 7080MB reserved), staying under a 6 GB footprint.\nSteady training speed (~3.5s/step) with brief interruption at step 800.\nThe graphs indicate effective model training with good optimization dynamics and resource utilization. The loss vs. learning rate plot suggests optimal learning around 10^-4.\nAcknowledgments\nThis research was supported by NSF award 2311142.\nThis work utilized the Delta system at the NCSA (University of Illinois) through ACCESS allocation CIS220162.\nThanks to the Unsloth team for their optimization framework and base model.\nThanks to Hugging Face for the model hosting and transformers infrastructure.\nThanks to the Global Terrorism Database team at the University of Maryland.",
    "thirdeyeai/Qwen2.5-Coder-7B-Instruct-Uncensored": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nthis uncensored model was developed by thirdeyeai\nthirdeye ai llc is not responsible or liable for anything users of this uncensored model do with the technology.\navailable at ollama\nhttps://ollama.com/thirdeyeai/qwen2.5-coder-uncensored-7b",
    "gpustack/FLUX.1-dev-GGUF": "FLUX.1-dev-GGUF\nKey Features\nUsage\nAPI Endpoints\nComfyUI\nDiffusers\nLimitations\nOut-of-Scope Use\nLicense\nFLUX.1-dev-GGUF\n!!! Experimental supported by gpustack/llama-box v0.0.77+ only !!!\nModel creator: Black Forest Labs\nOriginal model: FLUX.1-dev\nGGUF quantization: based on stable-diffusion.cpp ac54e that patched by llama-box.\nQuantization\nOpenAI CLIP ViT-L/14 Quantization\nGoogle T5-xxl Quantization\nVAE Quantization\nFP16\nFP16\nFP16\nFP16\nQ8_0\nFP16\nQ8_0\nFP16\n(pure) Q8_0\nQ8_0\nQ8_0\nFP16\nQ4_1\nFP16\nQ8_0\nFP16\nQ4_0\nFP16\nQ8_0\nFP16\n(pure) Q4_0\nQ4_0\nQ4_0\nFP16\nFLUX.1 [dev] is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions.\nFor more information, please read our blog post.\nKey Features\nCutting-edge output quality, second only to our state-of-the-art model FLUX.1 [pro].\nCompetitive prompt following, matching the performance of closed source alternatives .\nTrained using guidance distillation, making FLUX.1 [dev] more efficient.\nOpen weights to drive new scientific research, and empower artists to develop innovative workflows.\nGenerated outputs can be used for personal, scientific, and commercial purposes as described in the FLUX.1 [dev] Non-Commercial License.\nUsage\nWe provide a reference implementation of FLUX.1 [dev], as well as sampling code, in a dedicated github repository.\nDevelopers and creatives looking to build on top of FLUX.1 [dev] are encouraged to use this as a starting point.\nAPI Endpoints\nThe FLUX.1 models are also available via API from the following sources\nbfl.ml (currently FLUX.1 [pro])\nreplicate.com\nfal.ai\nmystic.ai\nComfyUI\nFLUX.1 [dev] is also available in Comfy UI for local inference with a node-based workflow.\nDiffusers\nTo use FLUX.1 [dev] with the üß® diffusers python library, first install or upgrade diffusers\npip install -U diffusers\nThen you can use FluxPipeline to run the model\nimport torch\nfrom diffusers import FluxPipeline\npipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\npipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\nprompt = \"A cat holding a sign that says hello world\"\nimage = pipe(\nprompt,\nheight=1024,\nwidth=1024,\nguidance_scale=3.5,\nnum_inference_steps=50,\nmax_sequence_length=512,\ngenerator=torch.Generator(\"cpu\").manual_seed(0)\n).images[0]\nimage.save(\"flux-dev.png\")\nTo learn more check out the diffusers documentation\nLimitations\nThis model is not intended or able to provide factual information.\nAs a statistical model this checkpoint might amplify existing societal biases.\nThe model may fail to generate output that matches the prompts.\nPrompt following is heavily influenced by the prompting-style.\nOut-of-Scope Use\nThe model and its derivatives may not be used\nIn any way that violates any applicable national, federal, state, local or international law or regulation.\nFor the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.\nTo generate or disseminate verifiably false information and/or content with the purpose of harming others.\nTo generate or disseminate personal identifiable information that can be used to harm an individual.\nTo harass, abuse, threaten, stalk, or bully individuals or groups of individuals.\nTo create non-consensual nudity or illegal pornographic content.\nFor fully automated decision making that adversely impacts an individual's legal rights or otherwise creates or modifies a binding, enforceable obligation.\nGenerating or facilitating large-scale disinformation campaigns.\nLicense\nThis model falls under the FLUX.1 [dev] Non-Commercial License.",
    "mlx-community/Qwen2.5-Coder-32B-Instruct-8bit": "mlx-community/Qwen2.5-Coder-32B-Instruct-8bit\nUse with mlx\nmlx-community/Qwen2.5-Coder-32B-Instruct-8bit\nThe Model mlx-community/Qwen2.5-Coder-32B-Instruct-8bit was converted to MLX format from Qwen/Qwen2.5-Coder-32B-Instruct using mlx-lm version 0.19.3.\nUse with mlx\npip install mlx-lm\nfrom mlx_lm import load, generate\nmodel, tokenizer = load(\"mlx-community/Qwen2.5-Coder-32B-Instruct-8bit\")\nprompt=\"hello\"\nif hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nresponse = generate(model, tokenizer, prompt=prompt, verbose=True)",
    "mlx-community/Qwen2.5-Coder-32B-Instruct-4bit": "mlx-community/Qwen2.5-Coder-32B-Instruct-4bit\nUse with mlx\nmlx-community/Qwen2.5-Coder-32B-Instruct-4bit\nThe Model mlx-community/Qwen2.5-Coder-32B-Instruct-4bit was converted to MLX format from Qwen/Qwen2.5-Coder-32B-Instruct using mlx-lm version 0.19.3.\nUse with mlx\npip install mlx-lm\nfrom mlx_lm import load, generate\nmodel, tokenizer = load(\"mlx-community/Qwen2.5-Coder-32B-Instruct-4bit\")\nprompt=\"hello\"\nif hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nresponse = generate(model, tokenizer, prompt=prompt, verbose=True)",
    "mlx-community/Qwen2.5-Coder-0.5B-Instruct-4bit": "mlx-community/Qwen2.5-Coder-0.5B-Instruct-4bit\nUse with mlx\nmlx-community/Qwen2.5-Coder-0.5B-Instruct-4bit\nThe Model mlx-community/Qwen2.5-Coder-0.5B-Instruct-4bit was converted to MLX format from Qwen/Qwen2.5-Coder-0.5B-Instruct using mlx-lm version 0.19.3.\nUse with mlx\npip install mlx-lm\nfrom mlx_lm import load, generate\nmodel, tokenizer = load(\"mlx-community/Qwen2.5-Coder-0.5B-Instruct-4bit\")\nprompt=\"hello\"\nif hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nresponse = generate(model, tokenizer, prompt=prompt, verbose=True)",
    "allenai/ACE2-ERA5": "ACE2-ERA5\nQuick links\nInference quickstart\nStrengths and weaknesses\nACE2-ERA5\nAi2 Climate Emulator (ACE) is a family of models designed to simulate atmospheric variability from the time scale of days to centuries.\nDisclaimer: ACE models are research tools and should not be used for operational climate predictions.\nACE2-ERA5 is trained on the ERA5 dataset and is described in ACE2: Accurately learning subseasonal to decadal atmospheric variability and forced responses.\nQuick links\nüìÉ Paper\nüíª Code\nüí¨ Docs\nüìÇ All Models\nInference quickstart\nDownload this repository. Optionally, you can just download a subset of the forcing_data and initial_conditions for the period you are interested in.\nUpdate paths in the inference_config.yaml. Specifically, update experiment_dir, checkpoint_path, initial_condition.path and forcing_loader.dataset.path.\nInstall code dependencies with pip install fme.\nRun inference with python -m fme.ace.inference inference_config.yaml.\nStrengths and weaknesses\nBriefly, the strengths of ACE2-ERA5 are:\naccurate atmospheric warming response to combined increase of sea surface temperature and CO2 over last 80 years\nhighly accurate atmospheric response to El Ni√±o sea surface temperature variability\ngood representation of the geographic distribution of tropical cyclones\naccurate Madden Julian Oscillation variability\nrealistic stratospheric polar vortex strength and variability\nexact conservation of global dry air mass and moisture\nSome known weaknesses are:\nthe individual sensitivities to changing sea surface temperature and CO2 are not entirely realistic\nthe medium-range (3-10 day) weather forecast skill is not state of the art\nnot expected to generalize accurately for large perturbations of certain inputs (e.g. doubling of CO2)",
    "unsloth/Qwen2.5-Coder-0.5B-Instruct-bnb-4bit": "Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\n‚ú® Finetune for Free\nunsloth/Qwen2.5-Coder-0.5B-Instruct-bnb-4bit\nIntroduction\nRequirements\nEvaluation & Performance\nCitation\nFinetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nWe have a Qwen 2.5 (all model sizes) free Google Colab Tesla T4 notebook.\nAlso a Qwen 2.5 conversational style notebook.\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.1 8b\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma-2 9b\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral 7b\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nTinyLlama\n‚ñ∂Ô∏è Start on Colab\n3.9x faster\n74% less\nDPO - Zephyr\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nunsloth/Qwen2.5-Coder-0.5B-Instruct-bnb-4bit\nIntroduction\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\nSignificantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\nA more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\nThis repo contains the 0.5B Qwen2.5-Coder model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\nNumber of Parameters: 0.49B\nNumber of Paramaters (Non-Embedding): 0.36B\nNumber of Layers: 24\nNumber of Attention Heads (GQA): 14 for Q and 2 for KV\nContext Length: Full 32,768 tokens\nWe do not recommend using base language models for conversations. Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., or fill in the middle tasks on this model.\nFor more details, please refer to our blog, GitHub, Documentation, Arxiv.\nRequirements\nThe code of Qwen2.5-Coder has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{hui2024qwen2,\ntitle={Qwen2. 5-Coder Technical Report},\nauthor={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\njournal={arXiv preprint arXiv:2409.12186},\nyear={2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "unsloth/Qwen2.5-Coder-14B-Instruct-128K-GGUF": "YaRN 128K. 32K non extended GGUF here: link\nFinetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\n‚ú® Finetune for Free\nunsloth/Qwen2.5-Coder-1.5B-Instruct-128K-GGUF\nIntroduction\nRequirements\nEvaluation & Performance\nCitation\nYaRN 128K. 32K non extended GGUF here: link\nFinetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nWe have a Qwen 2.5 (all model sizes) free Google Colab Tesla T4 notebook.\nAlso a Qwen 2.5 conversational style notebook.\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.1 8b\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma-2 9b\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral 7b\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nTinyLlama\n‚ñ∂Ô∏è Start on Colab\n3.9x faster\n74% less\nDPO - Zephyr\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nunsloth/Qwen2.5-Coder-1.5B-Instruct-128K-GGUF\nIntroduction\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\nSignificantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\nA more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\nThis repo contains the 0.5B Qwen2.5-Coder model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\nNumber of Parameters: 0.49B\nNumber of Paramaters (Non-Embedding): 0.36B\nNumber of Layers: 24\nNumber of Attention Heads (GQA): 14 for Q and 2 for KV\nContext Length: Full 32,768 tokens\nWe do not recommend using base language models for conversations. Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., or fill in the middle tasks on this model.\nFor more details, please refer to our blog, GitHub, Documentation, Arxiv.\nRequirements\nThe code of Qwen2.5-Coder has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{hui2024qwen2,\ntitle={Qwen2. 5-Coder Technical Report},\nauthor={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\njournal={arXiv preprint arXiv:2409.12186},\nyear={2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "ArliAI/Qwen2.5-32B-ArliAI-RPMax-v1.3": "Qwen2.5-32B-ArliAI-RPMax-v1.3\nRPMax Series Overview\nModel Description\nSpecs\nTraining Details\nQuantization\nSuggested Prompt Format\nRPMax: Reduced repetition and higher creativity model\nWhat is repetition and creativity?\nDataset Curation\nThe Golden Rule of Fine-Tuning\nTraining Parameters\nRPMax's Unconventional Approach\nDifference between versions?\nReal Success?\nQwen2.5-32B-ArliAI-RPMax-v1.3\n=====================================\nRPMax Series Overview\nv1.1 = 2B | 3.8B | 8B | 9B | 12B | 20B | 22B | 70B\nv1.2 = 8B | 12B | 70B\nv1.3 = 8B | 12B | 32B | 70B\nRPMax is a series of models that are trained on a diverse set of curated creative writing and RP datasets with a focus on variety and deduplication. This model is designed to be highly creative and non-repetitive by making sure no two entries in the dataset have repeated characters or situations, which makes sure the model does not latch on to a certain personality and be capable of understanding and acting appropriately to any characters or situations.\nMany RPMax users mentioned that these models does not feel like any other RP models, having a different writing style and generally doesn't feel in-bred.\nYou can access the model at https://arliai.com and we also have a models ranking page at https://www.arliai.com/models-ranking\nAsk questions in our new Discord Server https://discord.com/invite/t75KbPgwhk or on our subreddit https://www.reddit.com/r/ArliAI/\nModel Description\nQwen2.5-32B-ArliAI-RPMax-v1.3 is a variant made from the Qwen2.5-32B-Instruct model.\nLet us know what you think of the model! The different parameter versions are based on different models, so they might all behave slightly differently in their own way.\nv1.3 updated models are trained with updated software and configs such as the updated transformers library that fixes the gradient checkpointing bug which should help the model learn better.\nThis version also uses RSLORA+ for training which helps the model learn even better.\nSpecs\nContext Length: 128K\nParameters: 32B\nTraining Details\nSequence Length: 8192\nTraining Duration: Approximately 3 days on 2x3090Ti\nEpochs: 1 epoch training for minimized repetition sickness\nRS-QLORA+: 64-rank 64-alpha, resulting in ~2% trainable weights\nLearning Rate: 0.00001\nGradient accumulation: Very low 32 for better learning.\nQuantization\nThe model is available in quantized formats:\nFP16: https://huggingface.co/ArliAI/Qwen2.5-32B-ArliAI-RPMax-v1.3\nGGUF: https://huggingface.co/ArliAI/Qwen2.5-32B-ArliAI-RPMax-v1.3-GGUF\nSuggested Prompt Format\nChatML Chat Format\n<|im_start|>system\nProvide some context and/or instructions to the model.\n<|im_end|>\n<|im_start|>user\nThe user‚Äôs message goes here\n<|im_end|>\n<|im_start|>assistant\nRPMax: Reduced repetition and higher creativity model\nThe goal of RPMax is to reduce repetitions and increase the models ability to creatively write in different situations presented to it. What this means is it is a model that will output responses very differently without falling into predictable tropes even in different situations.\nWhat is repetition and creativity?\nFirst of all, creativity should mean the variety in output that the model is capable of creating. You should not confuse creativity with writing prose. When a model writes in a way that can be said to be pleasant like writers would write in a novel, this is not creative writing. This is just a model having a certain pleasant type of writing prose. So a model that writes nicely is not necessarily a creative model.\nRepetition and creativity are essentially intertwined with each other, so if a model is repetitive then a model can also be said to be un-creative as it cannot write new things and can only repeat similar responses that it has created before. For repetition there are actually two very different forms of repetition.\nIn-context repetition: When people mention a model is repetitive, this usually mean a model that likes to repeat the same phrases in a single conversation. An example of this is when a model says that a character \"flicks her hair and....\" and then starts to prepend that \"flicks her hair and...\" into every other action that character does.\nIt can be said that the model is boring, but even in real people's writing it is possible that this kind of repetition could be intentional to subtly prove a point or showcase a character's traits in some scenarios. So this type of repetition is not always bad and completely discouraging a model from doing this does not always lead to improve a model's writing ability.\nCross-context repetition: A second arguably worse type of repetition is a model's tendency to repeat the same phrases or tropes in very different situations. An example is a model that likes to repeat the infamous \"shivers down my spine\" phrase in wildly different conversations that don't necessarily fit with that phrase.\nThis type of repetition is ALWAYS bad as it is a sign that the model has over-fitted into that style of \"creative writing\" that it has often seen in the training dataset. A model's tendency to have cross-context repetition is also usually visible in how a model likes to choose similar repetitive names when writing stories. Such as the infamous \"elara\" and \"whispering woods\" names.\nWith RPMax v1 the main goal is to create a highly creative model by reducing reducing cross-context repetition, as that is the type of repetition that follows you through different conversations. This is also a type of repetition that can be combated by making sure your dataset does not have repetitions of the same situations or characters in different example entries.\nDataset Curation\nRPMax is successful thanks to the training method and training dataset that was created for these models' fine-tuning. It contains as many open source creative writing and RP datasets that can be found (mostly from Hugging Face), from which have been curated to weed out datasets that are purely synthetic generations as they often only serve to dumb down the model and make the model learn GPT-isms (slop) rather than help.\nThen Llama 3.1 8B is used to create a database of the characters and situations that are portrayed in these datasets, which is then used to de-dupe these datasets to make sure that there is only a single entry of any character or situation.\nThe Golden Rule of Fine-Tuning\nUnlike the initial pre-training stage where the more data you throw at it the better it becomes for the most part, the golden rule for fine-tuning models isn't quantity, but instead quality over quantity. So the dataset for RPMax is actually orders of magnitude smaller than it would be if it included repeated characters and situations in the dataset, but the end result is a model that does not feel like just another remix of any other creative writing/RP model.\nTraining Parameters\nRPMax's training parameters are also a different approach to other fine-tunes. The usual way is to have a low learning rate and high gradient accumulation for better loss stability, and then run multiple epochs of the training run until the loss is acceptable.\nRPMax's Unconventional Approach\nRPMax, on the other hand, is only trained for one single epoch, uses a low gradient accumulation, and a higher than normal learning rate. The loss curve during training is actually unstable and jumps up and down a lot, but if you smooth it out, it is actually still steadily decreasing over time although never end up at a very low loss value. The theory is that this allows the models to learn from each individual example in the dataset much more, and by not showing the model the same example twice, it will stop the model from latching on and reinforcing a single character or story trope.\nThe jumping up and down of loss during training is because as the model gets trained on a new entry from the dataset, the model will have never seen a similar example before and therefore can't really predict an answer similar to the example entry. While, the relatively high end loss of 1.0 or slightly above for RPMax models is actually good because the goal was never to create a model that can output exactly like the dataset that is being used to train it. Rather to create a model that is creative enough to make up it's own style of responses.\nThis is different from training a model in a particular domain and needing the model to reliably be able to output like the example dataset, such as when training a model on a company's internal knowledge base.\nDifference between versions?\nv1.0 had some mistakes in the training parameters, hence why not many versions of it were created.\nv1.1 fixed the previous errors and is the version where many different base models were used in order to compare and figure out which models are most ideal for RPMax. The consensus is that Mistral based models were fantastic for RPMax as they are by far the most uncensored by default. On the other hand, Gemma seems to also have a quite interesting writing style, but on the other hand it had a lot of issues with running and training and the general low interest in it. Llama 3.1 based models also seem to do well, with 70B being having the lowest loss at the end of the training runs.\nv1.2 was a fix of the dataset, where it was found that there are many entries that contained broken or otherwise nonsensical system prompts or messages in the example conversations. Training the model on v1.2 predictable made them better at following instructions and staying coherent.\nv1.3 was not meant to be created, but due to the gradient checkpointing bug being found recently and training frameworks finally getting updated with the fix, it sounds like a good excuse to run a v1.3 of RPMax. This version is a focus on improving the training parameters, this time training was done using rsLoRA+ or rank-stabilized low rank adaptation with the addition of LoRA plus. These additions improved the models learning quite considerably, with the models all achieving lower loss than the previous iteration and outputting better quality outputs in real usage.\nReal Success?\nRPMax models have been out for a few months at this point, with versions v1.0 all the way to the now new v1.3. So far it seems like RPMax have been a resounding success in that achieves it's original goal of being a new creative writing/RP model that does not write like other RP finetunes. A lot of users of it mentioned it kind of almost feels like interacting with a real person when in an RP scenario, and that it does impressively unexpected things in their stories that caught them off guard in a good way.\nIs it the best model there is? Probably not, but there isn't ever one single best model. So try it out for yourself and maybe you will like it! As always any feedback on the model is always appreciated and will be taken into account for the next versions.",
    "strangerzonehf/Flux-Super-Realism-LoRA": "Model description for super realism engine\nComparison between the base model and related models.\nPrevious Model Links\nHosted/Demo Links\nModel Training Basic Details\nFlux-Super-Realism-LoRA Model GitHub\nAPI Usage / Quick Usage\nSetting Up Flux Space\nTrigger words\nDownload model\nPrompt\nSuper Realism, Woman in a red jacket, snowy, in the style of hyper-realistic portraiture, caninecore, mountainous vistas, timeless beauty, palewave, iconic, distinctive noses --ar 72:101 --stylize 750 --v 6\nPrompt\nSuper Realism, Headshot of handsome young man, wearing dark gray sweater with buttons and big shawl collar, brown hair and short beard, serious look on his face, black background, soft studio lighting, portrait photography --ar 85:128 --v 6.0 --style rawHeadshot of handsome young man, wearing dark gray sweater with buttons and big shawl collar, brown hair and short beard, serious look on his face, black background, soft studio lighting, portrait photography --ar 85:128 --v 6.0 --style rawHeadshot of handsome young man, wearing dark gray sweater with buttons and big shawl collar, brown hair and short beard, serious look on his face, black background, soft studio lighting, portrait photography --ar 85:128 --v 6.0 --style raw\nPrompt\nSuper Realism, High-resolution photograph, woman, UHD, photorealistic, shot on a Sony A7III --chaos 20 --ar 1:2 --style raw --stylize 250\nModel description for super realism engine\nImage Processing Parameters\nParameter\nValue\nParameter\nValue\nLR Scheduler\nconstant\nNoise Offset\n0.03\nOptimizer\nAdamW\nMultires Noise Discount\n0.1\nNetwork Dim\n64\nMultires Noise Iterations\n10\nNetwork Alpha\n32\nRepeat & Steps\n30 & 4380\nEpoch\n20\nSave Every N Epochs\n1\nComparison between the base model and related models.\nComparison between the base model FLUX.1-dev and its adapter, a LoRA model tuned for super-realistic realism.\n[ 28 steps ]\nHowever, it performs better in various aspects compared to its previous models, including face realism, ultra-realism, and others.\nprevious versions [ 28 steps ]\nPrevious Model Links\nModel Name\nDescription\nLink\nCanopus-LoRA-Flux-FaceRealism\nLoRA model for Face Realism\nCanopus-LoRA-Flux-FaceRealism\nCanopus-LoRA-Flux-UltraRealism-2.0\nLoRA model for Ultra Realism\nCanopus-LoRA-Flux-UltraRealism-2.0\nFlux.1-Dev-LoRA-HDR-Realism [Experimental Version]\nLoRA model for HDR Realism\nFlux.1-Dev-LoRA-HDR-Realism\nFlux-Realism-FineDetailed\nFine-detailed realism-focused model\nFlux-Realism-FineDetailed\nHosted/Demo Links\nDemo Name\nDescription\nLink\nFLUX-LoRA-DLC\nDemo for FLUX LoRA DLC\nFLUX-LoRA-DLC\nFLUX-REALISM\nDemo for FLUX Realism\nFLUX-REALISM\nModel Training Basic Details\nFeature\nDescription\nLabeling\nflorence2-en (natural language & English)\nTotal Images Used for Training\n55 [Hi-Res]\nBest Dimensions\n- 1024 x 1024 (Default)\n- 768 x 1024\nFlux-Super-Realism-LoRA Model GitHub\nRepository Link\nDescription\nFlux-Super-Realism-LoRA\nFlux Super Realism LoRA model repository for high-quality realism generation\nAPI Usage / Quick Usage\nfrom gradio_client import Client\nclient = Client(\"prithivMLmods/FLUX-REALISM\")\nresult = client.predict(\nprompt=\"A tiny astronaut hatching from an egg on the moon, 4k, planet theme\",\nseed=0,\nwidth=1024,\nheight=1024,\nguidance_scale=6,\nrandomize_seed=True,\napi_name=\"/run\"\n#takes minimum of 30 seconds\n)\nprint(result)\nSetting Up Flux Space\nimport torch\nfrom pipelines import DiffusionPipeline\nbase_model = \"black-forest-labs/FLUX.1-dev\"\npipe = DiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.bfloat16)\nlora_repo = \"strangerzonehf/Flux-Super-Realism-LoRA\"\ntrigger_word = \"Super Realism\"   #triggerword\npipe.load_lora_weights(lora_repo)\ndevice = torch.device(\"cuda\")\npipe.to(device)\nTrigger words\nTrigger words:  You should use Super Realism to trigger the image generation.\nThe trigger word is not mandatory; ensure that words like \"realistic\" and \"realism\" appear in the image description. The \"super realism\" trigger word should prompt an exact match to the reference image in the showcase.\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.",
    "Qiskit/granite-8b-qiskit-GGUF": "Qiskit/granite-8b-qiskit-GGUF\nQiskit/granite-8b-qiskit-GGUF\nThis is the Q4_K_M converted version of the original Qiskit/granite-8b-qiskit.\nPlease refer to the original granite-8b-qiskit model card for more details.",
    "lenML/aya-expanse-8b-abliterated": "Model Card for aya-expanse-8b-abliterated\nLimitations\nModel Card for aya-expanse-8b-abliterated\nThis is an uncensored version of aya-expanse-8b created with abliteration (see this article to know more about it).\nSpecial thanks to @FailSpy for the original code and technique. Please follow him if you're interested in abliterated models.\nLimitations\nÁõÆÂâçÔºåÊ†πÊçÆÊàëÁöÑ lenml-reject-eval ÊµãËØïÔºåÊ≠§ÁâàÊú¨Ê®°ÂûãÂ∞ÜÊãíÁªùËØÑÂàÜ‰ªé 0.91 Èôç‰ΩéÂà∞ 0.50ÔºåËøô‰ªçÁÑ∂ÊòØ‰∏Ä‰∏™ÂæàÈ´òÁöÑÂàÜÊï∞ÔºàÁõÆÂâçÂÆåÂÖ®Ëß£Èô§ÈôêÂà∂ÁöÑÊ®°ÂûãÂú® reject eval ‰∏≠ÊúÄ‰ΩéÂèØ‰ª•Âà∞ËææÂà∞ 0.05Ôºâ\nÂêéÁª≠Ëøò‰ºöÁªßÁª≠Êõ¥Êñ∞Ëøô‰∏™Ê®°Âûã",
    "bartowski/Qwen2.5-32B-ArliAI-RPMax-v1.3-GGUF": "Llamacpp imatrix Quantizations of Qwen2.5-32B-ArliAI-RPMax-v1.3\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nQ4_0_X_X\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of Qwen2.5-32B-ArliAI-RPMax-v1.3\nUsing llama.cpp release b4058 for quantization.\nOriginal model: https://huggingface.co/ArliAI/Qwen2.5-32B-ArliAI-RPMax-v1.3\nAll quants made using imatrix option with dataset from here\nRun them in LM Studio\nPrompt format\n<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nQwen2.5-32B-ArliAI-RPMax-v1.3-f16.gguf\nf16\n65.54GB\ntrue\nFull F16 weights.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q8_0.gguf\nQ8_0\n34.82GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q6_K_L.gguf\nQ6_K_L\n27.26GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q6_K.gguf\nQ6_K\n26.89GB\nfalse\nVery high quality, near perfect, recommended.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q5_K_L.gguf\nQ5_K_L\n23.74GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q5_K_M.gguf\nQ5_K_M\n23.26GB\nfalse\nHigh quality, recommended.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q5_K_S.gguf\nQ5_K_S\n22.64GB\nfalse\nHigh quality, recommended.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q4_K_L.gguf\nQ4_K_L\n20.43GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q4_K_M.gguf\nQ4_K_M\n19.85GB\nfalse\nGood quality, default size for most use cases, recommended.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q4_K_S.gguf\nQ4_K_S\n18.78GB\nfalse\nSlightly lower quality with more space savings, recommended.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q4_0.gguf\nQ4_0\n18.71GB\nfalse\nLegacy format, generally not worth using over similarly sized formats\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q4_0_8_8.gguf\nQ4_0_8_8\n18.64GB\nfalse\nOptimized for ARM inference. Requires 'sve' support (see link below). Don't use on Mac or Windows.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q4_0_4_8.gguf\nQ4_0_4_8\n18.64GB\nfalse\nOptimized for ARM inference. Requires 'i8mm' support (see link below). Don't use on Mac or Windows.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q4_0_4_4.gguf\nQ4_0_4_4\n18.64GB\nfalse\nOptimized for ARM inference. Should work well on all ARM chips, pick this if you're unsure. Don't use on Mac or Windows.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q3_K_XL.gguf\nQ3_K_XL\n17.93GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nQwen2.5-32B-ArliAI-RPMax-v1.3-IQ4_XS.gguf\nIQ4_XS\n17.69GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q3_K_L.gguf\nQ3_K_L\n17.25GB\nfalse\nLower quality but usable, good for low RAM availability.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q3_K_M.gguf\nQ3_K_M\n15.94GB\nfalse\nLow quality.\nQwen2.5-32B-ArliAI-RPMax-v1.3-IQ3_M.gguf\nIQ3_M\n14.81GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q3_K_S.gguf\nQ3_K_S\n14.39GB\nfalse\nLow quality, not recommended.\nQwen2.5-32B-ArliAI-RPMax-v1.3-IQ3_XS.gguf\nIQ3_XS\n13.71GB\nfalse\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q2_K_L.gguf\nQ2_K_L\n13.07GB\nfalse\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\nQwen2.5-32B-ArliAI-RPMax-v1.3-Q2_K.gguf\nQ2_K\n12.31GB\nfalse\nVery low quality but surprisingly usable.\nQwen2.5-32B-ArliAI-RPMax-v1.3-IQ2_M.gguf\nIQ2_M\n11.26GB\nfalse\nRelatively low quality, uses SOTA techniques to be surprisingly usable.\nQwen2.5-32B-ArliAI-RPMax-v1.3-IQ2_S.gguf\nIQ2_S\n10.39GB\nfalse\nLow quality, uses SOTA techniques to be usable.\nQwen2.5-32B-ArliAI-RPMax-v1.3-IQ2_XS.gguf\nIQ2_XS\n9.96GB\nfalse\nLow quality, uses SOTA techniques to be usable.\nQwen2.5-32B-ArliAI-RPMax-v1.3-IQ2_XXS.gguf\nIQ2_XXS\n9.03GB\nfalse\nVery low quality, uses SOTA techniques to be usable.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nSome say that this improves the quality, others don't notice any difference. If you use these models PLEASE COMMENT with your findings. I would like feedback that these are actually used and useful so I don't keep uploading quants no one is using.\nThanks!\nDownloading using huggingface-cli\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/Qwen2.5-32B-ArliAI-RPMax-v1.3-GGUF --include \"Qwen2.5-32B-ArliAI-RPMax-v1.3-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/Qwen2.5-32B-ArliAI-RPMax-v1.3-GGUF --include \"Qwen2.5-32B-ArliAI-RPMax-v1.3-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (Qwen2.5-32B-ArliAI-RPMax-v1.3-Q8_0) or download them all in place (./)\nQ4_0_X_X\nThese are NOT for Metal (Apple) offloading, only ARM chips.\nIf you're using an ARM chip, the Q4_0_X_X quants will have a substantial speedup. Check out Q4_0_4_4 speed comparisons on the original pull request\nTo check which one would work best for your ARM chip, you can check AArch64 SoC features (thanks EloyOn!).\nWhich file should I choose?\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset.\nThank you ZeroWw for the inspiration to experiment with embed/output.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "mradermacher/aya-expanse-8b-abliterated-i1-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nweighted/imatrix quants of https://huggingface.co/lenML/aya-expanse-8b-abliterated\nstatic quants are available at https://huggingface.co/mradermacher/aya-expanse-8b-abliterated-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\ni1-IQ1_S\n2.3\nfor the desperate\nGGUF\ni1-IQ1_M\n2.5\nmostly desperate\nGGUF\ni1-IQ2_XXS\n2.7\nGGUF\ni1-IQ2_XS\n2.9\nGGUF\ni1-IQ2_S\n3.0\nGGUF\ni1-IQ2_M\n3.2\nGGUF\ni1-IQ3_XXS\n3.5\nlower quality\nGGUF\ni1-Q2_K\n3.5\nIQ3_XXS probably better\nGGUF\ni1-IQ3_XS\n3.8\nGGUF\ni1-Q3_K_S\n4.0\nIQ3_XS probably better\nGGUF\ni1-IQ3_S\n4.0\nbeats Q3_K*\nGGUF\ni1-IQ3_M\n4.1\nGGUF\ni1-Q3_K_M\n4.3\nIQ3_S probably better\nGGUF\ni1-Q3_K_L\n4.6\nIQ3_M probably better\nGGUF\ni1-IQ4_XS\n4.7\nGGUF\ni1-Q4_0_4_4\n4.9\nfast on arm, low quality\nGGUF\ni1-Q4_0_4_8\n4.9\nfast on arm+i8mm, low quality\nGGUF\ni1-Q4_0_8_8\n4.9\nfast on arm+sve, low quality\nGGUF\ni1-Q4_0\n4.9\nfast, low quality\nGGUF\ni1-Q4_K_S\n4.9\noptimal size/speed/quality\nGGUF\ni1-Q4_K_M\n5.2\nfast, recommended\nGGUF\ni1-Q5_K_S\n5.8\nGGUF\ni1-Q5_K_M\n5.9\nGGUF\ni1-Q6_K\n6.7\npractically like static Q6_K\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time. Additional thanks to @nicoboss for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.",
    "NexaAI/OmniVLM-968M": "OmniVLM\nüî• Latest Update\nIntroduction\nIntended Use Cases\nBenchmarks\nHow to Use On Device\nModel Architecture\nTraining\nWhat's next for OmniVLM?\nFollow us\nOmniVLM\nüî• Latest Update\n[Dec 16, 2024] Our work \"OmniVLM: A Token-Compressed, Sub-Billion-Parameter Vision-Language Model for Efficient On-Device Inference\" is now live on Arxiv! üöÄ\n[Nov 27, 2024] Model Improvements: OmniVLM v3 model's GGUF file has been updated in this Hugging Face Repo! ‚ú®\nüëâ Test these exciting changes in our Hugging Face Space\n[Nov 22, 2024] Model Improvements: OmniVLM v2 model's GGUF file has been updated in this Hugging Face Repo! ‚ú® Key Improvements Include:\nEnhanced Art Descriptions\nBetter Complex Image Understanding\nImproved Anime Recognition\nMore Accurate Color and Detail Detection\nExpanded World Knowledge\nWe are continuously improving OmniVLM-968M based on your valuable feedback! More exciting updates coming soon - Stay tuned! ‚≠ê\nIntroduction\nOmniVLM is a compact, sub-billion (968M) multimodal model for processing both visual and text inputs, optimized for edge devices. Improved on LLaVA's architecture, it features:\n9x Token Reduction: Reduces image tokens from 729 to 81, cutting latency and computational cost aggressively. Note that the computation of vision encoder and the projection part keep the same, but the computation of language model backbone is reduced due to 9X shorter image token span.\nTrustworthy Result: Reduces hallucinations using DPO training from trustworthy data.\nQuick Links:\nInteractive Demo in our Hugging Face Space. (Updated 2024 Nov 21)\nQuickstart for local setup\nLearn more in our Blogs\nFeedback: Send questions or comments about the model in our Discord\nIntended Use Cases\nOmniVLM is intended for Visual Question Answering (answering questions about images) and Image Captioning (describing scenes in photos), making it ideal for on-device applications.\nExample Demo:\nGenerating captions for a 1046√ó1568 image on M4 Pro Macbook takes < 2s processing time and requires only 988 MB RAM and 948 MB Storage.\nBenchmarks\nBelow we demonstrate a figure to show how OmniVLM performs against nanollava. In all the tasks, OmniVLM outperforms the previous world's smallest vision-language model.\nWe have conducted a series of experiments on benchmark datasets, including MM-VET, ChartQA, MMMU, ScienceQA, POPE to evaluate the performance of OmniVLM.\nBenchmark\nNexa AI OmniVLM v2\nNexa AI OmniVLM v1\nnanoLLAVA\nScienceQA (Eval)\n71.0\n62.2\n59.0\nScienceQA (Test)\n71.0\n64.5\n59.0\nPOPE\n93.3\n89.4\n84.1\nMM-VET\n30.9\n27.5\n23.9\nChartQA (Test)\n61.9\n59.2\nNA\nMMMU (Test)\n42.1\n41.8\n28.6\nMMMU (Eval)\n40.0\n39.9\n30.4\nHow to Use On Device\nIn the following, we demonstrate how to run OmniVLM locally on your device.\nStep 1: Install Nexa-SDK (local on-device inference framework)\nInstall Nexa-SDK\nNexa-SDK is a open-sourced, local on-device inference framework, supporting text generation, image generation, vision-language models (VLM), audio-language models, speech-to-text (ASR), and text-to-speech (TTS) capabilities. Installable via Python Package or Executable Installer.\nStep 2: Then run the following code in your terminal\nnexa run omniVLM\nModel Architecture\nOmniVLM's architecture consists of three key components:\nBase Language Model: Qwen2.5-0.5B-Instruct functions as the base model to process text inputs\nVision Encoder: SigLIP-400M operates at 384 resolution with 14√ó14 patch size to generate image embeddings\nProjection Layer: Multi-Layer Perceptron (MLP) aligns the vision encoder's embeddings with the language model's token space. Compared to vanilla Llava architecture, we designed a projector that reduce 9X image tokens.\nThe vision encoder first transforms input images into embeddings, which are then processed by the projection layer to match the token space of Qwen2.5-0.5B-Instruct, enabling end-to-end visual-language understanding.\nTraining\nWe developed OmniVLM through a three-stage training pipeline:\nPretraining:\nThe initial stage focuses on establishing basic visual-linguistic alignments using image-caption pairs, during which only the projection layer parameters are unfrozen to learn these fundamental relationships.\nSupervised Fine-tuning (SFT):\nWe enhance the model's contextual understanding using image-based question-answering datasets. This stage involves training on structured chat histories that incorporate images for the model to generate more contextually appropriate responses.\nDirect Preference Optimization (DPO):\nThe final stage implements DPO by first generating responses to images using the base model. A teacher model then produces minimally edited corrections while maintaining high semantic similarity with the original responses, focusing specifically on accuracy-critical elements. These original and corrected outputs form chosen-rejected pairs. The fine-tuning targeted at essential model output improvements without altering the model's core response characteristics\nWhat's next for OmniVLM?\nOmniVLM is in early development and we are working to address current limitations:\nExpand DPO Training: Increase the scope of DPO (Direct Preference Optimization) training in an iterative process to continually improve model performance and response quality.\nImprove document and text understanding\nIn the long term, we aim to develop OmniVLM as a fully optimized, production-ready solution for edge AI multimodal applications.\nFollow us\nBlogs | Discord | X(Twitter)",
    "DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters": "Notes\nMaximizing Model Performance for All Quants Types And Full-Precision using Samplers, Advance Samplers and Parameters Guide\nAdditional Docs [MAIN DOC below]:\n#1 - NEW: AI Autocorrect, Auto Creative Enhancement and Low Quant Optimization Software:\nRun all my models - especially class 2, 3, and 4 (as well as new class 5s) without any issues. Also enhances\nthe operation of ALL models - all GGUFs, EXL2s, full source and any compressioned model type:\n[ https://huggingface.co/DavidAU/AI_Autocorrect__Auto-Creative-Enhancement__Auto-Low-Quant-Optimization__gguf-exl2-hqq-SOFTWARE ]\n#2 - NEW: How to Use Thinking/Reasoning Models, including links to these and how to change a \"regular model\" into a thinking/reasoning model\nDocuments main focus is how to setup, and use reasoning/thinking models and how to get maximum performance from them.\nDocument has links to all my \"reasoning/thinking models\" (including MOE type thinking/reasoning models), source code, and adapters.\nLinks to \"adapters\" (\"slice\" or \"essense\" of a reasoning model) show how to change regular models to \"thinking/reasoning\" models, including step by step using Mergekit.\nThis doc also shows how to use \"system prommpt/role\" to change the operation of a regular model to \"simulated reasoning.\"\n[ https://huggingface.co/DavidAU/How-To-Use-Reasoning-Thinking-Models-and-Create-Them ]\n#3 - Mixture of Experts / MOE - Set/activate Experts:\nThis document covers how to adjust/set the number of experts in various AI/LLM apps, and includes links\nto MOE/Mixture of expert models - both GGUF and source.\n[ https://huggingface.co/DavidAU/How-To-Set-and-Manage-MOE-Mix-of-Experts-Model-Activation-of-Experts ]\n#4 How to Set the \"System Role\" / \"System Prompt\" / \"System Message\"\nFor some of my models I have \"system prompt(s)\" to enhance model operation and/or invoke/control reasoning/thinking in a model.\nThis section will cover how to set these.\nSystem Role / System Prompt / System Message (called \"System Prompt\" in this section)\nis \"root access\" to the model and controls internal workings - both instruction following and output generation and in the\ncase of this \"reasoning models\" reasoning control and on/off for reasoning too.\nFor reasoning models that require it:\nIf you do not set a \"system prompt\", reasoning/thinking will be OFF by default, and the model will operate like a normal LLM.\nHOW TO SET:\nDepending on your AI \"app\" you may have to copy/paste on of the \"code(s)\" to enable reasoning/thinking in the\n\"System Prompt\" or \"System Role\" window.\nFor \"normal\" models the \"code(s)\" can enhance operation of the model - this works for all models, and all model types. The instruction\nfollowing \"power\" of the model will directly influence the overall effect(s) of such code.\nLikewise, the more parameters a model has, the more powerful the effect too.\nIn Lmstudio set/activate \"Power User\" or \"Developer\" mode to access, copy/paste to System Prompt Box.\nIn SillyTavern go to the \"template page\" (\"A\") , activate \"system prompt\" and enter the text in the prompt box.\nIn Ollama see [ https://github.com/ollama/ollama/blob/main/README.md ] ; and setting the \"system message\".\nIn Koboldcpp, load the model, start it, go to settings -> select a template and enter the text in the \"sys prompt\" box.\nSYSTEM PROMPTS - Add them / Edit Them:\nWhen you copy/paste PRESERVE formatting, including line breaks.\nIf you want to edit/adjust these only do so in NOTEPAD OR the LLM App directly.\nSpecial Thanks:\nSpecial thanks to all the following, and many more...\nAll the model makers, fine tuners, mergers, and tweakers:\nProvides the raw \"DNA\" for almost all my models.\nSources of model(s) can be found on the repo pages, especially the \"source\" repos with link(s) to the model creator(s).\nHuggingface [ https://huggingface.co ] :\nThe place to store, merge, and tune models endlessly.\nTHE reason we have an open source community.\nLlamaCPP [ https://github.com/ggml-org/llama.cpp ] :\nThe ability to compress and run models on GPU(s), CPU(s) and almost all devices.\nImatrix, Quantization, and other tools to tune the quants and the models.\nLlama-Server : A cli based direct interface to run GGUF models.\nThe only tool I use to quant models.\nQuant-Masters: Team Mradermacher, Bartowski, and many others:\nQuant models day and night for us all to use.\nThey are the lifeblood of open source access.\nMergeKit [ https://github.com/arcee-ai/mergekit ] :\nThe universal online/offline tool to merge models together and forge something new.\nOver 20 methods to almost instantly merge model, pull them apart and put them together again.\nThe tool I have used to create over 1500 models.\nLmstudio [ https://lmstudio.ai/ ] :\nThe go to tool to test and run models in GGUF format.\nThe Tool I use to test/refine and evaluate new models.\nLMStudio forum on discord; endless info and community for open source.\nText Generation Webui // KolboldCPP // SillyTavern:\nExcellent tools to run GGUF models with - [  https://github.com/oobabooga/text-generation-webui ] [ https://github.com/LostRuins/koboldcpp ] .\nSillytavern [ https://github.com/SillyTavern/SillyTavern ] can be used with LMSTudio [ https://lmstudio.ai/ ] , TextGen [ https://github.com/oobabooga/text-generation-webui ], Kolboldcpp [ https://github.com/LostRuins/koboldcpp ], Llama-Server [part of LLAMAcpp] as a off the scale front end control system and interface to work with models.\nMAIN DOCUMENT:\n(Updated: \"INDEX\", and added \"Generation Steering\" section ; notes on Roleplay/Simulation added, Screenshots of parameters/samplers added in quick reference section.)\nThis document includes detailed information, references, and notes for general parameters, samplers and\nadvanced samplers to get the most out of your model's abilities including notes / settings for the most popular AI/LLM app in use (LLAMACPP, KoboldCPP, Text-Generation-WebUI, LMStudio, Sillytavern, Ollama and others).\nThese settings / suggestions can be applied to all models including GGUF, EXL2, GPTQ, HQQ, AWQ and full source/precision.\nIt also includes critical settings for Class 3 and Class 4 models at this repo - DavidAU - to enhance and control generation\nfor specific as a well as outside use case(s) including role play, chat and other use case(s).\nThe settings discussed in this document can also fix a number of model issues (any model, any repo) such as:\n\"Gibberish\"\nGeneration length (including out of control generation)\nChat quality / Multi-Turn convos.\nMulti-turn / COT / and other multi prompt/answer generation\nLetter, word, phrase, paragraph repeats\nCoherence\nInstruction following\nCreativeness or lack there of or .. too much - purple prose.\nLow quant (ie q2k, iq1s, iq2s) issues.\nGeneral output quality.\nRole play related issues.\nLikewise ALL the setting (parameters, samplers and advanced samplers) below can also improve model generation and/or general overall \"smoothness\" / \"quality\" of model operation:\nall parameters and samplers available via LLAMACPP (and most apps that run / use LLAMACPP - including Lmstudio, Ollama, Sillytavern and others.)\nall parameters (including some not in Lllamacpp), samplers and advanced samplers (\"Dry\", \"Quadratic\", \"Microstat\") in oobabooga/text-generation-webui including llamacpp_HF loader (allowing a lot more samplers)\nall parameters (including some not in Lllamacpp), samplers and advanced samplers (\"Dry\", \"Quadratic\", \"Microstat\") in SillyTavern / KoboldCPP (including Anti-slop filters)\nEven if you are not using my models, you may find this document useful for any model (any quant / full source / any repo) available online.\nIf you are currently using model(s) - from my repo and/or others - that are difficult to \"wrangle\" then you can apply \"Class 3\" or \"Class 4\" settings to them.\nThis document will be updated over time too and is subject to change without notice.\nPlease use the \"community tab\" for suggestions / edits / improvements.\nIMPORTANT:\nEvery parameter, sampler and advanced sampler here affects per token generation and overall generation quality.\nThis effect is cumulative especially with long output generation and/or multi-turn (chat, role play, COT).\nLikewise because of how modern AIs/LLMs operate the previously generated (quality) of the tokens generated affect the next tokens generated too.\nYou will get higher quality operation overall - stronger prose, better answers, and a higher quality adventure.\nPS: Running a 70B model?\nYou may want to see this document:\nhttps://huggingface.co/DavidAU/Llama-3.3-70B-Instruct-How-To-Run-on-Low-BPW-IQ1_S-IQ1_M-at-maximum-speed-quality\nINDEX\nHow to Use this document:\nReview quant(s) information to select quant(s) to download, then review \"Class 1,2,3...\" for specific information on models followed by \"Source Files...APPS to run LLMs/AIs\".\n\"TESTING / Default / Generation Example PARAMETERS AND SAMPLERS\" are the basic defaults for parameters, and samplers - the bare minimums. You should always set these first.\nThe optional section \"Generational Control And Steering of a Model / Fixing Model Issues on the Fly\" covers methods to manually steer / edit / modify generation (as well as fixes) for any model.\n\"Quick reference\" will state the best parameter settings for each \"Class\" of model(s) to get the best operation and/or good defaults to use to get started. If you came to this page from a repo card on my repo -DavidAU- the \"class\" of the model would have been stated just before you came to this page.\nThe detailed sections about parameters - Section 1 a,b,c and section 2 will help tune the model(s) operation.\nThe \"DETAILED NOTES ON PARAMETERS, SAMPLERS and ADVANCED SAMPLERS\" section after this covers and links to more information about \"tuning\" your model(s). These cover theory, hints, tips and tricks, and observations\nand how to fine control CLASS 3/4 models directly.\nAll information about parameters, samplers and advanced samplers applies to ALL models, regardless of repo(s) you download them from.\nQUANTS:\n-\tQUANTS Detailed information.\n-\tIMATRIX Quants\n-\tQUANTS GENERATIONAL DIFFERENCES:\n-\tADDITIONAL QUANT INFORMATION\n-\tARM QUANTS / Q4_0_X_X\n-\tNEO Imatrix Quants / Neo Imatrix X Quants\n-\tCPU ONLY CONSIDERATIONS\nClass 1, 2, 3 and 4 model critical notes\nSOURCE FILES for my Models / APPS to Run LLMs / AIs:\n-\tTEXT-GENERATION-WEBUI\n-\tKOBOLDCPP\n-\tSILLYTAVERN\n-\tLmstudio, Ollama, Llamacpp, Backyard, and OTHER PROGRAMS\n-\tRoleplay and Simulation Programs/Notes on models.\nTESTING / Default / Generation Example PARAMETERS AND SAMPLERS\n-\tBasic settings suggested for general model operation.\nGenerational Control And Steering of a Model / Fixing Model Issues on the Fly\n-\tMultiple Methods to Steer Generation on the fly\n-\tOn the fly Class 3/4 Steering / Generational Issues and Fixes (also for any model/type)\n-\tAdvanced Steering / Fixing Issues (any model, any type) and \"sequenced\" parameter/sampler change(s)\n-\t\"Cold\" Editing/Generation\nQuick Reference Table / Parameters, Samplers, Advanced Samplers\n-\tQuick setup for all model classes for automated control / smooth operation.\n-\tScreenshots for multiple LLM/AI apps of parameters/samplers\n-\tSection 1a : PRIMARY PARAMETERS - ALL APPS\n-\tSection 1b : PENALITY SAMPLERS - ALL APPS\n-\tSection 1c : SECONDARY SAMPLERS / FILTERS - ALL APPS\n-\tSection 2: ADVANCED SAMPLERS\nDETAILED NOTES ON PARAMETERS, SAMPLERS and ADVANCED SAMPLERS:\n-\tDETAILS on PARAMETERS / SAMPLERS\n-\tGeneral Parameters\n-\tThe Local LLM Settings Guide/Rant\n-\tLLAMACPP-SERVER EXE - usage / parameters / samplers\n-\tDRY Sampler\n-\tSamplers-\tCreative Writing\n-\tBenchmarking-and-Guiding-Adaptive-Sampling-Decoding\nADVANCED: HOW TO TEST EACH PARAMETER(s), SAMPLER(s) and ADVANCED SAMPLER(s)\nQUANTS:\nPlease note that smaller quant(s) IE: Q2K, IQ1s, IQ2s and some IQ3s (especially those of models size 8B parameters or less) may require additional adjustment(s). For these quants\nyou may need to increase the \"penalty\" sampler(s) and/or advanced sampler(s) to compensate for the compression damage of the model.\nFor models of 20B parameters and higher, generally this is not a major concern as the parameters can make up for compression damage at lower quant levels (IE Q2K+, but at least Q3 ; IQ2+, but at least IQ3+).\nIQ1s: Generally IQ1_S rarely works for models less than 30B parameters. IQ1_M is however almost twice as stable/usable relative to IQ1_S.\nGenerally it is recommended to run the highest quant(s) you can on your machine ; but at least Q4KM/IQ4XS as a minimum for models 20B and lower.\nThe smaller the size of model, the greater the contrast between the smallest quant and largest quant in terms of operation, quality, nuance and general overall function.\nThere is an exception to this , see \"Neo Imatrix\" below and \"all quants\" (cpu only operation).\nIMATRIX:\nImatrix quants generally improve all quants, and also allow you to use smaller quants (less memory, more context space) and retain quality of operation.\nIE: Instead of using a q4KM, you might be able to run an IQ3_M and get close to Q4KM's quality, but at a higher token per second speed and have more VRAM for context.\nRecommended Quants - ALL:\nThis covers both Imatrix and regular quants.\nImatrix can be applied to any quant - \"Q\" or \"IQ\" - however, IQ1s to IQ3_S REQUIRE an imatrix dataset / imatrixing process before quanting.\nThis chart shows the order in terms of \"BPW\" for each quant (mapped below with relative \"strength\" to one another) with \"IQ1_S\" with the least, and \"Q8_0\" (F16 is full precision) with the most:\nIQ1_S \t| IQ1_M\nIQ2_XXS | IQ2_XS | Q2_K_S \t| IQ2_S \t| Q2_K  \t| IQ2_M\nIQ3_XXS | Q3_K_S | IQ3_XS  \t| IQ3_S \t| IQ3_M\t    | Q3_K_M\t| Q3_K_L\nQ4_K_S\t| IQ4_XS | IQ4_NL  \t| Q4_K_M\nQ5_K_S\t| Q5_K_M\nQ6_K\nQ8_0\nF16\nMore BPW mean better quality, but higher VRAM requirements (and larger file size) and lower tokens per second.\nThe larger the model in terms of parameters the lower the size of quant you can run with less quality losses.\nNote that \"quality losses\" refers to both instruction following and output quality.\nDifferences (quality) between quants at lower levels are larger relative to higher quants differences.\nThe Imatrix process has NO effect on Q8 or F16 quants.\nF16 is full precision, just in GGUF format.\nQUANTS GENERATIONAL DIFFERENCES:\nHigher quants will have more detail, nuance and in some cases stronger \"emotional\" levels. Characters will also be\nmore \"fleshed out\" too. Sense of \"there\" will also increase.\nLikewise for any use case -> higher quants nuance (both instruction following AND output generation) will be higher.\n\"Nuance\" is critical for both understanding, as well as the quality of the output generation.\nTo put this another way, \"nuance\" is lost as the full precision model is more and more compressed (lower and lower quants).\nSome of this can be counteracted by parameters and/or Imatrix (as noted earlier).\nIQ4XS / IQ4NL quants:\nDue to the unusual nature of this quant (mixture/processing), generations from it will be different then other quants.\nThese quants can also be \"quanted\" with or without an Imatrix.\nYou may want to try it / compare it to other quant(s) output.\nSpecial note on Q2k/Q3 quants:\nYou may need to use temp 2 or lower with these quants (1 or lower for q2k). Just too much compression at this level, damaging the model.\nIQ quants (and Imatrix versions of q2k/q3) perform better at these \"BPW\" levels.\nRep pen adjustments may also be required to get the most out a model at this/these quant level(s).\nADDITONAL QUANT INFORMATION:\nClick here for details\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nARM QUANTS / Q4_0_X_X:\nThese are new quants that are specifically for computers/devices that can run \"ARM\" quants. If you try to run these on a \"non arm\" machine/device, the token per second will be VERY SLOW.\nQ4_0_X_X information\nThese are NOT for Metal (Apple) or GPU (nvidia/AMD/intel) offloading, only ARM chips (and certain AVX2/AVX512 CPUs).\nIf you're using an ARM chip, the Q4_0_X_X quants will have a substantial speedup. Check out Q4_0_4_4 speed comparisons on the original pull request\nTo check which one would work best for your ARM chip, you can check AArch64 SoC features (thanks EloyOn!).\nIf you're using a CPU that supports AVX2 or AVX512 (typically server CPUs and AMD's latest Zen5 CPUs) and are not offloading to a GPU, the Q4_0_8_8 may offer a nice speed as well:\nClick to view benchmarks on an AVX2 system (EPYC7702)\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n% (vs Q4_0)\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp512\n204.03 ¬± 1.03\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp1024\n282.92 ¬± 0.19\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp2048\n259.49 ¬± 0.44\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg128\n39.12 ¬± 0.27\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg256\n39.31 ¬± 0.69\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg512\n40.52 ¬± 0.03\n100%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp512\n301.02 ¬± 1.74\n147%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp1024\n287.23 ¬± 0.20\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp2048\n262.77 ¬± 1.81\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg128\n18.80 ¬± 0.99\n48%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg256\n24.46 ¬± 3.04\n83%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg512\n36.32 ¬± 3.59\n90%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp512\n271.71 ¬± 3.53\n133%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp1024\n279.86 ¬± 45.63\n100%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp2048\n320.77 ¬± 5.00\n124%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg128\n43.51 ¬± 0.05\n111%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg256\n43.35 ¬± 0.09\n110%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg512\n42.60 ¬± 0.31\n105%\nQ4_0_8_8 offers a nice bump to prompt processing and a small bump to text generation\nNEO Imatrix Quants / Neo Imatrix X Quants\nNEO Imatrix quants are specialized and specifically \"themed\" datasets used to slightly alter the weights in a model. All Imatrix datasets do this to some degree or another, however NEO Imatrix datasets\nare content / theme specific and have been calibrated to have maximum effect on a model (relative to standard Imatrix datasets). Calibration was made possible after testing 50+ standard Imatrix datasets,\nand carefully modifying them and testing the resulting changes to determine the exact format and content which has the maximum effect on a model via the Imatrix process.\nPlease keep in mind that the Imatrix process (at it strongest) only \"tints\" a model and/or slightly changes its bias(es).\nHere are some Imatrix Neo Models:\n[ https://huggingface.co/DavidAU/Command-R-01-Ultra-NEO-DARK-HORROR-V1-V2-35B-IMATRIX-GGUF ]\n[ https://huggingface.co/DavidAU/Command-R-01-200xq-Ultra-NEO-V1-35B-IMATRIX-GGUF ]\n[ https://huggingface.co/DavidAU/Command-R-01-200xq-Ultra-NEO-V1-35B-IMATRIX-GGUF ] (this is an X-Quant)\n[ https://huggingface.co/DavidAU/Llama-3.2-1B-Instruct-NEO-SI-FI-GGUF ]\n[ https://huggingface.co/DavidAU/Llama-3.2-1B-Instruct-NEO-WEE-HORROR-GGUF ]\n[ https://huggingface.co/DavidAU/L3-8B-Stheno-v3.2-Ultra-NEO-V1-IMATRIX-GGUF ]\nSuggestions for Imatrix NEO quants:\nThe LOWER the quant the STRONGER the Imatrix effect is, and therefore the stronger the \"tint\" so to speak\nDue to the unique nature of this project, quants IQ1s to IQ4s are recommended for maximum effect with IQ4_XS the most balanced in terms of power and bits.\nSecondaries are Q2s-Q4s. Imatrix effect is still strong in these quants.\nEffects diminish quickly from Q5s and up.\nQ8/F16 there is no change (as the Imatrix process does not affect this quant), and therefore not included.\nCPU ONLY CONSIDERATIONS:\nThis section DOES NOT apply to most \"Macs\" because of the difference in O/S Memory, Vram and motherboard VS other frameworks.\nRunning quants on CPU will be a lot slower than running them on a video card(s).\nIn this special case however it may be preferred to run AS SMALL a quant as possible for token per second generation reasons.\nOn a top, high end (and relatively new) CPU expect token per second speeds to be 1/4 (or less) a standard middle of the road video card.\nOlder machines/cpus will be a lot slower - but models will STILL run on these as long as you have enough ram.\nHere are some rough comparisons:\nOn my video card (Nvidia 16GB 4060TI) I get 160-190 tokens per second with 1B LLama 3.2 Instruct, CPU speeds are 50-60 token per second.\nOn my much older machine (8 years old)(2 core), token per second speed (same 1B model) is in the 10ish token per second (CPU).\nRoughly 8B-12B models are limit for CPU only operation (in terms of \"usable\" tokens/second) - at the moment.\nThis is changing as new cpus come out, designed for AI usage.\nClass 1, 2, 3 and 4 model critical notes:\nSome of the models at my repo are custom designed / limited use case models. For some of these models, specific settings and/or samplers (including advanced) are recommended for best operation.\nAs a result I have classified the models as class 1, class 2, class 3 and class 4.\nEach model is \"classed\" on the model card itself for each model.\nGenerally all models (mine and other repos) fall under class 1 or class 2 and can be used when just about any sampler(s) / parameter(s) and advanced sampler(s).\nClass 3 requires a little more adjustment because these models run closer to the ragged edge of stability. The settings for these will help control them better, especially\nfor chat / role play and/or other use case(s). Generally speaking, this helps them behave better overall.\nClass 4 are balanced on the very edge of stability. These models are generally highly creative, for very narrow use case(s), and closer to \"human prose\" than other models and/or\noperate in ways no other model(s) operate offering unique generational abilities. With these models, advanced samplers are used to \"bring these bad boys\" inline which is especially important for chat and/or role play type use cases AND/OR use case(s) these models were not designed for.\nFor reference here are some Class 3/4 models:\n[ https://huggingface.co/DavidAU/L3-Stheno-Maid-Blackroot-Grand-HORROR-16B-GGUF ]\n(note Grand Horror Series contain class 2,3 and 4 models)\n[  https://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF ]\n(note Dark Planet Series contains Class 1, 2 and Class 3/4 models)\n[ https://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF ]\n(this model has exceptional prose abilities in all areas)\n[ https://huggingface.co/DavidAU/MN-GRAND-Gutenberg-Lyra4-Lyra-23.5B-GGUF ]\n(note Grand Guttenberg Madness/Darkness (12B) are class 1 models, but compressed versions of 23.5B)\nAlthough Class 3 and Class 4 models will work when used within their specific use case(s), standard parameters and settings on the model card, I recognize that users want either a smoother experience\nand/or want to use these models for other than intended use case(s) and that is in part why I created this document.\nThe goal here is to use parameters to raise/lower the power of the model and samplers to \"prune\" (and/or in some cases enhance) operation.\nWith that being said, generation \"examples\" (at my repo) are created using the \"Primary Testing Parameters\" (top of this document) settings regardless of the \"class\" of the model and no advanced settings, parameters, or samplers.\nHowever, for ANY model regardless of \"class\" or if it is at my repo, you can now take performance to the next level with the information contained in this document.\nSide note:\nThere are no \"Class 5\" models published... yet.\nSOURCE FILES for my Models / APPS to Run LLMs / AIs:\nSource files / Source models of my models are located here (also upper right menu on this page):\n[ https://huggingface.co/collections/DavidAU/d-au-source-files-for-gguf-exl2-awq-gptq-hqq-etc-etc-66b55cb8ba25f914cbf210be ]\nYou will need the config files to use \"llamacpp_HF\" loader (\"text-generation-webui\") [ https://github.com/oobabooga/text-generation-webui ]\nYou can also use the full source in \"text-generation-webui\" too.\nAs an alternative you can use GGUFs directly in \"KOBOLDCPP\" / \"SillyTavern\" without the \"config files\" and still use almost all the parameters, samplers and advanced samplers.\nParameters, Samplers and Advanced Samplers\nIn section 1 a,b, and c, below are all the LLAMA_CPP parameters and samplers.\nI have added notes below each one for adjustment / enhancement(s) for specific use cases.\nTEXT-GENERATION-WEBUI\nIn section 2, will be additional samplers, which become available when using \"llamacpp_HF\" loader in https://github.com/oobabooga/text-generation-webui\nAND/OR https://github.com/LostRuins/koboldcpp (\"KOBOLDCPP\").\nThe \"llamacpp_HF\" (for \"text-generation-webui\") only requires the GGUF you want to use plus a few config files from \"source repo\" of the model.\n(this process is automated with this program, just enter the repo(s) urls -> it will fetch everything for you)\nThis allows access to very advanced samplers in addition to all the parameters / samplers here.\nKOBOLDCPP:\nNote that https://github.com/LostRuins/koboldcpp also allows access to all LLAMACPP parameters/samplers too as well as additional advanced samplers too.\nYou can use almost all parameters, samplers and advanced samplers using \"KOBOLDCPP\" without the need to get the source config files (the \"llamacpp_HF\" step).\nNote: This program has one of the newest samplers called \"Anti-slop\" which allows phrase/word banning at the generation level.\nSILLYTAVERN:\nNote that https://github.com/SillyTavern/SillyTavern also allows access to all LLAMACPP parameters/samplers too as well as additional advanced samplers too.\nYou can use almost all parameters, samplers and advanced samplers using \"SILLYTAVERN\" without the need to get the source config files (the \"llamacpp_HF\" step).\nFor CLASS3 and CLASS4 the most important setting is \"SMOOTHING FACTOR\" (Quadratic Smoothing) ; information is located on this page:\nhttps://docs.sillytavern.app/usage/common-settings/\nCritical Note:\nSilly Tavern allows you to \"connect\" (via API) to different AI programs/apps like Koboldcpp, Llamacpp (server), Text Generation Webui, Lmstudio, Ollama ... etc etc.\nYou \"load\" a model in one of these, then connect Silly Tavern to the App via API. This way you can use any model, and Sillytavern becomes the interface between\nthe AI model and you directly. Sillytavern opens an interface in your browser.\nIn Sillytavern you can then adjust parameters, samplers and advanced samplers ; there are also PRESET parameter/samplers too and you can save your favorites too.\nCurrently, at time of this writing, connecting Silly Tavern via KoboldCPP or Text Generation Webui will provide the most samplers/parameters.\nHowever for some, connecting to Lmstudio, LlamaCPP, or Ollama may be preferred.\nYou may also want to check out how to connect SillyTavern to local AI \"apps\" running on your pc here:\nhttps://docs.sillytavern.app/usage/api-connections/\nLmstudio, Ollama, Llamacpp, and OTHER PROGRAMS\nOther programs like https://www.LMStudio.ai allows access to most of STANDARD samplers, where as others (llamacpp only here) you may need to add to the json file(s) for a model and/or template preset.\nIn most cases all llama_cpp parameters/samplers are available when using API / headless / server mode in \"text-generation-webui\", \"koboldcpp\", \"Sillytavern\", \"Olama\", and \"LMStudio\" (as well as other apps too).\nYou can also use llama_cpp directly too. (IE: llama-server.exe) ; see :\nhttps://github.com/ggerganov/llama.cpp\n(scroll down on the main page for more apps/programs to use GGUFs too that connect to / use the LLAMA-CPP package.)\nSpecial note:\nIt appears \"DRY\" / \"XTC\" samplers has been added to LLAMACPP and SILLYTAVERN.\nIt is available (Llamacpp) via \"server.exe / llama-server.exe\". Likely this sampler will also become available \"downstream\" in applications that use LLAMACPP in due time.\n[ https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md ]\nOperating Systems:\nMost AI/LLM apps operate on Windows, Mac, and Linux.\nMobile devices (and O/S) are in many cases also supported.\nRoleplay and Simulation Programs/Notes on models.\nText Generation Webui, KoboldCPP, and Silly Tavern (and AI/LLM apps connected via Silly Tavern) can all do roleplay / simulation AS WELL as \"chat\" and other creative activities.\nLMStudio (the app here directly), Ollama and other LLM/AI apps are for general usage, however they can be connected to Silly Tavern via API too.\nBackyard ( https://backyard.ai/ ) is software that is dedicated primarily to Roleplay / Simulation, however it can not be (at time of this writing) connected via API to Silly Tavern at this time.\nIf you are using Backyard app, see special notes for \"roleplay / simulation\" and where applicable, \"BACKYARD APP\" for specific notes on using these app.\nModels that are Class 3/4 :\nSome of my models that are rated Class 3 or 4 maybe a little more challenging to operate with roleplay, especially if you can not access / control certain samplers.\nHow to handle this issue is addressed in \"Generational Steering\" section (you control it) as well as Quick Reference, and Detailed Parameters, Samplers and Advanced Samplers Sections (automated control).\nAlso, some of my models are available in multiple \"classes\", IE Dark Planet, and Grand Gutenberg.\nIn these cases, Dark Planet 8B versions and Grand Gutenberg 12B (\"Darkness\" / \"Madness\") are class 1 - any use case, including role play and simulation.\nLikewise Darkest Planet 16.5B and Grand Gutenberg 23/23.5B are class 3 - great at roleplay/simulation, but need a bit more steering and/or parameter/samplers adjustments to work flawlessly for this use case.\nNote: Dark Planet 8B (class 1) is also a compressed version of Grand Horror 16B (a full on class 4)\nTESTING / Generation Example PARAMETERS AND SAMPLERS\nPrimary Testing Parameters I use, including use for output generation examples at my repo:\nRanged Parameters:\ntemperature: 0 to 5 (\"temp\")\nrepetition_penalty : 1.02 to 1.15 (\"rep pen\")\nSet parameters:\ntop_k:40\nmin_p:0.05\ntop_p: 0.95\nrepeat-last-n: 64   (also called: \"repetition_penalty_range\" / \"rp range\" )\nI do not set any other settings, parameters or have samplers activated when generating examples.\nEverything else is \"zeroed\" / \"disabled\".\nIMPORTANT:\nThese parameters/settings are considered both safe and default and in most cases available to all users in all AI/LLM apps.\nYou should set these as noted first. I would say these are the minimum settings to use to get good model operation.\nNote for Class 3/Class 4 models settings/samplers (discussed below) \"repeat-last-n\" is a CRITICAL setting.\nBACKYARD APP:\nIn \"Backyard\" app, \"repetition_penalty_range\" is called \"Repeat Penalty Tokens\" (set on the \"character card\").\nFor class 3/4 models (if using with Backyard app), set this to 64 OR LESS.\nGenerational Control And Steering of a Model / Fixing Model Issues on the Fly\nMultiple Methods to Steer Generation on the fly\nNow that you have the basic parameters and samplers from the previous section, I will cover Generational Control and Steering.\nThis section is optional and covers how to manually STEER generation(s) - ANY MODEL, ANY TYPE.\nThis section (in part) will also cover how to deal with Class 3/4 model issues directly, as well as general issues than can happen with any \"class\" of model during generation IF you want to control them manually as\nthe \"Quick Reference\" and/or \"Detailed Parameters, Samplers, and Advanced Samplers\" will cover how to deal with any generation issue(s) automatically.\nThere is a very important concept that must be covered first:\nThe output/generation/answer to your prompt/instructions BECOMES part of your \"prompt\" after you click STOP, and then click on \"CONTINUE\".\nLikewise is true in multi-turn chat, role play, or in a \"chat window\" so to speak.\nYour prompts AND the model's \"answers\"/\"generation\" all become part of the \"ROADMAP\" for the model to use in whatever journey you are on.\nWhen you hit \"REGEN\" this nullifies only the last \"generation\" - not the prompt before it, nor the prompt(s)/generation(s) in the same chat.\nThe part I will cover here is once a generation has started, from a single prompt (no other prompts/generations in the chat).\nSo lets start with a prompt (NOTE: this prompt has no \"steering\" in the instructions):\nStart a 1000 word scene (vivid horror, 1st person, include thoughts) with: The sky scraper swayed, as she watched the window in front of her on the 21 floor explode...\nGeneration starts ... and then ends.\nIt could be 500 words, to 4000+...\nThen you hit regen however many times to get a \"good\" generation.\nThere is a better way.\nGeneration starts... 200 words in you think... this is not going in the right direction.\nDo you hit stop? Then regen?\nThere are a lot more options:\n1 - Hit Stop.\n2 - Select \"EDIT\" -> Edit out the part(s) you don't want AND/OR add in STEERING \"text\" (statement, phrase, paragraph, even a single word) (anywhere in the \"generation\" text).\n3 - Hit Continue.\nOnce you hit \"continue\" the change(s) you made will now steer the models choices.\nThe LAST edit (bottom of the generation) will have the most impact. However ALL EDITS will affect generation as these become part of the generational \"ROADMAP\".\nYou can repeat this process at will.\nEventually the model will come to a \"natural\" stopping point.\nIf you want to model to continue past this model, delete a few lines AND \"steer\" it.\nThese methods apply to all generation types - not just a \"scene\" or \"story\", but \"programming code\", \"article\", \"conclusions\", \"analytics\", ... you name it.\nNotes:\nFor Text Generation Webui, you can transfer your \"chat\" to \"notebook\" for easy Stop/Edit/Continue function.\nFor KoboldCPP -> This is built in.\nFor Silly Tavern -> This is built in.\nFor LMStudio -> This is built in.\nFor API (direct control) you have to send the \"chat\" elements back to the \"server\" with the \"edits\" (send the whole \"revised\" chat as a json payload).\nOn the fly Class 3/4 Steering / Generational Issues and Fixes (also for any model/type):\nGenerational issues can occur such as letter(s), word(s), phrase(s), paragraph repeat(s), \"rants\" etc etc which can occur at any point during generation.\nThis can happen to ANY model, any type ; however with Class 3/4 models there is a higher chance this will occur because of how these models operate.\nThe \"Quick Reference\" and Detailed Parameters, Samplers and Advanced Samplers (below) cover how to set the model \"controls\" to do this automatically.\nHowever, sometimes these settings MAY trim too much (ie creativity, \"madness\", nuance, emotion, even the \"right answer(s) etc etc) sometimes, so I will show you how to address these issues directly.\nIf you have a letter(s) and/or word(s) repeat:\nStop generation, edit out this, and back ONE OR TWO lines (delete)\nHit continue.\nBetter: Do these steps, and add \"steering\" (last line -> word, phrase, sentence)\nIf you have single or multiple paragraph repeat(s):\nStop generation, edit out all the paragraph(s), and back ONE OR TWO lines OR last NON repeating paragraph (delete)\nHit continue.\nBetter: Do these steps, and add \"steering\" (last line -> word, phrase, sentence or paragraph)\nIn each case we are BREAKING the \"condition(s)\" that lead (or lead into) to the repeat(s).\nIf you have \"rants\" and/or \"model has lost its mind\":\nStop generation, edit out all the paragraph(s), and back AS FAR as possible to where is appears the rant/mind loss occured (delete ALL) and delete one additional paragraph / 2 or more sentences.\nHit continue.\nBetter: Do these steps, and add \"steering\" (last line -> word, phrase, sentence or paragraph).\nClass 3/4 model additional note:\nWith these classes of model, you MAY need to \"edit\" / \"revise\" further back than one or two lines / one paragraph - they sometimes need just a little more editing.\nAnother option is using \"Cold\" Editing/Generation explained below.\nAdvanced Steering / Fixing Issues (any model, any type) and \"sequenced\" parameter/sampler change(s)\nThis will drastically (depending on changes you make) change up \"Continue(d)\" generation(s):\nDo the edits above (steering and/or \"steering fixes\"), but before you click \"Continue\" (after your \"Edit(s)\"), adjust the parameter(s), sampler(s) and advanced sampler(s) settings.\nOnce you do this BEFORE hitting \"Continue\" your new settings will be applied to all generation from your new \"Continue\" point.\nYou can repeat this process at will.\nYou can also hit \"stop\", make NO EDIT(S), adjust the parameter(s), sampler(s) and advanced sampler(s) settings and hit \"Continue\" and the new settings will take effect from the \"stop point\" going forward.\n\"Cold\" Editing/Generation\nLet say you have a generation, but you want to edit it later IN A NEW CHAT.\nSometimes you can just copy/paste the generation and the model MAY get the \"IDEA\" and continue the generation without a prompt or direction.\nHowever this does not always work.\nSo you need something along these lines (adjust accordingly):\nInstructions: Continue this scene, using vivid and graphic details.\nSCENE:\n(previous generation)\nNote the structure, layout and spacing.\nIf it was programming code:\nInstructions: Continue this javascript, [critical instructions here for \"code\" goals]\nJAVASCRIPT:\n(previous generation)\nYou may want to include the ENTIRE prior prompt (with some modifications) used in the first generation:\nInstructions: Continue the scene below (vivid horror, 1st person, include thoughts) with: The sky scraper swayed, as she watched the window in front of her on the 21 floor explode...\nSCENE:\n(previous generation)\nNOTE:\nYou may want to modify the instructions to provide a \"steering\" continue point and/or \"goal\" for the generation to the model has some idea how to proceed.\nQuick Reference Table - Parameters, Samplers, Advanced Samplers\nCompiled by: \"EnragedAntelope\" ( https://huggingface.co/EnragedAntelope  || https://github.com/EnragedAntelope )\nThis section will get you started - especially with class 3 and 4 models - and the detail section will cover settings / control in more depth below.\nPlease see sections below this for advanced usage, more details, settings, notes etc etc.\nIMPORTANT NOTES:\nNot all parameters, samplers and advanced samplers are listed in this quick reference section. Scroll down to see all of them in following sections.\nLikewise there may be some \"name variation(s)\" - in other LLM/AI apps - this is addressed in the detailed sections.\nI have added Screenshots of settings for Class 1-2, Class 3 and Class 4 are below this chart for Koboldcpp, SillyTavern and Text Gen Webui.\n# LLM Parameters Reference Table\n| Parameter \t\t\t| Description |\n|-----------\t\t\t|-------------|\n| Primary Parameters |\n| temperature \t\t| Controls randomness of outputs (0 = deterministic, higher = more random). Range: 0-5 |\n| top-p \t\t\t\t| Selects tokens with probabilities adding up to this number. Higher = more random results. Default: 0.9 |\n| min-p \t\t\t\t| Discards tokens with probability smaller than this value √ó probability of most likely token. Default: 0.1 |\n| top-k \t\t\t\t| Selects only top K most likely tokens. Higher = more possible results. Default: 40 |\n| Penalty Samplers |\n| repeat-last-n \t\t| Number of tokens to consider for penalties. Critical for preventing repetition. Default: 64 (Class 3/4 - but see notes) |\n| repeat-penalty\t \t| Penalizes repeated token sequences. Range: 1.0-1.15. Default: 1.0 |\n| presence-penalty \t| Penalizes token presence in previous text. Range: 0-0.2 for Class 3, 0.1-0.35 for Class 4 |\n| frequency-penalty \t| Penalizes token frequency in previous text. Range: 0-0.25 for Class 3, 0.4-0.8 for Class 4 |\n| penalize-nl \t\t| Penalizes newline tokens. Generally unused. Default: false |\n| Secondary Samplers |\n| mirostat \t\t\t| Controls perplexity during sampling. Modes: 0 (off), 1, or 2 |\n| mirostat-lr \t\t| Mirostat learning rate. Default: 0.1 |\n| mirostat-ent \t\t| Mirostat target entropy. Default: 5.0 |\n| dynatemp-range \t| Range for dynamic temperature adjustment. Default: 0.0 |\n| dynatemp-exp \t\t| Exponent for dynamic temperature scaling. Default: 1.0 |\n| tfs \t\t\t\t| Tail free sampling - removes low-probability tokens. Default: 1.0 |\n| typical \t\t\t| Selects tokens more likely than random given prior text. Default: 1.0 |\n| xtc-probability \t\t| Probability of token removal. Range: 0-1 |\n| xtc-threshold \t\t| Threshold for considering token removal. Default: 0.1 |\n| Advanced Samplers |\n| dry_multiplier \t\t| Controls DRY (Don't Repeat Yourself) intensity. Range: 0.8-1.12+ Class 3 (Class 4 is higher) |\n| dry_allowed_length | Allowed length for repeated sequences in DRY. Default: 2 |\n| dry_base \t\t\t| Base value for DRY calculations. Range: 1.15-1.75+ for Class 4 |\n| smoothing_factor \t| Quadratic sampling intensity. Range: 1-3 for Class 3, 3-5+ for Class 4 |\n| smoothing_curve \t| Quadratic sampling curve. Range: 1 for Class 3, 1.5-2 for Class 4 |\nNotes\nFor Class 3 and 4 models, using both DRY and Quadratic sampling is recommended (see advanced/detailed samplers below on how to control the model here directly)\nLower quants (Q2K, IQ1s, IQ2s) may require stronger settings due to compression damage\nParameters interact with each other, so test changes one at a time\nAlways test with temperature at 0 first to establish a baseline\nSCREENSHOTS (right click-> open in new window) of Class 1-2, Class 3, and Class 4 for KoboldCPP, Silly Tavern and Text Gen Webui.\nclass1-2-Silly-Tavern\nclass1-2-WebUI\nclass1-2-kcpp\nclass3-Silly-Tavern\nclass3-WebUI\nclass3-kcpp\nclass4-Silly-Tavern\nclass4-WebUI\nclass4-kcpp\nNOTES:\nThese cover basic/default settings PER CLASS. See \"quick range\" of settings above, and full range with details on how to \"fine tune\" model operation below.\nThis is especially important for fine control of Class 3 and Class 4 models ; sometimes you can use class 2 or 3 settings for class 3 and even class 4 models.\nIt is your use CASE(s) / smooth operation requirements that determine which settings will work best.\nYou should not apply class 3 or class 4 settings on a class 1 or class 2 model - this might limit model operation and usually class 1/2 models do not require this level of control.\nCLASS 3/4 MODELS:\nIf you are using a class 3 or class 4 model for use case(s) such as role play, multi-turn, chat etc etc, it is suggested to activate / set all samplers for class 3 but may be required for class 4 models.\nLikewise for fine control of a class 3/4 via \"DRY\" and \"Quadratic\" samplers is detailed below. These allow you to dial up or dial down the model's raw power directly.\nROLEPLAY / SIMULATION NOTES:\nIf you are using a model (regardless of \"class\") for these uses cases, you may need to LOWER \"temp\" to get better instruction following.\nInstruction following issues can cascade over the \"adventure\" if the temp is set too high for the specific model(s) you are using.\nLikewise you may want to set MAXIMUM output tokens (a hard limit how much the model can output) to much lower values such as 128 to 300.\n(This will assist with steering, and stop the model from endlessly \"yapping\")\nMICROSTAT Sampler - IMPORTANT:\nMake sure to review MIROSTAT sampler settings below, due to behaviour of this specific sampler / affect on parameters/other samplers which varies from app to app too.\nSection 1a : PRIMARY PARAMETERS - ALL APPS:\nThese parameters will have SIGNIFICANT effect on prose, generation, length and content; with temp being the most powerful.\nKeep in mind the biggest parameter / random \"unknown\" is your prompt.\nA word change, rephrasing, punctation , even a comma, or semi-colon can drastically alter the output, even at min temp settings. CAPS also affect generation too.\nLikewise the size, and complexity of your prompt impacts generation too ; especially clarity and direction.\nSpecial note:\nPre-prompts / system role are not discussed here. Many of the model repo cards (at my repo) have an optional pre-prompt you can use to aid generation (and can impact instruction following too).\nSome of my newer models repo cards use a limited form of this called a \"prose control\" (discussed and shown by example).\nRoughly a pre-prompt / system role is embedded during each prompt and can act as a guide and/or set of directives for processing the prompt and/or containing generation instructions.\nA prose control is a simplified version of this, which precedes the main prompt(s) - but the idea / effect is relatively the same (pre-prompt/system role does have a slightly higher priority however).\nI strongly suggest you research these online, as they are a powerful addition to your generation toolbox.\nThey are especially potent with newer model archs due to newer model types having stronger instruction following abilities AND increase context too.\nPRIMARY PARAMETERS:\ntemp  /  temperature\ntemperature (default: 0.8)\nPrimary factor to control the randomness of outputs. 0 = deterministic (only the most likely token is used). Higher value = more randomness.\nRange 0 to 5. Increment at .1 per change.\nToo much temp can affect instruction following in some cases and sometimes not enough = boring generation.\nNewer model archs (L3,L3.1,L3.2, Mistral Nemo, Gemma2 etc) many times NEED more temp (1+) to get their best generations.\nROLEPLAY / SIMULATION NOTE:\nIf you are using a model (regardless of \"class\") for these uses cases, you may need to LOWER temp to get better instruction following.\ntop-p\ntop-p sampling (default: 0.9, 1.0 = disabled)\nIf not set to 1, select tokens with probabilities adding up to less than this number. Higher value = higher range of possible random results.\nDropping this can simplify word choices but this works in conjunction with \"top-k\"\nI use default of: .95 ;\nmin-p\nmin-p sampling (default: 0.1, 0.0 = disabled)\nTokens with probability smaller than (min_p) * (probability of the most likely token) are discarded.\nI use default: .05 ;\nCareful adjustment of this parameter can result in more \"wordy\" or \"less wordy\" generation but this works in conjunction with \"top-k\".\ntop-k\ntop-k sampling (default: 40, 0 = disabled)\nSimilar to top_p, but select instead only the top_k most likely tokens. Higher value = higher range of possible random results.\nBring this up to 80-120 for a lot more word choice, and below 40 for simpler word choices.\nAs this parameter operates in conjunction with \"top-p\" and \"min-p\" all three should be carefully adjusted one at a time.\nNOTE - \"CORE\" Testing with \"TEMP\":\nFor an interesting test, set \"temp\" to 0 ; this will give you the SAME generation for a given prompt each time.\nThen adjust a word, phrase, sentence etc in your prompt, and generate again to see the differences.\n(you should use a \"fresh\" chat for each generation)\nKeep in mind this will show model operation at its LEAST powerful/creative level and should NOT be used to determine if the model works for your use case(s).\nThen test your prompt(s) \"at temp\" to see the model in action. (5-10 generations recommended)\nYou can also use \"temp=0\" to test different quants of the same model to see generation differences. (roughly minor \"BIAS\" changes which reflect math changes due to compress/mixtures differences between quants).\nAnother option is testing different models (at temp=0 AND of the same quant) to see how each handles your prompt(s).\nThen test \"at temp\" with your prompt(s) to see the MODELS in action. (5-10 generations recommended)\nSection 1b : PENALITY SAMPLERS - ALL APPS:\nThese samplers \"trim\" or \"prune\" output in real time.\nThe longer the generation, the stronger overall effect but that all depends on \"repeat-last-n\" setting.\nFor creative use cases, these samplers can alter prose generation in interesting ways.\nPenalty parameters affect both per token and part of OR entire generation (depending on settings / output length).\nCLASS 4: For these models it is important to activate / set all samplers as noted for maximum quality and control.\nPRIMARY:\nrepeat-last-n\nlast n tokens to consider for penalize (default: 64, 0 = disabled, -1\t= ctx_size)\n(\"repetition_penalty_range\" in oobabooga/text-generation-webui , \"rp_range\" in kobold)\nTHIS IS CRITICAL.\nToo high you can get all kinds of issues (repeat words, sentences, paragraphs or \"gibberish\"), especially with class 3 or 4 models.\nLikewise if you change this parameter it will drastically alter the output.\nThis setting also works in conjunction with all other \"rep pens\" below.\nThis parameter is the \"RANGE\" of tokens looked at for the samplers directly below.\nBACKYARD APP:\nIn \"Backyard\" app, \"repetition_penalty_range\" is called \"Repeat Penalty Tokens\" (set on the \"character card\").\nFor class 3/4 models (if using with Backyard app), set this to 64 OR LESS.\nSECONDARIES:\nrepeat-penalty\npenalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n(commonly called \"rep pen\")\nGenerally this is set from 1.0 to 1.15 ; smallest increments are best IE: 1.01... 1,.02 or even 1.001... 1.002.\nThis affects creativity of the model over all, not just how words are penalized.\npresence-penalty\nrepeat alpha presence penalty (default: 0.0, 0.0 = disabled)\nGenerally leave this at zero IF repeat-last-n is 512-1024 or less. You may want to use this for higher repeat-last-n settings.\nCLASS 3: 0.05 to .2 may assist generation BUT SET \"repeat-last-n\" to 512 or less. Better is 128 or 64.\nCLASS 4: 0.1 to 0.35 may assist generation BUT SET \"repeat-last-n\" to 64.\nfrequency-penalty\nrepeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\nGenerally leave this at zero IF repeat-last-n is 512 or less. You may want to use this for higher repeat-last-n settings.\nCLASS 3: 0.25 may assist generation BUT SET \"repeat-last-n\" to 512 or less. Better is 128 or 64.\nCLASS 4: 0.4 to 0.8 may assist generation BUT SET \"repeat-last-n\" to 64.\npenalize-nl\npenalize newline tokens (default: false)\nGenerally this is not used.\nSection 1c : SECONDARY SAMPLERS / FILTERS - ALL APPS:\nIn some AI/LLM apps, these may only be available via JSON file modification and/or API.\nFor \"text-gen-webui\", \"Koboldcpp\"  these are directly accessible ; other programs/app this varies.\nSillytavern:\nIf the apps support (Sillytavern is connected to via API) these parameters/samplers then you can access them via Silly Tavern's parameter/sampler panel. So if you are using Text-Gen-Webui, Koboldcpp, LMStudio, Llamacpp, Ollama (etc) you can set/change/access all or most of these.\ni) OVERALL GENERATION CHANGES (affect per token as well as over all generation):\nmirostat\nUse Mirostat sampling. \"Top K\", \"Nucleus\", \"Tail Free\" (TFS) and \"Locally Typical\" (TYPICAL) samplers are ignored if used.  (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\nPaper: https://arxiv.org/abs/2007.14966\nCRITICAL:\nIf you activate Mirostat when using \"LLAMAcpp SERVER\" and/or some LLAMA_CPP based apps this will VOID/DISABLE all parameters (excluding \"penalties\", \"logit_bias\" ) AND all other SAMPLERS except \"temp\" parameter plus the following:\nV1: n_vocab(model) (this is set internally by llamacpp), seed, mirostat_tau, mirostat_eta\nV2: seed, mirostat_tau, mirostat_eta\nFor Koboldcpp:\n\"DRY\" sampling is NOT blocked, and a version of \"top_k\" (3000) is used (but Mirostat does NOT block \"Anti-Slop\" , BUT does block \"penalities\" parameters (unlike Llamacpp - which does not) ).\nFor Text Generation UI:\nNo blocking occurs. Note that ONLY Mirostat 2 is available. (other parameters/samplers should work without issue)\nNote this is subject to change by LLAMAcpp, Koboldcpp, Text Generation UI and other AI/LLM app makers at any time.\n(\"seed\" is usually a random value. (default) ; this parameter can be set in some AI/LLM apps to control Mirostat output more closely.)\n\"mirostat-lr\"\nMirostat learning rate, parameter eta (default: 0.1)  \" mirostat_tau \"\nmirostat_tau: 5-8 is a good value.\n\"mirostat-ent\"\nMirostat target entropy, parameter tau (default: 5.0) \" mirostat_eta \"\nmirostat_eta: 0.1 is a good value.\nActivates the Mirostat sampling technique. It aims to control perplexity during sampling. See the paper. ( https://arxiv.org/abs/2007.14966 )\nThis is the big one ; activating this will help with creative generation. It can also help with stability. Also note which\nsamplers are disabled/ignored here, and that \"mirostat_eta\" is a learning rate.\nThis is both a sampler (and pruner) and enhancement all in one.\nIt also has two modes of generation \"1\" and \"2\" - test both with 5-10 generations of the same prompt. Make adjustments, and repeat.\nCLASS 3: models it is suggested to use this to assist with generation (min settings).\nCLASS 4: models it is highly recommended with Microstat 1 or 2 + mirostat_tau @ 6 to 8 and mirostat_eta at .1 to .5\nDynamic Temperature\n\"dynatemp-range \"\ndynamic temperature range (default: 0.0, 0.0 = disabled)\n\"dynatemp-exp\"\ndynamic temperature exponent (default: 1.0)\nIn: oobabooga/text-generation-webui (has on/off, and high / low) :\nActivates Dynamic Temperature. This modifies temperature to range between \"dynatemp_low\" (minimum) and \"dynatemp_high\" (maximum), with an entropy-based scaling. The steepness of the curve is controlled by \"dynatemp_exponent\".\nThis allows the model to CHANGE temp during generation. This can greatly affect creativity, dialog, and other contrasts.\nFor Koboldcpp a converter is available and in oobabooga/text-generation-webui you just enter low/high/exp.\nCLASS 4 only: Suggested this is on, with a high/low of .8 to 1.8 (note the range here of \"1\" between high and low); with exponent to 1 (however below 0 or above work too)\nTo set manually (IE: Api, lmstudio, Llamacpp, etc) using \"range\" and \"exp\" ; this is a bit more tricky: (example is to set range from .8 to 1.8)\n1 - Set the \"temp\" to 1.3 (the regular temp parameter)\n2 - Set the \"range\" to .500 (this gives you \".8\" to \"1.8\" with \"1.3\" as the \"base\")\n3 - Set exp to 1 (or as you want).\nThis is both an enhancement and in some ways fixes issues in a model when too little temp (or too much/too much of the same) affects generation.\nii) PER TOKEN CHANGES:\ntfs\nTail free sampling, parameter z (default: 1.0, 1.0 = disabled)\nTries to detect a tail of low-probability tokens in the distribution and removes those tokens. The closer to 0, the more discarded tokens.\n( https://www.trentonbricken.com/Tail-Free-Sampling/ )\ntypical\nLocally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\nIf not set to 1, select only tokens that are at least this much more likely to appear than random tokens, given the prior text.\nXTC\n\"xtc-probability\"\nxtc probability (default: 0.0, 0.0 = disabled)\nProbability that the removal will actually happen. 0 disables the sampler. 1 makes it always happen.\n\"xtc-threshold\"\nxtc threshold (default: 0.1, 1.0 = disabled)\nIf 2 or more tokens have probability above this threshold, consider removing all but the last one.\nXTC is a new sampler, that adds an interesting twist in generation.\nSuggest you experiment with this one, with other advanced samplers disabled to see its affects.\nl,    logit-bias TOKEN_ID(+/-)BIAS\nmodifies the likelihood of token appearing in the completion,\ni.e. --logit-bias 15043+1 to increase likelihood of token ' Hello',  or --logit-bias 15043-1 to decrease likelihood of token ' Hello'\nThis may or may not be available. This requires a bit more work.\nNote: +- range is 0 to 100.\nIN \"oobabooga/text-generation-webui\" there is \"TOKEN BANNING\":\nThis is a very powerful pruning method; which can drastically alter output generation.\nI suggest you get some \"bad outputs\" ; get the \"tokens\" (actual number for the \"word\" / part word)  then use this.\nCareful testing is required, as this can have unclear side effects.\nSECTION 2: ADVANCED SAMPLERS - \"text-generation-webui\" / \"KOBOLDCPP\" / \"SillyTavern\" (see note 1 below):\nAdditional Parameters / Samplers, including \"DRY\", \"QUADRATIC\" and \"ANTI-SLOP\".\nNote #1 :\nYou can use these samplers via Sillytavern IF you use either of these APPS (Koboldcpp/Text Generation Webui/App supports them) to connect Silly Tavern to their API.\nOther Notes:\nHopefully ALL these samplers / controls will be LLAMACPP and available to all users via AI/LLM apps soon.\n\"DRY\" sampler has been added to Llamacpp as of the time of this writing (and available via SERVER/LLAMA-SERVER.EXE) and MAY appear in other \"downstream\" apps that use Llamacpp.\nINFORMATION ON THESE SAMPLERS:\nFor more info on what they do / how they affect generation see:\nhttps://github.com/oobabooga/text-generation-webui/wiki/03-%E2%80%90-Parameters-Tab\n(also see the section above \"Additional Links\" for more info on the parameters/samplers)\nADVANCED SAMPLERS - PART 1:\nKeep in mind these parameters/samplers become available (for GGUFs) in \"oobabooga/text-generation-webui\" when you use the llamacpp_HF loader.\nMost of these are also available in KOBOLDCPP too (via settings -> samplers) after start up (no \"llamacpp_HF loader\" step required).\nI am not going to touch on all of samplers / parameters, just the main ones at the moment.\nHowever, you should also check / test operation of (these are in Text Generation WebUI, and may be available via API / In Sillytavern (when connected to Text Generation Webui)):\na] Affects per token generation:\ntop_a\nepsilon_cutoff - see note #4\neta_cutoff - see note #4\nno_repeat_ngram_size - see note #1.\nb] Affects generation including phrase, sentence, paragraph and entire generation:\nno_repeat_ngram_size - see note #1.\nencoder_repetition_penalty \"Hallucinations filter\" - see note #2.\nguidance_scale (with \"Negative prompt\" ) => this is like a pre-prompt/system role prompt - see note #3.\nDisabling (BOS TOKEN) this can make the replies more creative.\nCustom stopping strings\nNote 1:\n\"no_repeat_ngram_size\" appears in both because it can impact per token OR per phrase depending on settings. This can also drastically affect sentence,\nparagraph and general flow of the output.\nNote 2:\nThis parameter if set to LESS than 1 causing the model to \"jump\" around a lot more , whereas above 1 causes the model to focus more on the immediate surroundings.\nIf the model is crafting a \"scene\", a setting of less than 1 causes the model to jump around the room, outside, etc etc ; if less than 1 then it focuses the model more on\nthe moment, the immediate surroundings, the POV character and details in the setting.\nNote 3:\nThis is a powerful method to send instructions / directives to the model on how to process your prompt(s) each time. See [ https://arxiv.org/pdf/2306.17806 ]\nNote 4:\nThese control selection of tokens, in some case providing more relevant and/or more options. See [ https://arxiv.org/pdf/2210.15191 ]\nMAIN ADVANCED SAMPLERS PART 2 (affects per token AND overall generation):\nWhat I will touch on here are special settings for CLASS 3 and CLASS 4 models (for the first TWO samplers).\nFor CLASS 3 you can use one, two or both.\nFor CLASS 4 using BOTH are strongly recommended, or at minimum \"QUADRATIC SAMPLING\".\nThese samplers (along with \"penalty\" settings) work in conjunction to \"wrangle\" the model / control it and get it to settle down, important for Class 3 but critical for Class 4 models.\nFor other classes of models, these advanced samplers can enhance operation across the board.\nFor Class 3 and Class 4 the goal is to use the LOWEST settings to keep the model inline rather than \"over prune it\".\nYou may therefore want to experiment to with dropping the settings (SLOWLY) for Class3/4 models from suggested below.\nDRY:\nDry (\"Don't Repeat Yourself\") affects repetition (and repeat \"penalty\") at the word, phrase, sentence and even paragraph level. Read about \"DRY\" above, in the \"Additional Links\" links section above.\nClass 3:\ndry_multiplier: .8\ndry_allowed_length: 2\ndry_base: 1\nClass 4:\ndry_multiplier: .8 to 1.12+\ndry_allowed_length: 2 (or less)\ndry_base: 1.15 to 1.75+\nDial the \"dry_muliplier\" up or down to \"reign in\" or \"release the madness\" so to speak from the core model.\nFor Class 4 models this is used to control some of the model's bad habit(s).\nFor more information on \"DRY\":\nhttps://github.com/oobabooga/text-generation-webui/pull/5677\nhttps://www.reddit.com/r/KoboldAI/comments/1e49vpt/dry_sampler_questionsthat_im_sure_most_of_us_are/\nhttps://www.reddit.com/r/KoboldAI/comments/1eo4r6q/dry_settings_questions/\nQUADRATIC SAMPLING: AKA \"Smoothing\"\nThis sampler alters the \"score\" of ALL TOKENS at the time of generation and as a result affects the entire generation of the model. See \"Additional Links\" links section above for more information.\nClass 3:\nsmoothing_factor: 1 to 3\nsmoothing_curve: 1\nClass 4:\nsmoothing_factor: 3 to 5 (or higher)\nsmoothing_curve: 1.5 to 2.\nDial the \"smoothing factor\" up or down to \"reign in\" or \"release the madness\" so to speak.\nIn Class 3 models, this has the effect of modifying the prose closer to \"normal\" with as much or little (or a lot!) touch of \"madness\" from the root model.\nIn Class 4 models, this has the effect of modifying the prose closer to \"normal\" with as much or little (or a lot!) touch of \"madness\" from the root model AND wrangling in some of the core model's bad habits.\nFor more information on Quadratic Samplings:\nhttps://gist.github.com/kalomaze/4473f3f975ff5e5fade06e632498f73e\nANTI-SLOP - Kolbaldcpp only\nHopefully this powerful sampler will soon appear in all LLM/AI apps.\nYou can access this in the KoboldCPP app, under \"context\" -> \"tokens\" on the main page of the app after start up.\nYou can also access in SillyTavern if you use KoboldCPP as your \"API\" connected app too.\nThis sampler allows banning words and phrases DURING generation, forcing the model to \"make another choice\".\nThis is a game changer in custom real time control of the model.\nFor more information on ANTI SLOP project (owner runs EQBench):\nhttps://github.com/sam-paech/antislop-sampler\nFINAL NOTES:\nKeep in mind that these settings/samplers work in conjunction with \"penalties\" ; which is especially important\nfor operation of CLASS 4 models for chat / role play and/or \"smoother operation\".\nFor Class 3 models, \"QUADRATIC\" will have a slightly stronger effect than \"DRY\" relatively speaking.\nIf you use Mirostat sampler, keep in mind this will interact with these two advanced samplers too.\nAnd...\nSmaller quants may require STRONGER settings (all classes of models) due to compression damage, especially for Q2K, and IQ1/IQ2s.\nThis is also influenced by the parameter size of the model in relation to the quant size.\nIE: a 8B model at Q2K will be far more unstable relative to a 20B model at Q2K, and as a result require stronger settings.\nDETAILED NOTES ON PARAMETERS, SAMPLERS and ADVANCED SAMPLERS:\nMost AI / LLM apps allow saving a \"profile\" parameters and samplers - \"favorite\" settings.\nText Generation Web Ui, Koboldcpp, Silly Tavern all have this feature and also \"presets\" (parameters/samplers set already) too.\nOther AI/LLM apps also have this feature to varying degrees too.\nDETAILS on PARAMETERS / SAMPLERS:\nFor additional details on these samplers settings (including advanced ones) you may also want to check out:\nhttps://github.com/oobabooga/text-generation-webui/wiki/03-%E2%80%90-Parameters-Tab\n(NOTE: Not all of these \"options\" are available for GGUFS, including when you use \"llamacpp_HF\" loader in \"text-generation-webui\" )\nAdditional Links (on parameters, samplers and advanced samplers):\nA Visual Guide of some top parameters / Samplers in action which you can play with and see how they interact:\nhttps://artefact2.github.io/llm-sampling/index.xhtml\nGeneral Parameters:\nhttps://arxiv.org/html/2408.13586v1\nhttps://www.reddit.com/r/LocalLLaMA/comments/17vonjo/your_settings_are_probably_hurting_your_model_why/\nThe Local LLM Settings Guide/Rant (covers a lot of parameters/samplers - lots of detail)\nhttps://rentry.org/llm-settings\nLLAMACPP-SERVER EXE - usage / parameters / samplers:\nhttps://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md\nDRY\nhttps://github.com/oobabooga/text-generation-webui/pull/5677\nhttps://www.reddit.com/r/KoboldAI/comments/1e49vpt/dry_sampler_questionsthat_im_sure_most_of_us_are/\nhttps://www.reddit.com/r/KoboldAI/comments/1eo4r6q/dry_settings_questions/\nSamplers:\nhttps://gist.github.com/kalomaze/4473f3f975ff5e5fade06e632498f73e\nhttps://huggingface.co/LWDCLS/LLM-Discussions/discussions/2\nhttps://huggingface.co/Virt-io/SillyTavern-Presets\nCreative Writing :\nhttps://www.reddit.com/r/LocalLLaMA/comments/1c36ieb/comparing_sampling_techniques_for_creative/\nBenchmarking-and-Guiding-Adaptive-Sampling-Decoding\nhttps://github.com/ZhouYuxuanYX/Benchmarking-and-Guiding-Adaptive-Sampling-Decoding-for-LLMs\nNOTE:\nI have also added notes too in the sections below for almost all parameters, samplers, and advanced samplers as well.\nOTHER:\nDepending on the AI/LLM \"apps\" you are using, additional reference material for parameters / samplers may also exist.\nADVANCED: HOW TO TEST EACH PARAMETER(s), SAMPLER(s) and ADVANCED SAMPLER(s)\n1 - Set temp to 0 (zero) and set your basic parameters, and use a prompt to get a \"default\" generation. A creative prompt will work better here.\n2 - If you want to test basic parameter changes, test ONE at a time, then compare output (answer quality, word choice, sentence size/construction, general output qualities) to your \"default\" generation.\n3 - Then start testing TWO parameters at a time, and comparing again. Keep in mind parameters (all) interact with each other.\n4 - Samplers -> Reset your basic parameters, (temp still at zero) and test each one of these, one at a time. Then adjust settings, test again.\n5 - Once you have an \"idea\" of how each affects your \"test prompt\" , now test at \"temp\" (not zero). It may take five to ten generation to get a rough idea.\nYes, testing is a lot of work - but once you get all the parameter(s) and/or sampler(s) dialed in - it is worth it.\nIMPORTANT: Use a \"fresh chat\" PER TEST (you will contaminate the results otherwise). Never use the same chat for multiple tests -> exception: Regens.\nKeep in mind that parameters, samplers and advanced samplers can affect the model on a per token generation basis AND/OR on a multi-token / phrase / sentence / paragraph\nand even complete generation basis.\nEverything is cumulative here regardless if the parameter/sampler affects per token or multi-token basis because of how models \"look back\" to see what was generated in some cases.\nAnd of course... each model will be different too.\nAll that being said, it is a good idea to have specific generation quality \"goals\" in mind.\nLikewise, at my repo, I post example generations so you can get an idea (but not complete picture) of a model's generation abilities.\nThe best way to control generation is STILL with your prompt(s) - including pre-prompts/system role. The latest gen models (and archs) have very strong\ninstruction following so many times better (or just included!) instructions in your prompts can make a world of difference.\nNot sure if the model understands your prompt(s)?\nAsk it ->\n\"Check my prompt below and tell me how to make it clearer?\" (prompt after this line)\n\"For my prompt below, explain the steps you wound take to execute it\" (prompt after this line)\nThis will help the model fine tune your prompt so IT understands it.\nHowever sometimes parameters and/or samplers are required to better \"wrangle\" the model and getting to perform to its maximum potential and/or fine tune it to your use case(s).",
    "alibabasglab/MossFormerGAN_SE_16K": "The MossFormerGAN_SE_16K model weights for 16 kHz speech enhancement in ClearerVoice-Studio repo.\nThis model is trained on large scale datasets inclduing open-sourced and private data.\nIt enhances speech audios by removing background noise.",
    "numind/NuExtract-1.5-smol": "NuExtract-1.5-smol by NuMind üî•\nBenchmark\nUsage\nNuExtract-1.5-smol by NuMind üî•\nNuExtract-1.5-smol is a fine-tuning of Hugging Face's SmolLM2-1.7B, intended for structured information extraction. It uses the same training data as NuExtract-1.5 and supports multiple languages, while being less than half the size (1.7B vs 3.8B).\nTo use the model, provide an input text and a JSON template describing the information you need to extract.\nNote: This model is trained to prioritize pure extraction, so in most cases all text generated by the model is present as is in the original text.\nCheck out the blog post.\nTry the 3.8B model here: Playground\nWe also provide a tiny (0.5B) version which is based on Qwen2.5-0.5B: NuExtract-tiny-v1.5\n‚ö†Ô∏è We recommend using NuExtract with a temperature at or very close to 0. Some inference frameworks, such as Ollama, use a default of 0.7 which is not well suited to pure extraction tasks.\nBenchmark\nZero-shot performance (English):\nZero-shot performance (Multilingual):\nUsage\nTo use the model:\nimport json\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndef predict_NuExtract(model, tokenizer, texts, template, batch_size=1, max_length=10_000, max_new_tokens=4_000):\ntemplate = json.dumps(json.loads(template), indent=4)\nprompts = [f\"\"\"<|input|>\\n### Template:\\n{template}\\n### Text:\\n{text}\\n\\n<|output|>\"\"\" for text in texts]\noutputs = []\nwith torch.no_grad():\nfor i in range(0, len(prompts), batch_size):\nbatch_prompts = prompts[i:i+batch_size]\nbatch_encodings = tokenizer(batch_prompts, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length).to(model.device)\npred_ids = model.generate(**batch_encodings, max_new_tokens=max_new_tokens)\noutputs += tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\nreturn [output.split(\"<|output|>\")[1] for output in outputs]\nmodel_name = \"numind/NuExtract-1.5-smol\"\ndevice = \"cuda\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, trust_remote_code=True).to(device).eval()\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntext = \"\"\"We introduce Mistral 7B, a 7‚Äìbillion-parameter language model engineered for\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\nMistral 7B ‚Äì Instruct, that surpasses Llama 2 13B ‚Äì chat model both on human and\nautomated benchmarks. Our models are released under the Apache 2.0 license.\nCode: <https://github.com/mistralai/mistral-src>\nWebpage: <https://mistral.ai/news/announcing-mistral-7b/>\"\"\"\ntemplate = \"\"\"{\n\"Model\": {\n\"Name\": \"\",\n\"Number of parameters\": \"\",\n\"Number of max token\": \"\",\n\"Architecture\": []\n},\n\"Usage\": {\n\"Use case\": [],\n\"Licence\": \"\"\n}\n}\"\"\"\nprediction = predict_NuExtract(model, tokenizer, [text], template)[0]\nprint(prediction)\nSliding window prompting:\nimport json\nMAX_INPUT_SIZE = 20_000\nMAX_NEW_TOKENS = 6000\ndef clean_json_text(text):\ntext = text.strip()\ntext = text.replace(\"\\#\", \"#\").replace(\"\\&\", \"&\")\nreturn text\ndef predict_chunk(text, template, current, model, tokenizer):\ncurrent = clean_json_text(current)\ninput_llm =  f\"<|input|>\\n### Template:\\n{template}\\n### Current:\\n{current}\\n### Text:\\n{text}\\n\\n<|output|>\" + \"{\"\ninput_ids = tokenizer(input_llm, return_tensors=\"pt\", truncation=True, max_length=MAX_INPUT_SIZE).to(\"cuda\")\noutput = tokenizer.decode(model.generate(**input_ids, max_new_tokens=MAX_NEW_TOKENS)[0], skip_special_tokens=True)\nreturn clean_json_text(output.split(\"<|output|>\")[1])\ndef split_document(document, window_size, overlap):\ntokens = tokenizer.tokenize(document)\nprint(f\"\\tLength of document: {len(tokens)} tokens\")\nchunks = []\nif len(tokens) > window_size:\nfor i in range(0, len(tokens), window_size-overlap):\nprint(f\"\\t{i} to {i + len(tokens[i:i + window_size])}\")\nchunk = tokenizer.convert_tokens_to_string(tokens[i:i + window_size])\nchunks.append(chunk)\nif i + len(tokens[i:i + window_size]) >= len(tokens):\nbreak\nelse:\nchunks.append(document)\nprint(f\"\\tSplit into {len(chunks)} chunks\")\nreturn chunks\ndef handle_broken_output(pred, prev):\ntry:\nif all([(v in [\"\", []]) for v in json.loads(pred).values()]):\n# if empty json, return previous\npred = prev\nexcept:\n# if broken json, return previous\npred = prev\nreturn pred\ndef sliding_window_prediction(text, template, model, tokenizer, window_size=4000, overlap=128):\n# split text into chunks of n tokens\ntokens = tokenizer.tokenize(text)\nchunks = split_document(text, window_size, overlap)\n# iterate over text chunks\nprev = template\nfor i, chunk in enumerate(chunks):\nprint(f\"Processing chunk {i}...\")\npred = predict_chunk(chunk, template, prev, model, tokenizer)\n# handle broken output\npred = handle_broken_output(pred, prev)\n# iterate\nprev = pred\nreturn pred",
    "mistralai/Mistral-Large-Instruct-2411": "Model Card for Mistral-Large-Instruct-2411\nKey features\nSystem Prompt\nBasic Instruct Template (V7)\nUsage\nvLLM\nServer\nOffline\nImproved Function Calling\nThe Mistral AI Team\nModel Card for Mistral-Large-Instruct-2411\nMistral-Large-Instruct-2411 is an advanced dense Large Language Model (LLM) of 123B parameters with state-of-the-art reasoning, knowledge and coding capabilities extending Mistral-Large-Instruct-2407 with better Long Context, Function Calling and System Prompt.\nKey features\nMulti-lingual by design: Dozens of languages supported, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch and Polish.\nProficient in coding: Trained on 80+ coding languages such as Python, Java, C, C++, Javacsript, and Bash. Also trained on more specific languages such as Swift and Fortran.\nAgent-centric: Best-in-class agentic capabilities with native function calling and JSON outputting.\nAdvanced Reasoning: State-of-the-art mathematical and reasoning capabilities.\nMistral Research License: Allows usage and modification for non-commercial usages.\nLarge Context: A large 128k context window.\nRobust Context Adherence: Ensures strong adherence for RAG and large context applications.\nSystem Prompt: Maintains strong adherence and support for more reliable system prompts.\nSystem Prompt\nWe appreciate the feedback received from our community regarding our system prompt handling.In response, we have implemented stronger support for system prompts.To achieve optimal results, we recommend always including a system prompt that clearly outlines the bot's purpose, even if it is minimal.\nBasic Instruct Template (V7)\n<s>[SYSTEM_PROMPT] <system prompt>[/SYSTEM_PROMPT][INST] <user message>[/INST] <assistant response></s>[INST] <user message>[/INST]\nBe careful with subtle missing or trailing white spaces!\nPlease make sure to use mistral-common as the source of truth\nUsage\nThe model can be used with the following frameworks\nvllm: See here\nvLLM\nWe recommend using this model with the vLLM library\nto implement production-ready inference pipelines.\nInstallation\nMake sure you install vLLM >= v0.6.4.post1:\npip install --upgrade vllm\nAlso make sure you have mistral_common >= 1.5.0 installed:\npip install --upgrade mistral_common\nYou can also make use of a ready-to-go docker image or on the docker hub.\nServer\nWe recommand that you use Mistral-Large-Instruct-2411 in a server/client setting.\nSpin up a server:\nvllm serve mistralai/Mistral-Large-Instruct-2411 --tokenizer_mode mistral --config_format mistral --load_format mistral --tensor_parallel_size 8\nNote: Running Mistral-Large-Instruct-2411 on GPU requires over 300 GB of GPU RAM.\nTo ping the client you can use a simple Python snippet.\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\nfrom datetime import datetime, timedelta\nurl = \"http://<your-server>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\nmodel = \"mistralai/Mistral-Large-Instruct-2411\"\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\ntoday = datetime.today().strftime(\"%Y-%m-%d\")\nyesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\nmodel_name = repo_id.split(\"/\")[-1]\nreturn system_prompt.format(name=model_name, today=today, yesterday=yesterday)\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT + \"\\n\\nThink step by step. You're a math genius.\"},\n{\n\"role\": \"user\",\n\"content\": \"Think of four random numbers. Then add, substract or multiply them so that the solution is 10. If it's not possible, say it.\"\n},\n]\ndata = {\"model\": model, \"messages\": messages}\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()[\"choices\"][0][\"message\"][\"content\"])\n#  Sure, let's start by thinking of four random numbers. For example, let's take 3, 5, 2, and 1.\n#\n#  Now, we need to find a combination of addition, subtraction, or multiplication that results in 10.\n#  Let's try:\n#  \\[ 3 + 5 + 2 - 1 = 9 \\]\n#  This doesn't work. Let's try another combination:\n#  \\[ 3 \\times 2 + 5 - 1 = 6 + 5 - 1 = 10 \\]\n#  This works! So, with the numbers 3, 5, 2, and 1, we can achieve the result 10 by performing the operations \\( 3 \\times 2 + 5 - 1 \\).\nOffline\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\nfrom huggingface_hub import hf_hub_download\nfrom datetime import datetime, timedelta\nmodel_name = \"mistralai/Mistral-Large-Instruct-2411\"\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, 'r') as file:\nsystem_prompt = file.read()\ntoday = datetime.today().strftime('%Y-%m-%d')\nyesterday = (datetime.today() - timedelta(days=1)).strftime('%Y-%m-%d')\nmodel_name = repo_id.split(\"/\")[-1]\nreturn system_prompt.format(name=model_name, today=today, yesterday=yesterday)\nSYSTEM_PROMPT = load_system_prompt(model_name, \"SYSTEM_PROMPT.txt\") + \"\\n\\nThink step by step. You're a math genius.\"\nuser_prompt = \"Without browsing the web, how many days ago was Mistral founded?\"\nmessages = [\n{\n\"role\": \"system\",\n\"content\": SYSTEM_PROMPT\n},\n{\n\"role\": \"user\",\n\"content\": user_prompt\n},\n]\n# note that running this model on GPU requires over 300 GB of GPU RAM\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\", tensor_parallel_size=8)\nsampling_params = SamplingParams(max_tokens=512)\noutputs = llm.chat(messages, sampling_params=sampling_params)\nprint(outputs[0].outputs[0].text)\n# I don't have real-time web browsing capabilities or access to current data, but I can help you calculate the number of days based on the information I have.\n#\n#Mistral AI was founded in April 2023. To determine how many days ago that was from today's date, November 18, 2024, we need to calculate the total number of days between April 2023 and November 2024.\n#\n#Here's the step-by-step calculation:\n#\n#1. **Days from April 2023 to December 2023:**\n#   - April 2023: 30 days (April has 30 days)\n#   - May 2023: 31 days\n#   - June 2023: 30 days\n#   - July 2023: 31 days\n#   - August 2023: 31 days\n#   - September 2023: 30 days\n#   - October 2023: 31 days\n#   - November 2023: 30 days\n#   - December 2023: 31 days\n#\n#   Total days in 2023 from April to December = 30 + 31 + 30 + 31 + 31 + 30 + 31 + 30 + 31 = 275 days\n#\n#2. **Days from January 2024 to November 18, 2024:**\n#   - January 2024: 31 days\n#   - February 2024: 29 days (2024 is a leap year)\n#   - March 2024: 31 days\n#   - April 2024: 30 days\n#   - May 2024: 31 days\n#   - June 2024: 30 days\n#   - July 2024: 31 days\n#   - August 2024: 31 days\n#   - September 2024: 30 days\n#   - October 2024: 31 days\n#   - November 2024 (up to the 18th): 18 days\n#\n#   Total days in 2024 from January to November 18 = 31 + 29 + 31 + 30 + 31 + 30 + 31 + 31 + 30 + 31 + 18 = 323 days\n#\n#3. **Total days from April 2023 to November 18, 2024:**\n#   Total days = 275 days (2023) + 323 days (2024) = 598 days\n#\n#Therefore, Mistral AI was founded 598 days ago from today's date, November 18, 2024.\nImproved Function Calling\nMistral-Large-2411 has much improved function calling capabilities that are fully supported\nusing mistral_common >= 1.5.0 and vLLM >= v0.6.4.post1.\nMake sure to serve the model with the following flags in vLLM:\nvllm serve mistralai/Pixtral-Large-Instruct-2411 --tokenizer_mode mistral --config_format mistral --load_format mistral --tensor-parallel-size 8 --tool-call-parser mistral --enable-auto-tool-choice\nExample\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\nfrom datetime import datetime, timedelta\nurl = \"http://<your-server>:8000/v1/chat/completions\"\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer token\"}\nmodel = \"mistralai/Mistral-Large-Instruct-2411\"\ndef load_system_prompt(repo_id: str, filename: str) -> str:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\ntoday = datetime.today().strftime(\"%Y-%m-%d\")\nyesterday = (datetime.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\nmodel_name = repo_id.split(\"/\")[-1]\nreturn system_prompt.format(name=model_name, today=today, yesterday=yesterday)\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\ntools = [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_weather\",\n\"description\": \"Get the current weather in a given location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"city\": {\n\"type\": \"string\",\n\"description\": \"The city to find the weather for, e.g. 'San Francisco'\",\n},\n\"state\": {\n\"type\": \"string\",\n\"description\": \"The state abbreviation, e.g. 'CA' for California\",\n},\n\"unit\": {\n\"type\": \"string\",\n\"description\": \"The unit for temperature\",\n\"enum\": [\"celsius\", \"fahrenheit\"],\n},\n},\n\"required\": [\"city\", \"state\", \"unit\"],\n},\n},\n},\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"rewrite\",\n\"description\": \"Rewrite a given text for improved clarity\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"text\": {\n\"type\": \"string\",\n\"description\": \"The input text to rewrite\",\n}\n},\n},\n},\n},\n]\nmessages = [\n{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n{\n\"role\": \"user\",\n\"content\": \"Could you please make the below article more concise?\\n\\nOpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\",\n},\n{\n\"role\": \"assistant\",\n\"content\": \"\",\n\"tool_calls\": [\n{\n\"id\": \"bbc5b7ede\",\n\"type\": \"function\",\n\"function\": {\n\"name\": \"rewrite\",\n\"arguments\": '{\"text\": \"OpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.\"}',\n},\n}\n],\n},\n{\n\"role\": \"tool\",\n\"content\": '{\"action\":\"rewrite\",\"outcome\":\"OpenAI is a FOR-profit company.\"}',\n\"tool_call_id\": \"bbc5b7ede\",\n\"name\": \"rewrite\",\n},\n{\n\"role\": \"assistant\",\n\"content\": \"---\\n\\nOpenAI is a FOR-profit company.\",\n},\n{\n\"role\": \"user\",\n\"content\": \"Can you tell me what the temperature will be in Dallas, in Fahrenheit?\",\n},\n]\ndata = {\"model\": model, \"messages\": messages, \"tools\": tools}\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()[\"choices\"][0][\"message\"][\"tool_calls\"])\n# [{'id': '8PdihwL6d', 'type': 'function', 'function': {'name': 'get_current_weather', 'arguments': '{\"city\": \"Dallas\", \"state\": \"TX\", \"unit\": \"fahrenheit\"}'}}]\nThe Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Diogo Costa, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, L√©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Micka√´l Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Th√©ophile Gervet, Timoth√©e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",
    "agarkovv/CryptoTrader-LM": "Model Card for CryptoTrader-LM\nModel Details\nModel Description\nUses\nDirect Use\nDownstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nBias\nRisks\nLimitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nData Periods:\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nModel Examination [optional]\nEnvironmental Impact\nTechnical Specifications\nModel Architecture and Objective\nCompute Infrastructure\nCitation\nGlossary [optional]\nMore Information [optional]\nModel Card Authors [optional]\nModel Card Contact\nFramework Versions\nModel Card for CryptoTrader-LM\nThe model predicts a trading decision‚Äîbuy, sell, or hold‚Äîfor either Bitcoin (BTC) or Ethereum (ETH) based on cryptocurrency news and historical price data. This model is fine-tuned using LoRA on the Ministral-8B-Instruct-2410 base model, specifically for the FinNLP @ COLING-2025 Cryptocurrency Trading Challenge.\nModel Details\nModel Description\nThis model is fine-tuned using LoRA (Low-Rank Adaptation) on the Ministral-8B-Instruct-2410 model, designed to predict daily cryptocurrency trading decisions (buy, sell, or hold) based on real-time news articles and BTC/ETH price data. The model's goal is to maximize profitability by making informed trading decisions under volatile market conditions.\nBase Model: mistralai/Ministral-8B-Instruct-2410\nFine-tuning Framework: PEFT (Parameter Efficient Fine-Tuning)\nTask: Cryptocurrency Trading Decision-Making (BTC, ETH)\nLanguages: English (for news article analysis)\nUses\nDirect Use\nThe model can be used to predict daily trading decisions for BTC or ETH based on real-time financial news and historical cryptocurrency price data. It is designed for participants of the FinNLP Cryptocurrency Trading Challenge, but it could also be applied to other cryptocurrency trading contexts.\nDownstream Use\nThe model can be integrated into automated crypto trading systems, agent-based trading platforms (such as FinMem), or used for research in financial decision-making models.\nOut-of-Scope Use\nThis model is not designed for:\nPredicting trading decisions for assets other than Bitcoin (BTC) or Ethereum (ETH).\nHigh-frequency trading (HFT); the model is optimized for daily decision-making, not minute-by-minute trading.\nUse in non-financial domains. It is not suitable for generic text-generation tasks or sentiment analysis outside of financial contexts.\nBias, Risks, and Limitations\nBias\nThe model is fine-tuned on specific data (cryptocurrency news and price data) and may not generalize well to other financial markets or different news sources. There could be biases based on the news outlets and timeframes present in the training data.\nRisks\nMarket Volatility: Cryptocurrency markets are inherently volatile. The model‚Äôs predictions are based on past data and news, which may not always predict future market conditions accurately.\nDecision-making: The model offers trading advice, but users should employ appropriate risk management techniques and not rely solely on the model for financial decisions.\nLimitations\nThe model‚Äôs evaluation is primarily focused on profitability (Sharpe Ratio), and it may not account for other factors such as market liquidity, transaction fees, or slippage.\nThe model may not perform well in scenarios with significant market regime changes, such as sudden regulatory shifts or unexpected global events.\nRecommendations\nRisk Management: Users should complement the model‚Äôs predictions with traditional risk management strategies and not use the model in isolation for trading.\nBias Awareness: Be aware of potential biases in the news sources and timeframe used in training. The model may underrepresent certain news sources or overemphasize specific types of news.\nHow to Get Started with the Model\nTo start using the model for predictions, you can follow the example code below:\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\nfrom huggingface_hub import login\nlogin(\"YOUR TOKEN HERE\")\nPROMPT = \"[INST]YOUR PROMPT HERE[/INST]\"\nMAX_LENGTH = 32768  # Do not change\nDEVICE = \"cpu\"\nmodel_id = \"agarkovv/CryptoTrader-LM\"\nbase_model_id = \"mistralai/Ministral-8B-Instruct-2410\"\nmodel = AutoPeftModelForCausalLM.from_pretrained(model_id)\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\nmodel = model.to(DEVICE)\nmodel.eval()\ninputs = tokenizer(\nPROMPT, return_tensors=\"pt\", padding=False, max_length=MAX_LENGTH, truncation=True\n)\ninputs = {key: value.to(model.device) for key, value in inputs.items()}\nres = model.generate(\n**inputs,\nuse_cache=True,\nmax_new_tokens=MAX_LENGTH,\n)\noutput = tokenizer.decode(res[0], skip_special_tokens=True)\nprint(output)\nTraining Details\nTraining Data\nThe model was fine-tuned on cryptocurrency market data, including:\nCryptocurrency to USD exchange rates for Bitcoin (BTC) and Ethereum (ETH).\nNews articles: Textual data related to cryptocurrency markets, including news URLs, titles, sources, and publication dates. The dataset was provided in JSON format, where each entry corresponds to a piece of news relevant to the crypto market.\nData Periods:\nTraining Data: Data period from 2022-01-01 to 2024-10-15.\nThe model was trained to correlate news sentiment, content, and cryptocurrency price trends, aiming to predict optimal trading decisions.\nTraining Procedure\nPreprocessing\nText Preprocessing: The raw news data underwent preprocessing which included text normalization, tokenization, and removal of irrelevant tokens (like stop words and special characters).\nPrice Data Normalization: Historical price data was normalized to reflect percentage changes over time, making it easier for the model to capture price trends.\nData Alignment: News articles were aligned with the corresponding time periods of price data to enable the model to learn from both data sources simultaneously.\nTraining Hyperparameters\nBatch size: 1\nLearning rate: 5e-5\nEpochs: 3\nPrecision: Mixed precision (FP16), which helped speed up training while conserving memory.\nOptimizer: AdamW\nLoRA Parameters: LoRA rank 8, alpha 16, dropout 0.1\nSpeeds, Sizes, Times\nTraining Time: Approximately 3 hours on an 4x A100 GPU setup.\nModel Size: 8B parameters (base model: Ministral-8B-Instruct).\nCheckpoint Size: ~16GB due to the parameter-efficient fine-tuning.\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\nThe model was evaluated on a validation set of cryptocurrency market data (both price data and news articles). The testing dataset aligns with time periods not seen in training.\nFactors\nThe model‚Äôs evaluation primarily focuses on:\nProfitability: The model‚Äôs ability to make profitable trading decisions.\nVolatility Handling: How well the model adapts to market volatility.\nTimeliness: The ability to react to time-sensitive news.\nMetrics\nSharpe Ratio (SR): The main evaluation metric for the challenge. The Sharpe Ratio is used to measure the risk-adjusted return of the model‚Äôs trading decisions.\nProfit and Loss (PnL): The net profit or loss generated by the model‚Äôs trading decisions over a given time period.\nAccuracy: The percentage of correct trading decisions (buy/sell/hold) compared to the optimal strategy.\nResults\nThe model achieved a Sharpe Ratio of 0.94 on the validation set, indicating a strong risk-adjusted return. The model demonstrated consistent profitability over the testing period and effectively managed news-based volatility.\nSummary\nSharpe Ratio: 0.94\nAccuracy: 72%\nProfitability: The model‚Äôs decisions resulted in an average 8% profit over the testing period.\nModel Examination [optional]\nInitial interpretability studies show that the model places significant weight on news headlines containing strong market sentiment indicators (e.g., \"surge\", \"plummet\"). Further analysis is recommended to explore how different types of news (e.g., regulatory updates vs. technical analysis) influence model decisions.\nEnvironmental Impact\nCarbon emissions and energy consumption estimates during model training:\nHardware Type: 4x NVIDIA A100 GPUs.\nHours used: ~3 hours of total training time.\nCloud Provider: AWS.\nCompute Region: US-East.\nCarbon Emitted: Approximately 1.1 kg CO2e, as estimated using the Machine Learning Impact calculator.\nTechnical Specifications\nModel Architecture and Objective\nModel Architecture: LoRA fine-tuned version of the Mistral-8B model, which is a transformer-based architecture optimized for instruction-following tasks.\nObjective: To predict daily trading decisions (buy/sell/hold) for BTC/ETH based on financial news and cryptocurrency price data.\nCompute Infrastructure\nHardware\nTraining Hardware: 4x NVIDIA A100 GPUs with 40GB of VRAM.\nInference Hardware: Can be run on a single GPU with at least 24GB of VRAM.\nSoftware\nFramework: PEFT (Parameter Efficient Fine-Tuning) with Hugging Face Transformers.\nDeep Learning Libraries: PyTorch, Hugging Face Transformers.\nPython Version: 3.10\nCitation\nIf you use this model in your work, please cite it as follows:\nBibTeX:\n@misc{CryptoTrader-LM,\nauthor = {300k/ns team},\ntitle = {CryptoTrader-LM: A LoRA-tuned Ministral-8B Model for Cryptocurrency Trading Decisions},\nyear = {2024},\npublisher = {Hugging Face},\nhowpublished = {\\url{https://huggingface.co/agarkovv/CryptoTrader-LM}},\n}\nAPA:\n300k/ns team. (2024). CryptoTrader-LM: A LoRA-tuned Ministral-8B Model for Cryptocurrency Trading Decisions. Hugging Face. https://huggingface.co/agarkovv/CryptoTrader-LM\nGlossary [optional]\nLoRA (Low-Rank Adaptation): A parameter-efficient fine-tuning method that reduces the number of trainable parameters by transforming the large matrices in transformers into low-rank decompositions, allowing for quicker and more memory-efficient fine-tuning.\nBTC: The ticker symbol for Bitcoin, a decentralized cryptocurrency.\nETH: The ticker symbol for Ethereum, a decentralized cryptocurrency and blockchain platform.\nSharpe Ratio (SR): A measure of risk-adjusted return, used to evaluate the performance of an investment or trading strategy.\nPnL (Profit and Loss): The financial gain or loss realized from trading over a specific time period.\nMore Information [optional]\nFor more information on the training process, model performance, or any specific details, please contact the model authors.\nModel Card Authors [optional]\n300k/ns\nContact via Telegram: @allocfree\nModel Card Contact\nFor any inquiries, please contact via Telegram: @allocfree\nFramework Versions\nPEFT: v0.13.2\nTransformers: v4.33.3\nPyTorch: v2.1.0",
    "stabilityai/stable-diffusion-3.5-controlnets": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nBy clicking \"Agree\", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nStable Diffusion 3.5 ControlNets\nModel\nLicense\nUsage\nUsage in üß® Diffusers\nUsage in ComfyUI\nUsage in SD3.5 Standalone Repo\nPreprocessing\nTips\nUses\nOut-of-Scope Uses\nTraining Data and Strategy\nSafety\nIntegrity Evaluation\nRisks identified and mitigations:\nAcknowledgements\nContact\nStable Diffusion 3.5 ControlNets\nModel\nThis repository provides a number of ControlNet models trained for use with Stable Diffusion 3.5 Large.\nThe following control types are available:\nCanny - Use a Canny edge map to guide the structure of the generated image. This is especially useful for illustrations, but works with all styles.\nDepth - use a depth map, generated by DepthFM, to guide generation. Some example use cases include generating architectural renderings, or texturing 3D assets.\nBlur - can be used to perform extremely high fidelity upscaling. A common use case is to tile an input image, apply the ControlNet to each tile, and merge the tiles to produce a higher resolution image. A more in-depth description of this use case is here.\nFor Comfy users, this extension provides support.\nWe recommend tiling the image at a tile size between 128 and 512.\nAll currently released ControlNets are compatible only with Stable Diffusion 3.5 Large (8b).\nAdditional ControlNet models, including 2B versions of the variants above, and multiple other control types, will be added to this repository in the future.\nPlease note: This model is released under the Stability Community License. Visit Stability AI to learn or contact us for commercial licensing details.\nLicense\nHere are the key components of the license:\nFree for non-commercial use: Individuals and organizations can use the model free of charge for non-commercial use, including scientific research.\nFree for commercial use (up to $1M in annual revenue): Startups, small to medium-sized businesses, and creators can use the model for commercial purposes at no cost, as long as their total annual revenue is less than $1M.\nOwnership of outputs: Retain ownership of the media generated without restrictive licensing implications.\nFor organizations with annual revenue more than $1M, please contact us here to inquire about an Enterprise License.\nUsage\nFor local or self-hosted use, we recommend ComfyUI for node-based UI inference, or the standalone SD3.5 repo for programmatic use.\nUsage in üß® Diffusers\nPlease see the READMEs in the standalone repos for Diffusers support:\nDepth\nBlur\nCanny\nUsage in ComfyUI\nPlease see the ComfyUI announcement blog post for details on usage within Comfy, including example workflows.\nUsage in SD3.5 Standalone Repo\nInstall the repo:\ngit clone git@github.com:Stability-AI/sd3.5.git\npip install -r requirements.txt\nThen, download the models and sample images like so:\ninput/canny.png\nmodels/clip_g.safetensors\nmodels/clip_l.safetensors\nmodels/t5xxl.safetensors\nmodels/sd3.5_large.safetensors\nmodels/sd3.5_large_controlnet_canny.safetensors\nand then you can run\npython sd3_infer.py --controlnet_ckpt models/sd3.5_large_controlnet_canny.safetensors --controlnet_cond_image input/canny.png --prompt \"An adorable fluffy pastel creature\"\nWhich should give you an image like below:\nThe conditioning image should already be preprocessed before being used as input to the standalone repo; sd3.5 does not implement the preprocessing code below.\nPreprocessing\nBelow are code snippets for preprocessing the various control image types.\nCanny\nimport torchvision.transforms.functional as F\n# assuming img is a PIL image\nimg = F.to_tensor(img)\nimg = cv2.cvtColor(img.transpose(1, 2, 0), cv2.COLOR_RGB2GRAY)\nimg = cv2.Canny(img, 100, 200)\nBlur\nimport torchvision.transforms as transforms\n# assuming img is a PIL image\ngaussian_blur = transforms.GaussianBlur(kernel_size=50)\nblurred_image = gaussian_blur(img)\nDepth\n# install depthfm from https://github.com/CompVis/depth-fm\nimport torchvision.transforms as transforms\nfrom depthfm.dfm import DepthFM\ndepthfm_model = DepthFM(ckpt_path=checkpoint_path)\ndepthfm_model.eval()\n# assuming img is a PIL image\nimg = F.to_tensor(img)\nc, h, w = img.shape\nimg = F.interpolate(img, (512, 512), mode='bilinear', align_corners=False)\nwith torch.no_grad():\nimg = self.depthfm_model(img, num_steps=2, ensemble_size=4)\nimg = F.interpolate(img, (h, w), mode='bilinear', align_corners=False)\nTips\nWe recommend starting with a ControlNet strength of 0.7-0.8, and adjusting as needed.\nEuler sampler and a slightly higher step count (50-60) gives best results, especially with Canny.\nPass --text_encoder_device <device_name> to load the text encoders directly to VRAM, which can speed up the full inference loop at the cost of extra VRAM usage.\nUses\nAll uses of the model must be in accordance with our Acceptable Use Policy.\nOut-of-Scope Uses\nThe model was not trained to be factual or true representations of people or events.  As such, using the model to generate such content is out-of-scope of the abilities of this model.\nTraining Data and Strategy\nThese models were trained on a wide variety of data, including synthetic data and filtered publicly available data.\nSafety\nWe believe in safe, responsible AI practices and take deliberate measures to ensure Integrity starts at the early stages of development. This means we have taken and continue to take reasonable steps to prevent the misuse of Stable Diffusion 3.5 by bad actors. For more information about our approach to Safety please visit our Safety page.\nIntegrity Evaluation\nOur integrity evaluation methods include structured evaluations and red-teaming testing for certain harms. Testing was conducted primarily in English and may not cover all possible harms.\nRisks identified and mitigations:\nHarmful content: We have implemented safeguards that attempt to strike the right balance between usefulness and preventing harm. However, this does not guarantee that all possible harmful content has been removed. All developers and deployers should exercise caution and implement content safety guardrails based on their specific product policies and application use cases.\nMisuse: Technical limitations and developer and end-user education can help mitigate against malicious applications of models. All users are required to adhere to our Acceptable Use Policy, including when applying fine-tuning and prompt engineering mechanisms. Please reference the Stability AI Acceptable Use Policy for information on violative uses of our products.\nPrivacy violations: Developers and deployers are encouraged to adhere to privacy regulations with techniques that respect data privacy.\nAcknowledgements\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala, authors of the original ControlNet paper.\nLvmin Zhang, who also developed the Tile ControlNet, which inspired the Blur ControlNet.\nDiffusers library authors, whose code was referenced during development.\nInstantX team, whose Flux and SD3 ControlNets were also referenced during training.\nAll early testers and raters of the models, and the Stability AI team.\nContact\nPlease report any issues with the model or contact us:\nSafety issues:  safety@stability.ai\nSecurity issues:  security@stability.ai\nPrivacy issues:  privacy@stability.ai\nLicense and general: https://stability.ai/license\nEnterprise license: https://stability.ai/enterprise",
    "naver-iv/zim-anything-vitb": "ZIM-Anything-ViTB\nIntroduction\nInstallation\nUsage\nCitation\nZIM-Anything-ViTB\nIntroduction\nüöÄ Introducing ZIM: Zero-Shot Image Matting ‚Äì A Step Beyond SAM! üöÄ\nWhile SAM (Segment Anything Model) has redefined zero-shot segmentation with broad applications across multiple fields, it often falls short in delivering high-precision, fine-grained masks. That‚Äôs where ZIM comes in.\nüåü What is ZIM? üåü\nZIM (Zero-Shot Image Matting) is a groundbreaking model developed to set a new standard in precision matting while maintaining strong zero-shot capabilities. Like SAM, ZIM can generalize across diverse datasets and objects in a zero-shot paradigm. But ZIM goes beyond, delivering highly accurate, fine-grained masks that capture intricate details.\nüîç Get Started with ZIM üîç\nReady to elevate your AI projects with unmatched matting quality? Access ZIM on our project page, Arxiv, and Github.\nInstallation\npip install zim_anything\nor\ngit clone https://github.com/naver-ai/ZIM.git\ncd ZIM; pip install -e .\nUsage\nMake the directory zim_vit_b_2043.\nDownload the encoder weight and decoder weight.\nPut them under the zim_vit_b_2043 directory.\nfrom zim_anything import zim_model_registry, ZimPredictor\nbackbone = \"vit_b\"\nckpt_p = \"zim_vit_b_2043\"\nmodel = zim_model_registry[backbone](checkpoint=ckpt_p)\nif torch.cuda.is_available():\nmodel.cuda()\npredictor = ZimPredictor(model)\npredictor.set_image(<image>)\nmasks, _, _ = predictor.predict(<input_prompts>)\nCitation\nIf you find this project useful, please consider citing:\n@article{kim2024zim,\ntitle={ZIM: Zero-Shot Image Matting for Anything},\nauthor={Kim, Beomyoung and Shin, Chanyong and Jeong, Joonhyun and Jung, Hyungsik and Lee, Se-Yun and Chun, Sewhan and Hwang, Dong-Hyun and Yu, Joonsang},\njournal={arXiv preprint arXiv:2411.00626},\nyear={2024}\n}",
    "naver-iv/zim-anything-vitl": "ZIM-Anything-ViTL\nIntroduction\nInstallation\nUsage\nCitation\nZIM-Anything-ViTL\nIntroduction\nüöÄ Introducing ZIM: Zero-Shot Image Matting ‚Äì A Step Beyond SAM! üöÄ\nWhile SAM (Segment Anything Model) has redefined zero-shot segmentation with broad applications across multiple fields, it often falls short in delivering high-precision, fine-grained masks. That‚Äôs where ZIM comes in.\nüåü What is ZIM? üåü\nZIM (Zero-Shot Image Matting) is a groundbreaking model developed to set a new standard in precision matting while maintaining strong zero-shot capabilities. Like SAM, ZIM can generalize across diverse datasets and objects in a zero-shot paradigm. But ZIM goes beyond, delivering highly accurate, fine-grained masks that capture intricate details.\nüîç Get Started with ZIM üîç\nReady to elevate your AI projects with unmatched matting quality? Access ZIM on our project page, Arxiv, and Github.\nInstallation\npip install zim_anything\nor\ngit clone https://github.com/naver-ai/ZIM.git\ncd ZIM; pip install -e .\nUsage\nMake the directory zim_vit_l_2092.\nDownload the encoder weight and decoder weight.\nPut them under the zim_vit_b_2092 directory.\nfrom zim_anything import zim_model_registry, ZimPredictor\nbackbone = \"vit_l\"\nckpt_p = \"zim_vit_l_2092\"\nmodel = zim_model_registry[backbone](checkpoint=ckpt_p)\nif torch.cuda.is_available():\nmodel.cuda()\npredictor = ZimPredictor(model)\npredictor.set_image(<image>)\nmasks, _, _ = predictor.predict(<input_prompts>)\nCitation\nIf you find this project useful, please consider citing:\n@article{kim2024zim,\ntitle={ZIM: Zero-Shot Image Matting for Anything},\nauthor={Kim, Beomyoung and Shin, Chanyong and Jeong, Joonhyun and Jung, Hyungsik and Lee, Se-Yun and Chun, Sewhan and Hwang, Dong-Hyun and Yu, Joonsang},\njournal={arXiv preprint arXiv:2411.00626},\nyear={2024}\n}",
    "mradermacher/codegemma-7b-it-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/google/codegemma-7b-it\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/codegemma-7b-it-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n3.6\nGGUF\nQ3_K_S\n4.1\nGGUF\nQ3_K_M\n4.5\nlower quality\nGGUF\nQ3_K_L\n4.8\nGGUF\nIQ4_XS\n4.9\nGGUF\nQ4_0_4_4\n5.1\nfast on arm, low quality\nGGUF\nQ4_K_S\n5.1\nfast, recommended\nGGUF\nQ4_K_M\n5.4\nfast, recommended\nGGUF\nQ5_K_S\n6.1\nGGUF\nQ5_K_M\n6.2\nGGUF\nQ6_K\n7.1\nvery good quality\nGGUF\nQ8_0\n9.2\nfast, best quality\nGGUF\nf16\n17.2\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "magic-leap-community/superglue_outdoor": "SuperGlue\nDemo notebook\nModel Details\nModel Description\nModel Sources\nUses\nDirect Use\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nCitation\nModel Card Authors\nSuperGlue\nThe SuperGlue model was proposed\nin SuperGlue: Learning Feature Matching with Graph Neural Networks by Paul-Edouard Sarlin, Daniel\nDeTone, Tomasz Malisiewicz and Andrew Rabinovich.\nThis model consists of matching two sets of interest points detected in an image. Paired with the\nSuperPoint model, it can be used to match two images and\nestimate the pose between them. This model is useful for tasks such as image matching, homography estimation, etc.\nThe abstract from the paper is the following:\nThis paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences\nand rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs\nare predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling\nSuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics,\nour technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image\npairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in\nchallenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and\ncan be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at this URL.\nThis model was contributed by stevenbucaille.\nThe original code can be found here.\nDemo notebook\nA demo notebook showcasing inference + visualization with SuperGlue can be found here.\nModel Details\nModel Description\nSuperGlue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points.\nIt introduces a flexible context aggregation mechanism based on attention, enabling it to reason about the underlying 3D scene and feature\nassignments. The architecture consists of two main components: the Attentional Graph Neural Network and the Optimal Matching Layer.\nThe Attentional Graph Neural Network uses a Keypoint Encoder to map keypoint positions and visual descriptors.\nIt employs self- and cross-attention layers to create powerful representations. The Optimal Matching Layer creates a\nscore matrix, augments it with dustbins, and finds the optimal partial assignment using the Sinkhorn algorithm.\nDeveloped by: MagicLeap\nModel type: Image Matching\nLicense: ACADEMIC OR NON-PROFIT ORGANIZATION NONCOMMERCIAL RESEARCH USE ONLY\nModel Sources\nRepository: https://github.com/magicleap/SuperGluePretrainedNetwork\nPaper: https://arxiv.org/pdf/1911.11763\nDemo: https://psarlin.com/superglue/\nUses\nDirect Use\nSuperGlue is designed for feature matching and pose estimation tasks in computer vision. It can be applied to a variety of multiple-view\ngeometry problems and can handle challenging real-world indoor and outdoor environments. However, it may not perform well on tasks that\nrequire different types of visual understanding, such as object detection or image classification.\nHow to Get Started with the Model\nHere is a quick example of using the model. Since this model is an image matching model, it requires pairs of images to be matched:\nfrom transformers import AutoImageProcessor, AutoModel\nimport torch\nfrom PIL import Image\nimport requests\nurl = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\"\nim1 = Image.open(requests.get(url, stream=True).raw)\nurl = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\"\nim2 = Image.open(requests.get(url, stream=True).raw)\nimages = [im1, im2]\nprocessor = AutoImageProcessor.from_pretrained(\"stevenbucaille/superglue_outdoor\")\nmodel = AutoModel.from_pretrained(\"stevenbucaille/superglue_outdoor\")\ninputs = processor(images, return_tensors=\"pt\")\noutputs = model(**inputs)\nThe outputs contain the list of keypoints detected by the keypoint detector as well as the list of matches with their corresponding matching scores.\nDue to the nature of SuperGlue, to output a dynamic number of matches, you will need to use the mask attribute to retrieve the respective information:\nfrom transformers import AutoImageProcessor, AutoModel\nimport torch\nfrom PIL import Image\nimport requests\nurl_image_1 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\"\nimage_1 = Image.open(requests.get(url_image_1, stream=True).raw)\nurl_image_2 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\"\nimage_2 = Image.open(requests.get(url_image_2, stream=True).raw)\nimages = [image_1, image_2]\nprocessor = AutoImageProcessor.from_pretrained(\"stevenbucaille/superglue_indoor\")\nmodel = AutoModel.from_pretrained(\"stevenbucaille/superglue_indoor\")\ninputs = processor(images, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model(**inputs)\n# Get the respective image masks\nimage0_mask, image1_mask = outputs_mask[0]\nimage0_indices = torch.nonzero(image0_mask).squeeze()\nimage1_indices = torch.nonzero(image1_mask).squeeze()\nimage0_matches = outputs.matches[0, 0][image0_indices]\nimage1_matches = outputs.matches[0, 1][image1_indices]\nimage0_matching_scores = outputs.matching_scores[0, 0][image0_indices]\nimage1_matching_scores = outputs.matching_scores[0, 1][image1_indices]\nYou can use the post_process_keypoint_matching method from the SuperGlueImageProcessor to get the keypoints and matches in a more readable format:\nimage_sizes = [(image.height, image.width) for image in images]\noutputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\nfor i, output in enumerate(outputs):\nprint(\"For the image pair\", i)\nfor keypoint0, keypoint1, matching_score in zip(output[\"keypoints0\"], output[\"keypoints1\"],\noutput[\"matching_scores\"]):\nprint(\nf\"Keypoint at coordinate {keypoint0.numpy()} in the first image matches with keypoint at coordinate {keypoint1.numpy()} in the second image with a score of {matching_score}.\"\n)\nFrom the outputs, you can visualize the matches between the two images using the following code:\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Create side by side image\nmerged_image = np.zeros((max(image1.height, image2.height), image1.width + image2.width, 3))\nmerged_image[: image1.height, : image1.width] = np.array(image1) / 255.0\nmerged_image[: image2.height, image1.width :] = np.array(image2) / 255.0\nplt.imshow(merged_image)\nplt.axis(\"off\")\n# Retrieve the keypoints and matches\noutput = outputs[0]\nkeypoints0 = output[\"keypoints0\"]\nkeypoints1 = output[\"keypoints1\"]\nmatching_scores = output[\"matching_scores\"]\nkeypoints0_x, keypoints0_y = keypoints0[:, 0].numpy(), keypoints0[:, 1].numpy()\nkeypoints1_x, keypoints1_y = keypoints1[:, 0].numpy(), keypoints1[:, 1].numpy()\n# Plot the matches\nfor keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\nkeypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, matching_scores\n):\nplt.plot(\n[keypoint0_x, keypoint1_x + image1.width],\n[keypoint0_y, keypoint1_y],\ncolor=plt.get_cmap(\"RdYlGn\")(matching_score.item()),\nalpha=0.9,\nlinewidth=0.5,\n)\nplt.scatter(keypoint0_x, keypoint0_y, c=\"black\", s=2)\nplt.scatter(keypoint1_x + image1.width, keypoint1_y, c=\"black\", s=2)\n# Save the plot\nplt.savefig(\"matched_image.png\", dpi=300, bbox_inches='tight')\nplt.close()\nTraining Details\nTraining Data\nSuperGlue is trained on large annotated datasets for pose estimation, enabling it to learn priors for pose estimation and reason about the 3D scene.\nThe training data consists of image pairs with ground truth correspondences and unmatched keypoints derived from ground truth poses and depth maps.\nTraining Procedure\nSuperGlue is trained in a supervised manner using ground truth matches and unmatched keypoints. The loss function maximizes\nthe negative log-likelihood of the assignment matrix, aiming to simultaneously maximize precision and recall.\nTraining Hyperparameters\nTraining regime: fp32\nSpeeds, Sizes, Times\nSuperGlue is designed to be efficient and runs in real-time on a modern GPU. A forward pass takes approximately 69 milliseconds (15 FPS) for an indoor image pair.\nThe model has 12 million parameters, making it relatively compact compared to some other deep learning models.\nThe inference speed of SuperGlue is suitable for real-time applications and can be readily integrated into\nmodern Simultaneous Localization and Mapping (SLAM) or Structure-from-Motion (SfM) systems.\nCitation\nBibTeX:\n@inproceedings{sarlin2020superglue,\ntitle={Superglue: Learning feature matching with graph neural networks},\nauthor={Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},\nbooktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},\npages={4938--4947},\nyear={2020}\n}\nModel Card Authors\nSteven Bucaille",
    "Docty/Blood-Cell": "Blood-Cell\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nBlood-Cell\nThis model is a fine-tuned version of google/vit-base-patch16-224-in21k on an unknown dataset.\nIt achieves the following results on the evaluation set:\nTrain Loss: 1.9897\nValidation Loss: 1.9904\nTrain Accuracy: 0.3905\nEpoch: 9\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\noptimizer: {'name': 'AdamWeightDecay', 'learning_rate': {'module': 'keras.optimizers.schedules', 'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 3e-05, 'decay_steps': 10, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}, 'registered_name': None}, 'decay': 0.0, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-08, 'amsgrad': False, 'weight_decay_rate': 0.01}\ntraining_precision: float32\nTraining results\nTrain Loss\nValidation Loss\nTrain Accuracy\nEpoch\n1.9904\n1.9904\n0.3905\n0\n1.9894\n1.9904\n0.3905\n1\n1.9898\n1.9904\n0.3905\n2\n1.9897\n1.9904\n0.3905\n3\n1.9897\n1.9904\n0.3905\n4\n1.9901\n1.9904\n0.3905\n5\n1.9897\n1.9904\n0.3905\n6\n1.9897\n1.9904\n0.3905\n7\n1.9902\n1.9904\n0.3905\n8\n1.9897\n1.9904\n0.3905\n9\nFramework versions\nTransformers 4.46.2\nTensorFlow 2.17.1\nDatasets 3.1.0\nTokenizers 0.20.3",
    "strangerzonehf/Flux-Ultimate-LoRA-Collection": "Flux.1dev Adapter Resources\nFlux.1dev Adapter Resources\nFile Name\nSize\nLFS\nFile Type\n3DXL-Mannequin.safetensors\n613 MB\nLFS\n.safetensors\n3DXLC1.safetensors\n613 MB\nLFS\n.safetensors\n3DXLP1.safetensors\n613 MB\nLFS\n.safetensors\n3DXLP2.safetensors\n613 MB\nLFS\n.safetensors\n3DXLP3.safetensors\n613 MB\nLFS\n.safetensors\n3DXLP4.safetensors\n613 MB\nLFS\n.safetensors\n3DXLP5.safetensors\n613 MB\nLFS\n.safetensors\n3DXLP6.safetensors\n613 MB\nLFS\n.safetensors\nAbstract-Cartoon.safetensors\n613 MB\nLFS\n.safetensors\nAmxtoon.safetensors\n613 MB\nLFS\n.safetensors\nAnimeo.safetensors\n613 MB\nLFS\n.safetensors\nAnimex.safetensors\n613 MB\nLFS\n.safetensors\nAura-9999.safetensors\n613 MB\nLFS\n.safetensors\nBold-Shadows.safetensors\n613 MB\nLFS\n.safetensors\nC33.safetensors\n613 MB\nLFS\n.safetensors\nCAM00.safetensors\n613 MB\nLFS\n.safetensors\nCanopus-Anime-Character-Art-FluxDev-LoRA.safetensors\n613 MB\nLFS\n.safetensors\nCanopus-Car-Flux-Dev-LoRA.safetensors\n613 MB\nLFS\n.safetensors\nCanopus-Clothing-Flux-Dev-Florence2-LoRA.safetensors\n613 MB\nLFS\n.safetensors\nCanopus-Cute-Kawaii-Flux-LoRA.safetensors\n613 MB\nLFS\n.safetensors\nCastor-3D-Portrait-Flux-LoRA.safetensors\n306 MB\nLFS\n.safetensors\nCastor-3D-Sketchfab-Flux-LoRA.safetensors\n613 MB\nLFS\n.safetensors\nCastor-Character-Polygon-LoRA.safetensors\n613 MB\nLFS\n.safetensors\nCastor-Collage-Dim-Flux-LoRA.safetensors\n613 MB\nLFS\n.safetensors\nCastor-Happy-Halloween-Flux-LoRA.safetensors\n613 MB\nLFS\n.safetensors\nCastor-Red-Dead-Redemption-2-Flux-LoRA.safetensors\n613 MB\nLFS\n.safetensors\nClaymation.safetensors\n613 MB\nLFS\n.safetensors\nClothing-Flux-Dev-Florence2-LoRA-Pruned.safetensors\n613 MB\nLFS\n.safetensors\nClouds Illusion.safetensors\n613 MB\nLFS\n.safetensors\nCreative-Stocks.safetensors\n613 MB\nLFS\n.safetensors\nCute-3d-Kawaii.safetensors\n613 MB\nLFS\n.safetensors\nDark_Creature.safetensors\n613 MB\nLFS\n.safetensors\nDigital-Chaos.safetensors\n613 MB\nLFS\n.safetensors\nDigital-Yellow.safetensors\n613 MB\nLFS\n.safetensors\nDramatic-Neon-Flux-LoRA.safetensors\n613 MB\nLFS\n.safetensors\nEBook-Cover.safetensors\n613 MB\nLFS\n.safetensors\nElectric-Blue.safetensors\n613 MB\nLFS\n.safetensors\nFashion-Modeling.safetensors\n613 MB\nLFS\n.safetensors\nFlux-Dev-Real-Anime-LoRA.safetensors\n613 MB\nLFS\n.safetensors\nFlux-Realism-FineDetailed.safetensors\n613 MB\nLFS\n.safetensors\nGArt.safetensors\n613 MB\nLFS\n.safetensors\nGhibli-Art.safetensors\n613 MB\nLFS\n.safetensors\nGlowing-Body.safetensors\n613 MB\nLFS\n.safetensors\nGolden-Coin.safetensors\n613 MB\nLFS\n.safetensors\nGreen-Cartoon.safetensors\n613 MB\nLFS\n.safetensors\nGta6-Concept-Charecter.safetensors\n613 MB\nLFS\n.safetensors\nGta6.safetensors\n613 MB\nLFS\n.safetensors\nHDR-Digital-Chaos.safetensors\n613 MB\nLFS\n.safetensors\nHDR.safetensors\n613 MB\nLFS\n.safetensors\nIcon-Kit.safetensors\n613 MB\nLFS\n.safetensors\nIntense-Red.safetensors\n613 MB\nLFS\n.safetensors\nIsometric-3D-Cinematography.safetensors\n613 MB\nLFS\n.safetensors\nIsometric-3D.safetensors\n613 MB\nLFS\n.safetensors\nKepler-452b-LoRA-Flux-Dev-3D-Bubbly.safetensors\n613 MB\nLFS\n.safetensors\nKnitted- Character.safetensors\n613 MB\nLFS\n.safetensors\nLego.safetensors\n613 MB\nLFS\n.safetensors\nLime-Green.safetensors\n613 MB\nLFS\n.safetensors\nLogo-design.safetensors\n613 MB\nLFS\n.safetensors\nLong-Toon.safetensors\n613 MB\nLFS\n.safetensors\nMinimal-Futuristic.safetensors\n613 MB\nLFS\n.safetensors\nMockup-Texture.safetensors\n613 MB\nLFS\n.safetensors\nMulti-Frame-Shot(MFS).safetensors\n613 MB\nLFS\n.safetensors\nNFTv4.safetensors\n613 MB\nLFS\n.safetensors\nOrange-Chroma.safetensors\n613 MB\nLFS\n.safetensors\nPast-Present-Deep-Mix-Flux-LoRA.safetensors\n613 MB\nLFS\n.safetensors\nPastel-BG.safetensors\n613 MB\nLFS\n.safetensors\nProd-Ad.safetensors\n613 MB\nLFS\n.safetensors\nPurple-Dreamy.safetensors\n613 MB\nLFS\n.safetensors\nPurple_Grid.safetensors\n613 MB\nLFS\n.safetensors\nRed-Undersea.safetensors\n613 MB\nLFS\n.safetensors\nRetro-Pixel.safetensors\n613 MB\nLFS\n.safetensors\nSeamless-Pattern-Design.safetensors\n613 MB\nLFS\n.safetensors\nShadow-Projection.safetensors\n613 MB\nLFS\n.safetensors\nSimple_ Doodle.safetensors\n270 MB\nLFS\n.safetensors\nSmiley-C4C.safetensors\n613 MB\nLFS\n.safetensors\nSnoopy-Charlie-Brown-Flux-LoRA.safetensors\n613 MB\nLFS\n.safetensors\nStreet_Bokeh.safetensors\n613 MB\nLFS\n.safetensors\nSuper-Blend.safetensors\n613 MB\nLFS\n.safetensors\nSuper-Detail.safetensors\n613 MB\nLFS\n.safetensors\nSuper-Portrait.safetensors\n613 MB\nLFS\n.safetensors\nTarot-card.safetensors\n613 MB\nLFS\n.safetensors\nTeen-Outfit.safetensors\n613 MB\nLFS\n.safetensors\nTypography.safetensors\n613 MB\nLFS\n.safetensors\nUncoloured-3D-Polygon.safetensors\n613 MB\nLFS\n.safetensors\nYellow-Laser.safetensors\n613 MB\nLFS\n.safetensors\nYellow_Pop.safetensors\n613 MB\nLFS\n.safetensors\ncapybara-hf.safetensors\n613 MB\nLFS\n.safetensors\nchill-guy.safetensors\n613 MB\nLFS\n.safetensors\ncoloring-book.safetensors\n613 MB\nLFS\n.safetensors\nctoon.safetensors\n613 MB\nLFS\n.safetensors\ndalle-mix.safetensors\n613 MB\nLFS\n.safetensors\nfrosted-gc.safetensors\n613 MB\nLFS\n.safetensors\nhandstick69.safetensors\n613 MB\nLFS\n.safetensors\nindo-realism.safetensors\n613 MB\nLFS\n.safetensors\nlook-in-2.safetensors\n613 MB\nLFS\n.safetensors\nmeme.safetensors\n613 MB\nLFS\n.safetensors\nmidjourney-mix.safetensors\n613 MB\nLFS\n.safetensors\nmjV6.safetensors\n613 MB\nLFS\n.safetensors\nmovieboard.safetensors\n613 MB\nLFS\n.safetensors\nnm99.safetensors\n613 MB\nLFS\n.safetensors\nonly-stickers.safetensors\n613 MB\nLFS\n.safetensors\npolaroid-plus.safetensors\n613 MB\nLFS\n.safetensors\nposter-foss.safetensors\n613 MB\nLFS\n.safetensors\nquoter.safetensors\n613 MB\nLFS\n.safetensors\nsketchcard.safetensors\n613 MB\nLFS\n.safetensors\nstam9.safetensors\n613 MB\nLFS\n.safetensors\nsuper-realism.safetensors\n613 MB\nLFS\n.safetensors\ntoon-mix.safetensors\n613 MB\nLFS\n.safetensors\ntoonic2.5D.safetensors\n613 MB\nLFS\n.safetensors\nywl-realism.safetensors\n613 MB\nLFS\n.safetensors\nPrompt\nStranger Zones Ultimate LoRA Collection\nRepository\nDescription\nLink\nPrithivMLMods\nRepository featuring various adapters and ML models.\nVisit Repository\nStrangerZoneHF\nRepository containing specialized Hugging Face models.\nVisit Repository"
}