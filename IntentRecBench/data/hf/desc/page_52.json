{
    "pyannote/overlapped-speech-detection": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nThe collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers apply for grants to improve it further. If you are an academic researcher, please cite the relevant papers in your own publications using the model. If you work for a company, please consider contributing back to pyannote.audio development (e.g. through unrestricted gifts). We also provide scientific consulting services around speaker diarization and machine listening.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nüéπ Overlapped speech detection\nSupport\nCitation\nUsing this open-source model in production?Consider switching to pyannoteAI for better and faster options.\nüéπ Overlapped speech detection\nRelies on pyannote.audio 2.1: see installation instructions.\n# 1. visit hf.co/pyannote/segmentation and accept user conditions\n# 2. visit hf.co/settings/tokens to create an access token\n# 3. instantiate pretrained overlapped speech detection pipeline\nfrom pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(\"pyannote/overlapped-speech-detection\",\nuse_auth_token=\"ACCESS_TOKEN_GOES_HERE\")\noutput = pipeline(\"audio.wav\")\nfor speech in output.get_timeline().support():\n# two or more speakers are active between speech.start and speech.end\n...\nSupport\nFor commercial enquiries and scientific consulting, please contact me.For technical questions and bug reports, please check pyannote.audio Github repository.\nCitation\n@inproceedings{Bredin2021,\nTitle = {{End-to-end speaker segmentation for overlap-aware resegmentation}},\nAuthor = {{Bredin}, Herv{\\'e} and {Laurent}, Antoine},\nBooktitle = {Proc. Interspeech 2021},\nAddress = {Brno, Czech Republic},\nMonth = {August},\nYear = {2021},\n}\n@inproceedings{Bredin2020,\nTitle = {{pyannote.audio: neural building blocks for speaker diarization}},\nAuthor = {{Bredin}, Herv{\\'e} and {Yin}, Ruiqing and {Coria}, Juan Manuel and {Gelly}, Gregory and {Korshunov}, Pavel and {Lavechin}, Marvin and {Fustes}, Diego and {Titeux}, Hadrien and {Bouaziz}, Wassim and {Gill}, Marie-Philippe},\nBooktitle = {ICASSP 2020, IEEE International Conference on Acoustics, Speech, and Signal Processing},\nAddress = {Barcelona, Spain},\nMonth = {May},\nYear = {2020},\n}",
    "pyannote/segmentation": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nThe collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers apply for grants to improve it further. If you are an academic researcher, please cite the relevant papers in your own publications using the model. If you work for a company, please consider contributing back to pyannote.audio development (e.g. through unrestricted gifts). We also provide scientific consulting services around speaker diarization and machine listening.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nüéπ Speaker segmentation\nUsage\nVoice activity detection\nOverlapped speech detection\nResegmentation\nRaw scores\nCitation\nReproducible research\nUsing this open-source model in production?Consider switching to pyannoteAI for better and faster options.\nüéπ Speaker segmentation\nPaper | Demo | Blog post\nUsage\nRelies on pyannote.audio 2.1.1: see installation instructions.\n# 1. visit hf.co/pyannote/segmentation and accept user conditions\n# 2. visit hf.co/settings/tokens to create an access token\n# 3. instantiate pretrained model\nfrom pyannote.audio import Model\nmodel = Model.from_pretrained(\"pyannote/segmentation\",\nuse_auth_token=\"ACCESS_TOKEN_GOES_HERE\")\nVoice activity detection\nfrom pyannote.audio.pipelines import VoiceActivityDetection\npipeline = VoiceActivityDetection(segmentation=model)\nHYPER_PARAMETERS = {\n# onset/offset activation thresholds\n\"onset\": 0.5, \"offset\": 0.5,\n# remove speech regions shorter than that many seconds.\n\"min_duration_on\": 0.0,\n# fill non-speech regions shorter than that many seconds.\n\"min_duration_off\": 0.0\n}\npipeline.instantiate(HYPER_PARAMETERS)\nvad = pipeline(\"audio.wav\")\n# `vad` is a pyannote.core.Annotation instance containing speech regions\nOverlapped speech detection\nfrom pyannote.audio.pipelines import OverlappedSpeechDetection\npipeline = OverlappedSpeechDetection(segmentation=model)\npipeline.instantiate(HYPER_PARAMETERS)\nosd = pipeline(\"audio.wav\")\n# `osd` is a pyannote.core.Annotation instance containing overlapped speech regions\nResegmentation\nfrom pyannote.audio.pipelines import Resegmentation\npipeline = Resegmentation(segmentation=model,\ndiarization=\"baseline\")\npipeline.instantiate(HYPER_PARAMETERS)\nresegmented_baseline = pipeline({\"audio\": \"audio.wav\", \"baseline\": baseline})\n# where `baseline` should be provided as a pyannote.core.Annotation instance\nRaw scores\nfrom pyannote.audio import Inference\ninference = Inference(model)\nsegmentation = inference(\"audio.wav\")\n# `segmentation` is a pyannote.core.SlidingWindowFeature\n# instance containing raw segmentation scores like the\n# one pictured above (output)\nCitation\n@inproceedings{Bredin2021,\nTitle = {{End-to-end speaker segmentation for overlap-aware resegmentation}},\nAuthor = {{Bredin}, Herv{\\'e} and {Laurent}, Antoine},\nBooktitle = {Proc. Interspeech 2021},\nAddress = {Brno, Czech Republic},\nMonth = {August},\nYear = {2021},\n@inproceedings{Bredin2020,\nTitle = {{pyannote.audio: neural building blocks for speaker diarization}},\nAuthor = {{Bredin}, Herv{\\'e} and {Yin}, Ruiqing and {Coria}, Juan Manuel and {Gelly}, Gregory and {Korshunov}, Pavel and {Lavechin}, Marvin and {Fustes}, Diego and {Titeux}, Hadrien and {Bouaziz}, Wassim and {Gill}, Marie-Philippe},\nBooktitle = {ICASSP 2020, IEEE International Conference on Acoustics, Speech, and Signal Processing},\nAddress = {Barcelona, Spain},\nMonth = {May},\nYear = {2020},\n}\nReproducible research\nIn order to reproduce the results of the paper \"End-to-end speaker segmentation for overlap-aware resegmentation\n\", use pyannote/segmentation@Interspeech2021 with the following hyper-parameters:\nVoice activity detection\nonset\noffset\nmin_duration_on\nmin_duration_off\nAMI Mix-Headset\n0.684\n0.577\n0.181\n0.037\nDIHARD3\n0.767\n0.377\n0.136\n0.067\nVoxConverse\n0.767\n0.713\n0.182\n0.501\nOverlapped speech detection\nonset\noffset\nmin_duration_on\nmin_duration_off\nAMI Mix-Headset\n0.448\n0.362\n0.116\n0.187\nDIHARD3\n0.430\n0.320\n0.091\n0.144\nVoxConverse\n0.587\n0.426\n0.337\n0.112\nResegmentation of VBx\nonset\noffset\nmin_duration_on\nmin_duration_off\nAMI Mix-Headset\n0.542\n0.527\n0.044\n0.705\nDIHARD3\n0.592\n0.489\n0.163\n0.182\nVoxConverse\n0.537\n0.724\n0.410\n0.563\nExpected outputs (and VBx baseline) are also provided in the /reproducible_research sub-directories.",
    "rifkat/uztext-3Gb-BPE-Roberta": "UzRoBerta model.\nPre-prepared model in Uzbek (Cyrillic and latin script) to model the masked language and predict the next sentences.\nHow to use.\nYou can use this model directly with a pipeline for masked language modeling:\nfrom transformers import pipeline\nunmasker = pipeline('fill-mask', model='rifkat/uztext-3Gb-BPE-Roberta')\nunmasker(\"–ê–ª–∏—à–µ—Ä –ù–∞–≤–æ–∏–π ‚Äì —É–ª—É“ì —û–∑–±–µ–∫ –≤–∞ –±–æ—à“õ–∞ —Ç—É—Ä–∫–∏–π —Ö–∞–ª“õ–ª–∞—Ä–Ω–∏–Ω–≥ [mask], –º—É—Ç–∞—Ñ–∞–∫–∫–∏—Ä–∏ –≤–∞ –¥–∞–≤–ª–∞—Ç –∞—Ä–±–æ–±–∏ –±—û–ª–≥–∞–Ω.\")\n[{'score': 0.5902208685874939,\n'sequence': '–ê–ª–∏—à–µ—Ä –ù–∞–≤–æ–∏–π ‚Äì —É–ª—É“ì —û–∑–±–µ–∫ –≤–∞ –±–æ—à“õ–∞ —Ç—É—Ä–∫–∏–π —Ö–∞–ª“õ–ª–∞—Ä–Ω–∏–Ω–≥ —à–æ–∏—Ä–∏, –º—É—Ç–∞—Ñ–∞–∫–∫–∏—Ä–∏ –≤–∞ –¥–∞–≤–ª–∞—Ç –∞—Ä–±–æ–±–∏ –±—û–ª–≥–∞–Ω.',\n'token': 28809,\n'token_str': ' —à–æ–∏—Ä–∏'},\n{'score': 0.08303504437208176,\n'sequence': '–ê–ª–∏—à–µ—Ä –ù–∞–≤–æ–∏–π ‚Äì —É–ª—É“ì —û–∑–±–µ–∫ –≤–∞ –±–æ—à“õ–∞ —Ç—É—Ä–∫–∏–π —Ö–∞–ª“õ–ª–∞—Ä–Ω–∏–Ω–≥ —É—Å—Ç–æ–∑–∏, –º—É—Ç–∞—Ñ–∞–∫–∫–∏—Ä–∏ –≤–∞ –¥–∞–≤–ª–∞—Ç –∞—Ä–±–æ–±–∏ –±—û–ª–≥–∞–Ω.',\n'token': 17484,\n'token_str': ' —É—Å—Ç–æ–∑–∏'},\n{'score': 0.035882771015167236,\n'sequence': '–ê–ª–∏—à–µ—Ä –ù–∞–≤–æ–∏–π ‚Äì —É–ª—É“ì —û–∑–±–µ–∫ –≤–∞ –±–æ—à“õ–∞ —Ç—É—Ä–∫–∏–π —Ö–∞–ª“õ–ª–∞—Ä–Ω–∏–Ω–≥ –∞—Ä–±–æ–±–∏, –º—É—Ç–∞—Ñ–∞–∫–∫–∏—Ä–∏ –≤–∞ –¥–∞–≤–ª–∞—Ç –∞—Ä–±–æ–±–∏ –±—û–ª–≥–∞–Ω.',\n'token': 34552,\n'token_str': ' –∞—Ä–±–æ–±–∏'},\n{'score': 0.03447483479976654,\n'sequence': '–ê–ª–∏—à–µ—Ä –ù–∞–≤–æ–∏–π ‚Äì —É–ª—É“ì —û–∑–±–µ–∫ –≤–∞ –±–æ—à“õ–∞ —Ç—É—Ä–∫–∏–π —Ö–∞–ª“õ–ª–∞—Ä–Ω–∏–Ω–≥ –∞—Å–æ—Å—á–∏—Å–∏, –º—É—Ç–∞—Ñ–∞–∫–∫–∏—Ä–∏ –≤–∞ –¥–∞–≤–ª–∞—Ç –∞—Ä–±–æ–±–∏ –±—û–ª–≥–∞–Ω.',\n'token': 14034,\n'token_str': ' –∞—Å–æ—Å—á–∏—Å–∏'},\n{'score': 0.03044942207634449,\n'sequence': '–ê–ª–∏—à–µ—Ä –ù–∞–≤–æ–∏–π ‚Äì —É–ª—É“ì —û–∑–±–µ–∫ –≤–∞ –±–æ—à“õ–∞ —Ç—É—Ä–∫–∏–π —Ö–∞–ª“õ–ª–∞—Ä–Ω–∏–Ω–≥ –¥—û—Å—Ç–∏, –º—É—Ç–∞—Ñ–∞–∫–∫–∏—Ä–∏ –≤–∞ –¥–∞–≤–ª–∞—Ç –∞—Ä–±–æ–±–∏ –±—û–ª–≥–∞–Ω.',\n'token': 28100,\n'token_str': ' –¥—û—Å—Ç–∏'}]\nunmasker(\"Kuchli yomg‚Äòirlar tufayli bir qator [mask] kuchli sel oqishi kuzatildi.\")\n[{'score': 0.410250186920166,\n'sequence': 'Kuchli yomg‚Äòirlar tufayli bir qator hududlarda kuchli sel oqishi kuzatildi.',\n'token': 11009,\n'token_str': ' hududlarda'},\n{'score': 0.2023029774427414,\n'sequence': 'Kuchli yomg‚Äòirlar tufayli bir qator tumanlarda kuchli sel oqishi kuzatildi.',\n'token': 35370,\n'token_str': ' tumanlarda'},\n{'score': 0.129830002784729,\n'sequence': 'Kuchli yomg‚Äòirlar tufayli bir qator viloyatlarda kuchli sel oqishi kuzatildi.',\n'token': 33584,\n'token_str': ' viloyatlarda'},\n{'score': 0.04539087787270546,\n'sequence': 'Kuchli yomg‚Äòirlar tufayli bir qator mamlakatlarda kuchli sel oqishi kuzatildi.',\n'token': 19315,\n'token_str': ' mamlakatlarda'},\n{'score': 0.0369882769882679,\n'sequence': 'Kuchli yomg‚Äòirlar tufayli bir qator joylarda kuchli sel oqishi kuzatildi.',\n'token': 5853,\n'token_str': ' joylarda'}]\nTraining data.\nUzBERT model was pretrained on ‚âà2M news articles (‚âà3Gb).\n@misc {rifkat_davronov_2022,\nauthor       = { {Adilova Fatima,Rifkat Davronov, Samariddin Kushmuratov, Ruzmat Safarov} },\ntitle        = { uztext-3Gb-BPE-Roberta (Revision 0c87494) },\nyear         = 2022,\nurl          = { https://huggingface.co/rifkat/uztext-3Gb-BPE-Roberta },\ndoi          = { 10.57967/hf/0140 },\npublisher    = { Hugging Face }\n}",
    "rinna/japanese-gpt-1b": "japanese-gpt-1b\nHow to use the model\nModel architecture\nTraining\nTokenization\nRelease date\nHow to cite\nLicenese\njapanese-gpt-1b\nThis repository provides a 1.3B-parameter Japanese GPT model. The model was trained by rinna Co., Ltd.\nHow to use the model\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt-1b\", use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt-1b\")\nif torch.cuda.is_available():\nmodel = model.to(\"cuda\")\ntext = \"Ë•øÁî∞ÂπæÂ§öÈÉé„ÅØ„ÄÅ\"\ntoken_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\nwith torch.no_grad():\noutput_ids = model.generate(\ntoken_ids.to(model.device),\nmax_length=100,\nmin_length=100,\ndo_sample=True,\ntop_k=500,\ntop_p=0.95,\npad_token_id=tokenizer.pad_token_id,\nbos_token_id=tokenizer.bos_token_id,\neos_token_id=tokenizer.eos_token_id,\nbad_words_ids=[[tokenizer.unk_token_id]]\n)\noutput = tokenizer.decode(output_ids.tolist()[0])\nprint(output)\n# sample output: Ë•øÁî∞ÂπæÂ§öÈÉé„ÅØ„ÄÅ„Åù„ÅÆ‰∏ªËëó„ÅÆ„ÄåÂñÑ„ÅÆÁ†îÁ©∂„Äç„Å™„Å©„Åß„ÄÅ‰∫∫Èñì„ÅÆÂÜÖÈù¢„Å´Ëá™ÁÑ∂„Å®„Åù„ÅÆÊ†πÊ∫ê„Åå„ÅÇ„Çã„Å®ÊåáÊëò„Åó„ÄÅ„Åù„ÅÆÊ†πÊ∫êÁöÑ„Å™ÊÄßÊ†º„ÅØ„ÄÅ„Åì„ÅÆË•øÁî∞Âì≤Â≠¶„ÇíË±°Âæ¥„Åó„Å¶„ÅÑ„Çã„Å®„Åó„Å¶„ÄÅ„Ç´„É≥„Éà„ÅÆ„ÄåÁ¥îÁ≤ãÁêÜÊÄßÊâπÂà§„Äç„Å®„ÄåÂà§Êñ≠ÂäõÊâπÂà§„Äç„ÇíÂØæÊØî„Åó„Å¶Êçâ„Åà„Åæ„Åô„ÄÇ„Åù„Çå„ÅØ„ÄÅ„Äå‰∫∫„ÅåÁêÜÊÄßÁöÑÂ≠òÂú®„Åß„ÅÇ„Çã„Åã„Åé„Çä„Å´„Åä„ÅÑ„Å¶„ÄÅ‰∫∫„ÅØ„Åù„ÅÆÂΩì‰∫∫„Å´Âõ∫Êúâ„Å™ÈÅìÂæ≥ÁöÑ„Å´Ëá™Ë¶ö„Åï„Çå„ÅüÂñÑÊÇ™„ÅÆÂü∫Ê∫ñ„ÇíÊåÅ„Å£„Å¶„ÅÑ„Çã„Äç„Å®„Åô„Çã„ÇÇ„ÅÆ„Åß„ÄÅ„Åì„ÅÆÁêÜÊÄßÁöÑ„Å™ÂñÑÊÇ™„ÅÆË¶≥Âøµ„ÇíÂê¶ÂÆö„Åô„Çã„ÅÆ„Åå„Ç´„É≥„Éà„ÅÆ\nModel architecture\nA 24-layer, 2048-hidden-size transformer-based language model.\nTraining\nThe model was trained on Japanese C4, Japanese CC-100 and Japanese Wikipedia to optimize a traditional language modelling objective. It reaches around 14 perplexity on a chosen validation set from the same data.\nTokenization\nThe model uses a sentencepiece-based tokenizer. The vocabulary was first trained on a selected subset from the training data using the official sentencepiece training script, and then augmented with emojis and symbols.\nRelease date\nJanuary 26, 2022\nHow to cite\n@misc{rinna-japanese-gpt-1b,\ntitle = {rinna/japanese-gpt-1b},\nauthor = {Zhao, Tianyu and Sawada, Kei},\nurl = {https://huggingface.co/rinna/japanese-gpt-1b}\n}\n@inproceedings{sawada2024release,\ntitle = {Release of Pre-Trained Models for the {J}apanese Language},\nauthor = {Sawada, Kei and Zhao, Tianyu and Shing, Makoto and Mitsui, Kentaro and Kaga, Akio and Hono, Yukiya and Wakatsuki, Toshiaki and Mitsuda, Koh},\nbooktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},\nmonth = {5},\nyear = {2024},\npages = {13898--13905},\nurl = {https://aclanthology.org/2024.lrec-main.1213},\nnote = {\\url{https://arxiv.org/abs/2404.01657}}\n}\nLicenese\nThe MIT license",
    "savasy/bert-base-turkish-ner-cased": "For Turkish language, here is an easy-to-use NER application.\nCitation\nother detail\nUsage\nSome results\nFor Turkish language, here is an easy-to-use NER application.\n** T√ºrk√ße i√ßin kolay bir python  NER (Bert + Transfer Learning)  (ƒ∞sim Varlƒ±k Tanƒ±ma) modeli...\nCitation\nPlease cite if you use it in your study\n@misc{yildirim2024finetuning,\ntitle={Fine-tuning Transformer-based Encoder for Turkish Language Understanding Tasks},\nauthor={Savas Yildirim},\nyear={2024},\neprint={2401.17396},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\n@book{yildirim2021mastering,\ntitle={Mastering Transformers: Build state-of-the-art models from scratch with advanced natural language processing techniques},\nauthor={Yildirim, Savas and Asgari-Chenaghlu, Meysam},\nyear={2021},\npublisher={Packt Publishing Ltd}\n}\nother detail\nThanks to @stefan-it, I applied the followings for training\ncd tr-data\nfor file in train.txt dev.txt test.txt labels.txt\ndo\nwget https://schweter.eu/storage/turkish-bert-wikiann/$file\ndone\ncd ..\nIt will download the pre-processed datasets with training, dev and test splits and put them in a tr-data folder.\nRun pre-training\nAfter downloading the dataset, pre-training can be started. Just set the following environment variables:\nexport MAX_LENGTH=128\nexport BERT_MODEL=dbmdz/bert-base-turkish-cased\nexport OUTPUT_DIR=tr-new-model\nexport BATCH_SIZE=32\nexport NUM_EPOCHS=3\nexport SAVE_STEPS=625\nexport SEED=1\nThen run pre-training:\npython3 run_ner_old.py --data_dir ./tr-data3 \\\n--model_type bert \\\n--labels ./tr-data/labels.txt \\\n--model_name_or_path $BERT_MODEL \\\n--output_dir $OUTPUT_DIR-$SEED \\\n--max_seq_length $MAX_LENGTH \\\n--num_train_epochs $NUM_EPOCHS \\\n--per_gpu_train_batch_size $BATCH_SIZE \\\n--save_steps $SAVE_STEPS \\\n--seed $SEED \\\n--do_train \\\n--do_eval \\\n--do_predict \\\n--fp16\nUsage\nfrom transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\nmodel = AutoModelForTokenClassification.from_pretrained(\"savasy/bert-base-turkish-ner-cased\")\ntokenizer = AutoTokenizer.from_pretrained(\"savasy/bert-base-turkish-ner-cased\")\nner=pipeline('ner', model=model, tokenizer=tokenizer)\nner(\"Mustafa Kemal Atat√ºrk 19 Mayƒ±s 1919'da Samsun'a ayak bastƒ±.\")\nSome results\nData1:  For the data above\nEval Results:\nprecision = 0.916400580551524\nrecall = 0.9342309684101502\nf1 = 0.9252298787412536\nloss = 0.11335893666411284\nTest Results:\nprecision = 0.9192058759362955\nrecall = 0.9303010230367262\nf1 = 0.9247201697271198\nloss = 0.11182546521618497\nData2:\nhttps://github.com/stefan-it/turkish-bert/files/4558187/nerdata.txt\nThe performance for the data given by @kemalaraz is as follows\nsavas@savas-lenova:~/Desktop/trans/tr-new-model-1$ cat eval_results.txt\nprecision = 0.9461980692049029\nrecall = 0.959309358847465\nf1 = 0.9527086063783312\nloss = 0.037054269206847804\nsavas@savas-lenova:~/Desktop/trans/tr-new-model-1$ cat test_results.txt\nprecision = 0.9458370635631155\nrecall = 0.9588201928530913\nf1 = 0.952284378344882\nloss = 0.035431676572445225",
    "savasy/bert-base-turkish-sentiment-cased": "Bert-base Turkish Sentiment Model\nCitation\nDataset\nThe dataset is used by following papers\nTraining\nResults\nCode Usage\nTest\nData\nCode\nBert-base Turkish Sentiment Model\nhttps://huggingface.co/savasy/bert-base-turkish-sentiment-cased\nThis model is used for Sentiment Analysis, which is based on BERTurk for Turkish Language https://huggingface.co/dbmdz/bert-base-turkish-cased\nCitation\nPlease cite if you use it in your study\n@misc{yildirim2024finetuning,\ntitle={Fine-tuning Transformer-based Encoder for Turkish Language Understanding Tasks},\nauthor={Savas Yildirim},\nyear={2024},\neprint={2401.17396},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\n@book{yildirim2021mastering,\ntitle={Mastering Transformers: Build state-of-the-art models from scratch with advanced natural language processing techniques},\nauthor={Yildirim, Savas and Asgari-Chenaghlu, Meysam},\nyear={2021},\npublisher={Packt Publishing Ltd}\n}\nDataset\nThe dataset is taken from the studies [2] and [3], and merged.\nThe study [2] gathered movie and product reviews. The products are book, DVD, electronics, and kitchen.\nThe movie dataset is taken from a cinema Web page (Beyazperde) with\n5331 positive and 5331 negative sentences. Reviews in the Web page are marked in\nscale from 0 to 5 by the users who made the reviews. The study considered a review\nsentiment positive if the rating is equal to or bigger than 4, and negative if it is less\nor equal to 2. They also built Turkish product review dataset from an online retailer\nWeb page. They constructed benchmark dataset consisting of reviews regarding some\nproducts (book, DVD, etc.). Likewise, reviews are marked in the range from 1 to 5,\nand majority class of reviews are 5. Each category has 700 positive and 700 negative\nreviews in which average rating of negative reviews is 2.27 and of positive reviews\nis 4.5. This dataset is also used by the study [1].\nThe study [3] collected tweet dataset. They proposed a new approach for automatically classifying the sentiment of microblog messages. The proposed approach is based on utilizing robust feature representation and fusion.\nMerged Dataset\nsize\ndata\n8000\ndev.tsv\n8262\ntest.tsv\n32000\ntrain.tsv\n48290\ntotal\nThe dataset is used by following papers\n[1] Yildirim, SavasÃß. (2020). Comparing Deep Neural Networks to Traditional Models for Sentiment Analysis in Turkish Language. 10.1007/978-981-15-1216-2_12.\n[2] Demirtas, Erkin and Mykola Pechenizkiy. 2013. Cross-lingual polarity detection with machine translation. In Proceedings of the Second International Workshop on Issues of Sentiment\nDiscovery and Opinion Mining (WISDOM ‚Äô13)\n[3] Hayran, A.,   Sert, M. (2017), \"Sentiment Analysis on Microblog Data based on Word Embedding and Fusion Techniques\", IEEE 25th Signal Processing and Communications Applications Conference (SIU 2017), Belek, Turkey\nTraining\nexport GLUE_DIR=\"./sst-2-newall\"\nexport TASK_NAME=SST-2\npython3 run_glue.py \\\n--model_type bert \\\n--model_name_or_path dbmdz/bert-base-turkish-uncased\\\n--task_name \"SST-2\" \\\n--do_train \\\n--do_eval \\\n--data_dir \"./sst-2-newall\" \\\n--max_seq_length 128 \\\n--per_gpu_train_batch_size 32 \\\n--learning_rate 2e-5 \\\n--num_train_epochs 3.0 \\\n--output_dir \"./model\"\nResults\n05/10/2020 17:00:43 - INFO - transformers.trainer -   ***** Running Evaluation *****05/10/2020 17:00:43 - INFO - transformers.trainer -     Num examples = 799905/10/2020 17:00:43 - INFO - transformers.trainer -     Batch size = 8Evaluation: 100% 1000/1000 [00:34<00:00, 29.04it/s]05/10/2020 17:01:17 - INFO - __main__ -   ***** Eval results sst-2 *****05/10/2020 17:01:17 - INFO - __main__ -     acc = 0.953994249281160205/10/2020 17:01:17 - INFO - __main__ -     loss = 0.16348013816401363\nAccuracy is about 95.4%\nCode Usage\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\nmodel = AutoModelForSequenceClassification.from_pretrained(\"savasy/bert-base-turkish-sentiment-cased\")\ntokenizer = AutoTokenizer.from_pretrained(\"savasy/bert-base-turkish-sentiment-cased\")\nsa= pipeline(\"sentiment-analysis\", tokenizer=tokenizer, model=model)\np = sa(\"bu telefon modelleri √ßok kaliteli , her par√ßasƒ± √ßok √∂zel bence\")\nprint(p)\n# [{'label': 'LABEL_1', 'score': 0.9871089}]\nprint(p[0]['label'] == 'LABEL_1')\n# True\np = sa(\"Film √ßok k√∂t√º ve √ßok sahteydi\")\nprint(p)\n# [{'label': 'LABEL_0', 'score': 0.9975505}]\nprint(p[0]['label'] == 'LABEL_1')\n# False\nTest\nData\nSuppose your file has lots of lines of comment and label (1 or 0) at the end  (tab seperated)\ncomment1 ... \\t labelcomment2 ... \\t label...\nCode\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\nmodel = AutoModelForSequenceClassification.from_pretrained(\"savasy/bert-base-turkish-sentiment-cased\")\ntokenizer = AutoTokenizer.from_pretrained(\"savasy/bert-base-turkish-sentiment-cased\")\nsa = pipeline(\"sentiment-analysis\", tokenizer=tokenizer, model=model)\ninput_file = \"/path/to/your/file/yourfile.tsv\"\ni, crr = 0, 0\nfor line in open(input_file):\nlines = line.strip().split(\"\\t\")\nif len(lines) == 2:\ni = i + 1\nif i%100 == 0:\nprint(i)\npred = sa(lines[0])\npred = pred[0][\"label\"].split(\"_\")[1]\nif pred == lines[1]:\ncrr = crr + 1\nprint(crr, i, crr/i)",
    "ai-forever/ruRoberta-large": "ruRoberta-large\nAuthors\nCite us\nruRoberta-large\nThe model architecture design, pretraining, and evaluation are documented in our preprint: A Family of Pretrained Transformer Language Models for Russian.\nThe model is pretrained by the SberDevices team.\nTask: mask filling\nType: encoder\nTokenizer: BBPE\nDict size: 50 257\nNum Parameters: 355 M\nTraining Data Volume 250 GB\nAuthors\nNLP core team RnD Telegram channel:\nDmitry Zmitrovich\nCite us\n@misc{zmitrovich2023family,\ntitle={A Family of Pretrained Transformer Language Models for Russian},\nauthor={Dmitry Zmitrovich and Alexander Abramov and Andrey Kalmykov and Maria Tikhonova and Ekaterina Taktasheva and Danil Astafurov and Mark Baushenko and Artem Snegirev and Tatiana Shavrina and Sergey Markov and Vladislav Mikhailov and Alena Fenogenova},\nyear={2023},\neprint={2309.10931},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "sentence-transformers/LaBSE": "LaBSE\nUsage (Sentence-Transformers)\nFull Model Architecture\nCiting & Authors\nLaBSE\nThis is a port of the LaBSE model to PyTorch. It can be used to map 109 languages to a shared vector space.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('sentence-transformers/LaBSE')\nembeddings = model.encode(sentences)\nprint(embeddings)\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n(2): Dense({'in_features': 768, 'out_features': 768, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n(3): Normalize()\n)\nCiting & Authors\nHave a look at LaBSE for the respective publication that describes LaBSE.",
    "sentence-transformers/all-MiniLM-L12-v2": "all-MiniLM-L12-v2\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nBackground\nIntended uses\nTraining procedure\nPre-training\nFine-tuning\nall-MiniLM-L12-v2\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nBackground\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised\ncontrastive learning objective. We used the pretrained microsoft/MiniLM-L12-H384-uncased model and fine-tuned in on a\n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\nWe developped this model during the\nCommunity week using JAX/Flax for NLP & CV,\norganized by Hugging Face. We developped this model as part of the project:\nTrain the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\nIntended uses\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures\nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\nBy default, input text longer than 256 word pieces is truncated.\nTraining procedure\nPre-training\nWe use the pretrained microsoft/MiniLM-L12-H384-uncased model. Please refer to the model card for more detailed information about the pre-training procedure.\nFine-tuning\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\nHyper parameters\nWe trained ou model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: train_script.py.\nTraining data\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the data_config.json file.\nDataset\nPaper\nNumber of training tuples\nReddit comments (2015-2018)\npaper\n726,484,430\nS2ORC Citation pairs (Abstracts)\npaper\n116,288,806\nWikiAnswers Duplicate question pairs\npaper\n77,427,422\nPAQ (Question, Answer) pairs\npaper\n64,371,441\nS2ORC Citation pairs (Titles)\npaper\n52,603,982\nS2ORC (Title, Abstract)\npaper\n41,769,185\nStack Exchange (Title, Body) pairs\n-\n25,316,456\nStack Exchange (Title+Body, Answer) pairs\n-\n21,396,559\nStack Exchange (Title, Answer) pairs\n-\n21,396,559\nMS MARCO triplets\npaper\n9,144,553\nGOOAQ: Open Question Answering with Diverse Answer Types\npaper\n3,012,496\nYahoo Answers (Title, Answer)\npaper\n1,198,260\nCode Search\n-\n1,151,414\nCOCO Image captions\npaper\n828,395\nSPECTER citation triplets\npaper\n684,100\nYahoo Answers (Question, Answer)\npaper\n681,164\nYahoo Answers (Title, Question)\npaper\n659,896\nSearchQA\npaper\n582,261\nEli5\npaper\n325,475\nFlickr 30k\npaper\n317,695\nStack Exchange Duplicate questions (titles)\n304,525\nAllNLI (SNLI and MultiNLI\npaper SNLI, paper MultiNLI\n277,230\nStack Exchange Duplicate questions (bodies)\n250,519\nStack Exchange Duplicate questions (titles+bodies)\n250,460\nSentence Compression\npaper\n180,000\nWikihow\npaper\n128,542\nAltlex\npaper\n112,696\nQuora Question Triplets\n-\n103,663\nSimple Wikipedia\npaper\n102,225\nNatural Questions (NQ)\npaper\n100,231\nSQuAD2.0\npaper\n87,599\nTriviaQA\n-\n73,346\nTotal\n1,170,060,424",
    "sentence-transformers/all-distilroberta-v1": "all-distilroberta-v1\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nBackground\nIntended uses\nTraining procedure\nPre-training\nFine-tuning\nall-distilroberta-v1\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-distilroberta-v1')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-distilroberta-v1')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nBackground\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised\ncontrastive learning objective. We used the pretrained distilroberta-base model and fine-tuned in on a\n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\nWe developped this model during the\nCommunity week using JAX/Flax for NLP & CV,\norganized by Hugging Face. We developped this model as part of the project:\nTrain the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\nIntended uses\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures\nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\nBy default, input text longer than 128 word pieces is truncated.\nTraining procedure\nPre-training\nWe use the pretrained distilroberta-base. Please refer to the model card for more detailed information about the pre-training procedure.\nFine-tuning\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\nHyper parameters\nWe trained ou model on a TPU v3-8. We train the model during 920k steps using a batch size of 512 (64 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: train_script.py.\nTraining data\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the data_config.json file.\nDataset\nPaper\nNumber of training tuples\nReddit comments (2015-2018)\npaper\n726,484,430\nS2ORC Citation pairs (Abstracts)\npaper\n116,288,806\nWikiAnswers Duplicate question pairs\npaper\n77,427,422\nPAQ (Question, Answer) pairs\npaper\n64,371,441\nS2ORC Citation pairs (Titles)\npaper\n52,603,982\nS2ORC (Title, Abstract)\npaper\n41,769,185\nStack Exchange (Title, Body) pairs\n-\n25,316,456\nMS MARCO triplets\npaper\n9,144,553\nGOOAQ: Open Question Answering with Diverse Answer Types\npaper\n3,012,496\nYahoo Answers (Title, Answer)\npaper\n1,198,260\nCode Search\n-\n1,151,414\nCOCO Image captions\npaper\n828,395\nSPECTER citation triplets\npaper\n684,100\nYahoo Answers (Question, Answer)\npaper\n681,164\nYahoo Answers (Title, Question)\npaper\n659,896\nSearchQA\npaper\n582,261\nEli5\npaper\n325,475\nFlickr 30k\npaper\n317,695\nStack Exchange Duplicate questions (titles)\n304,525\nAllNLI (SNLI and MultiNLI\npaper SNLI, paper MultiNLI\n277,230\nStack Exchange Duplicate questions (bodies)\n250,519\nStack Exchange Duplicate questions (titles+bodies)\n250,460\nSentence Compression\npaper\n180,000\nWikihow\npaper\n128,542\nAltlex\npaper\n112,696\nQuora Question Triplets\n-\n103,663\nSimple Wikipedia\npaper\n102,225\nNatural Questions (NQ)\npaper\n100,231\nSQuAD2.0\npaper\n87,599\nTriviaQA\n-\n73,346\nTotal\n1,124,818,467",
    "sentence-transformers/all-roberta-large-v1": "all-roberta-large-v1\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nBackground\nIntended uses\nTraining procedure\nPre-training\nFine-tuning\nall-roberta-large-v1\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 1024 dimensional dense vector space and can be used for tasks like clustering or semantic search.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-roberta-large-v1')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nBackground\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised\ncontrastive learning objective. We used the pretrained roberta-large model and fine-tuned in on a\n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\nWe developped this model during the\nCommunity week using JAX/Flax for NLP & CV,\norganized by Hugging Face. We developped this model as part of the project:\nTrain the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\nIntended uses\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures\nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\nBy default, input text longer than 128 word pieces is truncated.\nTraining procedure\nPre-training\nWe use the pretrained roberta-large. Please refer to the model card for more detailed information about the pre-training procedure.\nFine-tuning\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\nHyper parameters\nWe trained ou model on a TPU v3-8. We train the model during 400k steps using a batch size of 256 (32 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: train_script.py.\nTraining data\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the data_config.json file.\nDataset\nPaper\nNumber of training tuples\nReddit comments (2015-2018)\npaper\n726,484,430\nS2ORC Citation pairs (Abstracts)\npaper\n116,288,806\nWikiAnswers Duplicate question pairs\npaper\n77,427,422\nPAQ (Question, Answer) pairs\npaper\n64,371,441\nS2ORC Citation pairs (Titles)\npaper\n52,603,982\nS2ORC (Title, Abstract)\npaper\n41,769,185\nStack Exchange (Title, Body) pairs\n-\n25,316,456\nMS MARCO triplets\npaper\n9,144,553\nGOOAQ: Open Question Answering with Diverse Answer Types\npaper\n3,012,496\nYahoo Answers (Title, Answer)\npaper\n1,198,260\nCode Search\n-\n1,151,414\nCOCO Image captions\npaper\n828,395\nSPECTER citation triplets\npaper\n684,100\nYahoo Answers (Question, Answer)\npaper\n681,164\nYahoo Answers (Title, Question)\npaper\n659,896\nSearchQA\npaper\n582,261\nEli5\npaper\n325,475\nFlickr 30k\npaper\n317,695\nStack Exchange Duplicate questions (titles)\n304,525\nAllNLI (SNLI and MultiNLI\npaper SNLI, paper MultiNLI\n277,230\nStack Exchange Duplicate questions (bodies)\n250,519\nStack Exchange Duplicate questions (titles+bodies)\n250,460\nSentence Compression\npaper\n180,000\nWikihow\npaper\n128,542\nAltlex\npaper\n112,696\nQuora Question Triplets\n-\n103,663\nSimple Wikipedia\npaper\n102,225\nNatural Questions (NQ)\npaper\n100,231\nSQuAD2.0\npaper\n87,599\nTriviaQA\n-\n73,346\nTotal\n1,124,818,467",
    "sentence-transformers/allenai-specter": "allenai-specter\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nFull Model Architecture\nCiting & Authors\nallenai-specter\nThis model is a conversion of the AllenAI SPECTER model to sentence-transformers. It can be used to map the titles & abstracts of scientific publications to a vector space such that similar papers are close.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('sentence-transformers/allenai-specter')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\ndef cls_pooling(model_output, attention_mask):\nreturn model_output[0][:,0]\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/allenai-specter')\nmodel = AutoModel.from_pretrained('sentence-transformers/allenai-specter')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = cls_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\nCiting & Authors\nSee  AllenAI SPECTER",
    "sentence-transformers/multi-qa-MiniLM-L6-dot-v1": "multi-qa-MiniLM-L6-dot-v1\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nTechnical Details\nBackground\nIntended uses\nTraining procedure\nPre-training\nmulti-qa-MiniLM-L6-dot-v1\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources. For an introduction to semantic search, have a look at: SBERT.net - Semantic Search\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer, util\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n#Load the model\nmodel = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-dot-v1')\n#Encode query and documents\nquery_emb = model.encode(query)\ndoc_emb = model.encode(docs)\n#Compute dot score between query and all document embeddings\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n#Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n#Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n#Output passages & scores\nfor doc, score in doc_score_pairs:\nprint(score, doc)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n#CLS Pooling - Take output from first token\ndef cls_pooling(model_output):\nreturn model_output.last_hidden_state[:,0]\n#Encode text\ndef encode(texts):\n# Tokenize sentences\nencoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input, return_dict=True)\n# Perform pooling\nembeddings = cls_pooling(model_output)\nreturn embeddings\n# Sentences we want sentence embeddings for\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-dot-v1\")\nmodel = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-dot-v1\")\n#Encode query and docs\nquery_emb = encode(query)\ndoc_emb = encode(docs)\n#Compute dot score between query and all document embeddings\nscores = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()\n#Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n#Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n#Output passages & scores\nfor doc, score in doc_score_pairs:\nprint(score, doc)\nTechnical Details\nIn the following some technical details how this model must be used:\nSetting\nValue\nDimensions\n384\nProduces normalized embeddings\nNo\nPooling-Method\nCLS pooling\nSuitable score functions\ndot-product (e.g. util.dot_score)\nBackground\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised\ncontrastive learning objective. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\nWe developped this model during the\nCommunity week using JAX/Flax for NLP & CV,\norganized by Hugging Face. We developped this model as part of the project:\nTrain the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\nIntended uses\nOur model is intented to be used for semantic search: It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages.\nNote that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text.\nTraining procedure\nThe full training script is accessible in this current repository: train_script.py.\nPre-training\nWe use the pretrained nreimers/MiniLM-L6-H384-uncased model. Please refer to the model card for more detailed information about the pre-training procedure.\nTraining\nWe use the concatenation from multiple datasets to fine-tune our model. In total we have about 215M (question, answer) pairs.\nWe sampled each dataset given a weighted probability which configuration is detailed in the data_config.json file.\nThe model was trained with MultipleNegativesRankingLoss using CLS-pooling, dot-product as similarity function, and a scale of 1.\nDataset\nNumber of training tuples\nWikiAnswers Duplicate question pairs from WikiAnswers\n77,427,422\nPAQ Automatically generated (Question, Paragraph) pairs for each paragraph in Wikipedia\n64,371,441\nStack Exchange (Title, Body) pairs from all StackExchanges\n25,316,456\nStack Exchange (Title, Answer) pairs from all StackExchanges\n21,396,559\nMS MARCO Triplets (query, answer, hard_negative) for 500k queries from Bing search engine\n17,579,773\nGOOAQ: Open Question Answering with Diverse Answer Types (query, answer) pairs for 3M Google queries and Google featured snippet\n3,012,496\nAmazon-QA (Question, Answer) pairs from Amazon product pages\n2,448,839\nYahoo Answers (Title, Answer) pairs from Yahoo Answers\n1,198,260\nYahoo Answers (Question, Answer) pairs from Yahoo Answers\n681,164\nYahoo Answers (Title, Question) pairs from Yahoo Answers\n659,896\nSearchQA (Question, Answer) pairs for 140k questions, each with Top5 Google snippets on that question\n582,261\nELI5 (Question, Answer) pairs from Reddit ELI5 (explainlikeimfive)\n325,475\nStack Exchange Duplicate questions pairs (titles)\n304,525\nQuora Question Triplets (Question, Duplicate_Question, Hard_Negative) triplets for Quora Questions Pairs dataset\n103,663\nNatural Questions (NQ) (Question, Paragraph) pairs for 100k real Google queries with relevant Wikipedia paragraph\n100,231\nSQuAD2.0 (Question, Paragraph) pairs from SQuAD2.0 dataset\n87,599\nTriviaQA (Question, Evidence) pairs\n73,346\nTotal\n214,988,242",
    "sentence-transformers/paraphrase-MiniLM-L6-v2": "sentence-transformers/paraphrase-MiniLM-L6-v2\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nFull Model Architecture\nCiting & Authors\nsentence-transformers/paraphrase-MiniLM-L6-v2\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel\n(1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\nCiting & Authors\nThis model was trained by sentence-transformers.\nIf you find this model helpful, feel free to cite our publication Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks:\n@inproceedings{reimers-2019-sentence-bert,\ntitle = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\nauthor = \"Reimers, Nils and Gurevych, Iryna\",\nbooktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\nmonth = \"11\",\nyear = \"2019\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"http://arxiv.org/abs/1908.10084\",\n}",
    "shibing624/text2vec-base-chinese": "shibing624/text2vec-base-chinese\nEvaluation\nUsage (text2vec)\nUsage (HuggingFace Transformers)\nUsage (sentence-transformers)\nModel speed up\nFull Model Architecture\nIntended uses\nTraining procedure\nPre-training\nFine-tuning\nCiting & Authors\nshibing624/text2vec-base-chinese\nThis is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese.\nIt maps sentences to a 768 dimensional dense vector space and can be used for tasks\nlike sentence embeddings, text matching or semantic search.\nEvaluation\nFor an automated evaluation of this model, see the Evaluation Benchmark: text2vec\nchinese text matching taskÔºö\nArch\nBaseModel\nModel\nATEC\nBQ\nLCQMC\nPAWSX\nSTS-B\nSOHU-dd\nSOHU-dc\nAvg\nQPS\nWord2Vec\nword2vec\nw2v-light-tencent-chinese\n20.00\n31.49\n59.46\n2.57\n55.78\n55.04\n20.70\n35.03\n23769\nSBERT\nxlm-roberta-base\nsentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n18.42\n38.52\n63.96\n10.14\n78.90\n63.01\n52.28\n46.46\n3138\nInstructor\nhfl/chinese-roberta-wwm-ext\nmoka-ai/m3e-base\n41.27\n63.81\n74.87\n12.20\n76.96\n75.83\n60.55\n57.93\n2980\nCoSENT\nhfl/chinese-macbert-base\nshibing624/text2vec-base-chinese\n31.93\n42.67\n70.16\n17.21\n79.30\n70.27\n50.42\n51.61\n3008\nCoSENT\nhfl/chinese-lert-large\nGanymedeNil/text2vec-large-chinese\n32.61\n44.59\n69.30\n14.51\n79.44\n73.01\n59.04\n53.12\n2092\nCoSENT\nnghuyong/ernie-3.0-base-zh\nshibing624/text2vec-base-chinese-sentence\n43.37\n61.43\n73.48\n38.90\n78.25\n70.60\n53.08\n59.87\n3089\nCoSENT\nnghuyong/ernie-3.0-base-zh\nshibing624/text2vec-base-chinese-paraphrase\n44.89\n63.58\n74.24\n40.90\n78.93\n76.70\n63.30\n63.08\n3066\nCoSENT\nsentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\nshibing624/text2vec-base-multilingual\n32.39\n50.33\n65.64\n32.56\n74.45\n68.88\n51.17\n53.67\n4004\nËØ¥ÊòéÔºö\nÁªìÊûúËØÑÊµãÊåáÊ†áÔºöspearmanÁ≥ªÊï∞\nshibing624/text2vec-base-chineseÊ®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫éhfl/chinese-macbert-baseÂú®‰∏≠ÊñáSTS-BÊï∞ÊçÆËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠ÊñáSTS-BÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞ËæÉÂ•ΩÊïàÊûúÔºåËøêË°åexamples/training_sup_text_matching_model.py‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠ÊñáÈÄöÁî®ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®\nshibing624/text2vec-base-chinese-sentenceÊ®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫énghuyong/ernie-3.0-base-zhÁî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑ‰∏≠ÊñáSTSÊï∞ÊçÆÈõÜshibing624/nli-zh-all/text2vec-base-chinese-sentence-datasetËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠ÊñáÂêÑNLIÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞ËæÉÂ•ΩÊïàÊûúÔºåËøêË°åexamples/training_sup_text_matching_model_jsonl_data.py‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠Êñás2s(Âè•Â≠êvsÂè•Â≠ê)ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®\nshibing624/text2vec-base-chinese-paraphraseÊ®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫énghuyong/ernie-3.0-base-zhÁî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑ‰∏≠ÊñáSTSÊï∞ÊçÆÈõÜshibing624/nli-zh-all/text2vec-base-chinese-paraphrase-datasetÔºåÊï∞ÊçÆÈõÜÁõ∏ÂØπ‰∫éshibing624/nli-zh-all/text2vec-base-chinese-sentence-datasetÂä†ÂÖ•‰∫Üs2p(sentence to paraphrase)Êï∞ÊçÆÔºåÂº∫Âåñ‰∫ÜÂÖ∂ÈïøÊñáÊú¨ÁöÑË°®ÂæÅËÉΩÂäõÔºåÂπ∂Âú®‰∏≠ÊñáÂêÑNLIÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞SOTAÔºåËøêË°åexamples/training_sup_text_matching_model_jsonl_data.py‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠Êñás2p(Âè•Â≠êvsÊÆµËêΩ)ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®\nsentence-transformers/paraphrase-multilingual-MiniLM-L12-v2Ê®°ÂûãÊòØÁî®SBERTËÆ≠ÁªÉÔºåÊòØparaphrase-MiniLM-L12-v2Ê®°ÂûãÁöÑÂ§öËØ≠Ë®ÄÁâàÊú¨ÔºåÊîØÊåÅ‰∏≠Êñá„ÄÅËã±ÊñáÁ≠â\nw2v-light-tencent-chineseÊòØËÖæËÆØËØçÂêëÈáèÁöÑWord2VecÊ®°ÂûãÔºåCPUÂä†ËΩΩ‰ΩøÁî®ÔºåÈÄÇÁî®‰∫é‰∏≠ÊñáÂ≠óÈù¢ÂåπÈÖç‰ªªÂä°ÂíåÁº∫Â∞ëÊï∞ÊçÆÁöÑÂÜ∑ÂêØÂä®ÊÉÖÂÜµ\nUsage (text2vec)\nUsing this model becomes easy when you have text2vec installed:\npip install -U text2vec\nThen you can use the model like this:\nfrom text2vec import SentenceModel\nsentences = ['Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°', 'Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°']\nmodel = SentenceModel('shibing624/text2vec-base-chinese')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout text2vec, you can use the model like this:\nFirst, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nInstall transformers:\npip install transformers\nThen load model and predict:\nfrom transformers import BertTokenizer, BertModel\nimport torch\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0]  # First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Load model from HuggingFace Hub\ntokenizer = BertTokenizer.from_pretrained('shibing624/text2vec-base-chinese')\nmodel = BertModel.from_pretrained('shibing624/text2vec-base-chinese')\nsentences = ['Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°', 'Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°']\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nUsage (sentence-transformers)\nsentence-transformers is a popular library to compute dense vector representations for sentences.\nInstall sentence-transformers:\npip install -U sentence-transformers\nThen load model and predict:\nfrom sentence_transformers import SentenceTransformer\nm = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\nsentences = ['Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°', 'Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°']\nsentence_embeddings = m.encode(sentences)\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nModel speed up\nModel\nATEC\nBQ\nLCQMC\nPAWSX\nSTSB\nshibing624/text2vec-base-chinese (fp32, baseline)\n0.31928\n0.42672\n0.70157\n0.17214\n0.79296\nshibing624/text2vec-base-chinese (onnx-O4, #29)\n0.31928\n0.42672\n0.70157\n0.17214\n0.79296\nshibing624/text2vec-base-chinese (ov, #27)\n0.31928\n0.42672\n0.70157\n0.17214\n0.79296\nshibing624/text2vec-base-chinese (ov-qint8, #30)\n0.30778 (-3.60%)\n0.43474 (+1.88%)\n0.69620 (-0.77%)\n0.16662 (-3.20%)\n0.79396 (+0.13%)\nIn short:\n‚úÖ shibing624/text2vec-base-chinese (onnx-O4), ONNX Optimized to O4 does not reduce performance, but gives a ~2x speedup on GPU.\n‚úÖ shibing624/text2vec-base-chinese (ov), OpenVINO does not reduce performance, but gives a 1.12x speedup on CPU.\nüü° shibing624/text2vec-base-chinese (ov-qint8), int8 quantization with OV incurs a small performance hit on some tasks, and a tiny performance gain on others, when quantizing with Chinese STSB. Additionally, it results in a 4.78x speedup on CPU.\nusage: shibing624/text2vec-base-chinese (onnx-O4), for gpu\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\n\"shibing624/text2vec-base-chinese\",\nbackend=\"onnx\",\nmodel_kwargs={\"file_name\": \"model_O4.onnx\"},\n)\nembeddings = model.encode([\"Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°\", \"Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°\", \"‰Ω†ÊòØË∞Å\"])\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\nusage: shibing624/text2vec-base-chinese (ov), for cpu\n# pip install 'optimum[openvino]'\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\n\"shibing624/text2vec-base-chinese\",\nbackend=\"openvino\",\n)\nembeddings = model.encode([\"Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°\", \"Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°\", \"‰Ω†ÊòØË∞Å\"])\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\nusage: shibing624/text2vec-base-chinese (ov-qint8), for cpu\n# pip install optimum\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\n\"shibing624/text2vec-base-chinese\",\nbackend=\"onnx\",\nmodel_kwargs={\"file_name\": \"model_qint8_avx512_vnni.onnx\"},\n)\nembeddings = model.encode([\"Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°\", \"Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°\", \"‰Ω†ÊòØË∞Å\"])\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\nFull Model Architecture\nCoSENT(\n(0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel\n(1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_mean_tokens': True})\n)\nIntended uses\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures\nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\nBy default, input text longer than 256 word pieces is truncated.\nTraining procedure\nPre-training\nWe use the pretrained hfl/chinese-macbert-base model.\nPlease refer to the model card for more detailed information about the pre-training procedure.\nFine-tuning\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each\npossible sentence pairs from the batch.\nWe then apply the rank loss by comparing with true pairs and false pairs.\nHyper parameters\ntraining dataset: https://huggingface.co/datasets/shibing624/nli_zh\nmax_seq_length: 128\nbest epoch: 5\nsentence embedding dim: 768\nCiting & Authors\nThis model was trained by text2vec.\nIf you find this model helpful, feel free to cite:\n@software{text2vec,\nauthor = {Xu Ming},\ntitle = {text2vec: A Tool for Text to Vector},\nyear = {2022},\nurl = {https://github.com/shibing624/text2vec},\n}",
    "siebert/sentiment-roberta-large-english": "Overview\nPredictions on a data set\nUse in a Hugging Face pipeline\nUse for further fine-tuning\nPerformance\nFine-tuning hyperparameters\nCitation and contact\nSiEBERT - English-Language Sentiment Classification\nOverview\nThis model (\"SiEBERT\", prefix for \"Sentiment in English\") is a fine-tuned checkpoint of RoBERTa-large (Liu et al. 2019). It enables reliable binary sentiment analysis for various types of English-language text. For each instance, it predicts either positive (1) or negative (0) sentiment. The model was fine-tuned and evaluated on 15 data sets from diverse text sources to enhance generalization across different types of texts (reviews, tweets, etc.). Consequently, it outperforms models trained on only one type of text (e.g., movie reviews from the popular SST-2 benchmark) when used on new data as shown below.\nPredictions on a data set\nIf you want to predict sentiment for your own data, we provide an example script via Google Colab. You can load your data to a Google Drive and run the script for free on a Colab GPU. Set-up only takes a few minutes. We suggest that you manually label a subset of your data to evaluate performance for your use case. For performance benchmark values across various sentiment analysis contexts, please refer to our paper (Hartmann et al. 2023).\nUse in a Hugging Face pipeline\nThe easiest way to use the model for single predictions is Hugging Face's sentiment analysis pipeline, which only needs a couple lines of code as shown in the following example:\nfrom transformers import pipeline\nsentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\nprint(sentiment_analysis(\"I love this!\"))\nUse for further fine-tuning\nThe model can also be used as a starting point for further fine-tuning of RoBERTa on your specific data. Please refer to Hugging Face's documentation for further details and example code.\nPerformance\nTo evaluate the performance of our general-purpose sentiment analysis model, we set aside an evaluation set from each data set, which was not used for training. On average, our model outperforms a DistilBERT-based model (which is solely fine-tuned on the popular SST-2 data set) by more than 15 percentage points (78.1 vs. 93.2 percent, see table below). As a robustness check, we evaluate the model in a leave-one-out manner (training on 14 data sets, evaluating on the one left out), which decreases model performance by only about 3 percentage points on average and underscores its generalizability. Model performance is given as evaluation set accuracy in percent.\nDataset\nDistilBERT SST-2\nThis model\nMcAuley and Leskovec (2013) (Reviews)\n84.7\n98.0\nMcAuley and Leskovec (2013) (Review Titles)\n65.5\n87.0\nYelp Academic Dataset\n84.8\n96.5\nMaas et al. (2011)\n80.6\n96.0\nKaggle\n87.2\n96.0\nPang and Lee (2005)\n89.7\n91.0\nNakov et al. (2013)\n70.1\n88.5\nShamma (2009)\n76.0\n87.0\nBlitzer et al. (2007) (Books)\n83.0\n92.5\nBlitzer et al. (2007) (DVDs)\n84.5\n92.5\nBlitzer et al. (2007) (Electronics)\n74.5\n95.0\nBlitzer et al. (2007) (Kitchen devices)\n80.0\n98.5\nPang et al. (2002)\n73.5\n95.5\nSperiosu et al. (2011)\n71.5\n85.5\nHartmann et al. (2019)\n65.5\n98.0\nAverage\n78.1\n93.2\nFine-tuning hyperparameters\nlearning_rate = 2e-5\nnum_train_epochs = 3.0\nwarmump_steps = 500\nweight_decay = 0.01\nOther values were left at their defaults as listed here.\nCitation and contact\nPlease cite this paper (Published in the IJRM) when you use our model. Feel free to reach out to christian.siebert@uni-hamburg.de with any questions or feedback you may have.\n@article{hartmann2023,\ntitle = {More than a Feeling: Accuracy and Application of Sentiment Analysis},\njournal = {International Journal of Research in Marketing},\nvolume = {40},\nnumber = {1},\npages = {75-87},\nyear = {2023},\ndoi = {https://doi.org/10.1016/j.ijresmar.2022.05.005},\nurl = {https://www.sciencedirect.com/science/article/pii/S0167811622000477},\nauthor = {Jochen Hartmann and Mark Heitmann and Christian Siebert and Christina Schamp},\n}",
    "speechbrain/emotion-recognition-wav2vec2-IEMOCAP": "Emotion Recognition with wav2vec2 base on IEMOCAP\nPipeline description\nInstall SpeechBrain\nPerform Emotion recognition\nInference on GPU\nTraining\nLimitations\nCiting SpeechBrain\nAbout SpeechBrain\nEmotion Recognition with wav2vec2 base on IEMOCAP\nThis repository provides all the necessary tools to perform emotion recognition with a fine-tuned wav2vec2 (base) model using SpeechBrain.\nIt is trained on IEMOCAP training data.\nFor a better experience, we encourage you to learn more about\nSpeechBrain. The model performance on IEMOCAP test set is:\nRelease\nAccuracy(%)\n19-10-21\n78.7 (Avg: 75.3)\nPipeline description\nThis system is composed of an wav2vec2 model. It is a combination of convolutional and residual blocks. The embeddings are extracted using attentive statistical pooling. The system is trained with Additive Margin Softmax Loss.  Speaker Verification is performed using cosine distance between speaker embeddings.\nThe system is trained with recordings sampled at 16kHz (single channel).\nThe code will automatically normalize your audio (i.e., resampling + mono channel selection) when calling classify_file if needed.\nInstall SpeechBrain\nFirst of all, please install the development version of SpeechBrain with the following command:\npip install git+https://github.com/speechbrain/speechbrain.git@develop\nPlease notice that we encourage you to read our tutorials and learn more about\nSpeechBrain.\nPerform Emotion recognition\nAn external py_module_file=custom.py is used as an external Predictor class into this HF repos. We use foreign_class function from speechbrain.pretrained.interfaces that allow you to load you custom model.\nfrom speechbrain.inference.interfaces import foreign_class\nclassifier = foreign_class(source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", pymodule_file=\"custom_interface.py\", classname=\"CustomEncoderWav2vec2Classifier\")\nout_prob, score, index, text_lab = classifier.classify_file(\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP/anger.wav\")\nprint(text_lab)\nThe prediction tensor will contain a tuple of (embedding, id_class, label_name).\nInference on GPU\nTo perform inference on the GPU, add  run_opts={\"device\":\"cuda\"}  when calling the from_hparams method.\nTraining\nThe model was trained with SpeechBrain (aa018540).\nTo train it from scratch follows these steps:\nClone SpeechBrain:\ngit clone https://github.com/speechbrain/speechbrain/\nInstall it:\ncd speechbrain\npip install -r requirements.txt\npip install -e .\nRun Training:\ncd  recipes/IEMOCAP/emotion_recognition\npython train_with_wav2vec2.py hparams/train_with_wav2vec2.yaml --data_folder=your_data_folder\nYou can find our training results (models, logs, etc) here.\nLimitations\nThe SpeechBrain team does not provide any warranty on the performance achieved by this model when used on other datasets.\nCiting SpeechBrain\nPlease, cite SpeechBrain if you use it for your research or business.\n@misc{speechbrain,\ntitle={{SpeechBrain}: A General-Purpose Speech Toolkit},\nauthor={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and Fran√ßois Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\nyear={2021},\neprint={2106.04624},\narchivePrefix={arXiv},\nprimaryClass={eess.AS},\nnote={arXiv:2106.04624}\n}\nAbout SpeechBrain\nWebsite: https://speechbrain.github.io/\nCode: https://github.com/speechbrain/speechbrain/\nHuggingFace: https://huggingface.co/speechbrain/",
    "speechbrain/spkrec-ecapa-voxceleb": "Speaker Verification with ECAPA-TDNN embeddings on Voxceleb\nPipeline description\nInstall SpeechBrain\nCompute your speaker embeddings\nPerform Speaker Verification\nInference on GPU\nTraining\nLimitations\nCiting SpeechBrain\nAbout SpeechBrain\nSpeaker Verification with ECAPA-TDNN embeddings on Voxceleb\nThis repository provides all the necessary tools to perform speaker verification with a pretrained ECAPA-TDNN model using SpeechBrain.\nThe system can be used to extract speaker embeddings as well.\nIt is trained on Voxceleb 1+ Voxceleb2 training data.\nFor a better experience, we encourage you to learn more about\nSpeechBrain. The model performance on Voxceleb1-test set(Cleaned) is:\nRelease\nEER(%)\n05-03-21\n0.80\nPipeline description\nThis system is composed of an ECAPA-TDNN model. It is a combination of convolutional and residual blocks. The embeddings are extracted using attentive statistical pooling. The system is trained with Additive Margin Softmax Loss.  Speaker Verification is performed using cosine distance between speaker embeddings.\nInstall SpeechBrain\nFirst of all, please install SpeechBrain with the following command:\npip install git+https://github.com/speechbrain/speechbrain.git@develop\nPlease notice that we encourage you to read our tutorials and learn more about\nSpeechBrain.\nCompute your speaker embeddings\nimport torchaudio\nfrom speechbrain.inference.speaker import EncoderClassifier\nclassifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\nsignal, fs =torchaudio.load('tests/samples/ASR/spk1_snt1.wav')\nembeddings = classifier.encode_batch(signal)\nThe system is trained with recordings sampled at 16kHz (single channel).\nThe code will automatically normalize your audio (i.e., resampling + mono channel selection) when calling classify_file if needed. Make sure your input tensor is compliant with the expected sampling rate if you use encode_batch and classify_batch.\nPerform Speaker Verification\nfrom speechbrain.inference.speaker import SpeakerRecognition\nverification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")\nscore, prediction = verification.verify_files(\"tests/samples/ASR/spk1_snt1.wav\", \"tests/samples/ASR/spk2_snt1.wav\") # Different Speakers\nscore, prediction = verification.verify_files(\"tests/samples/ASR/spk1_snt1.wav\", \"tests/samples/ASR/spk1_snt2.wav\") # Same Speaker\nThe prediction is 1 if the two signals in input are from the same speaker and 0 otherwise.\nInference on GPU\nTo perform inference on the GPU, add  run_opts={\"device\":\"cuda\"}  when calling the from_hparams method.\nTraining\nThe model was trained with SpeechBrain (aa018540).\nTo train it from scratch follows these steps:\nClone SpeechBrain:\ngit clone https://github.com/speechbrain/speechbrain/\nInstall it:\ncd speechbrain\npip install -r requirements.txt\npip install -e .\nRun Training:\ncd  recipes/VoxCeleb/SpeakerRec\npython train_speaker_embeddings.py hparams/train_ecapa_tdnn.yaml --data_folder=your_data_folder\nYou can find our training results (models, logs, etc) here.\nLimitations\nThe SpeechBrain team does not provide any warranty on the performance achieved by this model when used on other datasets.\nReferencing ECAPA-TDNN\n@inproceedings{DBLP:conf/interspeech/DesplanquesTD20,\nauthor    = {Brecht Desplanques and\nJenthe Thienpondt and\nKris Demuynck},\neditor    = {Helen Meng and\nBo Xu and\nThomas Fang Zheng},\ntitle     = {{ECAPA-TDNN:} Emphasized Channel Attention, Propagation and Aggregation\nin {TDNN} Based Speaker Verification},\nbooktitle = {Interspeech 2020},\npages     = {3830--3834},\npublisher = {{ISCA}},\nyear      = {2020},\n}\nCiting SpeechBrain\nPlease, cite SpeechBrain if you use it for your research or business.\n@misc{speechbrain,\ntitle={{SpeechBrain}: A General-Purpose Speech Toolkit},\nauthor={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and Fran√ßois Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\nyear={2021},\neprint={2106.04624},\narchivePrefix={arXiv},\nprimaryClass={eess.AS},\nnote={arXiv:2106.04624}\n}\nAbout SpeechBrain\nWebsite: https://speechbrain.github.io/\nCode: https://github.com/speechbrain/speechbrain/\nHuggingFace: https://huggingface.co/speechbrain/",
    "superb/wav2vec2-base-superb-ks": "Wav2Vec2-Base for Keyword Spotting\nModel description\nTask and dataset description\nUsage examples\nEval results\nBibTeX entry and citation info\nWav2Vec2-Base for Keyword Spotting\nModel description\nThis is a ported version of\nS3PRL's Wav2Vec2 for the SUPERB Keyword Spotting task.\nThe base model is wav2vec2-base, which is pretrained on 16kHz\nsampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\nFor more information refer to SUPERB: Speech processing Universal PERformance Benchmark\nTask and dataset description\nKeyword Spotting (KS) detects preregistered keywords by classifying utterances into a predefined set of\nwords. The task is usually performed on-device for the fast response time. Thus, accuracy, model size, and\ninference time are all crucial. SUPERB uses the widely used\nSpeech Commands dataset v1.0 for the task.\nThe dataset consists of ten classes of keywords, a class for silence, and an unknown class to include the\nfalse positive.\nFor the original model's training and evaluation instructions refer to the\nS3PRL downstream task README.\nUsage examples\nYou can use the model via the Audio Classification pipeline:\nfrom datasets import load_dataset\nfrom transformers import pipeline\ndataset = load_dataset(\"anton-l/superb_demo\", \"ks\", split=\"test\")\nclassifier = pipeline(\"audio-classification\", model=\"superb/wav2vec2-base-superb-ks\")\nlabels = classifier(dataset[0][\"file\"], top_k=5)\nOr use the model directly:\nimport torch\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\nfrom torchaudio.sox_effects import apply_effects_file\neffects = [[\"channels\", \"1\"], [\"rate\", \"16000\"], [\"gain\", \"-3.0\"]]\ndef map_to_array(example):\nspeech, _ = apply_effects_file(example[\"file\"], effects)\nexample[\"speech\"] = speech.squeeze(0).numpy()\nreturn example\n# load a demo dataset and read audio files\ndataset = load_dataset(\"anton-l/superb_demo\", \"ks\", split=\"test\")\ndataset = dataset.map(map_to_array)\nmodel = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-ks\")\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-ks\")\n# compute attention masks and normalize the waveform if needed\ninputs = feature_extractor(dataset[:4][\"speech\"], sampling_rate=16000, padding=True, return_tensors=\"pt\")\nlogits = model(**inputs).logits\npredicted_ids = torch.argmax(logits, dim=-1)\nlabels = [model.config.id2label[_id] for _id in predicted_ids.tolist()]\nEval results\nThe evaluation metric is accuracy.\ns3prl\ntransformers\ntest\n0.9623\n0.9643\nBibTeX entry and citation info\n@article{yang2021superb,\ntitle={SUPERB: Speech processing Universal PERformance Benchmark},\nauthor={Yang, Shu-wen and Chi, Po-Han and Chuang, Yung-Sung and Lai, Cheng-I Jeff and Lakhotia, Kushal and Lin, Yist Y and Liu, Andy T and Shi, Jiatong and Chang, Xuankai and Lin, Guan-Ting and others},\njournal={arXiv preprint arXiv:2105.01051},\nyear={2021}\n}",
    "textattack/roberta-base-CoLA": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nTextAttack Model Cardand the glue dataset loaded using the nlp library. The model was fine-tuned\nTextAttack Model Cardand the glue dataset loaded using the nlp library. The model was fine-tuned\nfor 5 epochs with a batch size of 32, a learning\nrate of 2e-05, and a maximum sequence length of 128.\nSince this was a classification task, the model was trained with a cross-entropy loss function.\nThe best score the model achieved on this task was 0.850431447746884, as measured by the\neval set accuracy, found after 1 epoch.\nFor more information, check out TextAttack on Github.",
    "uer/gpt2-chinese-cluecorpussmall": "Chinese GPT2 Models\nModel description\nHow to use\nTraining data\nTraining procedure\nBibTeX entry and citation info\nChinese GPT2 Models\nModel description\nThe set of GPT2 models, except for GPT2-xlarge model, are pre-trained by UER-py, which is introduced in this paper. The GPT2-xlarge model is pre-trained by TencentPretrain introduced in this paper, which inherits UER-py to support models with parameters above one billion, and extends it to a multimodal pre-training framework. Besides, the other models could also be pre-trained by TencentPretrain.\nThe model is used to generate Chinese texts. You can download the set of Chinese GPT2 models either from the UER-py Modelzoo page, or via HuggingFace from the links below:\nLink\nGPT2-distil\nL=6/H=768\nGPT2\nL=12/H=768\nGPT2-medium\nL=24/H=1024\nGPT2-large\nL=36/H=1280\nGPT2-xlarge\nL=48/H=1600\nNote that the 6-layer model is called GPT2-distil model because it follows the configuration of distilgpt2, and the pre-training does not involve the supervision of larger models.\nHow to use\nYou can use the model directly with a pipeline for text generation (take the case of GPT2-distil):\n>>> from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n>>> tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")\n>>> model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-distil-chinese-cluecorpussmall\")\n>>> text_generator = TextGenerationPipeline(model, tokenizer)\n>>> text_generator(\"ËøôÊòØÂæà‰πÖ‰πãÂâçÁöÑ‰∫ãÊÉÖ‰∫Ü\", max_length=100, do_sample=True)\n[{'generated_text': 'ËøôÊòØÂæà‰πÖ‰πãÂâçÁöÑ‰∫ãÊÉÖ‰∫Ü „ÄÇ Êàë Áé∞ Âú® ÊÉ≥ Ëµ∑ Êù• Â∞± ËÆ© Ëá™ Â∑± Âæà ‰º§ ÂøÉ Ôºå Âæà Â§± Êúõ „ÄÇ Êàë Áé∞ Âú® ÊÉ≥ Âà∞ Ôºå Êàë Ëßâ Âæó Â§ß Â§ö Êï∞ ‰∫∫ ÁöÑ Áîü Ê¥ª ÊØî Êàë ÁöÑ Áîü ÂëΩ Ëøò Ë¶Å Èáç Ë¶Å Ôºå ÂØπ ‰∏Ä ‰∫õ ‰∫ã ÊÉÖ ÁöÑ Áúã Ê≥ï Ôºå ÂØπ ‰∏Ä ‰∫õ ‰∫∫ ÁöÑ Áúã Ê≥ï Ôºå ÈÉΩ ÊòØ Âú® Âèë Ê≥Ñ „ÄÇ ‰ΩÜ ÊòØ Ôºå Êàë ‰ª¨ ÁöÑ Áîü Ê¥ª ÊòØ ÈúÄ Ë¶Å ‰∏Ä ‰∏™ ‰ø° Áî® ‰Ωì Á≥ª ÁöÑ „ÄÇ Êàë ‰∏ç Áü•'}]\nTraining data\nCLUECorpusSmall is used as training data.\nTraining procedure\nThe GPT2-xlarge model is pre-trained by TencentPretrain, and the others are pre-trained by UER-py on Tencent Cloud. We pre-train 1,000,000 steps with a sequence length of 128 and then pre-train 250,000 additional steps with a sequence length of 1024.\nFor the models pre-trained by UER-py, take the case of GPT2-distil\nStage1:\npython3 preprocess.py --corpus_path corpora/cluecorpussmall.txt \\\n--vocab_path models/google_zh_vocab.txt \\\n--dataset_path cluecorpussmall_lm_seq128_dataset.pt \\\n--seq_length 128 --processes_num 32 --data_processor lm\npython3 pretrain.py --dataset_path cluecorpussmall_lm_seq128_dataset.pt \\\n--vocab_path models/google_zh_vocab.txt \\\n--config_path models/gpt2/distil_config.json \\\n--output_model_path models/cluecorpussmall_gpt2_distil_seq128_model.bin \\\n--world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \\\n--total_steps 1000000 --save_checkpoint_steps 100000 --report_steps 50000 \\\n--learning_rate 1e-4 --batch_size 64\nStage2:\npython3 preprocess.py --corpus_path corpora/cluecorpussmall.txt \\\n--vocab_path models/google_zh_vocab.txt \\\n--dataset_path cluecorpussmall_lm_seq1024_dataset.pt \\\n--seq_length 1024 --processes_num 32 --data_processor lm\npython3 pretrain.py --dataset_path cluecorpussmall_lm_seq1024_dataset.pt \\\n--vocab_path models/google_zh_vocab.txt \\\n--pretrained_model_path models/cluecorpussmall_gpt2_distil_seq128_model.bin-1000000 \\\n--config_path models/gpt2/distil_config.json \\\n--output_model_path models/cluecorpussmall_gpt2_distil_seq1024_model.bin \\\n--world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \\\n--total_steps 250000 --save_checkpoint_steps 50000 --report_steps 10000 \\\n--learning_rate 5e-5 --batch_size 16\nFinally, we convert the pre-trained model into Huggingface's format:\npython3 scripts/convert_gpt2_from_uer_to_huggingface.py --input_model_path models/cluecorpussmall_gpt2_distil_seq1024_model.bin-250000 \\\n--output_model_path pytorch_model.bin \\\n--layers_num 6\nFor GPT2-xlarge model, we use TencetPretrain.\nStage1:\npython3 preprocess.py --corpus_path corpora/cluecorpussmall.txt \\\n--vocab_path models/google_zh_vocab.txt \\\n--dataset_path cluecorpussmall_lm_seq128_dataset.pt \\\n--seq_length 128 --processes_num 32 --data_processor lm\ndeepspeed pretrain.py --deepspeed --deepspeed_config models/deepspeed_config.json \\\n--dataset_path corpora/cluecorpussmall_lm_seq128_dataset.pt \\\n--vocab_path models/google_zh_vocab.txt \\\n--config_path models/gpt2/xlarge_config.json \\\n--output_model_path models/cluecorpussmall_gpt2_xlarge_seq128_model \\\n--world_size 8 --batch_size 64 \\\n--total_steps 1000000 --save_checkpoint_steps 100000 --report_steps 50000 \\\n--deepspeed_checkpoint_activations --deepspeed_checkpoint_layers_num 24\nBefore stage2, we extract fp32 consolidated weights from a zero 2 and 3 DeepSpeed checkpoints:\npython3 models/cluecorpussmall_gpt2_xlarge_seq128_model/zero_to_fp32.py models/cluecorpussmall_gpt2_xlarge_seq128_model/ \\\nmodels/cluecorpussmall_gpt2_xlarge_seq128_model.bin\nStage2:\npython3 preprocess.py --corpus_path corpora/cluecorpussmall.txt \\\n--vocab_path models/google_zh_vocab.txt \\\n--dataset_path cluecorpussmall_lm_seq1024_dataset.pt \\\n--seq_length 1024 --processes_num 32 --data_processor lm\ndeepspeed pretrain.py --deepspeed --deepspeed_config models/deepspeed_config.json \\\n--dataset_path corpora/cluecorpussmall_lm_seq1024_dataset.pt \\\n--vocab_path models/google_zh_vocab.txt \\\n--config_path models/gpt2/xlarge_config.json \\\n--pretrained_model_path models/cluecorpussmall_gpt2_xlarge_seq128_model.bin \\\n--output_model_path models/cluecorpussmall_gpt2_xlarge_seq1024_model \\\n--world_size 8 --batch_size 16 --learning_rate 5e-5 \\\n--total_steps 250000 --save_checkpoint_steps 50000 --report_steps 10000 \\\n--deepspeed_checkpoint_activations --deepspeed_checkpoint_layers_num 6\nThen, we extract fp32 consolidated weights from a zero 2 and 3 DeepSpeed checkpoints:\npython3 models/cluecorpussmall_gpt2_xlarge_seq1024_model/zero_to_fp32.py models/cluecorpussmall_gpt2_xlarge_seq1024_model/ \\\nmodels/cluecorpussmall_gpt2_xlarge_seq1024_model.bin\nFinally, we convert the pre-trained model into Huggingface's format:\npython3 scripts/convert_gpt2_from_tencentpretrain_to_huggingface.py --input_model_path models/cluecorpussmall_gpt2_xlarge_seq1024_model.bin \\\n--output_model_path pytorch_model.bin \\\n--layers_num 48\nBibTeX entry and citation info\n@article{radford2019language,\ntitle={Language Models are Unsupervised Multitask Learners},\nauthor={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\nyear={2019}\n}\n@article{zhao2019uer,\ntitle={UER: An Open-Source Toolkit for Pre-training Models},\nauthor={Zhao, Zhe and Chen, Hui and Zhang, Jinbin and Zhao, Xin and Liu, Tao and Lu, Wei and Chen, Xi and Deng, Haotang and Ju, Qi and Du, Xiaoyong},\njournal={EMNLP-IJCNLP 2019},\npages={241},\nyear={2019}\n}\n@article{zhao2023tencentpretrain,\ntitle={TencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models of Different Modalities},\nauthor={Zhao, Zhe and Li, Yudong and Hou, Cheng and Zhao, Jing and others},\njournal={ACL 2023},\npages={217},\nyear={2023}",
    "uer/roberta-base-finetuned-jd-binary-chinese": "Chinese RoBERTa-Base Models for Text Classification\nModel description\nHow to use\nTraining data\nTraining procedure\nBibTeX entry and citation info\nChinese RoBERTa-Base Models for Text Classification\nModel description\nThis is the set of 5 Chinese RoBERTa-Base classification models fine-tuned by UER-py, which is introduced in this paper. Besides, the models could also be fine-tuned by TencentPretrain introduced in this paper, which inherits UER-py to support models with parameters above one billion, and extends it to a multimodal pre-training framework.\nYou can download the 5 Chinese RoBERTa-Base classification models either from the UER-py Modelzoo page, or via HuggingFace from the links below:\nDataset\nLink\nJD full\nroberta-base-finetuned-jd-full-chinese\nJD binary\nroberta-base-finetuned-jd-binary-chinese\nDianping\nroberta-base-finetuned-dianping-chinese\nIfeng\nroberta-base-finetuned-ifeng-chinese\nChinanews\nroberta-base-finetuned-chinanews-chinese\nHow to use\nYou can use this model directly with a pipeline for text classification (take the case of roberta-base-finetuned-chinanews-chinese):\n>>> from transformers import AutoModelForSequenceClassification,AutoTokenizer,pipeline\n>>> model = AutoModelForSequenceClassification.from_pretrained('uer/roberta-base-finetuned-chinanews-chinese')\n>>> tokenizer = AutoTokenizer.from_pretrained('uer/roberta-base-finetuned-chinanews-chinese')\n>>> text_classification = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n>>> text_classification(\"Âåó‰∫¨‰∏ä‰∏™ÊúàÂè¨ÂºÄ‰∫Ü‰∏§‰ºö\")\n[{'label': 'mainland China politics', 'score': 0.7211663722991943}]\nTraining data\n5 Chinese text classification datasets are used. JD full, JD binary, and Dianping datasets consist of user reviews of different sentiment polarities. Ifeng and Chinanews consist of first paragraphs of news articles of different topic classes. They are collected by Glyph project and more details are discussed in the corresponding paper.\nTraining procedure\nModels are fine-tuned by UER-py on Tencent Cloud. We fine-tune three epochs with a sequence length of 512 on the basis of the pre-trained model chinese_roberta_L-12_H-768. At the end of each epoch, the model is saved when the best performance on development set is achieved. We use the same hyper-parameters on different models.\nTaking the case of roberta-base-finetuned-chinanews-chinese\npython3 finetune/run_classifier.py --pretrained_model_path models/cluecorpussmall_roberta_base_seq512_model.bin-250000 \\\n--vocab_path models/google_zh_vocab.txt \\\n--train_path datasets/glyph/chinanews/train.tsv \\\n--dev_path datasets/glyph/chinanews/dev.tsv \\\n--output_model_path models/chinanews_classifier_model.bin \\\n--learning_rate 3e-5 --epochs_num 3 --batch_size 32 --seq_length 512\nFinally, we convert the pre-trained model into Huggingface's format:\npython3 scripts/convert_bert_text_classification_from_uer_to_huggingface.py --input_model_path models/chinanews_classifier_model.bin \\\n--output_model_path pytorch_model.bin \\\n--layers_num 12\nBibTeX entry and citation info\n@article{liu2019roberta,\ntitle={Roberta: A robustly optimized bert pretraining approach},\nauthor={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},\njournal={arXiv preprint arXiv:1907.11692},\nyear={2019}\n}\n@article{zhang2017encoding,\ntitle={Which encoding is the best for text classification in chinese, english, japanese and korean?},\nauthor={Zhang, Xiang and LeCun, Yann},\njournal={arXiv preprint arXiv:1708.02657},\nyear={2017}\n}\n@article{zhao2019uer,\ntitle={UER: An Open-Source Toolkit for Pre-training Models},\nauthor={Zhao, Zhe and Chen, Hui and Zhang, Jinbin and Zhao, Xin and Liu, Tao and Lu, Wei and Chen, Xi and Deng, Haotang and Ju, Qi and Du, Xiaoyong},\njournal={EMNLP-IJCNLP 2019},\npages={241},\nyear={2019}\n}\n@article{zhao2023tencentpretrain,\ntitle={TencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models of Different Modalities},\nauthor={Zhao, Zhe and Li, Yudong and Hou, Cheng and Zhao, Jing and others},\njournal={ACL 2023},\npages={217},\nyear={2023}",
    "vinai/bertweet-large": "BERTweet: A pre-trained language model for English Tweets\nMain results\nBERTweet: A pre-trained language model for English Tweets\nBERTweet is the first public large-scale language model pre-trained for English Tweets. BERTweet is trained based on the RoBERTa  pre-training procedure. The corpus used to pre-train BERTweet consists of 850M English Tweets (16B word tokens ~ 80GB), containing 845M Tweets streamed from 01/2012 to 08/2019 and 5M Tweets related to the COVID-19 pandemic. The general architecture and experimental results of BERTweet can be found in our paper:\n@inproceedings{bertweet,\ntitle     = {{BERTweet: A pre-trained language model for English Tweets}},\nauthor    = {Dat Quoc Nguyen and Thanh Vu and Anh Tuan Nguyen},\nbooktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},\npages     = {9--14},\nyear      = {2020}\n}\nPlease CITE our paper when BERTweet is used to help produce published results or is incorporated into other software.\nFor further information or requests, please go to BERTweet's homepage!\nMain results",
    "yikuan8/Clinical-Longformer": "Pre-training\nUsage\nCiting\nQuestions\nClinical-Longformer is a clinical knowledge enriched version of Longformer that was further pre-trained using MIMIC-III clinical notes. It allows up to 4,096 tokens as the model input. Clinical-Longformer consistently out-performs ClinicalBERT across 10 baseline dataset for at least 2 percent. Those downstream experiments broadly cover named entity recognition (NER), question answering (QA), natural language inference (NLI) and text classification tasks. For more details, please refer to our paper. We also provide a sister model at Clinical-BigBIrd\nPre-training\nWe initialized Clinical-Longformer from the pre-trained weights of the base version of Longformer. The pre-training process was distributed in parallel to 6 32GB Tesla V100 GPUs. FP16 precision was enabled to accelerate training. We pre-trained Clinical-Longformer for 200,000 steps with batch size of 6√ó3. The learning rates were 3e-5 for both models. The entire pre-training process took more than 2 weeks.\nUsage\nLoad the model directly from Transformers:\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\ntokenizer = AutoTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"yikuan8/Clinical-Longformer\")\nCiting\nIf you find our model helps, please consider citing this :)\n@article{li2023comparative,\ntitle={A comparative study of pretrained language models for long clinical text},\nauthor={Li, Yikuan and Wehbe, Ramsey M and Ahmad, Faraz S and Wang, Hanyin and Luo, Yuan},\njournal={Journal of the American Medical Informatics Association},\nvolume={30},\nnumber={2},\npages={340--347},\nyear={2023},\npublisher={Oxford University Press}\n}\nQuestions\nPlease email yikuanli2018@u.northwestern.edu",
    "jweb/japanese-soseki-gpt2-1b": "japanese-soseki-gpt2-1b\nHow to use the model\nModel architecture\nTraining\nFinetuning\nTokenization\nLicenese\njapanese-soseki-gpt2-1b\nThis repository provides a 1.3B-parameter finetuned Japanese GPT2 model.\nThe model was finetuned by jweb based on trained by rinna Co., Ltd.\nBoth pytorch(pytorch_model.bin) and Rust(rust_model.ot) models are provided\nHow to use the model\nNOTE: Use T5Tokenizer to initiate the tokenizer.\npython\nimport torch\nfrom transformers import T5Tokenizer, AutoModelForCausalLM\ntokenizer = T5Tokenizer.from_pretrained(\"jweb/japanese-soseki-gpt2-1b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"jweb/japanese-soseki-gpt2-1b\")\nif torch.cuda.is_available():\nmodel = model.to(\"cuda\")\ntext = \"Â§èÁõÆÊº±Áü≥„ÅØ„ÄÅ\"\ntoken_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\nwith torch.no_grad():\noutput_ids = model.generate(\ntoken_ids.to(model.device),\nmax_length=128,\nmin_length=40,\ndo_sample=True,\nrepetition_penalty= 1.6,\nearly_stopping= True,\nnum_beams= 5,\ntemperature= 1.0,\ntop_k=500,\ntop_p=0.95,\npad_token_id=tokenizer.pad_token_id,\nbos_token_id=tokenizer.bos_token_id,\neos_token_id=tokenizer.eos_token_id,\n)\noutput = tokenizer.decode(output_ids.tolist()[0])\nprint(output)\n# sample output: Â§èÁõÆÊº±Áü≥„ÅØ„ÄÅÊòéÊ≤ªÊôÇ‰ª£„Çí‰ª£Ë°®„Åô„ÇãÊñáË±™„Åß„Åô„ÄÇÂ§èÁõÆÊº±Áü≥„ÅÆ‰ª£Ë°®‰Ωú„ÅØ„ÄåÂêæËº©„ÅØÁå´„Åß„ÅÇ„Çã„Äç„ÇÑ„ÄåÂùä„Å£„Å°„ÇÉ„Çì„Äç„ÄÅ„ÄåËçâÊûï„Äç„Äå‰∏âÂõõÈÉé„Äç„ÄÅ„Åù„Çå„Å´„ÄåËôûÁæé‰∫∫Ëçâ(„Åê„Å≥„Åò„Çì„Åù„ÅÜ)„Äç„Å™„Å©„Åü„Åè„Åï„Çì„ÅÇ„Çä„Åæ„Åô„ÄÇ\nrust\nuse rust_bert::gpt2::GPT2Generator;\nuse rust_bert::pipelines::common::{ModelType, TokenizerOption};\nuse rust_bert::pipelines::generation_utils::{GenerateConfig, LanguageGenerator};\nuse rust_bert::resources::{ RemoteResource,  ResourceProvider};\nuse tch::Device;\nfn main() -> anyhow::Result<()> {\nlet model_resource = Box::new(RemoteResource {\nurl: \"https://huggingface.co/jweb/japanese-soseki-gpt2-1b/resolve/main/rust_model.ot\".into(),\ncache_subdir: \"japanese-soseki-gpt2-1b/model\".into(),\n});\nlet config_resource = Box::new(RemoteResource {\nurl: \"https://huggingface.co/jweb/japanese-soseki-gpt2-1b/resolve/main/config.json\".into(),\ncache_subdir: \"japanese-soseki-gpt2-1b/config\".into(),\n});\nlet vocab_resource = Box::new(RemoteResource {\nurl: \"https://huggingface.co/jweb/japanese-soseki-gpt2-1b/resolve/main/spiece.model\".into(),\ncache_subdir: \"japanese-soseki-gpt2-1b/vocab\".into(),\n});\nlet vocab_resource_token = vocab_resource.clone();\nlet merges_resource = vocab_resource.clone();\nlet generate_config = GenerateConfig {\nmodel_resource,\nconfig_resource,\nvocab_resource,\nmerges_resource, // not used\ndevice: Device::Cpu,\nrepetition_penalty: 1.6,\nmin_length: 40,\nmax_length: 128,\ndo_sample: true,\nearly_stopping: true,\nnum_beams: 5,\ntemperature: 1.0,\ntop_k: 500,\ntop_p: 0.95,\n..Default::default()\n};\nlet tokenizer = TokenizerOption::from_file(\nModelType::T5,\nvocab_resource_token.get_local_path().unwrap().to_str().unwrap(),\nNone,\ntrue,\nNone,\nNone,\n)?;\nlet mut gpt2_model = GPT2Generator::new_with_tokenizer(generate_config, tokenizer.into())?;\ngpt2_model.set_device(Device::cuda_if_available());\nlet input_text = \"Â§èÁõÆÊº±Áü≥„ÅØ„ÄÅ\";\nlet t1 = std::time::Instant::now();\nlet output = gpt2_model.generate(Some(&[input_text]), None);\nprintln!(\"{}\", output[0].text);\nprintln!(\"Elapsed Time(ms):{}\",t1.elapsed().as_millis());\nOk(())\n}\n// sample output: Â§èÁõÆÊº±Áü≥„ÅØ„ÄÅÊòéÊ≤ª„Åã„ÇâÂ§ßÊ≠£„Å´„Åã„Åë„Å¶Ê¥ªË∫ç„Åó„ÅüÊó•Êú¨„ÅÆÂ∞èË™¨ÂÆ∂„Åß„Åô„ÄÇÂΩº„ÅØ„ÄåÂêæËº©„ÅØÁå´„Åß„ÅÇ„Çã„Äç„ÇÑ„ÄåÂùä„Å£„Å°„ÇÉ„Çì„Äç„ÄÅ„ÄåËçâÊûï„Äç„Äå‰∏âÂõõÈÉé„Äç„ÄÅ„ÅÇ„Çã„ÅÑ„ÅØ„ÄåËôûÁæé‰∫∫Ëçâ„Äç„Å™„Å©„ÅÆÂ∞èË™¨„ÅßÁü•„Çâ„Çå„Å¶„ÅÑ„Åæ„Åô„Åå„ÄÅ„ÄåÊòéÊöó„Äç„ÅÆ„Çà„ÅÜ„Å™Â∞èË™¨„ÇÇÊõ∏„ÅÑ„Å¶„ÅÑ„Åæ„Åó„Åü„ÄÇ\nModel architecture\nA 24-layer, 2048-hidden-size transformer-based language model.\nTraining\nThe model was trained on Japanese C4, Japanese CC-100 and Japanese Wikipedia to optimize a traditional language modelling objective. It reaches around 14 perplexity on a chosen validation set from the same data.\nFinetuning\nThe model was finetuned on Aozorabunko, especially Natume Soseki books.\nTokenization\nThe model uses a sentencepiece-based tokenizer. The vocabulary was first trained on a selected subset from the training data using the official sentencepiece training script, and then augmented with emojis and symbols.\nLicenese\nThe MIT license",
    "akhaliq/RealBasicVSR_x4": "No model card",
    "tae898/emoberta-base": "Emotion Recognition in Coversation (ERC)\nPrerequisites\nEmoBERTa training\nResults on the test split (weighted f1 scores)\nDeployment\nHuggingface\nFlask app\nClient\nTroubleshooting\nContributing\nCite our work\nAuthors\nLicense\nCheck https://github.com/tae898/erc for the details\nWatch a demo video!\nEmotion Recognition in Coversation (ERC)\nAt the moment, we only use the text modality to correctly classify the emotion of the utterances.The experiments were carried out on two datasets (i.e. MELD and IEMOCAP)\nPrerequisites\nAn x86-64 Unix or Unix-like machine\nPython 3.8 or higher\nRunning in a virtual environment (e.g., conda, virtualenv, etc.) is highly recommended so that you don't mess up with the system python.\nmultimodal-datasets repo (submodule)\npip install -r requirements.txt\nEmoBERTa training\nFirst configure the hyper parameters and the dataset in train-erc-text.yaml and then,\nIn this directory run the below commands. I recommend you to run this in a virtualenv.\npython train-erc-text.py\nThis will subsequently call train-erc-text-hp.py and train-erc-text-full.py.\nResults on the test split (weighted f1 scores)\nModel\nMELD\nIEMOCAP\nEmoBERTa\nNo past and future utterances\n63.46\n56.09\nOnly past utterances\n64.55\n68.57\nOnly future utterances\n64.23\n66.56\nBoth past and future utterances\n65.61\n67.42\n‚Üí without speaker names\n65.07\n64.02\nAbove numbers are the mean values of five random seed runs.\nIf you want to see more training test details, check out ./results/\nIf you want to download the trained checkpoints and stuff, then here is where you can download them. It's a pretty big zip file.\nDeployment\nHuggingface\nWe have released our models on huggingface:\nemoberta-base\nemoberta-large\nThey are based on RoBERTa-base and RoBERTa-large, respectively. They were trained on both MELD and IEMOCAP datasets. Our deployed models are neither speaker-aware nor take previous utterances into account, meaning that it only classifies one utterance at a time without the speaker information (e.g., \"I love you\").\nFlask app\nYou can either run the Flask RESTful server app as a docker container or just as a python script.\nRunning the app as a docker container (recommended).\nThere are four images. Take what you need:\ndocker run -it --rm -p 10006:10006 tae898/emoberta-base\ndocker run -it --rm -p 10006:10006 --gpus all tae898/emoberta-base-cuda\ndocker run -it --rm -p 10006:10006 tae898/emoberta-large\ndocker run -it --rm -p 10006:10006 --gpus all tae898/emoberta-large-cuda\nRunning the app in your python environment:\nThis method is less recommended than the docker one.\nRun pip install -r requirements-deploy.txt first.\nThe app.py is a flask RESTful server. The usage is below:\napp.py [-h] [--host HOST] [--port PORT] [--device DEVICE] [--model-type MODEL_TYPE]\nFor example:\npython app.py --host 0.0.0.0 --port 10006 --device cpu --model-type emoberta-base\nClient\nOnce the app is running, you can send a text to the server. First install the necessary packages: pip install -r requirements-client.txt, and the run the client.py. The usage is as below:\nclient.py [-h] [--url-emoberta URL_EMOBERTA] --text TEXT\nFor example:\npython client.py --text \"Emotion recognition is so cool\\!\"\nwill give you:\n{\n\"neutral\": 0.0049800905,\n\"joy\": 0.96399665,\n\"surprise\": 0.018937444,\n\"anger\": 0.0071516023,\n\"sadness\": 0.002021492,\n\"disgust\": 0.001495996,\n\"fear\": 0.0014167271\n}\nTroubleshooting\nThe best way to find and solve your problems is to see in the github issue tab. If you can't find what you want, feel free to raise an issue. We are pretty responsive.\nContributing\nContributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated.\nFork the Project\nCreate your Feature Branch (git checkout -b feature/AmazingFeature)\nRun make style && quality in the root repo directory, to ensure code quality.\nCommit your Changes (git commit -m 'Add some AmazingFeature')\nPush to the Branch (git push origin feature/AmazingFeature)\nOpen a Pull Request\nCite our work\nCheck out the paper.\n@misc{kim2021emoberta,\ntitle={EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa},\nauthor={Taewoon Kim and Piek Vossen},\nyear={2021},\neprint={2108.12009},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\nAuthors\nTaewoon Kim\nLicense\nMIT",
    "microsoft/resnet-152": "ResNet-152 v1.5\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nResNet-152 v1.5\nResNet model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al.\nDisclaimer: The team releasing ResNet did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections. This enables to train much deeper models.\nThis is ResNet v1.5, which differs from the original model: in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec) according to Nvidia.\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\nfrom transformers import AutoFeatureExtractor, ResNetForImageClassification\nimport torch\nfrom datasets import load_dataset\ndataset = load_dataset(\"huggingface/cats-image\")\nimage = dataset[\"test\"][\"image\"][0]\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-152\")\nmodel = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-152\")\ninputs = feature_extractor(image, return_tensors=\"pt\")\nwith torch.no_grad():\nlogits = model(**inputs).logits\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])\nFor more code examples, we refer to the documentation.\nBibTeX entry and citation info\n@inproceedings{he2016deep,\ntitle={Deep residual learning for image recognition},\nauthor={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\nbooktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\npages={770--778},\nyear={2016}\n}",
    "SJ-Ray/Re-Punctuate": "Re-Punctuate:\nRe-Punctuate is a T5 model that attempts to correct Capitalization and Punctuations in the sentences.\nDataSet:\nDialogSum dataset (115056 Records) was used to fine-tune the model for Punctuation and Capitalization correction.\nUsage:\nfrom transformers import T5Tokenizer, TFT5ForConditionalGeneration\ntokenizer = T5Tokenizer.from_pretrained('SJ-Ray/Re-Punctuate')\nmodel = TFT5ForConditionalGeneration.from_pretrained('SJ-Ray/Re-Punctuate')\ninput_text = 'the story of this brave brilliant athlete whose very being was questioned so publicly is one that still captures the imagination'\ninputs = tokenizer.encode(\"punctuate: \" + input_text, return_tensors=\"tf\")\nresult = model.generate(inputs)\ndecoded_output = tokenizer.decode(result[0], skip_special_tokens=True)\nprint(decoded_output)\nExample:\nInput:  the story of this brave brilliant athlete whose very being was questioned so publicly is one that still captures the imagination\nOutput: The story of this brave, brilliant athlete, whose very being was questioned so publicly, is one that still captures the imagination.\nConnect on: LinkedIn : Suraj Kumar",
    "PoloHuggingface/French_grammar_error_corrector": "Finetuned T5 on the french part of Lang-8 to automatically correct sentences.\nFinetuned T5 on the french part of Lang-8 to automatically correct sentences.\nSince the Lang-8 dataset contains really short sentences, the model does not generalize well with sentences larger than 10 words.\nI'll upload soon the cleaned dataset that I've used for training."
}