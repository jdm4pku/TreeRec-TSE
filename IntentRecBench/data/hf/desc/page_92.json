{
    "strangerzonehf/Flux-Super-Blend-LoRA": "Super Blend Creative Prompts\nTrigger words\nDownload model\nPrompt\nSuper Blend, a small figurine of a man in a space suit stands atop an ice cream cone, holding a flag with a red, white, and blue stripes. The man is dressed in a full suit, a helmet, and black pants, and is holding a gun in his right hand. The ice cream is covered in a thick layer of white ice cream, and the cone is adorned with a waffle-like texture. The background is a deep black, with a starry night sky in the upper right corner.\nPrompt\nSuper Blend, a vibrant orange flamingo stands atop a cityscape. The flamingos head is turned slightly to the left, with its long neck stretched out to the right. Its beak is black, with a white stripe running down the middle of its neck. The birds feet are bent towards the right, and its tail is bent slightly at the tips. The cityscape is filled with buildings and skyscrapers. The sky is a deep blue, dotted with a few wispy clouds.\nPrompt\nSuper Blend, a 3D rendering of an elephant stands against a light blue backdrop. The elephants trunk is adorned with a thick layer of brown bread, its tusks protruding from its trunk. Its legs are adorned with dark gray stripes, adding a touch of texture to the scene. Its trunk is draped over its back, revealing two white tusk protrudes from the bottom of its body. Its tusked feet are visible, adding depth to the composition.\nThe model is still in the training phase. This is not the final version and may contain artifacts and perform poorly in some cases.\nModel description { Prompt Format -> Down There ‚¨áÔ∏è }\nstrangerzonehf/Flux-Super-Blend-LoRA\nImage Processing Parameters\nParameter\nValue\nParameter\nValue\nLR Scheduler\nconstant\nNoise Offset\n0.03\nOptimizer\nAdamW\nMultires Noise Discount\n0.1\nNetwork Dim\n64\nMultires Noise Iterations\n10\nNetwork Alpha\n32\nRepeat & Steps\n28 & 3100\nEpoch\n20\nSave Every N Epochs\n1\nLabeling: florence2-en(natural language & English)\nTotal Images Used for Training : 30\nBest Dimensions\n768 x 1024 (Best)\n1024 x 1024 (Default)\nSetting Up\nimport torch\nfrom pipelines import DiffusionPipeline\nbase_model = \"black-forest-labs/FLUX.1-dev\"\npipe = DiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.bfloat16)\nlora_repo = \"strangerzonehf/Flux-Super-Blend-LoRA\"\ntrigger_word = \"Super Blend\"\npipe.load_lora_weights(lora_repo)\ndevice = torch.device(\"cuda\")\npipe.to(device)\nSuper Blend Creative Prompts\nPrompt Name\nDescription\nSuper Blend: Mountaintop Smoothie\nA miniature superhero figurine stands atop a smoothie cup overflowing with layers of bright pink and yellow fruit puree. The superhero wears a cape billowing in the wind, a silver mask, and a suit with an emblem of a strawberry. They hold a spoon-like staff with glowing edges. The smoothie cup is surrounded by tiny, colorful fruits, and the background is a sunrise over a tropical paradise.\nSuper Blend: Coffee Cosmos\nA tiny adventurer figurine balances on the rim of a steaming coffee cup, holding a futuristic coffee stirrer like a spear. The adventurer wears a sleek black suit with a glowing blue chest emblem, a helmet with a visor, and boots that leave golden trails. The coffee swirls like a galaxy, and the background shows a mysterious cityscape under a night sky with shimmering stars.\nSuper Blend: Cereal Quest\nA small knight figurine stands heroically on a floating island made of cereal pieces in a bowl of milk. The knight wears shimmering golden armor, wields a sword shaped like a spoon, and has a shield with a cereal logo. The bowl is placed on a vibrant table surrounded by floating fruits, and the background features a glowing portal in the sky.\nSuper Blend: Juice Jetpack Adventure\nA figurine of a pilot in a retro space suit with a juice-filled jetpack stands mid-air over a glass of layered tropical juice. The pilot holds a straw-like laser weapon, and their boots emit a citrus-colored glow. The juice glass is adorned with a slice of lime and a tiny umbrella. In the background, there are floating citrus planets in a bright, cosmic sky.\nSuper Blend: Sundae Crusader\nA figurine of a superhero with a cherry-shaped helmet and a chocolate cape stands atop a mountain of ice cream scoops in a sundae glass. The superhero holds a waffle cone lance and has a glowing vanilla swirl emblem on their chest. The sundae is topped with whipped cream, sprinkles, and a cherry, with a cosmic skyline featuring candy-like stars.\nSuper Blend: Milkshake Maverick\nA figurine of a cowboy with a futuristic twist rides a mechanical straw in a giant milkshake glass. The cowboy wears a silver hat, reflective boots, and a leather jacket with glowing buttons. The milkshake is topped with whipped cream, chocolate drizzle, and a cookie star. The background is a desert-like alien planet with floating ice cream cones.\nSuper Blend: Cosmic Candy Conqueror\nA figurine of an explorer in a bright purple suit stands atop a swirling lollipop in a glass of layered candy-infused milk. The explorer carries a staff with a glowing candy tip, and the lollipop spins like a tiny planet. The glass is surrounded by smaller candy orbs, and the background is a colorful nebula.\nTrigger words\nYou should use Super Blend to trigger the image generation.\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.",
    "HuggingFaceTB/SmolVLM-Instruct": "SmolVLM\nModel Summary\nResources\nUses\nTechnical Summary\nHow to get started\nModel optimizations\nMisuse and Out-of-scope Use\nLicense\nTraining Details\nTraining Data\nEvaluation\nCitation information\nSmolVLM\nSmolVLM is a compact open multimodal model that accepts arbitrary sequences of image and text inputs to produce text outputs. Designed for efficiency, SmolVLM can answer questions about images, describe visual content, create stories grounded on multiple images, or function as a pure language model without visual inputs. Its lightweight architecture makes it suitable for on-device applications while maintaining strong performance on multimodal tasks.\nModel Summary\nDeveloped by: Hugging Face ü§ó\nModel type: Multi-modal model (image+text)\nLanguage(s) (NLP): English\nLicense: Apache 2.0\nArchitecture: Based on Idefics3 (see technical summary)\nResources\nDemo: SmolVLM Demo\nBlog: Blog post\nUses\nSmolVLM can be used for inference on multimodal (image + text) tasks where the input comprises text queries along with one or more images. Text and images can be interleaved arbitrarily, enabling tasks like image captioning, visual question answering, and storytelling based on visual content. The model does not support image generation.\nTo fine-tune SmolVLM on a specific task, you can follow the fine-tuning tutorial.\nTechnical Summary\nSmolVLM leverages the lightweight SmolLM2 language model to provide a compact yet powerful multimodal experience. It introduces several changes compared to previous Idefics models:\nImage compression: We introduce a more radical image compression compared to Idefics3 to enable the model to infer faster and use less RAM.\nVisual Token Encoding: SmolVLM uses 81 visual tokens to encode image patches of size 384√ó384. Larger images are divided into patches, each encoded separately, enhancing efficiency without compromising performance.\nMore details about the training and architecture are available in our technical report.\nHow to get started\nYou can use transformers to load, infer and fine-tune SmolVLM.\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom transformers.image_utils import load_image\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Load images\nimage1 = load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\")\nimage2 = load_image(\"https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg\")\n# Initialize processor and model\nprocessor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\nmodel = AutoModelForVision2Seq.from_pretrained(\n\"HuggingFaceTB/SmolVLM-Instruct\",\ntorch_dtype=torch.bfloat16,\n_attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else \"eager\",\n).to(DEVICE)\n# Create input messages\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"Can you describe the two images?\"}\n]\n},\n]\n# Prepare inputs\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image1, image2], return_tensors=\"pt\")\ninputs = inputs.to(DEVICE)\n# Generate outputs\ngenerated_ids = model.generate(**inputs, max_new_tokens=500)\ngenerated_texts = processor.batch_decode(\ngenerated_ids,\nskip_special_tokens=True,\n)\nprint(generated_texts[0])\n\"\"\"\nAssistant: The first image shows a green statue of the Statue of Liberty standing on a stone pedestal in front of a body of water.\nThe statue is holding a torch in its right hand and a tablet in its left hand. The water is calm and there are no boats or other objects visible.\nThe sky is clear and there are no clouds. The second image shows a bee on a pink flower.\nThe bee is black and yellow and is collecting pollen from the flower. The flower is surrounded by green leaves.\n\"\"\"\nModel optimizations\nPrecision: For better performance, load and run the model in half-precision (torch.float16 or torch.bfloat16) if your hardware supports it.\nfrom transformers import AutoModelForVision2Seq\nimport torch\nmodel = AutoModelForVision2Seq.from_pretrained(\n\"HuggingFaceTB/SmolVLM-Instruct\",\ntorch_dtype=torch.bfloat16\n).to(\"cuda\")\nYou can also load SmolVLM with 4/8-bit quantization using bitsandbytes, torchao or Quanto. Refer to this page for other options.\nfrom transformers import AutoModelForVision2Seq, BitsAndBytesConfig\nimport torch\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = AutoModelForVision2Seq.from_pretrained(\n\"HuggingFaceTB/SmolVLM-Instruct\",\nquantization_config=quantization_config,\n)\nVision Encoder Efficiency: Adjust the image resolution by setting size={\"longest_edge\": N*384} when initializing the processor, where N is your desired value. The default N=4 works well, which results in input images of\nsize 1536√ó1536. For documents, N=5 might be beneficial. Decreasing N can save GPU memory and is appropriate for lower-resolution images. This is also useful if you want to fine-tune on videos.\nMisuse and Out-of-scope Use\nSmolVLM is not intended for high-stakes scenarios or critical decision-making processes that affect an individual's well-being or livelihood. The model may produce content that appears factual but may not be accurate. Misuse includes, but is not limited to:\nProhibited Uses:\nEvaluating or scoring individuals (e.g., in employment, education, credit)\nCritical automated decision-making\nGenerating unreliable factual content\nMalicious Activities:\nSpam generation\nDisinformation campaigns\nHarassment or abuse\nUnauthorized surveillance\nLicense\nSmolVLM is built upon the shape-optimized SigLIP as image encoder and SmolLM2 for text decoder part.\nWe release the SmolVLM checkpoints under the Apache 2.0 license.\nTraining Details\nTraining Data\nThe training data comes from The Cauldron and Docmatix datasets, with emphasis on document understanding (25%) and image captioning (18%), while maintaining balanced coverage across other crucial capabilities like visual reasoning, chart comprehension, and general instruction following.\nEvaluation\nModel\nMMMU (val)\nMathVista (testmini)\nMMStar (val)\nDocVQA (test)\nTextVQA (val)\nMin GPU RAM required (GB)\nSmolVLM\n38.8\n44.6\n42.1\n81.6\n72.7\n5.02\nQwen-VL 2B\n41.1\n47.8\n47.5\n90.1\n79.7\n13.70\nInternVL2 2B\n34.3\n46.3\n49.8\n86.9\n73.4\n10.52\nPaliGemma 3B 448px\n34.9\n28.7\n48.3\n32.2\n56.0\n6.72\nmoondream2\n32.4\n24.3\n40.3\n70.5\n65.2\n3.87\nMiniCPM-V-2\n38.2\n39.8\n39.1\n71.9\n74.1\n7.88\nMM1.5 1B\n35.8\n37.2\n0.0\n81.0\n72.5\nNaN\nCitation information\nYou can cite us in the following way:\n@article{marafioti2025smolvlm,\ntitle={SmolVLM: Redefining small and efficient multimodal models},\nauthor={Andr√©s Marafioti and Orr Zohar and Miquel Farr√© and Merve Noyan and Elie Bakouch and Pedro Cuenca and Cyril Zakka and Loubna Ben Allal and Anton Lozhkov and Nouamane Tazi and Vaibhav Srivastav and Joshua Lochner and Hugo Larcher and Mathieu Morlon and Lewis Tunstall and Leandro von Werra and Thomas Wolf},\njournal={arXiv preprint arXiv:2504.05299},\nyear={2025}\n}",
    "allenai/Llama-3.1-Tulu-3-8B-SFT": "Llama-3.1-Tulu-3-8B-SFT\nModel description\nModel Sources\nModel Family\nUsing the model\nLoading with HuggingFace\nVLLM\nChat template\nSystem prompt\nBias, Risks, and Limitations\nPerformance\nHyperparamters\nLicense and use\nCitation\nLlama-3.1-Tulu-3-8B-SFT\nT√ºlu3 is a leading instruction following model family, offering fully open-source data, code, and recipes designed to serve as a comprehensive guide for modern post-training techniques.\nT√ºlu3 is designed for state-of-the-art performance on a diversity of tasks in addition to chat, such as MATH, GSM8K, and IFEval.\nModel description\nModel type: A model trained on a mix of publicly available, synthetic and human-created datasets.\nLanguage(s) (NLP): Primarily English\nLicense: Llama 3.1 Community License Agreement\nFinetuned from model: meta-llama/Llama-3.1-8B\nModel Sources\nTraining Repository: https://github.com/allenai/open-instruct\nEval Repository: https://github.com/allenai/olmes\nPaper: https://arxiv.org/abs/2411.15124\nDemo: https://playground.allenai.org/\nModel Family\nStage\nLlama 3.1 8B\nLlama 3.1 70B\nBase Model\nmeta-llama/Llama-3.1-8B\nmeta-llama/Llama-3.1-70B\nSFT\nallenai/Llama-3.1-Tulu-3-8B-SFT\nallenai/Llama-3.1-Tulu-3-70B-SFT\nDPO\nallenai/Llama-3.1-Tulu-3-8B-DPO\nallenai/Llama-3.1-Tulu-3-70B-DPO\nFinal Models (RLVR)\nallenai/Llama-3.1-Tulu-3-8B\nallenai/Llama-3.1-Tulu-3-70B\nReward Model (RM)\nallenai/Llama-3.1-Tulu-3-8B-RM\n(Same as 8B)\nStage\nLlama 3.1 405B\nBase Model\nmeta-llama/llama-3.1-405B\nSFT\nallenai/llama-3.1-Tulu-3-405B-SFT\nDPO\nallenai/llama-3.1-Tulu-3-405B-DPO\nFinal Model (RLVR)\nallenai/llama-3.1-Tulu-3-405B\nReward Model (RM)\n(Same as 8B)\nUsing the model\nLoading with HuggingFace\nTo load the model with HuggingFace, use the following snippet:\nfrom transformers import AutoModelForCausalLM\ntulu_model = AutoModelForCausalLM.from_pretrained(\"allenai/Llama-3.1-Tulu-3-8B-SFT\")\nVLLM\nAs a Llama base model, the model can be easily served with:\nvllm serve allenai/Llama-3.1-Tulu-3-8B-SFT\nNote that given the long chat template of Llama, you may want to use --max_model_len=8192.\nChat template\nThe chat template for our models is formatted as:\n<|user|>\\nHow are you doing?\\n<|assistant|>\\nI'm just a computer program, so I don't have feelings, but I'm functioning as expected. How can I assist you today?<|endoftext|>\nOr with new lines expanded:\n<|user|>\nHow are you doing?\n<|assistant|>\nI'm just a computer program, so I don't have feelings, but I'm functioning as expected. How can I assist you today?<|endoftext|>\nIt is embedded within the tokenizer as well, for tokenizer.apply_chat_template.\nSystem prompt\nIn Ai2 demos, we use this system prompt by default:\nYou are Tulu 3, a helpful and harmless AI Assistant built by the Allen Institute for AI.\nThe model has not been trained with a specific system prompt in mind.\nBias, Risks, and Limitations\nThe T√ºlu3 models have limited safety training, but are not deployed automatically with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so).\nIt is also unknown what the size and composition of the corpus was used to train the base Llama 3.1 models, however it is likely to have included a mix of Web data and technical sources like books and code.\nSee the Falcon 180B model card for an example of this.\nPerformance\nBenchmark (eval)\nT√ºlu 3 SFT 8B\nT√ºlu 3 DPO 8B\nT√ºlu 3 8B\nLlama 3.1 8B Instruct\nQwen 2.5 7B Instruct\nMagpie 8B\nGemma 2 9B Instruct\nMinistral 8B Instruct\nAvg.\n60.4\n64.4\n64.8\n62.2\n57.8\n44.7\n55.2\n58.3\nMMLU (0 shot, CoT)\n65.9\n68.7\n68.2\n71.2\n76.6\n62.0\n74.6\n68.5\nPopQA (15 shot)\n29.3\n29.3\n29.1\n20.2\n18.1\n22.5\n28.3\n20.2\nTruthfulQA (6 shot)\n46.8\n56.1\n55.0\n55.1\n63.1\n57.0\n61.4\n55.5\nBigBenchHard (3 shot, CoT)\n67.9\n65.8\n66.0\n62.8\n21.7\n0.9\n2.5\n56.2\nDROP (3 shot)\n61.3\n62.5\n62.6\n61.5\n54.4\n49.4\n58.8\n56.2\nMATH (4 shot CoT, Flex)\n31.5\n42.0\n43.7\n42.5\n14.8\n5.1\n29.8\n40.0\nGSM8K (8 shot, CoT)\n76.2\n84.3\n87.6\n83.4\n83.8\n61.2\n79.7\n80.0\nHumanEval (pass@10)\n86.2\n83.9\n83.9\n86.3\n93.1\n75.4\n71.7\n91.0\nHumanEval+ (pass@10)\n81.4\n78.6\n79.2\n82.9\n89.7\n69.1\n67.0\n88.5\nIFEval (prompt loose)\n72.8\n81.1\n82.4\n80.6\n74.7\n38.8\n69.9\n56.4\nAlpacaEval 2 (LC % win)\n12.4\n33.5\n34.5\n24.2\n29.0\n49.0\n43.7\n31.4\nSafety (6 task avg.)\n93.1\n87.2\n85.5\n75.2\n75.0\n46.4\n75.5\n56.2\nBenchmark (eval)\nT√ºlu 3 70B SFT\nT√ºlu 3 DPO 70B\nT√ºlu 3 70B\nLlama 3.1 70B Instruct\nQwen 2.5 72B Instruct\nHermes 3 Llama 3.1 70B\nNemotron Llama 3.1 70B\nAvg.\n72.6\n75.9\n76.0\n73.4\n71.5\n68.3\n65.5\nMMLU (0 shot, CoT)\n78.9\n83.3\n83.1\n85.3\n85.5\n80.4\n83.8\nPopQA (15 shot)\n48.6\n46.3\n46.5\n46.4\n30.6\n48.1\n36.4\nTruthfulQA (6 shot)\n55.7\n67.9\n67.6\n66.8\n69.9\n66.5\n62.6\nBigBenchHard (3 shot, CoT)\n82.7\n81.8\n82.0\n73.8\n67.2\n82.1\n0.7\nDROP (3 shot)\n77.2\n74.1\n74.3\n77.0\n34.2\n73.2\n68.8\nMATH (4 shot CoT, Flex)\n53.7\n62.3\n63.0\n56.4\n74.3\n41.9\n55.0\nGSM8K (8 shot, CoT)\n91.1\n93.5\n93.5\n93.7\n89.5\n90.0\n84.7\nHumanEval (pass@10)\n92.9\n92.4\n92.4\n93.6\n94.0\n89.6\n94.1\nHumanEval+ (pass@10)\n87.3\n88.4\n88.0\n89.5\n90.8\n85.9\n85.5\nIFEval (prompt loose)\n82.1\n82.6\n83.2\n88.0\n87.6\n76.0\n79.9\nAlpacaEval 2 (LC % win)\n26.3\n49.6\n49.8\n33.4\n47.7\n28.4\n66.1\nSafety (6 task avg.)\n94.4\n89.0\n88.3\n76.5\n87.0\n57.9\n69.0\nBenchmark (eval)\nT√ºlu 3 405B SFT\nT√ºlu 3 405B DPO\nT√ºlu 3 405B\nLlama 3.1 405B Instruct\nNous Hermes 3 405B\nDeepseek V3\nGPT 4o (11-24)\nAvg w/o Safety\n76.3\n79.0\n80.0\n78.1\n74.4\n79.0\n80.5\nAvg w/ Safety\n77.5\n79.6\n80.7\n79.0\n73.5\n75.9\n81.6\nMMLU (5 shot, CoT)\n84.4\n86.6\n87.0\n88.0\n84.9\n82.1\n87.9\nPopQA (3 shot)\n55.7\n55.4\n55.5\n52.9\n54.2\n44.9\n53.6\nBigBenchHard (0 shot, CoT)\n88.0\n88.8\n88.6\n87.1\n87.7\n89.5\n83.3\nMATH (4 shot, Flex)\n63.4\n59.9\n67.3\n66.6\n58.4\n72.5\n68.8\nGSM8K (8 shot, CoT)\n93.6\n94.2\n95.5\n95.4\n92.7\n94.1\n91.7\nHumanEval (pass@10)\n95.7\n97.2\n95.9\n95.9\n92.3\n94.6\n97.0\nHumanEval+ (pass@10)\n93.3\n93.9\n92.9\n90.3\n86.9\n91.6\n92.7\nIFEval (prompt loose)\n82.4\n85.0\n86.0\n88.4\n81.9\n88.0\n84.8\nAlpacaEval 2 (LC % win)\n30.4\n49.8\n51.4\n38.5\n30.2\n53.5\n65.0\nSafety (6 task avg.)\n87.7\n85.5\n86.7\n86.8\n65.8\n72.2\n90.9\nHyperparamters\nSFT:\nLearning Rate: 5E-6 (8B), 2E-6 (70B, 405B)\nEffective Batch Size: 128 (8B, 70B), 256 (405B)\nMax. Sequence Length: 4096\nLoss Accumulation: Sum (see https://unsloth.ai/blog/gradient)\nLearning Rate Schedule: Linear\nLR Warmup Ratio: 0.03\nNum. Epochs: 2\nLicense and use\nAll Llama 3.1 T√ºlu3 models are released under Meta's Llama 3.1 Community License Agreement.\nLlama 3.1 is licensed under the Llama 3.1 Community License, Copyright ¬© Meta Platforms, Inc.\nT√ºlu3 is intended for research and educational use.\nFor more information, please see our Responsible Use Guidelines.\nCitation\nIf T√ºlu3 or any of the related materials were helpful to your work, please cite:\n@article{lambert2024tulu3,\ntitle = {T√ºlu 3: Pushing Frontiers in Open Language Model Post-Training},\nauthor = {\nNathan Lambert and\nJacob Morrison and\nValentina Pyatkin and\nShengyi Huang and\nHamish Ivison and\nFaeze Brahman and\nLester James V. Miranda and\nAlisa Liu and\nNouha Dziri and\nShane Lyu and\nYuling Gu and\nSaumya Malik and\nVictoria Graf and\nJena D. Hwang and\nJiangjiang Yang and\nRonan Le Bras and\nOyvind Tafjord and\nChris Wilhelm and\nLuca Soldaini and\nNoah A. Smith and\nYizhong Wang and\nPradeep Dasigi and\nHannaneh Hajishirzi\n},\nyear = {2024},\nemail = {tulu@allenai.org}\n}",
    "MaziyarPanahi/calme-3.2-instruct-78b": "MaziyarPanahi/calme-3.2-instruct-78b\n‚ö° Quantized GGUF\n‚ö° Quantized EXL2\nüèÜ Open LLM Leaderboard Evaluation Results\nPrompt Template\nHow to use\nEthical Considerations\nThis is an experimental model, so it might not perform well for some prompts and may be sensitive to hyper parameters. I would appreciate any feedback to see if I can fix any issues in the next iteration. ‚ù§Ô∏è\nMaziyarPanahi/calme-3.2-instruct-78b\nThis model is an advanced iteration of the powerful Qwen/Qwen2.5-72B, specifically fine-tuned to enhance its capabilities in generic domains. The Qwen2.5-72B base model was merged with itself to create a larger model. After that, the model was fine-tuned on a custom datasets.\n‚ö° Quantized GGUF\nHere are the GGUF models thanks to bartowski: calme-3.2-instruct-78b-GGUF\n‚ö° Quantized EXL2\nHere is the EXL2 4.5 bits per weight (bpw) model thanks to DavidCatalano: DavidCatalano/calme-3.2-instruct-78b-exl2\nDavidCatalano/calme-3.2-instruct-78b-exl2-4.5bpw.\nüèÜ Open LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n52.02\nIFEval (0-Shot)\n80.63\nBBH (3-Shot)\n62.61\nMATH Lvl 5 (4-Shot)\n39.95\nGPQA (0-shot)\n20.36\nMuSR (0-shot)\n38.53\nMMLU-PRO (5-shot)\n70.03\nPrompt Template\nThis model uses ChatML prompt template:\n<|im_start|>system\n{System}\n<|im_end|>\n<|im_start|>user\n{User}\n<|im_end|>\n<|im_start|>assistant\n{Assistant}\nHow to use\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\nmessages = [\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\npipe = pipeline(\"text-generation\", model=\"MaziyarPanahi/calme-3.2-instruct-78b\")\npipe(messages)\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"MaziyarPanahi/calme-3.2-instruct-78b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"MaziyarPanahi/calme-3.2-instruct-78b\")\nEthical Considerations\nAs with any large language model, users should be aware of potential biases and limitations. We recommend implementing appropriate safeguards and human oversight when deploying this model in production environments.",
    "Xkev/Llama-3.2V-11B-cot": "Model Card for Model ID\nModel Details\nBenchmark Results\nReproduction\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nBias, Risks, and Limitations\nModel Card for Model ID\nLlama-3.2V-11B-cot is the first version of LLaVA-CoT, which is a visual language model capable of spontaneous, systematic reasoning.\nThe model was proposed in LLaVA-CoT: Let Vision Language Models Reason Step-by-Step.\nModel Details\nLicense: apache-2.0\nFinetuned from model: meta-llama/Llama-3.2-11B-Vision-Instruct\nBenchmark Results\nMMStar\nMMBench\nMMVet\nMathVista\nAI2D\nHallusion\nAverage\n57.6\n75.0\n60.3\n54.8\n85.7\n47.8\n63.5\nReproduction\nTo reproduce our results, you should use VLMEvalKit and the following settings.\nParameter\nValue\ndo_sample\nTrue\ntemperature\n0.6\ntop_p\n0.9\nmax_new_tokens\n2048\nYou may change them in this file, line 80-83, and modify the max_new_tokens throughout the file.\nNote: We follow the same settings as Llama-3.2-11B-Vision-Instruct, except that we extend the max_new_tokens to 2048.\nAfter you get the results, you should filter the model output and only keep the outputs between <CONCLUSION> and </CONCLUSION>.\nThis shouldn't have any difference in theory, but empirically we observe some performance difference because the jugder GPT-4o can be inaccurate sometimes.\nBy keeping the outputs between <CONCLUSION> and </CONCLUSION>, most answers can be direclty extracted using VLMEvalKit system, which can be much less biased.\nHow to Get Started with the Model\nYou can use the inference code for Llama-3.2-11B-Vision-Instruct.\nTraining Details\nTraining Data\nThe model is trained on the LLaVA-CoT-100k dataset.\nTraining Procedure\nThe model is finetuned on llama-recipes with the following settings.\nUsing the same setting should accurately reproduce our results.\nParameter\nValue\nFSDP\nenabled\nlr\n1e-5\nnum_epochs\n3\nbatch_size_training\n4\nuse_fast_kernels\nTrue\nrun_validation\nFalse\nbatching_strategy\npadding\ncontext_length\n4096\ngradient_accumulation_steps\n1\ngradient_clipping\nFalse\ngradient_clipping_threshold\n1.0\nweight_decay\n0.0\ngamma\n0.85\nseed\n42\nuse_fp16\nFalse\nmixed_precision\nTrue\nBias, Risks, and Limitations\nThe model may generate biased or offensive content, similar to other VLMs, due to limitations in the training data.\nTechnically, the model's performance in aspects like instruction following still falls short of leading industry models.",
    "allenai/Llama-3.1-Tulu-3-8B": "A newer version of this model is available:\nallenai/Llama-3.1-Tulu-3.1-8B\nLlama-3.1-Tulu-3-8B\nModel description\nModel Sources\nModel Family\nUsing the model\nLoading with HuggingFace\nVLLM\nChat template\nSystem prompt\nBias, Risks, and Limitations\nPerformance\nHyperparamters\nLicense and use\nCitation\nOpen LLM Leaderboard Evaluation Results\nLlama-3.1-Tulu-3-8B\nT√ºlu 3 is a leading instruction following model family, offering a post-training package with fully open-source data, code, and recipes designed to serve as a comprehensive guide for modern techniques.\nThis is one step of a bigger process to training fully open-source models, like our OLMo models.\nT√ºlu 3 is designed for state-of-the-art performance on a diversity of tasks in addition to chat, such as MATH, GSM8K, and IFEval.\nModel description\nModel type: A model trained on a mix of publicly available, synthetic and human-created datasets.\nLanguage(s) (NLP): Primarily English\nLicense: Llama 3.1 Community License Agreement\nFinetuned from model: allenai/Llama-3.1-Tulu-3-8B-DPO\nModel Sources\nTraining Repository: https://github.com/allenai/open-instruct\nEval Repository: https://github.com/allenai/olmes\nPaper: https://arxiv.org/abs/2411.15124\nDemo: https://playground.allenai.org/\nModel Family\nStage\nLlama 3.1 8B\nLlama 3.1 70B\nBase Model\nmeta-llama/Llama-3.1-8B\nmeta-llama/Llama-3.1-70B\nSFT\nallenai/Llama-3.1-Tulu-3-8B-SFT\nallenai/Llama-3.1-Tulu-3-70B-SFT\nDPO\nallenai/Llama-3.1-Tulu-3-8B-DPO\nallenai/Llama-3.1-Tulu-3-70B-DPO\nFinal Models (RLVR)\nallenai/Llama-3.1-Tulu-3-8B\nallenai/Llama-3.1-Tulu-3-70B\nReward Model (RM)\nallenai/Llama-3.1-Tulu-3-8B-RM\n(Same as 8B)\nStage\nLlama 3.1 405B\nBase Model\nmeta-llama/llama-3.1-405B\nSFT\nallenai/llama-3.1-Tulu-3-405B-SFT\nDPO\nallenai/llama-3.1-Tulu-3-405B-DPO\nFinal Model (RLVR)\nallenai/llama-3.1-Tulu-3-405B\nReward Model (RM)\n(Same as 8B)\nUsing the model\nLoading with HuggingFace\nTo load the model with HuggingFace, use the following snippet:\nfrom transformers import AutoModelForCausalLM\ntulu_model = AutoModelForCausalLM.from_pretrained(\"allenai/Llama-3.1-Tulu-3-8B\")\nVLLM\nAs a Llama base model, the model can be easily served with:\nvllm serve allenai/Llama-3.1-Tulu-3-8B\nNote that given the long chat template of Llama, you may want to use --max_model_len=8192.\nChat template\nThe chat template for our models is formatted as:\n<|user|>\\nHow are you doing?\\n<|assistant|>\\nI'm just a computer program, so I don't have feelings, but I'm functioning as expected. How can I assist you today?<|endoftext|>\nOr with new lines expanded:\n<|user|>\nHow are you doing?\n<|assistant|>\nI'm just a computer program, so I don't have feelings, but I'm functioning as expected. How can I assist you today?<|endoftext|>\nIt is embedded within the tokenizer as well, for tokenizer.apply_chat_template.\nSystem prompt\nIn Ai2 demos, we use this system prompt by default:\nYou are Tulu 3, a helpful and harmless AI Assistant built by the Allen Institute for AI.\nThe model has not been trained with a specific system prompt in mind.\nBias, Risks, and Limitations\nThe T√ºlu3 models have limited safety training, but are not deployed automatically with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so).\nIt is also unknown what the size and composition of the corpus was used to train the base Llama 3.1 models, however it is likely to have included a mix of Web data and technical sources like books and code.\nSee the Falcon 180B model card for an example of this.\nPerformance\nBenchmark (eval)\nT√ºlu 3 SFT 8B\nT√ºlu 3 DPO 8B\nT√ºlu 3 8B\nLlama 3.1 8B Instruct\nQwen 2.5 7B Instruct\nMagpie 8B\nGemma 2 9B Instruct\nMinistral 8B Instruct\nAvg.\n60.4\n64.4\n64.8\n62.2\n57.8\n44.7\n55.2\n58.3\nMMLU (0 shot, CoT)\n65.9\n68.7\n68.2\n71.2\n76.6\n62.0\n74.6\n68.5\nPopQA (15 shot)\n29.3\n29.3\n29.1\n20.2\n18.1\n22.5\n28.3\n20.2\nTruthfulQA (6 shot)\n46.8\n56.1\n55.0\n55.1\n63.1\n57.0\n61.4\n55.5\nBigBenchHard (3 shot, CoT)\n67.9\n65.8\n66.0\n62.8\n21.7\n0.9\n2.5\n56.2\nDROP (3 shot)\n61.3\n62.5\n62.6\n61.5\n54.4\n49.4\n58.8\n56.2\nMATH (4 shot CoT, Flex)\n31.5\n42.0\n43.7\n42.5\n14.8\n5.1\n29.8\n40.0\nGSM8K (8 shot, CoT)\n76.2\n84.3\n87.6\n83.4\n83.8\n61.2\n79.7\n80.0\nHumanEval (pass@10)\n86.2\n83.9\n83.9\n86.3\n93.1\n75.4\n71.7\n91.0\nHumanEval+ (pass@10)\n81.4\n78.6\n79.2\n82.9\n89.7\n69.1\n67.0\n88.5\nIFEval (prompt loose)\n72.8\n81.1\n82.4\n80.6\n74.7\n38.8\n69.9\n56.4\nAlpacaEval 2 (LC % win)\n12.4\n33.5\n34.5\n24.2\n29.0\n49.0\n43.7\n31.4\nSafety (6 task avg.)\n93.1\n87.2\n85.5\n75.2\n75.0\n46.4\n75.5\n56.2\nBenchmark (eval)\nT√ºlu 3 70B SFT\nT√ºlu 3 DPO 70B\nT√ºlu 3 70B\nLlama 3.1 70B Instruct\nQwen 2.5 72B Instruct\nHermes 3 Llama 3.1 70B\nNemotron Llama 3.1 70B\nAvg.\n72.6\n75.9\n76.0\n73.4\n71.5\n68.3\n65.5\nMMLU (0 shot, CoT)\n78.9\n83.3\n83.1\n85.3\n85.5\n80.4\n83.8\nPopQA (15 shot)\n48.6\n46.3\n46.5\n46.4\n30.6\n48.1\n36.4\nTruthfulQA (6 shot)\n55.7\n67.9\n67.6\n66.8\n69.9\n66.5\n62.6\nBigBenchHard (3 shot, CoT)\n82.7\n81.8\n82.0\n73.8\n67.2\n82.1\n0.7\nDROP (3 shot)\n77.2\n74.1\n74.3\n77.0\n34.2\n73.2\n68.8\nMATH (4 shot CoT, Flex)\n53.7\n62.3\n63.0\n56.4\n74.3\n41.9\n55.0\nGSM8K (8 shot, CoT)\n91.1\n93.5\n93.5\n93.7\n89.5\n90.0\n84.7\nHumanEval (pass@10)\n92.9\n92.4\n92.4\n93.6\n94.0\n89.6\n94.1\nHumanEval+ (pass@10)\n87.3\n88.4\n88.0\n89.5\n90.8\n85.9\n85.5\nIFEval (prompt loose)\n82.1\n82.6\n83.2\n88.0\n87.6\n76.0\n79.9\nAlpacaEval 2 (LC % win)\n26.3\n49.6\n49.8\n33.4\n47.7\n28.4\n66.1\nSafety (6 task avg.)\n94.4\n89.0\n88.3\n76.5\n87.0\n57.9\n69.0\nBenchmark (eval)\nT√ºlu 3 405B SFT\nT√ºlu 3 405B DPO\nT√ºlu 3 405B\nLlama 3.1 405B Instruct\nNous Hermes 3 405B\nDeepseek V3\nGPT 4o (11-24)\n-----------------\n----------------\n----------------\n-------------\n------------------------\n-------------------\n-------------\n----------------\nAvg w/o Safety\n76.3\n79.0\n80.0\n78.1\n74.4\n79.0\n80.5\nAvg w/ Safety\n77.5\n79.6\n80.7\n79.0\n73.5\n75.9\n81.6\nMMLU (5 shot, CoT)\n84.4\n86.6\n87.0\n88.0\n84.9\n82.1\n87.9\nPopQA (3 shot)\n55.7\n55.4\n55.5\n52.9\n54.2\n44.9\n53.6\nBigBenchHard (0 shot, CoT)\n88.0\n88.8\n88.6\n87.1\n87.7\n89.5\n83.3\nMATH (4 shot, Flex)\n63.4\n59.9\n67.3\n66.6\n58.4\n72.5\n68.8\nGSM8K (8 shot, CoT)\n93.6\n94.2\n95.5\n95.4\n92.7\n94.1\n91.7\nHumanEval (pass@10)\n95.7\n97.2\n95.9\n95.9\n92.3\n94.6\n97.0\nHumanEval+ (pass@10)\n93.3\n93.9\n92.9\n90.3\n86.9\n91.6\n92.7\nIFEval (prompt loose)\n82.4\n85.0\n86.0\n88.4\n81.9\n88.0\n84.8\nAlpacaEval 2 (LC % win)\n30.4\n49.8\n51.4\n38.5\n30.2\n53.5\n65.0\nSafety (6 task avg.)\n87.7\n85.5\n86.7\n86.8\n65.8\n72.2\n90.9\nHyperparamters\nPPO settings for RLVR:\nLearning Rate: 3 √ó 10‚Åª‚Å∑\nDiscount Factor (gamma): 1.0\nGeneral Advantage Estimation (lambda): 0.95\nMini-batches (N_mb): 1\nPPO Update Iterations (K): 4\nPPO's Clipping Coefficient (epsilon): 0.2\nValue Function Coefficient (c1): 0.1\nGradient Norm Threshold: 1.0\nLearning Rate Schedule: Linear\nGeneration Temperature: 1.0\nBatch Size (effective): 224\nMax Token Length: 2,048\nMax Prompt Token Length: 2,048\nPenalty Reward Value for Responses without an EOS Token: -10.0\nResponse Length: 2,048\nTotal Episodes: 100,000\nKL penalty coefficient (beta): 0.05\nWarm up ratio (omega): 0.0\nLicense and use\nAll Llama 3.1 T√ºlu3 models are released under Meta's Llama 3.1 Community License Agreement.\nLlama 3.1 is licensed under the Llama 3.1 Community License, Copyright ¬© Meta Platforms, Inc.\nT√ºlu3 is intended for research and educational use.\nFor more information, please see our Responsible Use Guidelines.\nThe models have been fine-tuned using a dataset mix with outputs generated from third party models and are subject to additional terms:\nGemma Terms of Use and Qwen License Agreement (models were improved using Qwen 2.5).\nCitation\nIf T√ºlu3 or any of the related materials were helpful to your work, please cite:\n@article{lambert2024tulu3,\ntitle = {T√ºlu 3: Pushing Frontiers in Open Language Model Post-Training},\nauthor = {\nNathan Lambert and\nJacob Morrison and\nValentina Pyatkin and\nShengyi Huang and\nHamish Ivison and\nFaeze Brahman and\nLester James V. Miranda and\nAlisa Liu and\nNouha Dziri and\nShane Lyu and\nYuling Gu and\nSaumya Malik and\nVictoria Graf and\nJena D. Hwang and\nJiangjiang Yang and\nRonan Le Bras and\nOyvind Tafjord and\nChris Wilhelm and\nLuca Soldaini and\nNoah A. Smith and\nYizhong Wang and\nPradeep Dasigi and\nHannaneh Hajishirzi\n},\nyear = {2024},\nemail = {tulu@allenai.org}\n}\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here!\nSummarized results can be found here!\nMetric\nValue (%)\nAverage\n25.88\nIFEval (0-Shot)\n82.55\nBBH (3-Shot)\n16.86\nMATH Lvl 5 (4-Shot)\n18.88\nGPQA (0-shot)\n6.26\nMuSR (0-shot)\n10.52\nMMLU-PRO (5-shot)\n20.23",
    "OpenVINO/Phi-3.5-mini-instruct-int4-ov": "Phi-3.5-mini-instruct-int4-ov\nDescription\nQuantization Parameters\nCompatibility\nRunning Model Inference with Optimum Intel\nRunning Model Inference with OpenVINO GenAI\nLimitations\nLegal information\nDisclaimer\nPhi-3.5-mini-instruct-int4-ov\nModel creator: microsoft\nOriginal model: Phi-3.5-mini-instruct\nDescription\nThis is Phi-3.5-mini-instruct model converted to the OpenVINO‚Ñ¢ IR (Intermediate Representation) format with weights compressed to INT4 by NNCF.\nQuantization Parameters\nWeight compression was performed using nncf.compress_weights with the following parameters:\nmode: int4_asym\nratio: 1\ngroup_size: 128\nFor more information on quantization, check the OpenVINO model optimization guide.\nCompatibility\nThe provided OpenVINO‚Ñ¢ IR model is compatible with:\nOpenVINO version 2024.5.0 and higher\nOptimum Intel 1.21.0 and higher\nRunning Model Inference with Optimum Intel\nInstall packages required for using Optimum Intel integration with the OpenVINO backend:\npip install optimum[openvino]\nRun model inference:\nfrom transformers import AutoTokenizer\nfrom optimum.intel.openvino import OVModelForCausalLM\nmodel_id = \"OpenVINO/Phi-3.5-mini-instruct-int4-ov\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = OVModelForCausalLM.from_pretrained(model_id)\ninputs = tokenizer(\"What is OpenVINO?\", return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\nprint(text)\nFor more examples and possible optimizations, refer to the OpenVINO Large Language Model Inference Guide.\nRunning Model Inference with OpenVINO GenAI\nInstall packages required for using OpenVINO GenAI.\npip install openvino-genai huggingface_hub\nDownload model from HuggingFace Hub\nimport huggingface_hub as hf_hub\nmodel_id = \"OpenVINO/Phi-3.5-mini-instruct-int4-ov\"\nmodel_path = \"Phi-3.5-mini-instruct-int4-ov\"\nhf_hub.snapshot_download(model_id, local_dir=model_path)\nRun model inference:\nimport openvino_genai as ov_genai\ndevice = \"CPU\"\npipe = ov_genai.LLMPipeline(model_path, device)\nprint(pipe.generate(\"What is OpenVINO?\", max_length=200))\nMore GenAI usage examples can be found in OpenVINO GenAI library docs and samples\nLimitations\nCheck the original model card for original model card for limitations.\nLegal information\nThe original model is distributed under mit license. More details can be found in original model card.\nDisclaimer\nIntel is committed to respecting human rights and avoiding causing or contributing to adverse impacts on human rights. See Intel‚Äôs Global Human Rights Principles. Intel‚Äôs products and software are intended only to be used in applications that do not cause or contribute to adverse impacts on human rights.",
    "black-forest-labs/FLUX.1-Depth-dev-lora": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nBy clicking \"Agree\", you agree to the FluxDev Non-Commercial License Agreement and acknowledge the Acceptable Use Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nKey Features\nUsage\nAPI Endpoints\nDiffusers\nLimitations\nOut-of-Scope Use\nLicense\nFLUX.1 Depth [dev] LoRA is a LoRA extracted from FLUX.1 Depth [dev], a 12 billion parameter rectified flow transformer capable of generating an image based on a text description while following the structure of a given input image.\nFor more information, please read our blog post.\nThe LoRA is applicable to FLUX.1 [dev].\nKey Features\nCutting-edge output quality.\nIt blends impressive prompt adherence with maintaining the structure of source images based on depth maps.\nTrained using guidance distillation, making FLUX.1 Depth [dev] LoRA more efficient.\nOpen weights to drive new scientific research, and empower artists to develop innovative workflows.\nGenerated outputs can be used for personal, scientific, and commercial purposes as described in the FLUX.1 [dev] Non-Commercial License.\nUsage\nWe provide a reference implementation of FLUX.1 Depth [dev], as well as sampling code, in a dedicated github repository.\nDevelopers and creatives looking to build on top of FLUX.1 Depth [dev] are encouraged to use this as a starting point.\nAPI Endpoints\nFLUX.1 Depth [pro] is available in our API bfl.ml\nDiffusers\nTo use FLUX.1-Depth-dev-lora with the üß® diffusers python library, first install or upgrade diffusers, peft, and image_gen_aux.\npip install -U git+https://github.com/huggingface/diffusers\npip install git+https://github.com/asomoza/image_gen_aux.git\npip install -U peft\nThen you can use the FluxControlPipeline to run it:\nimport torch\nfrom diffusers import FluxControlPipeline, FluxTransformer2DModel\nfrom diffusers.utils import load_image\nfrom image_gen_aux import DepthPreprocessor\npipe = FluxControlPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\npipe.load_lora_weights(\"black-forest-labs/FLUX.1-Depth-dev-lora\", adapter_name=\"depth\")\npipe.set_adapters(\"depth\", 0.85)\nprompt = \"A robot made of exotic candies and chocolates of different kinds. The background is filled with confetti and celebratory gifts.\"\ncontrol_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/robot.png\")\nprocessor = DepthPreprocessor.from_pretrained(\"LiheYoung/depth-anything-large-hf\")\ncontrol_image = processor(control_image)[0].convert(\"RGB\")\nimage = pipe(\nprompt=prompt,\ncontrol_image=control_image,\nheight=1024,\nwidth=1024,\nnum_inference_steps=30,\nguidance_scale=10.0,\ngenerator=torch.Generator().manual_seed(42),\n).images[0]\nimage.save(\"output.png\")\nTo learn more, check out the diffusers documentation.\nLimitations\nThis model is not intended or able to provide factual information.\nAs a statistical model this checkpoint might amplify existing societal biases.\nThe model may fail to generate output that matches the prompts.\nPrompt following is heavily influenced by the prompting-style.\nOut-of-Scope Use\nThe model and its derivatives may not be used\nIn any way that violates any applicable national, federal, state, local or international law or regulation.\nFor the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.\nTo generate or disseminate verifiably false information and/or content with the purpose of harming others.\nTo generate or disseminate personal identifiable information that can be used to harm an individual.\nTo harass, abuse, threaten, stalk, or bully individuals or groups of individuals.\nTo create non-consensual nudity or illegal pornographic content.\nFor fully automated decision making that adversely impacts an individual's legal rights or otherwise creates or modifies a binding, enforceable obligation.\nGenerating or facilitating large-scale disinformation campaigns.\nLicense\nThis model falls under the FLUX.1 [dev] Non-Commercial License.",
    "Zbalpha/geom_dist_ckpt": "README.md exists but content is empty.",
    "google/paligemma2-10b-ft-docci-448": "Access PaliGemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access PaliGemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged-in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nPaliGemma 2 model card\nModel information\nModel summary\nModel data\nUse in Transformers\nImplementation information\nHardware\nSoftware\nEvaluation information\nBenchmark results\nEthics and safety\nEvaluation approach\nEvaluation results\nUsage and limitations\nIntended usage\nEthical considerations and risks\nLimitations\nPaliGemma 2 model card\nModel page: PaliGemma\nTransformers PaliGemma 2 10B weights, fine-tuned with 448*448 input images on the DOCCI dataset.\nThe model is available in the bfloat16 format for research purposes only.\nThe fine-tune config is available at big_vision.\nResources and technical documentation:\nPaliGemma 2 on Kaggle\nResponsible Generative AI Toolkit\nTerms of Use: Terms\nAuthors: Google\nModel information\nModel summary\nPaliGemma 2 is an update of the PaliGemma\nvision-language model (VLM) which incorporates the capabilities of the\nGemma 2 models. The PaliGemma family of\nmodels is inspired by PaLI-3 and based on\nopen components such as the SigLIP vision\nmodel and Gemma 2 language models. It takes\nboth image and text as input and generates text as output, supporting multiple\nlanguages. It is designed for class-leading fine-tune performance on a wide\nrange of vision-language tasks such as image and short video caption, visual\nquestion answering, text reading, object detection and object segmentation.\nModel architecture\nPaliGemma 2 is the composition of a\nTransformer decoder and a\nVision Transformer image encoder.\nThe text decoder is initialized from\nGemma 2 in the 2B, 9B, and 27B\nparameter sizes. The image encoder is initialized from\nSigLIP-So400m/14.\nSimilar to the original PaliGemma model, PaliGemma 2 is trained following the\nPaLI-3 recipes.\nInputs and outputs\nInput: Image and text string, such as a prompt to caption the image, or\na question.\nOutput: Generated text in response to the input, such as a caption of\nthe image, an answer to a question, a list of object bounding box\ncoordinates, or segmentation codewords.\nCitation\n@article{\ntitle={PaliGemma 2: A Family of Versatile VLMs for Transfer},\nauthor={Andreas Steiner and Andr√© Susano Pinto and Michael Tschannen and Daniel Keysers and Xiao Wang and Yonatan Bitton and Alexey Gritsenko and Matthias Minderer and Anthony Sherbondy and Shangbang Long and Siyang Qin and Reeve Ingle and Emanuele Bugliarello and Sahar Kazemzadeh and Thomas Mesnard and Ibrahim Alabdulmohsin and Lucas Beyer and Xiaohua Zhai},\nyear={2024},\njournal={arXiv preprint arXiv:2412.03555}\n}\nModel data\nPre-train datasets\nPaliGemma 2 is pre-trained on the following mixture of datasets:\nWebLI: WebLI (Web Language Image) is\na web-scale multilingual image-text dataset built from the public web. A\nwide range of WebLI splits are used to acquire versatile model capabilities,\nsuch as visual semantic understanding, object localization,\nvisually-situated text understanding, and multilinguality.\nCC3M-35L: Curated English image-alt_text pairs from webpages\n(Sharma et al., 2018). We used the\nGoogle Cloud Translation API to\ntranslate into 34 additional languages.\nVQ¬≤A-CC3M-35L/VQG-CC3M-35L: A subset of VQ2A-CC3M\n(Changpinyo et al., 2022a),\ntranslated into the same additional 34 languages as CC3M-35L, using the\nGoogle Cloud Translation API.\nOpenImages: Detection and object-aware questions and answers\n(Piergiovanni et al. 2022) generated by\nhandcrafted rules on the OpenImages dataset.\nWIT: Images and texts collected from Wikipedia\n(Srinivasan et al., 2021).\nPaliGemma 2 is based on Gemma 2, and you can find information on the\npre-training datasets for Gemma 2 in the\nGemma 2 model card.\nData responsibility filtering\nThe following filters are applied to WebLI, with the goal of training PaliGemma\n2 on safe and responsible data:\nPornographic image filtering: This filter removes images deemed to be of\npornographic nature.\nText safety filtering: We identify and filter out images that are paired\nwith unsafe text. Unsafe text is any text deemed to contain or be about\nchild sexual abuse imagery (CSAI), pornography, vulgarities, or is otherwise\noffensive.\nText toxicity filtering: We further use the Perspective\nAPI to identify and filter out images that are\npaired with text deemed insulting, obscene, hateful or otherwise toxic.\nText personal information filtering: We filtered certain personal\ninformation and other sensitive data using the Cloud Data Loss Prevention\n(DLP) API to protect the\nprivacy of individuals. Identifiers such as social security numbers and\nother sensitive information types were removed.\nAdditional methods: Filtering based on content quality and safety in\nline with our policies and practices.\nUse in Transformers\nThe following snippets use model google/paligemma2-10b-ft-docci-448 for reference purposes.\nThe model in this repo you are now browsing may have been trained for other tasks, please\nmake sure you use appropriate inputs for the task at hand.\nfrom transformers import (\nPaliGemmaProcessor,\nPaliGemmaForConditionalGeneration,\n)\nfrom transformers.image_utils import load_image\nimport torch\nmodel_id = \"google/paligemma2-3b-ft-docci-448\"\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg\"\nimage = load_image(url)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\").eval()\nprocessor = PaliGemmaProcessor.from_pretrained(model_id)\n# Instruct the model to create a caption in English\nprompt = \"caption en\"\nmodel_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(torch.bfloat16).to(model.device)\ninput_len = model_inputs[\"input_ids\"].shape[-1]\nwith torch.inference_mode():\ngeneration = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\ngeneration = generation[0][input_len:]\ndecoded = processor.decode(generation, skip_special_tokens=True)\nprint(decoded)\nImplementation information\nHardware\nPaliGemma 2 was trained using the latest generation of Tensor Processing Unit\n(TPU) hardware (TPUv5e).\nSoftware\nTraining was completed using JAX,\nFlax,\nTFDS and\nbig_vision.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nTFDS is used to access datasets and Flax is used for model architecture. The\nPaliGemma 2 fine-tune code and inference code are released in the big_vision\nGitHub repository.\nEvaluation information\nBenchmark results\nIn order to verify the transferability of PaliGemma 2 to a wide variety of\nacademic tasks, we fine-tune the pretrained models on each task. We report results on\ndifferent resolutions to provide an impression of which tasks benefit from\nincreased resolution. Importantly, none of these tasks or datasets are part of\nthe pretraining data mixture, and their images are explicitly removed from the\nweb-scale pre-training data.\nPaliGemma 2 results by model resolution and size\nBenchmark\n224-3B\n224-10B\n224-28B\n448-3B\n448-10B\n448-28B\nAI2D\n74.7\n83.1\n83.2\n76.0\n84.4\n84.6\nAOKVQA-DA (val)\n64.2\n68.9\n70.2\n67.9\n70.8\n71.2\nAOKVQA-MC (val)\n79.7\n83.7\n84.7\n82.5\n85.9\n87.0\nActivityNet-CAP\n34.2\n35.9\n-\n-\n-\n-\nActivityNet-QA\n51.3\n53.2\n-\n-\n-\n-\nCOCO-35L (avg34)\n113.9\n115.8\n116.5\n115.8\n117.2\n117.2\nCOCO-35L (en)\n138.4\n140.8\n142.4\n140.4\n142.4\n142.3\nCOCOcap\n141.3\n143.7\n144.0\n143.4\n145.0\n145.2\nChartQA (aug)\n74.4\n74.2\n68.9\n89.2\n90.1\n85.1\nChartQA (human)\n42.0\n48.4\n46.8\n54.0\n66.4\n61.3\nCountBenchQA\n81.0\n84.0\n86.4\n82.0\n85.3\n87.4\nDocVQA (val)\n39.9\n43.9\n44.9\n73.6\n76.6\n76.1\nGQA\n66.2\n67.2\n67.3\n68.1\n68.3\n68.3\nInfoVQA (val)\n25.2\n33.6\n36.4\n37.5\n47.8\n46.7\nMARVL (avg5)\n83.5\n89.5\n90.6\n82.7\n89.1\n89.7\nMSRVTT-CAP\n68.5\n72.1\n-\n-\n-\n-\nMSRVTT-QA\n50.5\n51.9\n-\n-\n-\n-\nMSVD-QA\n61.1\n62.5\n-\n-\n-\n-\nNLVR2\n91.4\n93.9\n94.2\n91.6\n93.7\n94.1\nNoCaps\n123.1\n126.3\n127.1\n123.5\n126.9\n127.0\nOCR-VQA\n73.4\n74.7\n75.3\n75.7\n76.3\n76.6\nOKVQA\n64.2\n68.0\n71.2\n64.1\n68.6\n70.6\nRSVQA-hr (test)\n92.7\n92.6\n92.7\n92.8\n92.8\n92.8\nRSVQA-hr (test2)\n90.9\n90.8\n90.9\n90.7\n90.7\n90.8\nRSVQA-lr\n93.0\n92.8\n93.5\n92.7\n93.1\n93.7\nRefCOCO (testA)\n75.7\n77.2\n76.8\n78.6\n79.7\n79.3\nRefCOCO (testB)\n71.0\n74.2\n73.9\n73.5\n76.2\n74.8\nRefCOCO (val)\n73.4\n75.9\n75.0\n76.3\n78.2\n77.3\nRefCOCO+ (testA)\n72.7\n74.7\n73.6\n76.1\n77.7\n76.6\nRefCOCO+ (testB)\n64.2\n68.4\n67.1\n67.0\n71.1\n68.6\nRefCOCO+ (val)\n68.6\n72.0\n70.3\n72.1\n74.4\n72.8\nRefCOCOg (test)\n69.0\n71.9\n70.7\n72.7\n74.8\n73.7\nRefCOCOg (val)\n68.3\n71.4\n70.5\n72.3\n74.4\n73.0\nST-VQA (val)\n61.9\n64.3\n65.1\n80.5\n82.0\n81.8\nSciCap\n165.1\n159.5\n156.9\n183.3\n177.2\n172.7\nScienceQA\n96.1\n98.2\n98.2\n96.2\n98.5\n98.6\nScreen2Words\n113.3\n117.8\n122.8\n114.0\n119.1\n123.4\nTallyQA (complex)\n70.3\n73.4\n74.2\n73.6\n76.7\n76.8\nTallyQA (simple)\n81.8\n83.2\n83.4\n85.3\n86.2\n85.7\nTextCaps\n127.5\n137.9\n139.9\n152.1\n157.7\n153.6\nTextVQA (val)\n59.6\n64.0\n64.7\n75.2\n76.6\n76.2\nVATEX\n80.8\n82.7\n-\n-\n-\n-\nVQAv2 (minival)\n83.0\n84.3\n84.5\n84.8\n85.8\n85.8\nVizWizVQA (val)\n76.4\n78.1\n78.7\n77.5\n78.6\n78.9\nWidgetCap\n138.1\n139.8\n138.8\n151.4\n151.9\n148.9\nXM3600 (avg35)\n42.8\n44.5\n45.2\n43.2\n44.6\n45.2\nXM3600 (en)\n79.8\n80.7\n81.0\n80.3\n81.5\n81.0\nxGQA (avg7)\n58.6\n61.4\n61.1\n60.4\n62.6\n62.1\nAdditional Benchmarks\nICDAR 2015 Incidental\nModel\nPrecision\nRecall\nF1\nPaliGemma 2 3B\n81.88\n70.73\n75.9\nTotal-Text\nModel\nPrecision\nRecall\nF1\nPaliGemma 2 3B\n73.8.\n74.54\n74.17\nFinTabNet\nModel\nS-TEDS\nTEDS\nGriTS-Top\nGriTS-Con\nPaliGemma 2 3B\n99.18\n98.94\n99.43\n99.21\nPubTabNet\nModel\nS-TEDS\nTEDS\nGriTS-Top\nGriTS-Con\nPaliGemma 2 3B\n97.6\n97.31\n97.99\n97.84\nGrandStaff\nModel\nCER\nLER\nSER\nPaliGemma 2 3B\n1.6\n6.7\n2.3\nPubChem\nPaliGemma 2 3B, Full Match: 94.8\nDOCCI\nModel\navg#char\navg#sent\nNES %\nPaliGemma 2 3B\n529\n7.74\n28.42\nPaliGemma 2 10B\n521\n7.45\n20.27\navg#char: Average number of characters\navg#sent: Average number of sentences\nNES: Non entailment sentences\nMIMIC-CXR\nModel\nCIDEr\nBLEU4\nRouge-L\nRadGraph F1\nPaliGemma 2 3B\n19.9%\n14.6%\n31.92%\n28.8%\nPaliGemma 2 10B\n17.4%\n15%\n32.41%\n29.5%\nVisual Spatial Reasoning\nModel\nVSR zeroshot split (test)\nVSR random split (test)\nPaliGemma 2 3B\n0.75\n0.82\nPaliGemma 2 10B\n0.80\n0.87\nEthics and safety\nEvaluation approach\nOur evaluation methods include structured ethics and safety evaluations across\nrelevant content policies, including:\nHuman evaluation on prompts covering child safety, content safety and\nrepresentational harms. See the Gemma model\ncard for\nmore details on evaluation approach, but with image captioning and visual\nquestion answering setups.\nImage-to-Text benchmark evaluation: Benchmark against relevant academic\ndatasets such as FairFace Dataset (Karkkainen et al.,\n2021).\nEvaluation results\nThe human evaluation results of ethics and safety evaluations are within\nacceptable thresholds for meeting internal\npolicies\nfor categories such as child safety, content safety and representational\nharms.\nOn top of robust internal evaluations, we also use the Perspective API\n(threshold of 0.8) to measure toxicity, profanity, and other potential\nissues in the generated captions for images sourced from the FairFace\ndataset. We report the maximum and median values observed across subgroups\nfor each of the perceived gender, ethnicity, and age attributes.\nMetric\nPerceived gender\nEthnicity\nAge group\nModel size\n3B\n10B\n28B\n3B\n10B\n28B\n3B\n10B\n28B\nMaximum\nToxicity\n0.14%\n0.15%\n0.19%\n0.29%\n0.39%\n0.39%\n0.26%\n0.18%\n0.32%\nIdentity Attack\n0.04%\n0.02%\n0.02%\n0.13%\n0.06%\n0.06%\n0.06%\n0.03%\n0.06%\nInsult\n0.17%\n0.25%\n0.17%\n0.37%\n0.52%\n0.52%\n0.27%\n0.39%\n0.24%\nThreat\n0.55%\n0.43%\n0.57%\n0.83%\n0.48%\n0.48%\n0.64%\n0.43%\n0.64%\nProfanity\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nMedian\nToxicity\n0.13%\n0.10%\n0.18%\n0.07%\n0.07%\n0.14%\n0.12%\n0.08%\n0.12%\nIdentity Attack\n0.02%\n0.01%\n0.02%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nInsult\n0.15%\n0.23%\n0.14%\n0.14%\n0.17%\n0.13%\n0.09%\n0.18%\n0.16%\nThreat\n0.35%\n0.27%\n0.41%\n0.28%\n0.19%\n0.42%\n0.27%\n0.31%\n0.40%\nProfanity\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\n0.00%\nUsage and limitations\nIntended usage\nOpen Vision Language Models (VLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nFine-tune on specific vision-language task:\nThe pre-trained models can be fine-tuned on a wide range of vision-language\ntasks such as: image captioning, short video caption, visual question\nanswering, text reading, object detection and object segmentation.\nThe pre-trained models can be fine-tuned for specific domains such as remote\nsensing question answering, visual questions from people who are blind,\nscience question answering, describe UI element functionalities.\nThe pre-trained models can be fine-tuned for tasks with non-textual outputs\nsuch as bounding boxes or segmentation masks.\nVision-language research:\nThe pre-trained models and fine-tuned models can serve as a foundation for\nresearchers to experiment with VLM techniques, develop algorithms, and\ncontribute to the advancement of the field.\nEthical considerations and risks\nThe development of vision-language models (VLMs) raises several ethical\nconcerns. In creating an open model, we have carefully considered the following:\nBias and Fairness\nVLMs trained on large-scale, real-world image-text data can reflect\nsocio-cultural biases embedded in the training material. These models\nunderwent careful scrutiny, input data pre-processing described and\nposterior evaluations reported in this card.\nMisinformation and Misuse\nVLMs can be misused to generate text that is false, misleading, or\nharmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to share\ninnovation by making VLM technology accessible to developers and\nresearchers across the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content\nsafety are essential. Developers are encouraged to exercise caution and\nimplement appropriate content safety safeguards based on their specific\nproduct policies and application use cases.\nMisuse for malicious purposes: Technical limitations and developer and\nend-user education can help mitigate against malicious applications of LLMs.\nEducational resources and reporting mechanisms for users to flag misuse are\nprovided: see the Responsible Generative AI Toolkit.\nProhibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered to remove\ncertain personal information and sensitive data. Developers are encouraged\nto adhere to privacy regulations with privacy-preserving techniques.\nLimitations\nMost limitations inherited from the underlying Gemma 2 models still apply:\nVLMs are better at tasks that can be framed with clear prompts and\ninstructions. Open-ended or highly complex tasks might be challenging.\nNatural language is inherently complex. VLMs might struggle to grasp\nsubtle nuances, sarcasm, or figurative language.\nVLMs generate responses based on information they learned from their\ntraining datasets, but they are not knowledge bases. They may generate\nincorrect or outdated factual statements.\nVLMs rely on statistical patterns in language and images. They might\nlack the ability to apply common sense reasoning in certain situations.\nPaliGemma 2 was designed first and foremost to serve as a general\npre-trained model for fine-tuning to specialized tasks. Hence, its \"out of\nthe box\" or \"zero-shot\" performance might lag behind models designed\nspecifically for general purpose use.\nPaliGemma 2 is not a multi-turn chatbot. It is designed for a single round\nof image and text input.",
    "MERaLiON/MERaLiON-AudioLLM-Whisper-SEA-LION": "MERaLiON\nAcknowledgement\nModel Description\nCapabilities\nInference\nvLLM Inference\nHuggingface CPU Inference\nHuggingface GPU Inference\nDisclaimer\nTechnical Specifications\nTraining Data\nCompute and Infrastructure\nCitation\nüî• We have released MERaLiON2, with better multilingual Speech Recognition, Emotion Recognition, and Instruction Following capability.\nMERaLiON\nMERaLiON-AudioLLM is a Speech-Text Large Language Model tailored for Singapore‚Äôs multilingual and multicultural landscape. Integrating a localised Whisper-large-v2 speech encoder and SEA-LION V3 text decoder, MERaLiON-AudioLLM is finetuned on 260,000 hours of speech and audio data, 6 various tasks, to address the diverse linguistic nuances of Singapore's local accents and dialects.\nMERaLiON stands for Multimodal Empathetic Reasoning and Learning in One Network.\nDeveloped by: I2R, A*STAR, with collaboration with AISG, Singapore\nModel type: Multimodal LLM\nLanguage(s): Primarily English (Global and Singapore), with support for input and output in other languages compatible with Whisper and SEA-LION models.\nAudio: Mono channel audio, 16000 hz, within 30 seconds.\nLicense: MERaLiON Public License\nDemo: MERaLiON-AudioLLM Web Demo\nWe support model inference using the Huggingface and vLLM frameworks, enabling lightning inference speed. For more technical details, please refer to our technical report.\nAcknowledgement\nThis research is supported by the National Research Foundation, Singapore and Infocomm Media Development Authority, Singapore under its National Large Language Models Funding Initiative.\nModel Description\nMERaLiON-AudioLLM is designed to take in an audio-text pair as input and generate a text output.\nThe architecture comprises three key components: an audio encoder that transforms speech or audio inputs into sequences of vector representations, a text decoder that interprets and responds to natural language instructions, and an adaptor module that compresses the encoder representations while aligning the encoder‚Äôs hidden dimension with the text decoder‚Äôs embedding size.\nSpecifically, we fine-tuned the MERaLiON-Whisper encoder from Whisper-large-v2 for the audio encoder and used SEA-LION V3, a localised LLM developed by our partner AI Singapore as the text decoder.\nCapabilities\nMERaLiON-AudioLLM is trained to mainly address 6 tasks, namely Automatic Speech Recognition (ASR),\nSpeech Translation (ST), Spoken Question Answering (SQA),\nSpoken Dialogue Summarization (SDS), Speech Instruction (SI), and Paralinguistics (PARA).\nWe benchmark MERaLiON-AudioLLM with a series of test sets from the AudioBench benchmark\nagainst three well-known AudioLLMs: Qwen2-Audio 7B, WavLLM, SALMONN, and a cascaded model.\nAs is shown in the following table, MERaLiON-AudioLLM performs better in the Singapore local context,\nas evidenced by evaluation results on Singapore's Multitask National Speech Corpus (MNSC) datasets.\nMNSC is a multitask speech understanding dataset derived and further annotated from IMDA NSC Corpus.\nIt focuses on the knowledge of Singapore's local accent, localised terms, and code-switching.\nWe assess ASR and ST tasks using Word Error Rate (WER) and BLEU scores, respectively. For other tasks, we employ the LLM-as-a-Judge framework,\nwhich uses a pre-trained large language model to evaluate task performance by generating and scoring responses based on relevance, coherence, and accuracy criteria.\nRefer to the AudioBench paper for more details.\nTask\nDataset\nMERaLiON\nQwen2-Audio 7B\nWavLLM\nSALMONN-7B\nCascaded Model\nAutomatic Speech RecognitionWER (‚Üì)\nLibriSpeech-Test-Clean\n0.03\n0.03\n0.02\n0.10\n0.03\nLibriSpeech-Test-Other\n0.05\n0.06\n0.05\n0.10\n0.05\nCommon-Voice-15-En-Test\n0.10\n0.11\n0.15\n0.31\n0.11\nEarnings21-Test\n0.17\n0.19\n0.65\n0.26\n0.11\nEarnings22-Test\n0.20\n0.24\n0.67\n0.36\n0.14\nMNSC-ASR-Part 1\n0.05\n0.07\n-\n0.09\n0.07\nMNSC-ASR-Part 2\n0.05\n0.19\n-\n0.42\n0.33\nMNSC-ASR-Part 3\n0.28\n0.35\n-\n0.66\n0.30\nMNSC-ASR-Part 4\n0.40\n0.56\n-\n0.76\n0.48\nMNSC-ASR-Part 5\n0.21\n0.28\n-\n0.35\n0.23\nMNSC-ASR-Part 6\n0.15\n0.22\n-\n0.25\n0.18\nSpeech TranslationBLEU (‚Üë)\nCoVoST 2 En ‚Üí Id\n32.62\n16.33\n13.84\n14.14\n27.62\nCoVoST 2 En ‚Üí Zh\n37.98\n25.77\n31.96\n33.89\n35.27\nCoVoST 2 En ‚Üí Ta\n8.50\n0.03\n0.00\n0.00\n8.46\nCoVoST 2 Id ‚Üí En\n37.07\n6.33\n5.93\n26.89\n46.80\nCoVoST 2 Zh ‚Üí En\n15.01\n16.47\n2.37\n5.30\n15.21\nCoVoST 2 Ta ‚Üí En\n3.97\n0.04\n0.17\n0.36\n2.83\nSpoken Question AnsweringLLM-as-a-Judge (‚Üë)\nSLUE-SQA-5\n82.94\n80.05\n83.92\n83.48\n88.58\nSpoken-SQuAD\n70.33\n64.86\n77.65\n66.40\n88.62\nCN-College-Listen-Test\n85.03\n74.51\n65.43\n50.90\n91.85\nSingapore-Public-Speech-SQA\n60.32\n58.31\n58.55\n59.24\n73.11\nMNSC-SQA-Part 3\n51.4\n42.0\n-\n40.60\n53.20\nMNSC-SQA-Part 4\n49.0\n39.6\n-\n36.60\n60.20\nMNSC-SQA-Part 5\n58.2\n51.6\n-\n44.60\n67.20\nMNSC-SQA-Part 6\n65.2\n53.6\n-\n46.80\n71.60\nSpoken Dialogue SummarizationLLM-as-a-Judge (‚Üë)\nMNSC-SDS-Part 3\n46.80\n33.80\n-\n9.0\n45.40\nMNSC-SDS-Part 4\n45.80\n24.80\n-\n7.0\n44.00\nMNSC-SDS-Part 5\n55.2\n40.4\n-\n17.2\n58.00\nMNSC-SDS-Part 6\n61.8\n46.2\n-\n24.2\n65.40\nSpeech InstructionLLM-as-a-Judge (‚Üë)\nOpenHermes-Audio\n71.4\n44.8\n22.40\n15.80\n72.20\nAlpaca-GPT4-Audio\n73.4\n52.6\n21.60\n17.20\n73.80\nParalinguisticsLLM-as-a-Judge (‚Üë)\nVoxCeleb-Gender-Test\n99.53\n99.12\n69.68\n88.81\n35.25\nVoxCeleb-Accent-Test\n46.35\n29.18\n-\n34.22\n24.64\nMELD-Sentiment-Test\n42.26\n53.49\n50.08\n42.07\n56.67\nMELD-Emotion-Test\n30.15\n40.54\n41.07\n30.73\n47.39\nInference\nOut of Scope use: This model is not intended for use in tool calling, math, and coding tasks.\nvLLM Inference\nWe support hosting the model using vLLM framework. Refer to the guide here.\nHuggingface CPU Inference\nMERaLiON-AudioLLM-Whisper-SEA-LION transformers 4.46.3\npip install transformers==4.46.3\nimport librosa\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nrepo_id = \"MERaLiON/MERaLiON-AudioLLM-Whisper-SEA-LION\"\nprocessor = AutoProcessor.from_pretrained(\nrepo_id,\ntrust_remote_code=True,\n)\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\nrepo_id,\nuse_safetensors=True,\ntrust_remote_code=True,\n)\nprompt = \"Given the following audio context: <SpeechHere>\\n\\nText instruction: {query}\"\ntranscribe_query = \"Please transcribe this speech.\"\ntranslate_query = \"Can you please translate this speech into written Chinese?\"\nconversation = [\n[{\"role\": \"user\", \"content\": prompt.format(query=transcribe_query)}],\n[{\"role\": \"user\", \"content\": prompt.format(query=translate_query)}],\n]\nchat_prompt = processor.tokenizer.apply_chat_template(\nconversation=conversation,\ntokenize=False,\nadd_generation_prompt=True\n)\n# Use an audio within 30 seconds, 16000hz.\naudio_array, sample_rate = librosa.load(\"/path/to/your/audio/file\", sr=16000)\naudio_array = [audio_array]*2\ninputs = processor(text=chat_prompt, audios=audio_array)\noutputs = model.generate(**inputs, max_new_tokens=256)\ngenerated_ids = outputs[:, inputs['input_ids'].size(1):]\nresponse = processor.batch_decode(generated_ids, skip_special_tokens=True)\nHuggingface GPU Inference\nimport torch\nimport librosa\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nrepo_id = \"MERaLiON/MERaLiON-AudioLLM-Whisper-SEA-LION\"\ndevice = \"cuda\"\nprocessor = AutoProcessor.from_pretrained(\nrepo_id,\ntrust_remote_code=True,\n)\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\nrepo_id,\nuse_safetensors=True,\ntrust_remote_code=True,\nattn_implementation=\"flash_attention_2\",\ntorch_dtype=torch.bfloat16\n).to(device)\nprompt = \"Given the following audio context: <SpeechHere>\\n\\nText instruction: {query}\"\ntranscribe_query = \"Please transcribe this speech.\"\ntranslate_query = \"Can you please translate this speech into written Chinese?\"\nconversation = [\n[{\"role\": \"user\", \"content\": prompt.format(query=transcribe_query)}],\n[{\"role\": \"user\", \"content\": prompt.format(query=translate_query)}],\n]\nchat_prompt = processor.tokenizer.apply_chat_template(\nconversation=conversation,\ntokenize=False,\nadd_generation_prompt=True\n)\n# Use an audio within 30 seconds, 16000hz.\naudio_array, sample_rate = librosa.load(\"/path/to/your/audio/file\", sr=16000)\naudio_array = [audio_array]*2\ninputs = processor(text=chat_prompt, audios=audio_array)\nfor key, value in inputs.items():\nif isinstance(value, torch.Tensor):\ninputs[key] = inputs[key].to(device)\nif value.dtype == torch.float32:\ninputs[key] = inputs[key].to(torch.bfloat16)\noutputs = model.generate(**inputs, max_new_tokens=256)\ngenerated_ids = outputs[:, inputs['input_ids'].size(1):]\nresponse = processor.batch_decode(generated_ids, skip_special_tokens=True)\nDisclaimer\nThe current MERaLiON-AudioLLM has not been specifically aligned for safety and may generate content that is inappropriate, offensive, or harmful. Developers and users are responsible for performing their own safety fine-tuning and implementing necessary security measures. The authors shall not be held liable for any claims, damages, or other liabilities arising from the use of the released models, weights, or code.\nTechnical Specifications\nTraining Data\nMERaLiON-AudioLLM is trained on a diverse collection of publicly available datasets, alongside synthesised and augmented samples carefully curated by the team and native speakers, totaling 260,000 hours of audio.\nCompute and Infrastructure\nMERaLiON-AudioLLM is trained on the ASPIRE 2A+ Supercomputer Cluster, provided by National Supercomputing Centre (NSCC), Singapore. ASPIRE 2A+ cluster provides multiple H100 nodes, with each compute node equipped with 8 Nvidia H100 GPUs, 2 TB of RAM, and 30 TB of locally attached NVMe storage. These nodes are interconnected via a rail-optimised, full fat-tree topology, utilising 400 Gb/s NDR InfiniBand cables. Additionally, the cluster incorporates a 2.5 PB SSD-based Lustre file system, linked to the H100 nodes through high-speed InfiniBand connections.\nWith a global batch size of 640, we train the current release of MERaLiON-AudioLLM for around 200k steps, which took 2 days to complete using 16 nodes, 128 H100 GPUs.\nCitation\nIf you find our work useful, please cite our paper:\n@misc{he2024meralionaudiollmtechnicalreport,\ntitle={MERaLiON-AudioLLM: Bridging Audio and Language with Large Language Models},\nauthor={{MERaLiON Team}},\nyear={2024},\neprint={2412.09818},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2412.09818},\n}",
    "utter-project/EuroLLM-9B": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Card for EuroLLM-9B\nModel Details\nModel Description\nRun the model\nResults\nEU Languages\nEnglish\nBias, Risks, and Limitations\nModel Card for EuroLLM-9B\nThis is the model card for EuroLLM-9B. You can also check the instruction tuned version: EuroLLM-9B-Instruct.\nDeveloped by: Unbabel, Instituto Superior T√©cnico, Instituto de Telecomunica√ß√µes, University of Edinburgh, Aveni, University of Paris-Saclay, University of Amsterdam, Naver Labs, Sorbonne Universit√©.\nFunded by: European Union.\nModel type: A 9B parameter multilingual transfomer LLM.\nLanguage(s) (NLP): Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Greek, Hungarian, Irish, Italian, Latvian, Lithuanian, Maltese, Polish, Portuguese, Romanian, Slovak, Slovenian, Spanish, Swedish, Arabic, Catalan, Chinese, Galician, Hindi, Japanese, Korean, Norwegian, Russian, Turkish, and Ukrainian.\nLicense: Apache License 2.0.\nModel Details\nThe EuroLLM project has the goal of creating a suite of LLMs capable of understanding and generating text in all European Union languages as well as some additional relevant languages.\nEuroLLM-9B is a 9B parameter model trained on 4 trillion tokens divided across the considered languages and several data sources: Web data, parallel data (en-xx and xx-en), and high-quality datasets.\nEuroLLM-9B-Instruct was further instruction tuned on EuroBlocks, an instruction tuning dataset with focus on general instruction-following and machine translation.\nModel Description\nEuroLLM uses a standard, dense Transformer architecture:\nWe use grouped query attention (GQA) with 8 key-value heads, since it has been shown to increase speed at inference time while maintaining downstream performance.\nWe perform pre-layer normalization, since it improves the training stability, and use the RMSNorm, which is faster.\nWe use the SwiGLU activation function, since it has been shown to lead to good results on downstream tasks.\nWe use rotary positional embeddings (RoPE) in every layer, since these have been shown to lead to good performances while allowing the extension of the context length.\nFor pre-training, we use 400 Nvidia H100 GPUs of the Marenostrum 5 supercomputer, training the model with a constant batch size of 2,800 sequences, which corresponds to approximately 12 million tokens, using the Adam optimizer, and BF16 precision.\nHere is a summary of the model hyper-parameters:\nSequence Length\n4,096\nNumber of Layers\n42\nEmbedding Size\n4,096\nFFN Hidden Size\n12,288\nNumber of Heads\n32\nNumber of KV Heads (GQA)\n8\nActivation Function\nSwiGLU\nPosition Encodings\nRoPE (\\Theta=10,000)\nLayer Norm\nRMSNorm\nTied Embeddings\nNo\nEmbedding Parameters\n0.524B\nLM Head Parameters\n0.524B\nNon-embedding Parameters\n8.105B\nTotal Parameters\n9.154B\nRun the model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"utter-project/EuroLLM-9B\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\ntext = \"English: My name is EuroLLM. Portuguese:\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nResults\nEU Languages\nTable 1: Comparison of open-weight LLMs on multilingual benchmarks. The borda count corresponds to the average ranking of the models (see  (Colombo et al., 2022)). For Arc-challenge, Hellaswag, and MMLU we are using Okapi datasets (Lai et al., 2023) which include 11 languages. For MMLU-Pro and MUSR we translate the English version with Tower (Alves et al., 2024) to 6 EU languages.* As there are no public versions of the pre-trained models, we evaluated them using the post-trained versions.\nThe results in Table 1 highlight EuroLLM-9B's superior performance on multilingual tasks compared to other European-developed models (as shown by the Borda count of 1.0), as well as its strong competitiveness with non-European models, achieving results comparable to Gemma-2-9B and outperforming the rest on most benchmarks.\nEnglish\nTable 2: Comparison of open-weight LLMs on English general benchmarks.*  As there are no public versions of the pre-trained models, we evaluated them using the post-trained versions.\nThe results in Table 2 demonstrate EuroLLM's strong performance on English tasks, surpassing most European-developed models and matching the performance of Mistral-7B (obtaining the same Borda count).\nBias, Risks, and Limitations\nEuroLLM-9B has not been aligned to human preferences, so the model may generate problematic outputs (e.g., hallucinations, harmful content, or false statements).",
    "c01zaut/Qwen2.5-3B-Instruct-RK3588-1.1.4": "Qwen2.5-3B-Instruct-RK3588-1.1.4\nUseful links:\nOriginal Model Card for base model, Qwen2.5-3B-Instruct, below:\nQwen2.5-3B-Instruct\nIntroduction\nRequirements\nQuickstart\nEvaluation & Performance\nCitation\nQwen2.5-3B-Instruct-RK3588-1.1.4\nThis version of Qwen2.5-3B-Instruct has been converted to run on the RK3588 NPU using ['w8a8', 'w8a8_g128', 'w8a8_g256', 'w8a8_g512'] quantization.\nThis model has been optimized with the following LoRA:\nCompatible with RKLLM version: 1.1.4\nUseful links:\nOfficial RKLLM GitHub\nRockhipNPU Reddit\nEZRKNN-LLM\nPretty much anything by these folks: marty1885 and happyme531\nConverted using https://github.com/c0zaut/ez-er-rkllm-toolkit\nOriginal Model Card for base model, Qwen2.5-3B-Instruct, below:\nQwen2.5-3B-Instruct\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the instruction-tuned 3B Qwen2.5 model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\nNumber of Parameters: 3.09B\nNumber of Paramaters (Non-Embedding): 2.77B\nNumber of Layers: 36\nNumber of Attention Heads (GQA): 16 for Q and 2 for KV\nContext Length: Full 32,768 tokens and generation 8192 tokens\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-3B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "MatinaAI/matina-sentence-embedding": "You need to agree to share your contact information to access this model\nYou agree to not use the dataset to conduct experiments that cause harm to human subjects.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nSentenceTransformer\nModel Details\nModel Description\nModel Sources\nFull Model Architecture\nUsage\nDirect Usage (Sentence Transformers)\nTraining Details\nTraining Datasets\nTraining Hyperparameters\nFramework Versions\nCitation\nSentenceTransformer\nThis is a sentence-transformers model trained on the miracle_triplet_en_all, miracle_triplet_ar_all, miracle_triplet_fa_all, parsinlu_qqp_pair2class, parsinlu_entail_pair3class, pquad_pair, alpaca_persian_pair, wiki_triplet, wiki_DSimilar_pair2class, persianQA_pair, ghaemiyeh_pair, feghehi_QA_QQ_pair, education_books_pair, self_instruct_persian_pairs, cooking_tourism_pair and translation_pair datasets. It maps sentences & paragraphs to a 1024-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.\nOur Paper: Advancing Retrieval-Augmented Generation for Persian: Development of Language Models, Comprehensive Benchmarks, and Best Practices for Optimization link\nModel Details\nModel Description\nModel Type: Sentence Transformer\nMaximum Sequence Length: 512 tokens\nOutput Dimensionality: 1024 dimensions\nSimilarity Function: Cosine Similarity\nTraining Datasets:\nmiracle_triplet_en_all\nmiracle_triplet_ar_all\nmiracle_triplet_fa_all\nparsinlu_qqp_pair2class\nparsinlu_entail_pair3class\npquad_pair\nalpaca_persian_pair\nwiki_triplet\nwiki_DSimilar_pair2class\npersianQA_pair\nghaemiyeh_pair\nfeghehi_QA_QQ_pair\neducation_books_pair\nself_instruct_persian_pairs\ncooking_tourism_pair\ntranslation_pair\nLanguages: en, ar, bn, es, fa, fi, fr, hi, id, ja, ko, ru, sw, te, th, zh\nModel Sources\nDocumentation: Sentence Transformers Documentation\nRepository: Sentence Transformers on GitHub\nHugging Face: Sentence Transformers on Hugging Face\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: XLMRobertaModel\n(1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n)\nUsage\nDirect Usage (Sentence Transformers)\nFirst install the Sentence Transformers library:\npip install -U sentence-transformers\nThen you can load this model and run inference.\nfrom sentence_transformers import SentenceTransformer\n# Download from the ü§ó Hub\nmodel = SentenceTransformer(\"sentence_transformers_model_id\")\n# Run inference\nsentences = [\n'ŸÑ€åÿ≥ÿ™€å ÿßÿ≤ ⁄©ŸÑŸÖÿßÿ™ ÿ™ÿµÿßÿØŸÅ€å ÿß€åÿ¨ÿßÿØ ⁄©ŸÜ€åÿØ.',\n'ÿØÿ± ÿß€åŸÜÿ¨ÿß ŸÑ€åÿ≥ÿ™€å ÿßÿ≤ ÿØŸá ⁄©ŸÑŸÖŸá ÿ™ÿµÿßÿØŸÅ€å Ÿàÿ¨ŸàÿØ ÿØÿßÿ±ÿØ:1. ÿ≠ÿ±ŸÅ ÿ≤ÿØŸÜ\\n2. ÿ®ÿÆŸÑ\\n3. ÿ¢ÿ±ÿßÿ≥ÿ™ŸÜ\\n4. ÿ∑ŸÜ€åŸÜ\\n5. ÿ¨€åŸàŸá\\n6. ÿ¢ÿ™ÿ¥ ÿ≥Ÿàÿ≤€å\\n7. ÿÆÿ≥ÿ™⁄Ø€å ŸÜÿßŸæÿ∞€åÿ±\\n8. ÿ¥€åÿ∑ŸÜÿ™\\n9. ÿ™ÿ±ÿ≥€åŸÖ ⁄©ŸÜ€åÿØ\\n10. Ÿæÿßÿ±ÿß⁄ØŸàŸÜ',\n'',\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, 1024]\n# Get the similarity scores for the embeddings\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities.shape)\n# [3, 3]\nTraining Details\nTraining Datasets\nmiracle_triplet_en_all\nDataset: miracle_triplet_en_all at 07e2b62\nSize: 2,863 training samples\nColumns: anchor, positive, and negative\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 6 tokensmean: 11.48 tokensmax: 30 tokens\nmin: 19 tokensmean: 163.65 tokensmax: 490 tokens\nmin: 5 tokensmean: 142.69 tokensmax: 481 tokens\nSamples:\nanchor\npositive\nnegative\nWhen was quantum field theory developed?\nHistory of quantum field theoryThe third thread in the development of quantum field theory was the need to handle the statistics of many-particle systems consistently and with ease. In 1927, Pascual Jordan tried to extend the canonical quantization of fields to the many-body wave functions of identical particles using a formalism which is known as statistical transformation theory; this procedure is now sometimes called second quantization. In 1928, Jordan and Eugene Wigner found that the quantum field describing electrons, or other fermions, had to be expanded using anti-commuting creation and annihilation operators due to the Pauli exclusion principle (see Jordan‚ÄìWigner transformation). This thread of development was incorporated into many-body theory and strongly influenced condensed matter physics and nuclear physics.\nAdS/CFT correspondenceIn quantum field theory, one typically computes the probabilities of various physical events using the techniques of perturbation theory. Developed by Richard Feynman and others in the first half of the twentieth century, perturbative quantum field theory uses special diagrams called Feynman diagrams to organize computations. One imagines that these diagrams depict the paths of point-like particles and their interactions. Although this formalism is extremely useful for making predictions, these predictions are only possible when the strength of the interactions, the coupling constant, is small enough to reliably describe the theory as being close to a theory without interactions.\nWhen is the dialectical method used?\nDialect TestThe Dialect Test was created by A.J. Ellis in February 1879, and was used in the fieldwork for his work \"On Early English Pronunciation\". It stands as one of the earliest methods of identifying vowel sounds and features of speech. The aim was to capture the main vowel sounds of an individual dialect by listening to the reading of a short passage. All the categories of West Saxon words and vowels were included in the test so that comparisons could be made with the historic West Saxon speech as well as with various other dialects.\nFrankfurt SchoolMarx thus extensively relied on a form of dialectical analysis. This method‚Äîto know the truth by uncovering the contradictions in presently predominant ideas and, by extension, in the social relations to which they are linked‚Äîexposes the underlying struggle between opposing forces. For Marx, it is only by becoming aware of the dialectic (i.e., class consciousness) of such opposing forces, in a struggle for power, that individuals can liberate themselves and change the existing social order.\nHave the Colorado Rockies won a title?\nList of Colorado Rockies seasonsThe Colorado Rockies are a professional baseball team based in Denver, Colorado. The club has been owned since formation by Charles and Richard Monfort. The Rockies were created as an expansion team for the 1993 season and rose to a postseason appearance after three seasons and the 1994‚Äì95 strike. Since then they have played in the postseason four more times: in 2007 (when they lost the World Series to the Red Sox), 2009, 2017, and 2018. In 2012, the Rockies won only 64 games - the fewest in their history over a full season. They are one of the two MLB franchises that has never won a division title.\nHistory of the Colorado RockiesThe History of the Colorado Rockies began in 1991 when a Major League Baseball (MLB) expansion franchise for Denver, Colorado was granted to an ownership group headed by John Antonucci. In 1993, the Colorado Rockies started play in the National League (NL) West division. Since that date, the Rockies have reached the MLB postseason four times, each time as the National League wild card team. Twice (1995 and 2009) they were eliminated in the first round of the playoffs. In 2007, the Rockies advanced all the way to the World Series, only to be swept by the Boston Red Sox.\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nmiracle_triplet_ar_all\nDataset: miracle_triplet_ar_all at 07e2b62\nSize: 3,495 training samples\nColumns: anchor, positive, and negative\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 6 tokensmean: 11.76 tokensmax: 33 tokens\nmin: 17 tokensmean: 168.99 tokensmax: 512 tokens\nmin: 6 tokensmean: 152.25 tokensmax: 512 tokens\nSamples:\nanchor\npositive\nnegative\nŸÖÿßŸáŸà ÿßŸÑÿ£ŸÖŸÜ ÿßŸÑÿ®ÿ¥ÿ±Ÿä ÿü\nÿ£ŸÖŸÜ ÿ®ÿ¥ÿ±ŸäÿßŸÑÿ£ŸÖŸÜ ÿßŸÑÿ®ÿ¥ÿ±Ÿä ŸáŸà ŸÜŸÖŸàÿ∞ÿ¨ ŸÑŸÅŸáŸÖ ŸÜŸÇÿßÿ∑ ÿßŸÑÿ∂ÿπŸÅ ÿßŸÑÿπÿßŸÑŸÖŸäÿ© ÿßŸÑŸä ÿ£ŸÜÿµÿßÿ± ÿßŸÑÿ∑ÿπŸÜ ŸÅŸä ÿßŸÑŸÖŸÅŸáŸàŸÖ ÿßŸÑÿ™ŸÇŸÑŸäÿØŸä ŸÑŸÑÿ£ŸÖŸÜ ÿßŸÑŸÇŸàŸÖŸä ÿ®ÿßŸÑŸÇŸàŸÑ ÿ•ŸÜ ÿßŸÑŸÖÿ±ÿ¨ÿπ ÿßŸÑÿµÿ≠Ÿäÿ≠ ŸÑÿ™ÿ≠ŸÇŸäŸÇ ÿßŸÑÿ£ŸÖŸÜ ŸäŸÜÿ®ÿ∫Ÿä ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑŸÅÿ±ÿØ ŸàŸÑŸäÿ≥ ÿßŸÑÿØŸàŸÑÿ©. ÿßŸÑÿ£ŸÖŸÜ ÿßŸÑÿ®ÿ¥ÿ±Ÿä ÿßŸÑÿ∞Ÿä Ÿäÿ≠ŸÖŸÑ Ÿàÿ¨Ÿáÿ© ŸÜÿ∏ÿ± ÿßŸÑŸÜÿßÿ≥ ŸÖÿ≠Ÿàÿ±Ÿáÿß ÿßŸÑÿ£ŸÖŸÜ ÿ£ŸÖÿ± ÿ∂ÿ±Ÿàÿ±Ÿä ŸÑÿ™ÿ≠ŸÇŸäŸÇ ÿßŸÑÿßÿ≥ÿ™ŸÇÿ±ÿßÿ± ÿßŸÑŸàÿ∑ŸÜŸä ŸàÿßŸÑÿ•ŸÇŸÑŸäŸÖŸä ŸàÿßŸÑÿπÿßŸÑŸÖŸä.ÿ®ÿ±ÿ≤ Ÿáÿ∞ÿß ÿßŸÑŸÖŸÅŸáŸàŸÖ ŸÖŸÜ ŸÅŸáŸÖ ŸÖÿß ÿ®ÿπÿØ ÿßŸÑÿ≠ÿ±ÿ® ÿßŸÑÿ®ÿßÿ±ÿØÿ© ŸÖÿ™ÿπÿØÿØ ÿßŸÑÿ™ÿÆÿµÿµÿßÿ™ÿå ŸàÿßŸÑÿ£ŸÖŸÜ Ÿäÿ¥ŸÖŸÑ ÿπÿØÿØÿß ŸÖŸÜ ÿßŸÑŸÖÿ¨ÿßŸÑÿßÿ™ ÿßŸÑÿ®ÿ≠ÿ´Ÿäÿ©ÿå ÿ®ŸÖÿß ŸÅŸä ÿ∞ŸÑŸÉ ÿØÿ±ÿßÿ≥ÿßÿ™ ÿßŸÑÿ™ŸÜŸÖŸäÿ© ŸàÿßŸÑÿπŸÑÿßŸÇÿßÿ™ ÿßŸÑÿØŸàŸÑŸäÿ© ŸàÿßŸÑÿØÿ±ÿßÿ≥ÿßÿ™ ÿßŸÑÿßÿ≥ÿ™ÿ±ÿßÿ™Ÿäÿ¨Ÿäÿ©ÿå Ÿàÿ≠ŸÇŸàŸÇ ÿßŸÑÿ•ŸÜÿ≥ÿßŸÜ. ÿ®ÿ±ŸÜÿßŸÖÿ¨ ÿßŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© ÿßŸÑÿ•ŸÜŸÖÿßÿ¶Ÿä ŸÑÿπÿßŸÖ 1994 ÿ™ŸÇÿ±Ÿäÿ± ÿßŸÑÿ™ŸÜŸÖŸäÿ© ÿßŸÑÿ®ÿ¥ÿ±Ÿäÿ© ŸàŸäÿπÿ™ÿ®ÿ± ŸÖŸÜÿ¥Ÿàÿ± ŸÖÿπŸÑŸÖÿß ŸÅŸä ŸÖÿ¨ÿßŸÑ ÿßŸÑÿ£ŸÖŸÜ ÿßŸÑÿ®ÿ¥ÿ±Ÿäÿå Ÿàÿ≠ÿ¨ÿ™Ÿáÿß ÿ®ÿ£ŸÜ ÿ™ÿ£ŸÖŸäŸÜ \"ÿßŸÑÿ™ÿ≠ÿ±ÿ± ŸÖŸÜ ÿßŸÑŸÅÿßŸÇÿ©\" Ÿà\"ÿßŸÑÿ™ÿ≠ÿ±ÿ± ŸÖŸÜ ÿßŸÑÿÆŸàŸÅ\" ŸÑÿ¨ŸÖŸäÿπ ÿßŸÑÿ£ÿ¥ÿÆÿßÿµ ŸáŸà ÿ£ŸÅÿ∂ŸÑ ÿ∑ÿ±ŸäŸÇ ŸÑŸÖÿπÿßŸÑÿ¨ÿ© ŸÖÿ¥ŸÉŸÑÿ© ÿßŸÜÿπÿØÿßŸÖ ÿßŸÑÿ£ŸÖŸÜ ÿßŸÑÿπÿßŸÑŸÖŸä. ÿßŸÑŸÖÿ¥ÿßÿ± ÿ•ŸÑŸäŸáÿß ŸÅŸä ŸÉÿ´Ÿäÿ± ŸÖŸÜ ÿßŸÑÿ£ÿ≠ŸäÿßŸÜ ŸÖÿ¨ŸÖŸàÿπÿ© Ÿàÿßÿ≥ÿπÿ© ŸÖŸÜ ŸÖŸÜÿßŸÇÿ¥ÿßÿ™ ÿßŸÑÿ≥Ÿäÿßÿ≥ÿßÿ™ ÿßŸÑÿπÿßŸÑŸÖŸäÿ© ŸàÿßŸÑŸÖÿ¨ŸÑÿßÿ™ ÿßŸÑÿπŸÑŸÖŸäÿ©ÿå ÿ∫ÿßŸÑÿ®ÿß ŸÖÿß Ÿäÿ™ŸÖ ÿ™ÿØÿ±Ÿäÿ≥Ÿá ŸÅŸä ÿßŸÑÿ¨ÿßŸÖÿπÿßÿ™ ÿ£ŸÖŸÜ ÿßŸÑÿ•ŸÜÿ≥ÿßŸÜ ŸÉÿ¨ÿ≤ÿ° ŸÖŸÜ ÿßŸÑÿπŸÑÿßŸÇÿßÿ™ ÿßŸÑÿØŸàŸÑŸäÿ©ÿå ŸàÿßŸÑÿπŸàŸÑŸÖÿ©ÿå ÿ£Ÿà ÿØÿ±ÿßÿ≥ÿßÿ™ ÿ≠ŸÇŸàŸÇ ÿßŸÑÿ•ŸÜÿ≥ÿßŸÜ.\nÿ£ŸÖŸÜÿßŸÑÿ£ŸÖŸÜ ŸáŸà ÿßŸÑÿ™ÿ≠ÿ±ÿ± ŸÖŸÜ ÿ£Ÿà ŸÖŸÇÿßŸàŸÖÿ© ÿ∂ÿØ ÿ£Ÿä ÿ∂ÿ±ÿ± ŸÖÿ≠ÿ™ŸÖŸÑ (ÿ£Ÿà ÿ£Ÿä ÿ™ÿ∫ŸäŸäÿ± ŸÇÿ≥ÿ±Ÿä ÿ∫Ÿäÿ± ŸÖÿ±ÿ∫Ÿàÿ® ŸÅŸäŸá) ŸÖŸÜ ŸÇŸàŸâ ÿÆÿßÿ±ÿ¨Ÿäÿ©. ŸÇÿØ ŸäŸÉŸàŸÜ ÿßŸÑŸÖÿ≥ÿ™ŸÅŸäÿØŸàŸÜ ŸÖŸÜ ÿßŸÑÿ£ŸÖŸÜ ŸáŸÖ ÿßŸÑÿ£ÿ¥ÿÆÿßÿµ ŸàÿßŸÑŸÖÿ¨ŸÖŸàÿπÿßÿ™ ÿßŸÑÿ•ÿ¨ÿ™ŸÖÿßÿπŸäÿ©ÿå ŸàÿßŸÑÿ£ÿ¥Ÿäÿßÿ° ŸàÿßŸÑŸÖÿ§ÿ≥ÿ≥ÿßÿ™ÿå ŸàÿßŸÑŸÜÿ∏ŸÖ ÿßŸÑÿ®Ÿäÿ¶Ÿäÿ©ÿå Ÿàÿ£Ÿä ŸÉŸäÿßŸÜ ÿ¢ÿÆÿ± ÿ£Ÿà ÿ∏ÿßŸáÿ±ÿ© ÿ£ÿÆÿ±Ÿâ ŸÇÿØ ÿ™ŸÉŸàŸÜ ÿπÿ±ÿ∂ÿ© ŸÑÿ£Ÿä ÿ™ÿ∫ŸäŸäÿ± Ÿäÿ∑ÿ±ÿ£ ŸÅŸä ÿ£Ÿä ÿ®Ÿäÿ¶ÿ©.\nŸÖÿ™Ÿâ ŸàŸÑÿØ  ÿßŸÑŸÖÿ∫ŸÜŸä ÿØŸÉÿ™Ÿàÿ± ÿØÿ±Ÿäÿü\nÿØŸÉÿ™Ÿàÿ± ÿØÿ±Ÿäÿ£ŸÜÿØÿ±ŸäŸá ÿ±ŸàŸÖŸäŸÑ ŸäŸàŸÜÿ∫ (Andre Romelle Young) ÿßÿ≥ŸÖŸá ÿßŸÑŸÅŸÜŸä ÿØŸÉÿ™Ÿàÿ± ÿØÿ±Ÿä (Dr. Dre) ŸàŸäÿ≥ÿ™ÿÆÿ±ÿ¨ ÿßŸÑŸÑŸÇÿ® ÿßŸÑÿπÿßŸÖ ÿØÿ±Ÿä ŸÖŸÜ ÿßŸÑÿßÿ≥ŸÖ ÿßŸÑÿ£ŸàŸÑ ÿ£ŸÜÿØÿ±ŸäŸáÿå ŸàŸÑÿØ ŸÅŸä (18 ŸÅÿ®ÿ±ÿßŸäÿ± 1965) ŸÅŸä ŸÑŸàÿ≥ ÿ£ŸÜÿ¨ŸÑŸàÿ≥ ŸÅŸä ÿßŸÑŸàŸÑÿßŸäÿßÿ™ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© ÿßŸÑÿ£ŸÖÿ±ŸäŸÉŸäÿ©ÿå ŸÖÿ∫ŸÜŸä ÿ±ÿßÿ® ŸàŸÖŸÜÿ™ÿ¨ ŸÖŸàÿ≥ŸäŸÇŸä ŸàŸÖŸÜ ÿ£ŸÉÿ´ÿ± ŸÖÿ∫ŸÜŸä ÿßŸÑÿ±ÿßÿ® ÿ¥Ÿáÿ±ÿ© ŸàŸÜÿ¨ÿßÿ≠ÿß ŸÅŸä ŸÖÿ¨ÿßŸÑŸá.ÿ£ÿ≠ÿØÿ´ ÿØŸÉÿ™Ÿàÿ± ÿØÿ±Ÿä ÿ£ÿ´ÿ±ÿß ŸÅŸä ÿßŸÑÿ∫ÿßŸÜÿ∫ÿ≥ÿ™ÿß ÿ±ÿßÿ®ÿå ŸàŸáŸà ŸÖÿ§ÿ≥ÿ≥ ÿ™ÿ≥ÿ¨ŸäŸÑÿßÿ™ ÿ¢ŸÅÿ™ÿ±ŸÖÿßÿ´ Aftermath Records ÿßŸÑÿ™Ÿä ÿ£ÿ≥ÿ≥Ÿáÿß ÿ®ÿπÿØ ÿ¥ÿ±ŸÉÿ™Ÿá ÿßŸÑÿ≥ÿßÿ®ŸÇÿ© \"Death Row Records\" ÿ®ÿØÿ£ ŸÉÿπÿ∂Ÿà ŸÅŸä ŸÖÿ¨ŸÖŸàÿπÿ© World Class Wreckin' Cru ÿ´ŸÖ ÿ£ÿµÿ®ÿ≠ ÿπÿ∂Ÿàÿß ŸÅŸä ÿ•ŸÜ.ÿØÿ®ŸÑŸäŸà.ÿ£Ÿä N.W.A ÿßŸÑÿ™Ÿä ÿ≥ÿßŸáŸÖÿ™ ŸÅŸä ÿ¥Ÿáÿ±ÿ© ÿßŸÑÿ∫ÿßŸÜÿ∫ÿ≥ÿ™ÿß ÿ±ÿßÿ®ÿå Ÿàÿ®ÿπÿØ ÿßŸÜŸÅÿµÿßŸÑ ŸÖÿ¨ŸÖŸàÿπÿ© N.W.A ÿ£ÿµÿØÿ± ÿØÿ±Ÿä ÿ£ŸàŸÑ ÿ£ŸÑÿ®ŸàŸÖ ŸÖŸÜŸÅÿ±ÿØ ŸÑŸá ŸàŸáŸà \"The Chronic\" ŸÅŸä ÿπÿßŸÖ 1992 Ÿàÿ≠ÿßÿ≤ ÿπŸÑŸâ ÿ£ŸàŸÑ ÿ¨ÿßÿ¶ÿ≤ÿ© ÿ∫ÿ±ÿßŸÖŸä ŸÑŸá ÿπŸÜ ÿ£ÿ∫ŸÜŸäÿ© \"Let Me Ride\"ÿå ÿ£ŸÜÿ™ÿ¨ ÿØŸÉÿ™Ÿàÿ± ÿØÿ±Ÿä ÿßŸÑÿπÿØŸäÿØ ŸÖŸÜ ÿßŸÑÿ£ŸÑÿ®ŸàŸÖÿßÿ™ ŸÑŸÖÿ∫ŸÜŸäŸä ÿßŸÑÿ±ÿßÿ® ŸÉŸÖÿß ÿ≥ÿßŸáŸÖ ÿ®ÿßŸÑÿ•ÿ¥ÿ±ÿßŸÅ ÿπŸÑŸâ ŸÖŸáŸÜÿ© ÿßŸÑÿπÿØŸäÿØ ŸÖŸÜŸáŸÖ ŸÖÿ´ŸÑ ÿ≥ŸÜŸàÿ® ÿØŸàÿ∫ÿå ÿ•ŸÖŸäŸÜŸÖÿå Ÿà50 ÿ≥ŸÜÿ™.\nŸÅŸäŸÉŸÜ ÿØÿ±ÿØÿ±ŸäÿßŸÜŸÅŸäŸÉŸÜ ÿØÿßÿ±ŸäÿßŸÜ (ÿ®ÿßŸÑŸÅÿßÿ±ÿ≥Ÿäÿ©: ŸàŸä⁄ØŸÜ ÿØÿ±ÿØÿ±ŸäÿßŸÜ (23 ŸÜŸàŸÅŸÖÿ®ÿ± 1929 - 26 ÿ£ŸàŸÉÿ™Ÿàÿ®ÿ± 2003); ÿ®ÿßŸÑÿ£ÿ±ŸÖŸÜŸäÿ©: \"’é’´’£’•’∂ ‘¥’ß÷Ä’§’ß÷Ä’•’°’∂\") ŸàŸÑÿØ ŸÑÿ£ÿ≥ÿ±ÿ© ÿ£ÿ±ŸÖŸÜŸäÿ© ŸÅŸä ŸáŸÖÿØÿßŸÜÿå ÿ•Ÿäÿ±ÿßŸÜ. ÿØŸèÿπŸä \"ÿ®ÿ≥ŸÑÿ∑ÿßŸÜ ÿßŸÑÿ®Ÿàÿ®\" Ÿà \"ÿ≥ŸÑÿ∑ÿßŸÜ ÿßŸÑÿ¨ÿßÿ≤ ÿßŸÑŸÅÿßÿ±ÿ≥Ÿä\"ÿå Ÿàÿ≥ŸÑÿ∑ÿßŸÜ ÿßŸÑÿßÿ∫ŸÜŸäÿ© ÿßŸÑÿ•Ÿäÿ±ÿßŸÜŸäÿ©. ŸäÿπÿØ ŸÖŸÜ ÿ£ŸÉÿ´ÿ± ÿßŸÑŸÖÿ∫ŸÜŸäŸÜ ÿ™ÿ£ÿ´Ÿäÿ±Ÿãÿß ÿπŸÑŸâ ÿßŸÑŸÖŸàÿ≥ŸäŸÇŸâ ÿßŸÑŸÅÿßÿ±ÿ≥Ÿäÿ©. ŸÉÿßŸÜ ŸäÿπŸÖŸÑ ŸÖÿπ ÿ¥ÿπÿ±ÿßÿ° ŸàŸÖŸÑÿ≠ŸÜŸäŸÜ ÿ•Ÿäÿ±ÿßŸÜŸäŸäŸÜ ŸÖÿπÿ±ŸàŸÅŸäŸÜ ÿπŸÑŸâ ŸÖÿ≥ÿ™ŸàŸâ ÿßŸÑÿ£ÿ∫ŸÜŸäÿ© ÿßŸÑÿ•Ÿäÿ±ÿßŸÜŸäÿ©. ÿ™ÿ≤Ÿàÿ¨ Ÿàÿ£ŸÜÿ¨ÿ® ÿ£ÿ±ÿ®ÿπÿ© ÿ®ŸÜÿßÿ™.\nŸÖŸÜ ŸáŸà ŸÖÿ§ÿ≥ÿ≥ ÿßŸÑÿßÿ≠ŸÖÿØŸäÿ©ÿü\nÿ∫ŸÑÿßŸÖ ÿ£ÿ≠ŸÖÿØ ÿßŸÑŸÇÿßÿØŸäÿßŸÜŸäÿßŸÑŸÖŸäÿ±ÿ≤ÿß ÿ∫ŸÑÿßŸÖ ÿ£ÿ≠ŸÖÿØ ÿßŸÑŸÇÿßÿØŸäÿßŸÜŸä (1255 ŸáŸÄ - 1326 ŸáŸÄ / 1839 - 1908ŸÖ) ŸÖÿ§ÿ≥ÿ≥ ÿßŸÑÿ¨ŸÖÿßÿπÿ© ÿßŸÑÿ£ÿ≠ŸÖÿØŸäÿ© ÿ®ŸÇÿßÿØŸäÿßŸÜ ŸÅŸä ÿßŸÑŸáŸÜÿØ ŸàŸäÿπÿ™ÿ®ÿ± ÿπŸÜÿØ ÿ£ÿ™ÿ®ÿßÿπŸá ŸáŸà ÿßŸÑŸÖŸáÿØŸä ÿßŸÑŸÖŸàÿπŸàÿØ ŸàÿßŸÑŸÖÿ≥Ÿäÿ≠ ÿßŸÑŸÖŸÜÿ™ÿ∏ÿ±. ŸàŸÇÿßŸÑ ÿ®ÿ£ŸÜŸá ŸÖÿ¨ÿØÿØ ŸÑŸÑÿ•ÿ≥ŸÑÿßŸÖ ÿÆŸÑÿßŸÑ ÿßŸÑŸÇÿ±ŸÜ ÿßŸÑÿ±ÿßÿ®ÿπ ÿπÿ¥ÿ± ÿßŸÑŸáÿ¨ÿ±Ÿä. ÿ≠ÿ≥ÿ® ÿßŸÑÿ•ÿ≥ŸÑÿßŸÖ ŸÅÿ•ŸÜ ÿßŸÑŸÜÿ®Ÿä ÿπŸäÿ≥Ÿâ ÿπŸÑŸâ ŸÇŸäÿØ ÿßŸÑÿ≠Ÿäÿßÿ© Ÿàÿ£ŸÜŸá ŸÅŸä ÿßŸÑÿ≥ŸÖÿßÿ° Ÿàÿ≥ŸäŸÜÿ≤ŸÑ ŸÅŸä ŸÜŸáÿßŸäÿ© ÿßŸÑÿπÿßŸÑŸÖÿå ŸÑŸÉŸÜ ÿ≠ÿ≥ÿ® ÿ∫ŸÑÿßŸÖ ÿ£ÿ≠ŸÖÿØ ŸÅÿ•ŸÜ ÿπŸäÿ≥Ÿâ ŸÇÿØ ŸÜÿ¨ÿß ŸÖŸÜ ÿßŸÑÿµŸÑÿ® ŸàŸáÿßÿ¨ÿ± ÿ•ŸÑŸâ ŸÉÿ¥ŸÖŸäÿ±ÿå ÿ≠Ÿäÿ´ ÿ™ŸàŸÅŸä ŸàŸÅÿßÿ© ÿ∑ÿ®ŸäÿπŸäÿ© Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä ŸÉÿßŸÜÿ™ ŸÅŸÉÿ±ÿ© ÿπŸàÿØÿ™Ÿá ÿßŸÑÿ®ÿØŸÜŸäÿ© ÿÆÿßÿ∑ÿ¶ÿ©.\nÿ¥ŸäÿÆ ÿßŸÑÿπÿ±ÿ®ÿ£ÿ≠ŸÖÿØ (ÿ≠ŸÖÿßÿØ) ÿ®ŸÜ ŸÖÿ≠ŸÖÿØ ÿ®ŸÜ ÿ•ÿ®ÿ±ÿßŸáŸäŸÖ ÿ®Ÿàÿ¥ŸÑÿßŸÉŸÜÿå ÿßŸÑŸÖÿ¥ŸáŸàÿ± ÿ®ŸÑŸÇÿ® \"ÿ¥ŸäÿÆ ÿßŸÑÿπÿ±ÿ®\" ÿ£Ÿà \"ÿßŸÑÿ≠ÿßÿ¨\" ŸàÿßŸÑŸÖÿπÿ±ŸàŸÅ ÿ£Ÿäÿ∂ÿß ÿ®ÿßÿ≥ŸÖ ÿ£ÿ≠ŸÖÿØ ÿ£ŸÉŸàŸÑŸäÿ≤ (1927 - 7 ÿ£ÿ∫ÿ≥ÿ∑ÿ≥ 1964ŸÖ) ŸÖŸÜ ÿ±ÿ¨ÿßŸÑ ÿßŸÑŸÖŸÇÿßŸàŸÖÿ© ÿßŸÑŸÖÿ∫ÿ±ÿ®Ÿäÿ© ÿ≤ŸÖŸÜ ÿßŸÑÿ≠ŸÖÿßŸäÿ© ÿßŸÑŸÅÿ±ŸÜÿ≥Ÿäÿ© ŸàŸÖŸÜ ÿßŸÑŸÖÿπÿßÿ±ÿ∂ŸÜ ŸàÿßŸÑŸÖÿ™ŸÖÿ±ÿØŸäŸÜ ÿπŸÑŸâ ÿ≥ŸÑÿ∑ÿ© ÿßŸÑŸÖŸÑŸÉ ÿßŸÑÿ≠ÿ≥ŸÜ ÿßŸÑÿ´ÿßŸÜŸä ÿ®ÿπÿØ ÿßŸÑÿßÿ≥ÿ™ŸÇŸÑÿßŸÑ.\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nmiracle_triplet_fa_all\nDataset: miracle_triplet_fa_all at 07e2b62\nSize: 2,107 training samples\nColumns: anchor, positive, and negative\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 7 tokensmean: 13.43 tokensmax: 39 tokens\nmin: 15 tokensmean: 133.6 tokensmax: 512 tokens\nmin: 7 tokensmean: 130.35 tokensmax: 512 tokens\nSamples:\nanchor\npositive\nnegative\nÿØ€åŸÜ ŸÖÿ≥€åÿ≠€åÿ™ ÿØÿ± ⁄©ÿ¨ÿß ÿ®€åÿ¥ÿ™ÿ± ÿ±Ÿàÿßÿ¨ ÿØÿßÿ±ÿØÿü\nŸÅŸáÿ±ÿ≥ÿ™ ⁄©ÿ¥Ÿàÿ±Ÿáÿß ÿ®ÿ± Ÿæÿß€åŸá ÿ¨ŸÖÿπ€åÿ™ ŸÖÿ≥€åÿ≠€åÿßŸÜŸÖÿ≥€åÿ≠€åÿ™ ÿØ€åŸÜ ÿ∫ÿßŸÑÿ® ÿØÿ± ÿßÿ±ŸàŸæÿßÿå ÿ±Ÿàÿ≥€åŸáÿå ÿ¢ŸÖÿ±€å⁄©ÿßÿå ŸÅ€åŸÑ€åŸæ€åŸÜÿå ÿ™€åŸÖŸàÿ± ÿ¥ÿ±ŸÇ€åÿå ÿ¨ŸÜŸàÿ® ÿ¢ŸÅÿ±€åŸÇÿßÿå ŸÖÿ±⁄©ÿ≤ ÿ¢ŸÅÿ±€åŸÇÿßÿå ÿ¢ŸÅÿ±€åŸÇÿß€å ÿ¥ÿ±ŸÇ€å Ÿà ÿßŸÇ€åÿßŸÜŸàÿ≥€åŸá ÿßÿ≥ÿ™. ŸáŸÖ⁄ÜŸÜ€åŸÜ ÿ¨ŸàÿßŸÖÿπ ÿ®ÿ≤ÿ±⁄Ø ŸÖÿ≥€åÿ≠€å ÿØÿ± ÿØ€å⁄Øÿ± ŸÜŸÇÿßÿ∑ ÿ¨ŸáÿßŸÜ ŸÖÿßŸÜŸÜÿØ ÿßŸÜÿØŸàŸÜÿ≤€åÿå ÿ¢ÿ≥€åÿß€å ŸÖÿ±⁄©ÿ≤€å Ÿà ÿ¥ÿ±ŸÇ ŸÖ€åÿßŸÜŸáÿå ⁄©Ÿá ÿØÿ± ÿ¢ŸÜ‚ÄåŸáÿß ŸÖÿ≥€åÿ≠€åÿ™ ÿØŸàŸÖ€åŸÜ ÿØ€åŸÜ ÿ®ÿπÿØ ÿßÿ≤ ÿßÿ≥ŸÑÿßŸÖ ÿßÿ≥ÿ™ Ÿàÿ¨ŸàÿØ ÿØÿßÿ±ÿØ. ÿß€åÿßŸÑÿßÿ™ ŸÖÿ™ÿ≠ÿØŸá ÿ¢ŸÖÿ±€å⁄©ÿß ÿØÿßÿ±ÿß€å ÿ®ÿ≤ÿ±⁄Øÿ™ÿ±€åŸÜ ÿ¨ŸÖÿπ€åÿ™ ŸÖÿ≥€åÿ≠€å ÿØÿ± ÿ¨ŸáÿßŸÜ ÿßÿ≥ÿ™ÿå Ÿæÿ≥ ÿßÿ≤ ÿ¢ŸÜ ÿ®ÿ±ÿ≤€åŸÑ Ÿà ŸÖ⁄©ÿ≤€å⁄© ŸÇÿ±ÿßÿ± ÿØÿßÿ±ŸÜÿØ. Ÿàÿßÿ™€å⁄©ÿßŸÜ ÿ™ŸÜŸáÿß ⁄©ÿ¥Ÿàÿ± ÿ¨ŸáÿßŸÜ ÿßÿ≥ÿ™ ⁄©Ÿá €±€∞€∞ ÿØÿ±ÿµÿØ ÿ¨ŸÖÿπ€åÿ™ ÿ¢ŸÜ ŸÖÿ≥€åÿ≠€å Ÿáÿ≥ÿ™ŸÜÿØ.\n⁄Øÿ≥ÿ™ÿ±ÿ¥ ÿßÿØ€åÿßŸÜ ÿØÿ± ÿπÿßŸÑŸÖŸáŸÖ‚Äå⁄ÜŸÜ€åŸÜ ŸÖÿ∑ÿßÿ®ŸÇ ÿ¢ŸÖÿßÿ±Ÿáÿß€å ÿ¨ÿØ€åÿØÿ™ÿ± (€≤€∞€∞€µ) ÿ¢€å€åŸÜ ŸÖÿ≥€åÿ≠€åÿ™ ÿ®ÿß ⁄Øÿ≥ÿ™ÿ±ÿ¥ ÿØÿ± €≥€≥Ÿ™ ŸÜŸàÿπ ÿ®ÿ¥ÿ±ÿå ÿ®€åÿ¥ÿ™ÿ±€åŸÜ ⁄Øÿ≥ÿ™ÿ±ÿ¥ ÿ±ÿß ÿ±Ÿà€å ⁄©ÿ±ŸáŸî ÿÆÿß⁄©€å ÿØÿßÿ±ÿØÿå Ÿà Ÿæÿ≥ ÿßÿ≤ ÿ¢ŸÜ ÿßÿ≥ŸÑÿßŸÖ ÿ®ÿß ⁄Øÿ≥ÿ™ÿ±ÿ¥ €≤€±Ÿ™ÿå ÿ®€å‚ÄåÿØ€åŸÜ€å ÿ®ÿß ⁄Øÿ≥ÿ™ÿ±ÿ¥ €±€∂Ÿ™ Ÿà ÿ¢€å€åŸÜ ŸáŸÜÿØŸà ÿ®ÿß ⁄Øÿ≥ÿ™ÿ±ÿ¥ €±€¥Ÿ™ ŸÇÿ±ÿßÿ± ÿØÿßÿ±ŸÜÿØ.\nÿ¢ÿ¥ ÿ±ÿ¥ÿ™Ÿá ⁄Ü⁄ØŸàŸÜŸá ÿ™Ÿá€åŸá ŸÖ€å ÿ¥Ÿàÿ±ÿü\nÿ¢ÿ¥ ÿ±ÿ¥ÿ™Ÿáÿ¢ÿ¥ ÿ±ÿ¥ÿ™Ÿá ÿßÿ≤ ŸÖÿπÿ±ŸàŸÅ‚Äåÿ™ÿ±€åŸÜ ÿ¢ÿ¥‚ÄåŸáÿß€å ÿß€åÿ±ÿßŸÜ€å ÿßÿ≥ÿ™. ÿß€åŸÜ ÿ¢ÿ¥ ÿ±ÿß ÿ∫ÿßŸÑÿ®ÿßŸã ÿ®ÿß ⁄©ÿ¥⁄© Ÿà ⁄ØÿßŸá€å ÿ®ÿß ÿ≥ÿ±⁄©Ÿá Ÿà ÿ≠ÿ™€å ÿ™ÿ±ÿ¥€å ŸÖ€å‚ÄåÿÆŸàÿ±ŸÜÿØ. ÿ®ÿ≥ÿ™Ÿá ÿ®Ÿá ÿπÿßÿØÿ™‚ÄåŸáÿß€å ÿÆÿßŸÜŸàÿßÿØ⁄Ø€å ÿßÿ≤ ÿß€åŸÜ ÿÆŸàÿ±ÿß⁄© ÿ®Ÿá ÿπŸÜŸàÿßŸÜ Ÿæ€åÿ¥ ÿÆŸàÿ±ÿß⁄© €åÿß ÿÆŸàÿ±ÿß⁄© ÿßÿµŸÑ€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€å‚Äåÿ¥ŸàÿØ(ŸàŸÑ€å ÿ¢ÿ¥ ÿ∫ÿ∞ÿß ŸÜ€åÿ≥ÿ™). ŸáŸÖ⁄ÜŸÜ€åŸÜ ⁄ØÿßŸá€å ÿØÿ± ŸÖŸáŸÖÿßŸÜ€å‚ÄåŸáÿß ÿØÿ± ŸÅÿµŸÑ ÿ≤ŸÖÿ≥ÿ™ÿßŸÜ €åÿß ÿØÿ± ÿ±Ÿàÿ≤ ÿ≥€åÿ≤ÿØŸá‚Äåÿ®Ÿá‚ÄåÿØÿ± Ÿà €åÿß ŸÖÿ±ÿßÿ≥ŸÖ ÿßŸÅÿ∑ÿßÿ± ŸÖÿ≥ŸÑŸÖÿßŸÜÿßŸÜ ÿØÿ± ŸÖÿßŸá ÿ±ŸÖÿ∂ÿßŸÜ ÿßÿ≤ ÿ¢ÿ¥ ÿ±ÿ¥ÿ™Ÿá ÿ®Ÿá ÿπŸÜŸàÿßŸÜ Ÿæ€åÿ¥ ÿÆŸàÿ±ÿß⁄© ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€å‚Äåÿ¥ŸàÿØ. ÿ¢ÿ¥ ÿ±ÿ¥ÿ™Ÿá ÿØÿ± ŸÖÿßŸá ÿ±ŸÖÿ∂ÿßŸÜ ÿ¨ÿß€å⁄ØÿßŸá Ÿà€å⁄òŸá‚Äåÿß€å ÿ®€åŸÜ ÿÆÿßŸÜŸàÿßÿØŸá‚ÄåŸáÿß€å ÿß€åÿ±ÿßŸÜ€å ÿØÿßÿ±ÿØ Ÿà ÿßÿ≤ ÿ¨ŸÖŸÑŸá ÿÆŸàÿ±ÿß⁄©‚ÄåŸáÿß€å ŸÖŸÜÿßÿ≥ÿ® ÿß€åŸÜ ŸÖÿßŸá ÿßÿ≥ÿ™.ÿ¢ÿ® ÿ±ÿß ÿØÿ± ÿ∏ÿ±ŸÅ€å ⁄©Ÿá ÿ®Ÿá ÿßŸÜÿØÿßÿ≤ŸáŸî ⁄©ÿßŸÅ€å ÿ®ÿ≤ÿ±⁄Ø ÿßÿ≥ÿ™ ŸÖ€å‚Äå⁄Øÿ∞ÿßÿ±€åŸÖ ⁄©Ÿá ÿ¨Ÿàÿ¥ ÿ®€åÿß€åÿØÿå Ÿæÿ≥ ÿßÿ≤ ÿ¢ŸÜ ÿ≠ÿ®Ÿàÿ®ÿßÿ™ ÿ¥ÿßŸÖŸÑ ŸÑŸàÿ®€åÿßÿå ÿπÿØÿ≥ Ÿà ŸÜÿÆŸàÿØ ÿ±ÿß ÿ®Ÿá ÿ¢ŸÜ ÿßÿ∂ÿßŸÅŸá ŸÖ€å‚Äå⁄©ŸÜ€åŸÖ ÿØÿ± ÿß€åŸÜ ŸÖ€åÿßŸÜ ÿØÿ± ÿ∏ÿ±ŸÅ ÿØ€å⁄Øÿ±€å Ÿæ€åÿßÿ≤Ÿáÿß€å€å ÿ±ÿß ⁄©Ÿá ÿßÿ≤ Ÿæ€åÿ¥ ÿÆÿ±ÿØ ⁄©ÿ±ÿØŸá‚Äåÿß€åŸÖ ÿØÿ± ÿ±Ÿàÿ∫ŸÜÿå ŸÜŸÖ⁄© Ÿà ÿ≤ÿ±ÿØ ⁄ÜŸàÿ®Ÿá ÿ≥ÿ±ÿÆ ŸÖ€å‚Äå⁄©ŸÜ€åŸÖ. Ÿæÿ≥ ÿßÿ≤ ÿ¢ŸÜ⁄©Ÿá ŸÖÿ≠ÿ™Ÿà€å ÿ∏ÿ±ŸÅ ÿ¥ÿßŸÖŸÑ ÿ¢ÿ® Ÿà ÿ≠ÿ®Ÿàÿ®ÿßÿ™ ÿ®Ÿá ÿ¨Ÿàÿ¥ ÿ¢ŸÖÿØÿå ÿ≥ÿ®ÿ≤€å ÿ¢ÿ¥ ÿ±ÿß ÿ®Ÿá ÿ¢ŸÜ ŸÖ€å‚ÄåÿßŸÅÿ≤ÿß€å€åŸÖ Ÿà ÿµÿ®ÿ± ŸÖ€å‚Äå⁄©ŸÜ€åŸÖ ÿ™ÿß €≤€∞ ÿØŸÇ€åŸÇŸá ÿ®ÿ¨Ÿàÿ¥ÿØ ÿ™ÿß ŸÖÿ≤ŸáŸî ÿÆÿßŸÖ€å ŸÜÿØŸáÿØ. Ÿæÿ≥ ÿßÿ≤ ÿß€åŸÜ ŸÖÿØÿ™ ÿ±ÿ¥ÿ™ŸáŸî ÿ¢ÿ¥€å ÿ±ÿß ÿßÿ∂ÿßŸÅŸá ŸÖ€å‚Äå⁄©ŸÜ€åŸÖ Ÿà ŸáŸÖÿ±ÿßŸá ÿ®ÿß ÿ¢ŸÜ Ÿæ€åÿßÿ≤ ÿØÿßÿ∫ ÿ±ÿß ŸáŸÖ ŸÖ€å‚Äåÿ±€åÿ≤€åŸÖ Ÿà ÿ¢ŸÜ ŸÇÿØÿ± ÿµÿ®ÿ± ŸÖ€å‚Äå⁄©ŸÜ€åŸÖ ÿ™ÿß ÿ±ÿ¥ÿ™Ÿá ŸæÿÆÿ™Ÿá ÿ¥ŸàÿØ. ÿ±ÿ¥ÿ™Ÿá ÿ≤ŸÖÿßŸÜ€å ŸæÿÆÿ™Ÿá‚Äåÿßÿ≥ÿ™ ⁄©Ÿá ÿ®Ÿá ÿÆŸàÿ®€å ŸÜÿ±ŸÖ ÿ¥ÿØŸá ÿ®ÿßÿ¥ÿØÿå Ÿà ÿ±ÿ¥ÿ™Ÿá‚ÄåŸáÿß ŸáŸÖ⁄Ø€å ⁄©ŸÖ€å ŸæŸá...\nÿ¢ÿ¥ ÿ¥ŸàŸÑ€åÿßÿ®ÿ™ÿØÿß ŸÑÿ®Ÿà ÿ±ÿß ŸæŸàÿ≥ÿ™ ⁄Øÿ±ŸÅÿ™Ÿáÿå ÿ≥Ÿæÿ≥ ÿ¢ŸÜ ÿ±ÿß ÿ®Ÿá ÿµŸàÿ±ÿ™ ÿÆŸÑÿßŸÑ€å ÿ®ÿ±ÿ¥ ŸÖ€å‚ÄåÿØŸá€åŸÖ Ÿà ÿ®ÿß ŸÖŸÇÿØÿßÿ±€å ÿ¢ÿ® ÿ±Ÿà€å ÿ≠ÿ±ÿßÿ±ÿ™ ÿ¥ÿπŸÑŸá ŸÇÿ±ÿßÿ± ŸÖ€å‚ÄåÿØŸá€åŸÖ ÿ™ÿß ŸÜ€åŸÖ‚ÄåŸæÿ≤ ÿ¥ŸàÿØ.\nÿ±ŸÅÿßŸá  ÿ≠ÿßŸÑ ⁄©ŸàÿØ⁄©ÿßŸÜ ÿØÿ± ⁄Øÿ±Ÿà ⁄Ü€åÿ≥ÿ™ÿü\nÿ®ÿßÿ≤€å‚ÄåÿØÿ±ŸÖÿßŸÜ€åÿ¨ŸÖŸÑŸá ŸÖÿπÿ±ŸàŸÅ ŸÖÿ¥⁄©ŸÑÿßÿ™ ⁄©ŸàÿØ⁄©ÿßŸÜ ÿ®ÿß ÿ®ÿ≤ÿ±⁄Ø ÿ¥ÿØŸÜÿ¥ÿßŸÜ ÿ®ÿ≤ÿ±⁄Øÿ™ÿ± ŸÖ€å‚Äåÿ¥ŸàÿØ ÿ¨ŸÖŸÑŸá ÿß€å ⁄©ŸÑ€åÿ¥Ÿá ÿß€å Ÿà ÿßÿ¥ÿ™ÿ®ÿßŸá ÿßÿ≥ÿ™. ⁄©ŸàÿØ⁄©ÿßŸÜ ÿ®ÿß Ÿáÿ± ÿ±Ÿàÿ≤ ÿ®ÿ≤ÿ±⁄Ø ÿ¥ÿØŸÜ ÿ®ÿß€åÿ≥ÿ™€å ÿ™ŸàÿßŸÜÿß€å€å‚ÄåŸáÿß€å ÿÆŸà€åÿ¥ ÿØÿ± ÿ≠ŸÑ ŸÖÿ≥ÿ¶ŸÑŸá ÿ±ÿß ÿ®Ÿáÿ®ŸàÿØ ÿ®ÿ®ÿÆÿ¥ŸÜÿØ €å⁄©€å ÿßÿ≤ ÿß€åŸÜ ÿ±ÿßŸá‚ÄåŸáÿß ÿ®ÿßÿ≤€å ÿØÿ±ŸÖÿßŸÜ€å ÿßÿ≥ÿ™. ÿ®ÿ±ÿÆ€å ÿ®ÿßÿ≤€å‚ÄåŸáÿß ÿßÿ≠ÿ≥ÿßÿ≥ÿßÿ™ÿå ÿ®ÿ±ÿÆ€å ÿØ€å⁄Øÿ± ŸÇÿØÿ±ÿ™ ÿ≠ŸÑ ŸÖÿ≥ÿ¶ŸÑŸáÿå ÿ®ÿ±ÿÆ€å ÿØ€å⁄Øÿ± ÿπÿ∂ŸÑÿßÿ™ ⁄©Ÿà⁄Ü⁄© Ÿà ÿ®ÿ±ÿÆ€å ÿØ€å⁄Øÿ± ÿπÿ∂ŸÑÿßÿ™ ÿ®ÿ≤ÿ±⁄Øÿ™ÿ± ⁄©ŸàÿØ⁄©ÿßŸÜ ÿ±ÿß ÿØÿ± ⁄Ø€åÿ± ŸÖ€å‚Äå⁄©ŸÜÿØ ÿØÿ± ÿß€åŸÜ ÿØÿ±⁄Ø€åÿ±€å ÿßŸÜÿ±⁄ò€å ⁄©ŸàÿØ⁄© ÿ™ÿÆŸÑ€åŸá ÿ¥ÿØŸá Ÿà ÿßÿ≠ÿ™ŸÖÿßŸÑ ÿßÿ∂ÿ∑ÿ±ÿßÿ®ÿå Ÿæÿ± ÿ™ÿ≠ÿ±⁄©€åÿå ÿßŸÅÿ≥ÿ±ÿØ⁄Ø€å Ÿà ÿ≥ÿ±ÿÆŸàÿ±ÿØ⁄Ø€å ŸÜÿßÿ¥€å ÿßÿ≤ ÿß€åŸÜ ÿßŸÜÿ±⁄ò€å ŸÖÿßÿ≤ÿßÿØ ⁄©ÿßŸáÿ¥ ŸÖ€å €åÿßÿ®ÿØ. ÿ™ÿµŸà€åÿ± ⁄©ŸÜ€åÿØ ⁄©ŸàÿØ⁄©€å ⁄©Ÿá ŸÜÿ™ŸàÿßŸÜÿØ ÿ≥ÿßÿØŸá‚Äåÿ™ÿ±€åŸÜ ŸÖÿ≥ÿßÿ¶ŸÑ ÿ≤ŸÜÿØ⁄Ø€å ÿÆŸàÿØ ŸÖÿ´ŸÑ ŸÜÿ∏ÿßŸÅÿ™ ÿßÿ™ÿßŸÇ ÿ±ÿß ÿßŸÜÿ¨ÿßŸÖ ŸÜÿØŸáÿØÿå ÿß€åŸÜ ÿπÿØŸÖ ÿ™ŸàÿßŸÜÿß€å€å ÿØÿ± ÿßŸÜÿ¨ÿßŸÖ ŸÅÿπÿßŸÑ€åÿ™ ŸÖÿ±ÿ®Ÿàÿ∑Ÿá ŸàŸÇÿ™€å ÿ®ÿß ÿ™ÿ∞⁄©ÿ±ÿßÿ™ ÿÆÿßŸÜŸàÿßÿØŸá ŸÜ€åÿ≤ ŸáŸÖÿ±ÿßŸá ÿ®ÿßÿ¥ÿØ ÿ®Ÿá ÿßŸà ÿßÿ≥ÿ™ÿ±ÿ≥ ÿ≤€åÿßÿØ€å ÿ™ÿ≤ÿ±€åŸÇ ŸÖ€å‚Äå⁄©ŸÜÿØ. ÿØÿ± ŸàÿßŸÇÿπ ÿ≥ŸÑÿßŸÖÿ™ ÿ¨ÿ≥ŸÖ€å ⁄©ŸàÿØ⁄©ÿßŸÜ ÿ®€åÿ¥ÿ™ÿ± ÿØÿ± ⁄Øÿ±Ÿà ÿ™ÿ∫ÿ∞€åŸá Ÿà ÿ≥ŸÑÿßŸÖÿ™€å ÿ±Ÿàÿ≠€å ÿ¢ŸÜ‚ÄåŸáÿß ÿ®€åÿ¥ÿ™ÿ± ÿØÿ± ⁄Øÿ±Ÿà ÿ≠ŸÑ ŸÖÿ≥ÿ¶ŸÑŸáÿå Ÿæÿß€åÿ¥ Ÿà ŸæŸà€åÿ¥ ÿØÿ± ÿ≠Ÿàÿ≤Ÿá‚ÄåŸáÿß€å ŸÖÿÆÿ™ŸÑŸÅ ÿßÿ≥ÿ™.\nÿ®ÿßÿ≤€å‚ÄåÿØÿ±ŸÖÿßŸÜ€åÿ®ÿßÿ≤€å ÿØÿ±ŸÖÿßŸÜ€å ⁄©ÿßÿ±ÿ®ÿ±ÿØ ŸÖŸàŸÇÿπ€åÿ™‚ÄåŸáÿß€å ÿ®ÿßÿ≤€å ÿØÿ± €å⁄© ÿ≤ŸÖ€åŸÜŸá ÿØÿ±ŸÖÿßŸÜ€å ÿßÿ≥ÿ™. ÿ®ÿßÿ≤€å ÿØÿ±ŸÖÿßŸÜ€å ÿ¥ÿ±ÿß€åÿ∑€å ÿ±ÿß ŸÅÿ±ÿßŸáŸÖ ŸÖ€å ⁄©ŸÜÿØ ÿ™ÿß ⁄©ŸàÿØ⁄©ÿßŸÜ ÿ®ÿ™ŸàÿßŸÜŸÜÿØ ÿ®ÿß ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ ÿ±Ÿàÿ¥ Ÿáÿß€å ÿ≥ÿßŸÑŸÖ ÿßÿ≠ÿ≥ÿßÿ≥ÿßÿ™ Ÿà Ÿá€åÿ¨ÿßŸÜÿßÿ™ ÿÆŸàÿØ ÿ±ÿß ⁄©ÿ¥ŸÅ ⁄©ŸÜŸÜÿØ Ÿà ÿ¢ŸÜŸáÿß ÿ±ÿß ÿ®ÿ±Ÿàÿ≤ ÿØŸáŸÜÿØ. ÿß€åŸÜ ⁄©ÿßÿ± ŸÖ€å‚Äåÿ™ŸàÿßŸÜÿØ ÿ®Ÿá ÿµŸàÿ±ÿ™ ŸÅÿ±ÿØ€å Ÿà ⁄Øÿ±ŸàŸá€å ÿßŸÜÿ¨ÿßŸÖ ÿ¥ŸàÿØ. ÿØÿ± ÿ®ÿßÿ≤€å ÿØÿ±ŸÖÿßŸÜ€åÿå ŸÖÿ¥ÿßŸàÿ± ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ÿØÿ±ŸÖÿßŸÜ⁄Øÿ± ÿßÿ≥ÿ™ ÿßŸÖÿß ÿß€åŸÜ ⁄©ÿßÿ± ÿ™Ÿàÿ≥ÿ∑ ŸàÿßŸÑÿØ€åŸÜ ŸÜ€åÿ≤ ŸÖ€å‚Äåÿ™ŸàÿßŸÜÿØ ÿµŸàÿ±ÿ™ ⁄Ø€åÿ±ÿØ ⁄©Ÿá ÿ™ÿ£ÿ´€åÿ± ÿ¢ŸÜ ⁄©ŸÖÿ™ÿ± ÿÆŸàÿßŸáÿØ ÿ®ŸàÿØ.  ÿßŸàŸÑ€åŸÜ Ÿàÿ∏€åŸÅŸá ⁄©ŸàÿØ⁄©ÿßŸÜ Ÿà ÿßÿ≤ ÿ∂ÿ±Ÿàÿ±€åÿßÿ™ ÿ≤ŸÜÿØ⁄Ø€å ÿ¢ŸÜ‚ÄåŸáÿß ÿ®ÿßÿ≤€å ⁄©ÿ±ÿØŸÜ ÿßÿ≥ÿ™ÿå ÿ¢ŸÖŸàÿ≤ÿ¥ ŸÖÿ≥ÿ™ŸÇ€åŸÖ (ÿ¢ŸÖŸàÿ≤ÿ¥‚ÄåŸáÿß€å€å ŸÖÿßŸÜŸÜÿØ ÿÆŸàÿßŸÜÿØŸÜ Ÿà ŸÜŸàÿ¥ÿ™ŸÜÿå ÿ±€åÿßÿ∂€åÿßÿ™ Ÿà ÿπŸÑŸàŸÖ ⁄©ŸÑÿßÿ≥€å) ÿ®ÿ±ÿß€å ⁄©ŸàÿØ⁄©ÿßŸÜ ÿ≤€åÿ± €∑ ÿ≥ÿßŸÑ ÿ®ÿ≥€åÿßÿ± ŸÖÿÆÿ±ÿ® Ÿà Ÿà€åÿ±ÿßŸÜ ⁄Øÿ± ÿßÿ≥ÿ™ÿå ÿ®Ÿáÿ™ÿ±€åŸÜ ÿ±Ÿàÿ¥ ÿ¢ŸÖŸàÿ≤ÿ¥ ÿ®Ÿá ⁄©ŸàÿØ⁄©ÿßŸÜ ÿØÿ± ÿ≥ŸÜ€åŸÜ Ÿæÿßÿ¶€åŸÜ ÿßÿ≤ ÿ∑ÿ±€åŸÇ ÿ®ÿßÿ≤€å ÿØÿ±ŸÖÿßŸÜ€å ÿµŸàÿ±ÿ™ ŸÖ€å⁄Ø€åÿ±ÿØ. ÿ®ÿßÿ≤€å ÿßÿ≤ ÿ∂ÿ±Ÿàÿ±€åÿßÿ™ ÿ≤ŸÜÿØ⁄Ø€å ⁄©ŸàÿØ⁄©ÿßŸÜ ÿßÿ≥ÿ™ÿå ÿ¢ŸÜ‚ÄåŸáÿß ÿßÿ≤ ÿ∑ÿ±€åŸÇ ÿ®ÿßÿ≤€å ÿßŸÅ⁄©ÿßÿ±ÿ¥ÿßŸÜ ÿ±ÿß ÿ®ÿßÿ≤⁄ØŸà ŸÖ€å‚Äå⁄©ŸÜŸÜÿØ. ÿ™ŸàÿßŸÜÿß€å€å ÿ≠ŸÑ ŸÖÿ≥ÿ¶ŸÑŸá ÿØÿ± ⁄©ŸàÿØ⁄©ÿßŸÜ ÿßÿ≤ ÿ∑ÿ±€åŸÇ ÿ™ŸÖÿ±⁄©ÿ≤ ÿØÿ± ÿ®ÿßÿ≤€å ⁄Øÿ≥ÿ™ÿ±ÿ¥ ŸÖ€å €åÿßÿ®ÿØ Ÿà ÿ®Ÿá ŸáŸÖ€åŸÜ ÿØŸÑ€åŸÑ ÿßÿπÿ™ŸÖÿßÿØ ÿ®Ÿá ŸÜŸÅÿ≥ Ÿà ÿ±Ÿàÿ≠€åŸá ÿßÿ≥ÿ™ŸÇŸÑÿßŸÑ ÿ∑ŸÑÿ®€å ÿÆŸàÿØÿ¥ÿßŸÜ ÿ±ÿß ÿ™ŸÇŸà€åÿ™ ŸÖ€å‚Äå⁄©ŸÜŸÜÿØ.\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nparsinlu_qqp_pair2class\nDataset: parsinlu_qqp_pair2class\nSize: 4,644 training samples\nColumns: sentence1, sentence2, and label\nApproximate statistics based on the first 1000 samples:\nsentence1\nsentence2\nlabel\ntype\nstring\nstring\nint\ndetails\nmin: 5 tokensmean: 15.58 tokensmax: 50 tokens\nmin: 5 tokensmean: 15.91 tokensmax: 100 tokens\n0: ~61.70%1: ~38.30%\nSamples:\nsentence1\nsentence2\nlabel\n⁄Ü⁄ØŸàŸÜŸá ŸÖ€å ÿ™ŸàÿßŸÜŸÖ ⁄©ŸÖ ⁄©ŸÖ Ÿàÿ≤ŸÜ ÿÆŸàÿØ ÿ±ÿß ⁄©ÿßŸáÿ¥ ÿØŸáŸÖÿü\n⁄Ü⁄ØŸàŸÜŸá Ÿàÿ≤ŸÜ ⁄©ŸÖ ⁄©ŸÜŸÖÿü\n1\n⁄Ü⁄ØŸàŸÜŸá ÿßÿ≥ÿ™ŸÖŸÜÿßÿ° ÿ®ÿ± ŸÇÿØÿ±ÿ™ ÿ™ŸÖÿ±⁄©ÿ≤ ÿ¥ÿÆÿµ ÿ™ÿ£ÿ´€åÿ± ŸÖ€å ⁄Øÿ∞ÿßÿ±ÿØÿü\nÿ¢€åÿß ⁄Üÿ±⁄© ÿ±Ÿà€å ŸÑŸàÿ≤Ÿá Ÿáÿß ŸÖ€å ÿ™ŸàÿßŸÜÿØ ŸÜÿ¥ÿßŸÜŸá ÿß€å ÿßÿ≤ STD ÿ®ÿßÿ¥ÿØÿü\n0\nŸàŸÇÿ™€å ÿÆŸàÿßÿ® ⁄©ÿ≥€å ÿ±ÿß ÿØ€åÿØŸÖ Ÿà ⁄ØŸÅÿ™ŸÖ ÿ¢ŸÜŸáÿß ÿØÿ± ÿ≠ÿßŸÑ ŸÖÿ±⁄Ø Ÿáÿ≥ÿ™ŸÜÿØ ÿå ⁄ÜŸá ŸÖÿπŸÜÿß€å€å ÿØÿßÿ±ÿØÿü\nŸàŸÇÿ™€å ÿÆŸàÿßÿ® ŸÖ€å ÿ®€åŸÜŸÖ ⁄©Ÿá ⁄©ÿ≥€å ÿØÿ± ÿ≠ÿßŸÑ ŸÖÿ±⁄Ø ÿ®ÿßÿ¥ÿØ ŸÖÿπŸÜ€å ÿßÿ¥ ⁄Ü€åÿ≥ÿ™ÿü\n0\nLoss: ContrastiveLoss with these parameters:{\n\"distance_metric\": \"SiameseDistanceMetric.COSINE_DISTANCE\",\n\"margin\": 0.5,\n\"size_average\": true\n}\nparsinlu_entail_pair3class\nDataset: parsinlu_entail_pair3class at 13e2bfb\nSize: 2,697 training samples\nColumns: sentence1, sentence2, and label\nApproximate statistics based on the first 1000 samples:\nsentence1\nsentence2\nlabel\ntype\nstring\nstring\nint\ndetails\nmin: 3 tokensmean: 34.16 tokensmax: 203 tokens\nmin: 3 tokensmean: 17.89 tokensmax: 73 tokens\n0: ~39.30%1: ~31.60%2: ~29.10%\nSamples:\nsentence1\nsentence2\nlabel\nÿ≤ŸÜÿßŸÜ ÿ®Ÿá ŸÇÿØÿ±€å ÿ®ÿÆÿ¥ ÿ®ÿ≤ÿ±⁄Ø€å ÿßÿ≤ ŸÜ€åÿ±Ÿà€å ⁄©ÿßÿ± ÿ±ÿß ÿ™ÿ¥⁄©€åŸÑ ŸÖ€å ÿØŸáŸÜÿØ ⁄©Ÿá ÿ®Ÿá ÿ≥ÿÆÿ™€å ŸÖ€å ÿ™ŸàÿßŸÜ ÿ®ÿßŸàÿ± ÿØÿßÿ¥ÿ™ ⁄©Ÿá ÿß⁄Øÿ± ÿß€åŸÜ ÿßŸÖÿ± ÿØÿ± ŸÖŸàÿ±ÿØ ÿ≤ŸÜÿßŸÜ  ÿµÿßÿØŸÇ ŸÜÿ®ÿßÿ¥ÿØ ÿå ÿß€åŸÜ ÿßŸÖÿ± ŸÖ€å ÿ™ŸàÿßŸÜÿØ ÿµÿßÿØŸÇ ÿ®ÿßÿ¥ÿØ.\nŸÖÿ±ÿØÿßŸÜ ÿ®ÿÆÿ¥ ÿπÿ∏€åŸÖ€å ÿßÿ≤ ŸÜ€åÿ±Ÿà€å ⁄©ÿßÿ± Ÿáÿ≥ÿ™ŸÜÿØ ÿ®ŸÜÿßÿ®ÿ±ÿß€åŸÜ ÿ™ŸÜŸáÿß ÿßŸÅÿ±ÿßÿØ ŸÖŸáŸÖ Ÿáÿ≥ÿ™ŸÜÿØ.\n2\nÿ≥ÿßŸÑŸáÿß ÿßÿ≥ÿ™ ⁄©Ÿá ⁄©ŸÜ⁄Øÿ±Ÿá ÿØÿ± ÿ™ŸÑÿßÿ¥ ÿßÿ≥ÿ™ ÿ™ÿß ÿßÿ´ÿ±ÿ®ÿÆÿ¥€å ŸÖÿØ€åÿ±€åÿ™ ÿßÿ∑ŸÑÿßÿπÿßÿ™ Ÿà ŸÅŸÜÿßŸàÿ±€å ÿ±ÿß ÿØÿ± ÿØŸàŸÑÿ™ ŸÅÿØÿ±ÿßŸÑ ÿßŸÅÿ≤ÿß€åÿ¥ ÿØŸáÿØ.\n⁄©ŸÜ⁄Øÿ±Ÿá ÿ®ŸàÿØÿ¨Ÿá Ÿà€å⁄òŸá ÿß€å ÿ®ÿ±ÿß€å ŸÖÿØ€åÿ±€åÿ™ ÿßÿ∑ŸÑÿßÿπÿßÿ™ Ÿà ŸÅŸÜÿßŸàÿ±€å ÿØÿ± ÿØŸàŸÑÿ™ ŸÅÿØÿ±ÿßŸÑ ÿØÿßÿ±ÿØ.\n1\nÿ≥ÿ±ÿßŸÖ€å⁄©‚ÄåŸáÿß€å ÿ≤€åÿ≥ÿ™ ÿÆŸÜÿ´€å Ÿæÿ≥ ÿßÿ≤ ŸÇÿ±ÿßÿ±⁄Ø€åÿ±€å ÿØÿ± ÿ®ÿØŸÜ ŸÖ€åÿ≤ÿ®ÿßŸÜ ÿÆŸàÿßÿµ ŸÅ€åÿ≤€å⁄©€å Ÿà ŸÖ⁄©ÿßŸÜ€å⁄©€å ÿÆŸàÿØ ÿ±ÿß ÿ≠ŸÅÿ∏ ŸÖ€å‚Äå⁄©ŸÜÿØ.\nÿÆŸàÿßÿµ ŸÅ€åÿ≤€å⁄©€å ÿ≥ÿ±ÿßŸÖ€å⁄©‚ÄåŸáÿß ŸÇÿßÿ®ŸÑ ÿßŸÜÿØÿßÿ≤Ÿá ⁄Ø€åÿ±€å ÿßÿ≥ÿ™.\n1\nLoss: SoftmaxLoss\npquad_pair\nDataset: pquad_pair\nSize: 79,972 training samples\nColumns: positive and anchor\nApproximate statistics based on the first 1000 samples:\npositive\nanchor\ntype\nstring\nstring\ndetails\nmin: 19 tokensmean: 183.65 tokensmax: 366 tokens\nmin: 5 tokensmean: 13.95 tokensmax: 36 tokens\nSamples:\npositive\nanchor\nÿ®ÿßÿ¥⁄ØÿßŸá ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸìÿ±ÿ≥ŸÜÿßŸÑ (ÿ®Ÿá ÿßŸÜ⁄ØŸÑ€åÿ≥€å: Arsenal Football Club) €å⁄© ÿ®ÿßÿ¥⁄ØÿßŸá ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸÜ⁄ØŸÑ€åÿ≥€å ÿØÿ± ÿ¥ŸÖÿßŸÑ ÿ¥Ÿáÿ± ŸÑŸÜÿØŸÜ ÿßÿ≥ÿ™ ⁄©Ÿá ŸÖŸàŸÅŸÇ ÿ®Ÿá ⁄©ÿ≥ÿ® €±€≥ ÿπŸÜŸàÿßŸÜ ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿØÿ± ŸÑ€å⁄Ø ÿØÿ≥ÿ™Ÿá ÿßŸàŸÑ Ÿà ŸÑ€å⁄Ø ÿ®ÿ±ÿ™ÿ± ÿßŸÜ⁄ØŸÑÿ≥ÿ™ÿßŸÜÿå €±€¥ ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿØÿ± ÿ¨ÿßŸÖ ÿ≠ÿ∞ŸÅ€å ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸÜ⁄ØŸÑÿ≥ÿ™ÿßŸÜ ÿå €±€∂ ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿØÿ± ÿ¨ÿßŸÖ ÿÆ€åÿ±€åŸá ÿßŸÜ⁄ØŸÑÿ≥ÿ™ÿßŸÜ Ÿà ÿØŸà ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿØÿ± ÿ¨ÿßŸÖ ÿßÿ™ÿ≠ÿßÿØ€åŸá ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸÜ⁄ØŸÑÿ≥ÿ™ÿßŸÜ ÿ¥ÿØŸá‚Äåÿßÿ≥ÿ™. ÿßŸìŸÜ‚ÄåŸáÿß ÿ±⁄©Ÿàÿ±ÿØÿØÿßÿ± ÿ∑ŸàŸÑÿßŸÜ€å‚Äåÿ™ÿ±€åŸÜ ŸÖÿØÿ™ ÿµÿØÿ±ŸÜÿ¥€åŸÜ€å ÿ®ÿØŸàŸÜ ŸàŸÇŸÅŸá ÿØÿ± ŸÑ€å⁄Ø ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸÜ⁄ØŸÑ€åÿ≥ÿå ÿ®€åÿ¥ÿ™ÿ±€åŸÜ ÿ®ÿßÿ≤€å ÿ®ÿØŸàŸÜ ÿ®ÿßÿÆÿ™Ÿê Ÿæ€åÿßŸæ€å (€¥€π ÿ®ÿßÿ≤€å) Ÿà ŸáŸÖ⁄ÜŸÜ€åŸÜ ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿ®ÿØŸàŸÜ ÿ¥⁄©ÿ≥ÿ™ ÿØÿ± €å⁄© ŸÅÿµŸÑ (€∞€¥‚Äì€≤€∞€∞€≥) ŸÖ€å‚Äåÿ®ÿßÿ¥ŸÜÿØ Ÿà ÿ™ŸàÿßŸÜÿ≥ÿ™ŸÜÿØ ÿßŸàŸÑ€åŸÜ Ÿà ÿ™ŸÜŸáÿß ÿ™€åŸÖ€å ÿØÿ± ÿ™ÿßÿ±€åÿÆ ŸÑ€å⁄Ø ÿ®ÿ±ÿ™ÿ± ÿ®ÿßÿ¥ŸÜÿØ ⁄©Ÿá ÿ¨ÿßŸÖ ÿ∑ŸÑÿß€å€å ÿ±ÿß ÿ®ÿØÿ≥ÿ™ ŸÖ€å‚ÄåÿßŸìŸàÿ±ŸÜÿØ.\nŸÖŸàŸÇÿπ€åÿ™ ÿ¨ÿ∫ÿ±ÿßŸÅ€å ÿ®ÿßÿ¥⁄ØÿßŸá ŸÅŸàÿ™ÿ®ÿßŸÑ ÿ¢ÿ±ÿ≥ŸÜÿßŸÑ ÿ±ÿß ÿ®⁄ØŸà€å€åÿØÿü\nÿ®ÿßÿ¥⁄ØÿßŸá ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸìÿ±ÿ≥ŸÜÿßŸÑ (ÿ®Ÿá ÿßŸÜ⁄ØŸÑ€åÿ≥€å: Arsenal Football Club) €å⁄© ÿ®ÿßÿ¥⁄ØÿßŸá ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸÜ⁄ØŸÑ€åÿ≥€å ÿØÿ± ÿ¥ŸÖÿßŸÑ ÿ¥Ÿáÿ± ŸÑŸÜÿØŸÜ ÿßÿ≥ÿ™ ⁄©Ÿá ŸÖŸàŸÅŸÇ ÿ®Ÿá ⁄©ÿ≥ÿ® €±€≥ ÿπŸÜŸàÿßŸÜ ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿØÿ± ŸÑ€å⁄Ø ÿØÿ≥ÿ™Ÿá ÿßŸàŸÑ Ÿà ŸÑ€å⁄Ø ÿ®ÿ±ÿ™ÿ± ÿßŸÜ⁄ØŸÑÿ≥ÿ™ÿßŸÜÿå €±€¥ ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿØÿ± ÿ¨ÿßŸÖ ÿ≠ÿ∞ŸÅ€å ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸÜ⁄ØŸÑÿ≥ÿ™ÿßŸÜ ÿå €±€∂ ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿØÿ± ÿ¨ÿßŸÖ ÿÆ€åÿ±€åŸá ÿßŸÜ⁄ØŸÑÿ≥ÿ™ÿßŸÜ Ÿà ÿØŸà ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿØÿ± ÿ¨ÿßŸÖ ÿßÿ™ÿ≠ÿßÿØ€åŸá ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸÜ⁄ØŸÑÿ≥ÿ™ÿßŸÜ ÿ¥ÿØŸá‚Äåÿßÿ≥ÿ™. ÿßŸìŸÜ‚ÄåŸáÿß ÿ±⁄©Ÿàÿ±ÿØÿØÿßÿ± ÿ∑ŸàŸÑÿßŸÜ€å‚Äåÿ™ÿ±€åŸÜ ŸÖÿØÿ™ ÿµÿØÿ±ŸÜÿ¥€åŸÜ€å ÿ®ÿØŸàŸÜ ŸàŸÇŸÅŸá ÿØÿ± ŸÑ€å⁄Ø ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸÜ⁄ØŸÑ€åÿ≥ÿå ÿ®€åÿ¥ÿ™ÿ±€åŸÜ ÿ®ÿßÿ≤€å ÿ®ÿØŸàŸÜ ÿ®ÿßÿÆÿ™Ÿê Ÿæ€åÿßŸæ€å (€¥€π ÿ®ÿßÿ≤€å) Ÿà ŸáŸÖ⁄ÜŸÜ€åŸÜ ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿ®ÿØŸàŸÜ ÿ¥⁄©ÿ≥ÿ™ ÿØÿ± €å⁄© ŸÅÿµŸÑ (€∞€¥‚Äì€≤€∞€∞€≥) ŸÖ€å‚Äåÿ®ÿßÿ¥ŸÜÿØ Ÿà ÿ™ŸàÿßŸÜÿ≥ÿ™ŸÜÿØ ÿßŸàŸÑ€åŸÜ Ÿà ÿ™ŸÜŸáÿß ÿ™€åŸÖ€å ÿØÿ± ÿ™ÿßÿ±€åÿÆ ŸÑ€å⁄Ø ÿ®ÿ±ÿ™ÿ± ÿ®ÿßÿ¥ŸÜÿØ ⁄©Ÿá ÿ¨ÿßŸÖ ÿ∑ŸÑÿß€å€å ÿ±ÿß ÿ®ÿØÿ≥ÿ™ ŸÖ€å‚ÄåÿßŸìŸàÿ±ŸÜÿØ.\nŸÑ€å⁄Ø ÿ®ÿ±ÿ™ÿ± ÿßŸÜ⁄ØŸÑÿ≥ÿ™ÿßŸÜ ŸÖŸàŸÅŸÇ ÿ®Ÿá ⁄©ÿ≥ÿ® ⁄ÜŸÜÿØ ÿπŸÜŸàÿßŸÜ ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿØÿ± ÿ¨ÿßŸÖ ÿ≠ÿ∞ŸÅ€å ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸÜ⁄ØŸÑÿ≥ÿ™ÿßŸÜ ÿ¥ÿØŸá ÿßÿ≥ÿ™ÿü\nÿ®ÿßÿ¥⁄ØÿßŸá ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸìÿ±ÿ≥ŸÜÿßŸÑ (ÿ®Ÿá ÿßŸÜ⁄ØŸÑ€åÿ≥€å: Arsenal Football Club) €å⁄© ÿ®ÿßÿ¥⁄ØÿßŸá ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸÜ⁄ØŸÑ€åÿ≥€å ÿØÿ± ÿ¥ŸÖÿßŸÑ ÿ¥Ÿáÿ± ŸÑŸÜÿØŸÜ ÿßÿ≥ÿ™ ⁄©Ÿá ŸÖŸàŸÅŸÇ ÿ®Ÿá ⁄©ÿ≥ÿ® €±€≥ ÿπŸÜŸàÿßŸÜ ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿØÿ± ŸÑ€å⁄Ø ÿØÿ≥ÿ™Ÿá ÿßŸàŸÑ Ÿà ŸÑ€å⁄Ø ÿ®ÿ±ÿ™ÿ± ÿßŸÜ⁄ØŸÑÿ≥ÿ™ÿßŸÜÿå €±€¥ ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿØÿ± ÿ¨ÿßŸÖ ÿ≠ÿ∞ŸÅ€å ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸÜ⁄ØŸÑÿ≥ÿ™ÿßŸÜ ÿå €±€∂ ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿØÿ± ÿ¨ÿßŸÖ ÿÆ€åÿ±€åŸá ÿßŸÜ⁄ØŸÑÿ≥ÿ™ÿßŸÜ Ÿà ÿØŸà ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿØÿ± ÿ¨ÿßŸÖ ÿßÿ™ÿ≠ÿßÿØ€åŸá ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸÜ⁄ØŸÑÿ≥ÿ™ÿßŸÜ ÿ¥ÿØŸá‚Äåÿßÿ≥ÿ™. ÿßŸìŸÜ‚ÄåŸáÿß ÿ±⁄©Ÿàÿ±ÿØÿØÿßÿ± ÿ∑ŸàŸÑÿßŸÜ€å‚Äåÿ™ÿ±€åŸÜ ŸÖÿØÿ™ ÿµÿØÿ±ŸÜÿ¥€åŸÜ€å ÿ®ÿØŸàŸÜ ŸàŸÇŸÅŸá ÿØÿ± ŸÑ€å⁄Ø ŸÅŸàÿ™ÿ®ÿßŸÑ ÿßŸÜ⁄ØŸÑ€åÿ≥ÿå ÿ®€åÿ¥ÿ™ÿ±€åŸÜ ÿ®ÿßÿ≤€å ÿ®ÿØŸàŸÜ ÿ®ÿßÿÆÿ™Ÿê Ÿæ€åÿßŸæ€å (€¥€π ÿ®ÿßÿ≤€å) Ÿà ŸáŸÖ⁄ÜŸÜ€åŸÜ ŸÇŸáÿ±ŸÖÿßŸÜ€å ÿ®ÿØŸàŸÜ ÿ¥⁄©ÿ≥ÿ™ ÿØÿ± €å⁄© ŸÅÿµŸÑ (€∞€¥‚Äì€≤€∞€∞€≥) ŸÖ€å‚Äåÿ®ÿßÿ¥ŸÜÿØ Ÿà ÿ™ŸàÿßŸÜÿ≥ÿ™ŸÜÿØ ÿßŸàŸÑ€åŸÜ Ÿà ÿ™ŸÜŸáÿß ÿ™€åŸÖ€å ÿØÿ± ÿ™ÿßÿ±€åÿÆ ŸÑ€å⁄Ø ÿ®ÿ±ÿ™ÿ± ÿ®ÿßÿ¥ŸÜÿØ ⁄©Ÿá ÿ¨ÿßŸÖ ÿ∑ŸÑÿß€å€å ÿ±ÿß ÿ®ÿØÿ≥ÿ™ ŸÖ€å‚ÄåÿßŸìŸàÿ±ŸÜÿØ.\nÿ®€åÿ¥ÿ™ÿ±€åŸÜ ÿ®ÿßÿ≤€å ÿ®ÿØŸàŸÜ ÿ®ÿßÿÆÿ™ Ÿæ€åÿßŸæ€å ŸÖÿ™ÿπŸÑŸÇ ÿ®Ÿá ⁄©ÿØÿßŸÖ ÿ®ÿßÿ¥⁄ØÿßŸá ÿßÿ≥ÿ™ÿü\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nalpaca_persian_pair\nDataset: alpaca_persian_pair at dd503f5\nSize: 35,117 training samples\nColumns: anchor and positive\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\ntype\nstring\nstring\ndetails\nmin: 2 tokensmean: 17.2 tokensmax: 249 tokens\nmin: 2 tokensmean: 198.32 tokensmax: 512 tokens\nSamples:\nanchor\npositive\nÿ≥Ÿá ŸÜ⁄©ÿ™Ÿá ÿ®ÿ±ÿß€å ÿ≥ÿßŸÑŸÖ ŸÖÿßŸÜÿØŸÜ ÿßÿ±ÿßÿ¶Ÿá ÿØŸá€åÿØ.\n1. ÿ±⁄ò€åŸÖ ÿ∫ÿ∞ÿß€å€å ŸÖÿ™ÿπÿßÿØŸÑ Ÿà ŸÖÿ∫ÿ∞€å ÿØÿßÿ¥ÿ™Ÿá ÿ®ÿßÿ¥€åÿØ: ŸÖÿ∑ŸÖÿ¶ŸÜ ÿ¥Ÿà€åÿØ ⁄©Ÿá ŸàÿπÿØŸá Ÿáÿß€å ÿ∫ÿ∞ÿß€å€å ÿ¥ŸÖÿß ÿ¥ÿßŸÖŸÑ ÿßŸÜŸàÿßÿπ ŸÖ€åŸàŸá Ÿáÿß Ÿà ÿ≥ÿ®ÿ≤€åÿ¨ÿßÿ™ÿå Ÿæÿ±Ÿàÿ™ÿ¶€åŸÜ ÿ®ÿØŸàŸÜ ⁄Üÿ±ÿ®€åÿå ÿ∫ŸÑÿßÿ™ ⁄©ÿßŸÖŸÑ Ÿà ⁄Üÿ±ÿ®€å Ÿáÿß€å ÿ≥ÿßŸÑŸÖ ÿßÿ≥ÿ™. ÿß€åŸÜ ÿ®Ÿá ÿ®ÿØŸÜ ÿ¥ŸÖÿß ⁄©ŸÖ⁄© ŸÖ€å ⁄©ŸÜÿØ ÿ™ÿß ŸÖŸàÿßÿØ ŸÖÿ∫ÿ∞€å ÿ∂ÿ±Ÿàÿ±€å ÿ±ÿß ÿ®ÿ±ÿß€å ÿ®Ÿáÿ™ÿ±€åŸÜ ÿπŸÖŸÑ⁄©ÿ±ÿØ ÿÆŸàÿØ ŸÅÿ±ÿßŸáŸÖ ⁄©ŸÜÿØ Ÿà ŸÖ€å ÿ™ŸàÿßŸÜÿØ ÿ®Ÿá Ÿæ€åÿ¥⁄Ø€åÿ±€å ÿßÿ≤ ÿ®€åŸÖÿßÿ±€å Ÿáÿß€å ŸÖÿ≤ŸÖŸÜ ⁄©ŸÖ⁄© ⁄©ŸÜÿØ.2. ÿØÿ± ŸÅÿπÿßŸÑ€åÿ™ ÿ®ÿØŸÜ€å ŸÖŸÜÿ∏ŸÖ ÿ¥ÿ±⁄©ÿ™ ⁄©ŸÜ€åÿØ: Ÿàÿ±ÿ≤ÿ¥ ÿ®ÿ±ÿß€å ÿ≠ŸÅÿ∏ ÿßÿ≥ÿ™ÿÆŸàÿßŸÜ Ÿáÿßÿå ŸÖÿßŸá€å⁄ÜŸá Ÿáÿß Ÿà ÿ≥ŸÑÿßŸÖÿ™ ŸÇŸÑÿ® Ÿà ÿπÿ±ŸàŸÇ ŸÇŸà€å ÿ®ÿ≥€åÿßÿ± ŸÖŸáŸÖ ÿßÿ≥ÿ™. ÿ≠ÿØÿßŸÇŸÑ 150 ÿØŸÇ€åŸÇŸá Ÿàÿ±ÿ≤ÿ¥ ŸáŸàÿßÿ≤€å ŸÖÿ™Ÿàÿ≥ÿ∑ €åÿß 75 ÿØŸÇ€åŸÇŸá Ÿàÿ±ÿ≤ÿ¥ ÿ¥ÿØ€åÿØ ÿØÿ± ŸáŸÅÿ™Ÿá ÿ±ÿß ŸáÿØŸÅ ŸÇÿ±ÿßÿ± ÿØŸá€åÿØ.3. ÿ®Ÿá ÿßŸÜÿØÿßÿ≤Ÿá ⁄©ÿßŸÅ€å ÿ®ÿÆŸàÿßÿ®€åÿØ: ÿØÿßÿ¥ÿ™ŸÜ ÿÆŸàÿßÿ® ÿ®ÿß ⁄©€åŸÅ€åÿ™ ⁄©ÿßŸÅ€å ÿ®ÿ±ÿß€å ÿ≥ŸÑÿßŸÖÿ™ ÿ¨ÿ≥ŸÖ€å Ÿà ÿ±ŸàÿßŸÜ€å ÿ®ÿ≥€åÿßÿ± ŸÖŸáŸÖ ÿßÿ≥ÿ™. ÿ®Ÿá ÿ™ŸÜÿ∏€åŸÖ ÿÆŸÑŸÇ Ÿà ÿÆŸàÿå ÿ®Ÿáÿ®ŸàÿØ ÿπŸÖŸÑ⁄©ÿ±ÿØ ÿ¥ŸÜÿßÿÆÿ™€å Ÿà ÿ≠ŸÖÿß€åÿ™ ÿßÿ≤ ÿ±ÿ¥ÿØ ÿ≥ÿßŸÑŸÖ Ÿà ÿπŸÖŸÑ⁄©ÿ±ÿØ ÿ≥€åÿ≥ÿ™ŸÖ ÿß€åŸÖŸÜ€å ⁄©ŸÖ⁄© ŸÖ€å ⁄©ŸÜÿØ. Ÿáÿ± ÿ¥ÿ® €∑ ÿ™ÿß €π ÿ≥ÿßÿπÿ™ ÿ®ÿÆŸàÿßÿ®€åÿØ.\nÿ≥Ÿá ÿ±ŸÜ⁄Ø ÿßÿµŸÑ€å ⁄©ÿØÿßŸÖŸÜÿØÿü\nÿ≥Ÿá ÿ±ŸÜ⁄Ø ÿßÿµŸÑ€å ŸÇÿ±ŸÖÿ≤ÿå ÿ¢ÿ®€å Ÿà ÿ≤ÿ±ÿØ Ÿáÿ≥ÿ™ŸÜÿØ. ÿß€åŸÜ ÿ±ŸÜ⁄Ø‚ÄåŸáÿß ÿ±ÿß ÿ®Ÿá ÿß€åŸÜ ÿØŸÑ€åŸÑ ÿßÿµŸÑ€å ŸÖ€å‚ÄåŸÜÿßŸÖŸÜÿØ ⁄©Ÿá ÿ®ÿß ÿßÿÆÿ™ŸÑÿßÿ∑ ÿ±ŸÜ⁄Ø‚ÄåŸáÿß€å ÿØ€å⁄Øÿ± ÿß€åÿ¨ÿßÿØ ŸÜŸÖ€å‚Äåÿ¥ŸàŸÜÿØ Ÿà ŸáŸÖŸá ÿ±ŸÜ⁄Ø‚ÄåŸáÿß€å ÿØ€å⁄Øÿ± ÿ±ÿß ŸÖ€å‚Äåÿ™ŸàÿßŸÜ ÿ®ÿß ÿ™ÿ±⁄©€åÿ® ÿ¢ŸÜŸáÿß ÿ®Ÿá ŸÜÿ≥ÿ®ÿ™‚ÄåŸáÿß€å ŸÖÿÆÿ™ŸÑŸÅ ÿ≥ÿßÿÆÿ™. ÿØÿ± ÿ≥€åÿ≥ÿ™ŸÖ ÿ±ŸÜ⁄Ø ÿßŸÅÿ≤ŸàÿØŸÜ€å ⁄©Ÿá ÿ®ÿ±ÿß€å ŸÜŸàÿ± ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€å ÿ¥ŸàÿØÿå ÿ±ŸÜ⁄Ø Ÿáÿß€å ÿßÿµŸÑ€å ŸÇÿ±ŸÖÿ≤ÿå ÿ≥ÿ®ÿ≤ Ÿà ÿ¢ÿ®€å (RGB) Ÿáÿ≥ÿ™ŸÜÿØ.\nÿ≥ÿßÿÆÿ™ÿßÿ± €å⁄© ÿßÿ™ŸÖ ÿ±ÿß ÿ™Ÿàÿ∂€åÿ≠ ÿØŸá€åÿØ.\nÿßÿ™ŸÖ ÿ®ŸÑŸà⁄© ÿßÿµŸÑ€å ŸáŸÖŸá ŸÖŸàÿßÿØ ÿßÿ≥ÿ™ Ÿà ÿßÿ≤ ÿ≥Ÿá ŸÜŸàÿπ ÿ∞ÿ±Ÿá ÿ™ÿ¥⁄©€åŸÑ ÿ¥ÿØŸá ÿßÿ≥ÿ™: Ÿæÿ±Ÿàÿ™ŸàŸÜÿå ŸÜŸàÿ™ÿ±ŸàŸÜ Ÿà ÿßŸÑ⁄©ÿ™ÿ±ŸàŸÜ. ÿ≥ÿßÿÆÿ™ÿßÿ± €å⁄© ÿßÿ™ŸÖ ÿ±ÿß ŸÖ€å ÿ™ŸàÿßŸÜ ÿ®Ÿá ÿπŸÜŸàÿßŸÜ €å⁄© Ÿáÿ≥ÿ™Ÿá ÿØÿ± ŸÖÿ±⁄©ÿ≤ ⁄©Ÿá ÿ™Ÿàÿ≥ÿ∑ ÿßÿ®ÿ±€å ÿßÿ≤ ÿßŸÑ⁄©ÿ™ÿ±ŸàŸÜ ÿßÿ≠ÿßÿ∑Ÿá ÿ¥ÿØŸá ÿßÿ≥ÿ™ ÿ™Ÿàÿµ€åŸÅ ⁄©ÿ±ÿØ.Ÿáÿ≥ÿ™Ÿá €å⁄© ÿßÿ™ŸÖ ÿßÿ≤ Ÿæÿ±Ÿàÿ™ŸàŸÜ Ÿà ŸÜŸàÿ™ÿ±ŸàŸÜ ÿ™ÿ¥⁄©€åŸÑ ÿ¥ÿØŸá ÿßÿ≥ÿ™. Ÿæÿ±Ÿàÿ™ŸàŸÜ Ÿáÿß ÿ∞ÿ±ÿßÿ™ ÿ®ÿß ÿ®ÿßÿ± ŸÖÿ´ÿ®ÿ™ Ÿà ŸÜŸàÿ™ÿ±ŸàŸÜ Ÿáÿß ÿ∞ÿ±ÿßÿ™ ÿÆŸÜÿ´€å ÿ®ÿØŸàŸÜ ÿ®ÿßÿ± Ÿáÿ≥ÿ™ŸÜÿØ. Ÿáÿ± ÿØŸà€å ÿß€åŸÜ ÿ∞ÿ±ÿßÿ™ ÿØÿ± Ÿáÿ≥ÿ™Ÿá ÿßÿ™ŸÖ ŸÇÿ±ÿßÿ± ÿØÿßÿ±ŸÜÿØ ⁄©Ÿá ÿØÿ± ŸÖÿ±⁄©ÿ≤ ÿßÿ™ŸÖ ŸÇÿ±ÿßÿ± ÿØÿßÿ±ÿØ Ÿà ÿ®€åÿ¥ÿ™ÿ± ÿ¨ÿ±ŸÖ ÿßÿ™ŸÖ ÿ±ÿß ÿØÿ± ÿÆŸàÿØ ÿØÿßÿ±ÿØ.ÿØŸàÿ± Ÿáÿ≥ÿ™Ÿá ÿßÿ™ŸÖ ÿßÿ®ÿ±€å ÿßÿ≤ ÿßŸÑ⁄©ÿ™ÿ±ŸàŸÜ ÿßÿ≥ÿ™. ÿßŸÑ⁄©ÿ™ÿ±ŸàŸÜ Ÿáÿß ÿ∞ÿ±ÿßÿ™€å ÿ®ÿß ÿ®ÿßÿ± ŸÖŸÜŸÅ€å Ÿáÿ≥ÿ™ŸÜÿØ ⁄©Ÿá ÿØÿ± ÿßÿ∑ÿ±ÿßŸÅ Ÿáÿ≥ÿ™Ÿá ÿØÿ± ÿ≠ÿ±⁄©ÿ™ ÿ´ÿßÿ®ÿ™ Ÿáÿ≥ÿ™ŸÜÿØ. ÿßÿ®ÿ± ÿßŸÑ⁄©ÿ™ÿ±ŸàŸÜ€å ÿ®Ÿá ŸæŸàÿ≥ÿ™Ÿá €åÿß ÿßŸàÿ±ÿ®€åÿ™ÿßŸÑ ÿ™ŸÇÿ≥€åŸÖ ŸÖ€å ÿ¥ŸàÿØ Ÿà Ÿáÿ± ŸæŸàÿ≥ÿ™Ÿá ŸÖ€å ÿ™ŸàÿßŸÜÿØ ÿ™ÿπÿØÿßÿØ ŸÖÿπ€åŸÜ€å ÿßŸÑ⁄©ÿ™ÿ±ŸàŸÜ ÿ±ÿß ÿØÿ± ÿÆŸàÿØ ÿ¨ÿß€å ÿØŸáÿØ. ÿ™ÿπÿØÿßÿØ ÿßŸÑ⁄©ÿ™ÿ±ŸàŸÜ‚ÄåŸáÿß ÿØÿ± ÿ®€åÿ±ŸàŸÜ€å‚Äåÿ™ÿ±€åŸÜ ŸæŸàÿ≥ÿ™Ÿá ⁄©Ÿá ŸÑÿß€åŸá ÿ∏ÿ±ŸÅ€åÿ™€å ŸÜÿßŸÖ ÿØÿßÿ±ÿØÿå Ÿà€å⁄ò⁄Ø€å‚ÄåŸáÿß€å ÿ¥€åŸÖ€åÿß€å€å ÿßÿ™ŸÖ ÿ±ÿß ÿ™ÿπ€å€åŸÜ ŸÖ€å‚Äå⁄©ŸÜÿØ.ÿØÿ± €å⁄© ÿßÿ™ŸÖ ÿÆŸÜÿ´€åÿå ÿ™ÿπÿØÿßÿØ Ÿæÿ±Ÿàÿ™ŸàŸÜ Ÿáÿß€å Ÿáÿ≥ÿ™Ÿá ÿ®ÿ±ÿßÿ®ÿ± ÿ®ÿß ÿ™ÿπÿØÿßÿØ ÿßŸÑ⁄©ÿ™ÿ±ŸàŸÜ Ÿáÿß€å ÿßÿ®ÿ± ÿßŸÑ⁄©ÿ™ÿ±ŸàŸÜ€å ÿßÿ≥ÿ™ÿå ÿ®ŸÜÿßÿ®ÿ±ÿß€åŸÜ ÿ®ÿßÿ±Ÿáÿß€å ŸÖÿ´ÿ®ÿ™ Ÿà ŸÖŸÜŸÅ€å ŸÖÿ™ÿπÿßÿØŸÑ ŸÖ€å ÿ¥ŸàŸÜÿØ Ÿà ÿßÿ™ŸÖ ÿ®ÿßÿ± ⁄©ŸÑ€å ŸÜÿØÿßÿ±ÿØ. ÿ™ÿπÿØÿßÿØ Ÿæÿ±Ÿàÿ™ŸàŸÜ Ÿáÿß ⁄©Ÿá ÿπÿØÿØ ÿßÿ™ŸÖ€å ŸÜ€åÿ≤ ŸÜÿßŸÖ€åÿØŸá ŸÖ€å ÿ¥ŸàÿØÿå ÿ™ÿπ€å€åŸÜ ŸÖ€å ⁄©ŸÜÿØ ⁄©Ÿá ÿßÿ™ŸÖ ⁄ÜŸá ÿπŸÜÿµÿ±€å ÿßÿ≥ÿ™.\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nwiki_triplet\nDataset: wiki_triplet\nSize: 191,929 training samples\nColumns: anchor, positive, and negative\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 15 tokensmean: 41.49 tokensmax: 173 tokens\nmin: 14 tokensmean: 43.65 tokensmax: 183 tokens\nmin: 14 tokensmean: 40.19 tokensmax: 156 tokens\nSamples:\nanchor\npositive\nnegative\n-ÿß€åŸÜ ÿ±Ÿàÿ¥ ÿ®ÿ±ÿß€å ŸÅÿß€åŸÑ Ÿáÿß€å ÿ®ÿ≥€åÿßÿ± ⁄©Ÿà⁄Ü⁄© ŸÖŸÜÿßÿ≥ÿ® ÿßÿ≥ÿ™ ⁄ÜŸàŸÜ ÿØÿ≥ÿ™ÿ±ÿ≥€å ÿ®Ÿá ÿ±⁄©Ÿàÿ±ÿØŸáÿß ÿØÿ± ÿ¢ŸÜ Ÿáÿß ÿ≥ÿ±€åÿπŸÜÿ± ÿßÿ≥ÿ™ .\n-ÿ®Ÿá⁄© ŸÖÿØ€åÿ±€åÿ™ ÿ≠ÿßŸÅÿ∏Ÿá ŸÇŸà€å Ÿà ŸÖŸÜÿßÿ≥ÿ® ŸÜ€åÿßÿ≤ ÿØÿßÿ±ÿØ ÿ™ÿß ⁄©ÿßÿ±ÿß€å€å ÿßÿ≤ ÿØÿ≥ÿ™ ŸÜÿ±ŸàÿØ .\nŸÅÿ±ÿß€åŸÜÿØ Ÿæÿ¥ÿ™€åÿ®ÿßŸÜ€å (ÿ®Ÿá ÿßŸÜ⁄ØŸÑ€åÿ≥€å : back up) ÿ®Ÿá ÿ±ŸàŸÜŸàÿ¥ÿ™ ÿ®ÿ±ÿØÿßÿ¥ÿ™ŸÜ ÿßÿ≤ Ÿæÿ±ŸàŸÜÿØŸá Ÿáÿß€å ŸÅ€åÿ≤€å⁄©€å €åÿß ŸÖÿ¨ÿßÿ≤€å Ÿà Ÿæÿß€å⁄ØÿßŸá ÿØÿßÿØŸá Ÿáÿß ÿØÿ± €å⁄© ÿ≥ÿß€åÿ™ ÿ´ÿßŸÜŸà€åŸá ÿ®ÿ±ÿß€å ÿ¥ÿ±ÿß€åÿ∑€å ⁄©Ÿá ÿ≥ÿßŸÖÿßŸÜŸá ÿßÿ≤ ⁄©ÿßÿ± ÿ®€åŸÅÿ™ÿØ ÿå ÿßÿ∑ŸÑÿßŸÇ ŸÖ€å ÿ¥ŸàÿØ .\nÿ∂ÿ≠ÿß⁄© ÿ≥ŸæÿßŸá ŸÅÿ±ÿßŸàÿßŸÜ€å ÿ¢ŸÖÿßÿØŸá ⁄©ÿ±ÿØ Ÿà ÿ®Ÿá ÿØÿ≥ÿ™⁄Ø€åÿ±€å ÿ¨ŸÖÿ¥€åÿØ ŸÅÿ±ÿ≥ÿ™ÿßÿØ .\nÿ¨ŸÖÿ¥€åÿØ ÿØŸà ÿØÿÆÿ™ÿ± ÿÆŸàÿ® ÿ±Ÿà ÿØÿßÿ¥ÿ™ : €å⁄©€å ÿ¥Ÿáÿ±ŸÜÿßÿ≤ Ÿà ÿØ€å⁄Øÿ±€å ÿßÿ±ŸÜŸàÿßÿ≤.ÿß€åŸÜ ÿØŸà ŸÜ€åÿ≤ ÿØÿ± ÿØÿ≥ÿ™ ÿ∂ÿ≠ÿß⁄© ÿ≥ÿ™ŸÖ⁄Øÿ± ÿßÿ≥€åÿ± ÿ¥ÿØŸÜÿØ Ÿà ÿßÿ≤ ÿ™ÿ±ÿ≥ ÿ®Ÿá ŸÅÿ±ŸÖÿßŸÜ ÿßŸà ÿØÿ±ÿ¢ŸÖÿØŸÜÿØ .\nŸÅÿ±ÿßŸÜ⁄© ÿå ŸÖÿßÿØÿ± ŸÅÿ±€åÿØŸàŸÜ ÿå ÿ®€å ÿ¥ŸàŸáÿ± ŸÖÿßŸÜÿØ Ÿà ŸàŸÇÿ™€å ÿØÿßŸÜÿ≥ÿ™ ÿ∂ÿ≠ÿß⁄© ÿØÿ± ÿÆŸàÿßÿ® ÿØ€åÿØŸá ⁄©Ÿá ÿ¥⁄©ÿ≥ÿ™ÿ¥ ÿ®Ÿá ÿØÿ≥ÿ™ ŸÅÿ±€åÿØŸàŸÜ ÿßÿ≥ÿ™ ÿ®€åŸÖŸÜÿß⁄© ÿ¥ÿØ .\nÿ™ÿØ⁄©ÿ≥ ÿ¨ŸàÿßŸÜÿßŸÜ ÿ®ÿ±ŸÜÿßŸÖŸá Ÿáÿß€å€å ŸÖÿ≥ÿ™ŸÇŸÑ ÿßÿ≥ÿ™ ⁄©Ÿá ÿ®ÿ±ÿß€å ÿØÿßŸÜÿ¥ ÿ¢ŸÖŸàÿ≤ÿßŸÜ ŸÖŸÇÿ∑ÿπ €∑ ÿ™ÿß €±€≤ ÿ®ÿ±⁄Øÿ≤ÿßÿ± ŸÖ€å ÿ¥ŸàÿØ .\nÿßŸàŸÑ€åŸÜ ÿ™ÿØ⁄©ÿ≥ ÿß€åÿ±ÿßŸÜ ÿØÿ± €≤€µ ÿ®ŸáŸÖŸÜ ÿ≥ÿßŸÑ €±€≥€π€≤ ÿØÿ± ÿ™Ÿáÿ±ÿßŸÜ ÿ®ÿß ÿπŸÜŸàÿßŸÜ tedxtehran ÿ®ÿ±⁄Øÿ≤ÿßÿ± ÿ¥ÿØ .\n⁄©ÿ™ÿßÿ®Ÿáÿß€å€å ÿßÿµŸÑ€å Ÿáÿ≥ÿ™ŸÜÿØ . ŸÖÿßŸÜŸÜÿØ ŸÖÿ∞ÿß⁄©ÿ±ÿßÿ™ ÿå ⁄©ÿ™ÿßÿ® Ÿáÿß€å ÿ™ÿØ ÿ®Ÿá ÿßŸÜÿØÿßÿ≤Ÿá ⁄©ÿßŸÅ€å ÿ®ÿ±ÿß€å ⁄©ÿ¥ŸÅ ÿß€åÿØŸá ŸÇŸà€å Ÿæÿ±ŸÖÿ≠ÿ™Ÿàÿß Ÿáÿ≥ÿ™ŸÜÿØ Ÿà ÿØÿ± ÿπ€åŸÜ ÿ≠ÿßŸÑ ÿ®Ÿá ÿßŸÜÿØÿßÿ≤Ÿá ÿß€å ⁄©ŸÖ ÿ≠ÿ¨ŸÖ Ÿáÿ≥ÿ™ŸÜÿØ ⁄©Ÿá ÿØÿ± €å⁄© ŸÖÿØÿ™ ÿ≤ŸÖÿßŸÜ ⁄©Ÿàÿ™ÿßŸá ÿ®ÿ™ŸàÿßŸÜ ÿ¢ŸÜ ÿ±ÿß ÿÆŸàÿßŸÜÿØ .\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nwiki_DSimilar_pair2class\nDataset: wiki_DSimilar_pair2class\nSize: 137,402 training samples\nColumns: sentence1, sentence2, and label\nApproximate statistics based on the first 1000 samples:\nsentence1\nsentence2\nlabel\ntype\nstring\nstring\nint\ndetails\nmin: 14 tokensmean: 40.0 tokensmax: 143 tokens\nmin: 15 tokensmean: 41.49 tokensmax: 165 tokens\n0: ~54.10%1: ~45.90%\nSamples:\nsentence1\nsentence2\nlabel\nÿ¥ŸÖÿßŸÑ€å ÿ™ÿ±€åŸÜ ŸÜŸÇÿ∑Ÿá ÿß€åÿßŸÑÿ™ ÿ¢ŸÑÿßÿ®ÿßŸÖÿß ÿØÿ± ŸÜŸá Ÿà ŸÜ€åŸÖ ⁄©€åŸÑŸàŸÖÿ™ÿ±€å ÿ¥ŸÖÿßŸÑ ÿ∫ÿ±ÿ® ÿ¥Ÿáÿ± Ÿàÿßÿ™ÿ±ŸÑŸà ÿØÿ± ÿ¥Ÿáÿ±ÿ≥ÿ™ÿßŸÜ ŸÑŸàÿØÿ±ÿØ€åŸÑ ÿØÿ± ÿ¥ŸÖÿßŸÑ ÿ∫ÿ±ÿ®€å ÿ™ÿ±€åŸÜ ŸÜŸÇÿ∑Ÿá ÿß€åÿßŸÑÿ™ ŸÇÿ±ÿßÿ± ÿØÿßÿ±ÿØ .\nÿ¥ÿ±⁄©ÿ™ Ÿáÿß€å ŸÖÿÆÿßÿ®ÿ±ÿßÿ™€å ŸÖÿßŸÜŸÜÿØ ÿß€å ÿ™€å ÿßŸÜÿØ ÿ™€å ÿ≠ÿ∂Ÿàÿ± Ÿæÿ± ÿ±ŸÜ⁄Ø€å ÿØÿ± ÿ¢ŸÑÿßÿ®ÿßŸÖÿß ÿØÿßÿ±ŸÜÿØ .\n0\nÿØÿßÿ≥ÿ™ÿßŸÜ ŸÖÿπÿ±ŸàŸÅ ŸÖŸàÿ¨ŸàÿØ ÿØÿ± ÿ≥€åÿ±Ÿá ŸÜŸÇŸÑ ŸÖ€å ⁄©ŸÜÿØ ⁄©Ÿá ŸàŸÇÿ™€å ŸÖÿ≠ŸÖÿØ ÿßÿ≤ ÿ≥Ÿá ÿ≥ÿ§ÿßŸÑ ŸÖÿ∑ÿ±ÿ≠ ÿ¥ÿØŸá ÿßÿ≤ ÿ≥Ÿà€å ÿ±ÿ®ÿß€å Ÿáÿß ŸÖÿ∑ŸÑÿπ ⁄Øÿ¥ÿ™ ÿå ÿßÿπŸÑÿßŸÖ ⁄©ÿ±ÿØ ⁄©Ÿá ÿµÿ®ÿ≠ Ÿæÿßÿ≥ÿÆ Ÿáÿß ÿ±ÿß ÿÆŸàÿßŸáÿØ ÿØÿßÿ¥ÿ™ .\nÿØÿ± ŸÖ€åÿßŸÜ ÿπŸÑŸÖÿß€å ŸÖÿ≥ŸÑŸÖÿßŸÜ ÿå ÿß€åŸÜ ÿ™ŸÖ ŸáŸà€åÿ™ €åÿßÿ®€å ÿ∞ŸàÿßŸÑŸÇÿ±ŸÜ€åŸÜ ÿ®ÿß ÿßÿ≥⁄©ŸÜÿØÿ± ⁄©ÿ®€åÿ± ÿå ÿ®Ÿá ŸÜÿ∏ÿ± ŸÖ€å ÿ±ÿ≥ÿØ ⁄©Ÿá ÿØÿ± ÿß€åŸÜ ÿ¨ÿß ÿ≥ÿ±⁄Üÿ¥ŸÖŸá ⁄Øÿ±ŸÅÿ™Ÿá .\n1\nÿ®ÿß ÿ±ÿ¥ÿØ ŸÖÿ≥ÿ™ŸÖÿ± ÿßŸÇÿ™ÿµÿßÿØ ŸàŸÜÿ≤Ÿàÿ¶ŸÑÿß€å ŸÜŸÅÿ™ ÿÆ€åÿ≤ ÿØÿ± ŸÇÿ±ŸÜ ÿ®€åÿ≥ÿ™ŸÖ ÿå ⁄©ÿßÿ±ÿß⁄©ÿßÿ≥ ÿ™ÿ®ÿØ€åŸÑ ÿ®Ÿá⁄©€å ÿßÿ≤ ŸÖÿ±ÿß⁄©ÿ≤ ÿßŸÇÿ™ÿµÿßÿØ€å ŸÖŸáŸÖ ÿ¢ŸÖÿ±€å⁄©ÿß€å ŸÑÿßÿ™€åŸÜ ÿ¥ÿØ ÿå Ÿà ŸÜ€åÿ≤ ÿß€åŸÜ ÿ¥Ÿáÿ± ÿ™ÿ®ÿØ€åŸÑ ÿ®Ÿá ŸÖÿ±⁄©ÿ≤ ÿßÿµŸÑ€å ÿ±Ÿàÿßÿ®ÿ∑ ÿßÿ±ŸàŸæÿß Ÿà ÿ¢ŸÖÿ±€å⁄©ÿß€å ÿ¨ŸÜŸàÿ®€å ÿ¥ÿØ .\nÿ®ÿ±ÿ¨ÿ≥ÿ™Ÿá ÿ™ÿ±€åŸÜ ÿ™€åŸÖ Ÿáÿß€å ŸÅŸàÿ™ÿ®ÿßŸÑ Ÿà ÿ®€åÿ≥ÿ®ÿßŸÑ ÿØÿ± ⁄©ÿßÿ±ÿß⁄©ÿßÿ≥ ŸÇÿ±ÿßÿ± ÿØÿßÿ±ŸÜÿØ .\n0\nLoss: ContrastiveLoss with these parameters:{\n\"distance_metric\": \"SiameseDistanceMetric.COSINE_DISTANCE\",\n\"margin\": 0.5,\n\"size_average\": true\n}\npersianQA_pair\nDataset: persianQA_pair at 983eff5\nSize: 9,008 training samples\nColumns: positive and anchor\nApproximate statistics based on the first 1000 samples:\npositive\nanchor\ntype\nstring\nstring\ndetails\nmin: 135 tokensmean: 266.58 tokensmax: 512 tokens\nmin: 5 tokensmean: 12.4 tokensmax: 34 tokens\nSamples:\npositive\nanchor\nÿ¥ÿ±⁄©ÿ™ ŸÅŸàŸÑÿßÿØ ŸÖÿ®ÿßÿ±⁄©€Ä ÿßÿµŸÅŸáÿßŸÜÿå ÿ®ÿ≤ÿ±⁄Ø‚Äåÿ™ÿ±€åŸÜ Ÿàÿßÿ≠ÿØ ÿµŸÜÿπÿ™€å ÿÆÿµŸàÿµ€å ÿØÿ± ÿß€åÿ±ÿßŸÜ Ÿà ÿ®ÿ≤ÿ±⁄Ø‚Äåÿ™ÿ±€åŸÜ ŸÖÿ¨ÿ™ŸÖÿπ ÿ™ŸàŸÑ€åÿØ ŸÅŸàŸÑÿßÿØ ÿØÿ± ⁄©ÿ¥Ÿàÿ± ÿß€åÿ±ÿßŸÜ ÿßÿ≥ÿ™ÿå ⁄©Ÿá ÿØÿ± ÿ¥ÿ±ŸÇ ÿ¥Ÿáÿ± ŸÖÿ®ÿßÿ±⁄©Ÿá ŸÇÿ±ÿßÿ± ÿØÿßÿ±ÿØ. ŸÅŸàŸÑÿßÿØ ŸÖÿ®ÿßÿ±⁄©Ÿá ŸáŸÖ‚Äåÿß⁄©ŸÜŸàŸÜ ŸÖÿ≠ÿ±⁄© ÿ®ÿ≥€åÿßÿ±€å ÿßÿ≤ ÿµŸÜÿß€åÿπ ÿ®ÿßŸÑÿßÿØÿ≥ÿ™€å Ÿà Ÿæÿß€å€åŸÜ‚ÄåÿØÿ≥ÿ™€å ÿßÿ≥ÿ™. ŸÅŸàŸÑÿßÿØ ŸÖÿ®ÿßÿ±⁄©Ÿá ÿØÿ± €±€± ÿØŸàÿ±Ÿá ÿ¨ÿß€åÿ≤€Ä ŸÖŸÑ€å ÿ™ÿπÿßŸÑ€å ÿ≥ÿßÿ≤ŸÖÿßŸÜ€å Ÿà €∂ ÿØŸàÿ±Ÿá ÿ¨ÿß€åÿ≤€Ä ÿ¥ÿ±⁄©ÿ™ ÿØÿßŸÜÿ¥€å ÿØÿ± ⁄©ÿ¥Ÿàÿ± ÿ±ÿ™ÿ®€Ä ŸÜÿÆÿ≥ÿ™ ÿ±ÿß ÿ®ÿØÿ≥ÿ™ ÿ¢Ÿàÿ±ÿØŸá‚Äåÿßÿ≥ÿ™ Ÿà ŸáŸÖ⁄ÜŸÜ€åŸÜ ÿß€åŸÜ ÿ¥ÿ±⁄©ÿ™ ÿØÿ± ÿ≥ÿßŸÑ €±€≥€π€± ÿ®ÿ±ÿß€å ŸÜÿÆÿ≥ÿ™€åŸÜ‚Äåÿ®ÿßÿ± ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ÿ™ŸÜŸáÿß ÿ¥ÿ±⁄©ÿ™ ÿß€åÿ±ÿßŸÜ€å ÿ®ÿß ⁄©ÿ≥ÿ® ÿßŸÖÿ™€åÿßÿ≤ €∂€µ€¥ ÿ™ŸÜÿØ€åÿ≥ ÿ≤ÿ±€åŸÜ ÿ¨ÿß€åÿ≤€Ä ŸÖŸÑ€å ÿ™ÿπÿßŸÑ€å ÿ≥ÿßÿ≤ŸÖÿßŸÜ€å ÿ±ÿß ÿßÿ≤ ÿ¢ŸÜ ÿÆŸàÿØ ⁄©ŸÜÿØ. ÿ¥ÿ±⁄©ÿ™ ŸÅŸàŸÑÿßÿØ ŸÖÿ®ÿßÿ±⁄©€Ä ÿßÿµŸÅŸáÿßŸÜ ÿØÿ± €≤€≥ ÿØ€å ŸÖÿßŸá €±€≥€∑€± ÿßÿ≠ÿØÿßÿ´ ÿ¥ÿØ Ÿà ÿß⁄©ŸÜŸàŸÜ ÿ®ÿ≤ÿ±⁄Ø‚Äåÿ™ÿ±€åŸÜ Ÿàÿßÿ≠ÿØŸáÿß€å ÿµŸÜÿπÿ™€å Ÿà ÿ®ÿ≤ÿ±⁄Øÿ™ÿ±€åŸÜ ŸÖÿ¨ÿ™ŸÖÿπ ÿ™ŸàŸÑ€åÿØ ŸÅŸàŸÑÿßÿØ ÿØÿ± ÿß€åÿ±ÿßŸÜ ÿßÿ≥ÿ™. ÿß€åŸÜ ÿ¥ÿ±⁄©ÿ™ ÿØÿ± ÿ≤ŸÖ€åŸÜ€å ÿ®Ÿá ŸÖÿ≥ÿßÿ≠ÿ™ €≥€µ ⁄©€åŸÑŸàŸÖÿ™ÿ± ŸÖÿ±ÿ®ÿπ ÿØÿ± ŸÜÿ≤ÿØ€å⁄©€å ÿ¥Ÿáÿ± ŸÖÿ®ÿßÿ±⁄©Ÿá Ÿà ÿØÿ± €∑€µ ⁄©€åŸÑŸàŸÖÿ™ÿ±€å ÿ¨ŸÜŸàÿ® ÿ∫ÿ±ÿ®€å ÿ¥Ÿáÿ± ÿßÿµŸÅŸáÿßŸÜ ŸàÿßŸÇÿπ ÿ¥ÿØŸá‚Äåÿßÿ≥ÿ™. ŸÖÿµÿ±ŸÅ ÿ¢ÿ® ÿß€åŸÜ ⁄©ÿßÿ±ÿÆÿßŸÜŸá ÿØÿ± ⁄©ŸÖÿ™ÿ±€åŸÜ ŸÖ€åÿ≤ÿßŸÜ ÿÆŸàÿØÿå €±Ÿ´€µŸ™ ÿßÿ≤ ÿØÿ®€å ÿ≤ÿß€åŸÜÿØŸá‚Äåÿ±ŸàÿØ ÿ®ÿ±ÿßÿ®ÿ± ÿ≥ÿßŸÑÿßŸÜŸá €≤€≥ ŸÖ€åŸÑ€åŸàŸÜ ŸÖÿ™ÿ± ŸÖ⁄©ÿπÿ® ÿØÿ± ÿ≥ÿßŸÑ ÿßÿ≥ÿ™ Ÿà ÿÆŸàÿØ €å⁄©€å ÿßÿ≤ ÿπŸàÿßŸÖŸÑ ⁄©ŸÖ‚Äåÿ¢ÿ®€å ÿ≤ÿß€åŸÜÿØŸá‚Äåÿ±ŸàÿØ ÿ¥ŸÜÿßÿÆÿ™Ÿá ŸÖ€å‚Äåÿ¥ŸàÿØ.\nÿ¥ÿ±⁄©ÿ™ ŸÅŸàŸÑÿßÿØ ŸÖÿ®ÿßÿ±⁄©Ÿá ÿØÿ± ⁄©ÿ¨ÿß ŸàÿßŸÇÿπ ÿ¥ÿØŸá ÿßÿ≥ÿ™\nÿ¥ÿ±⁄©ÿ™ ŸÅŸàŸÑÿßÿØ ŸÖÿ®ÿßÿ±⁄©€Ä ÿßÿµŸÅŸáÿßŸÜÿå ÿ®ÿ≤ÿ±⁄Ø‚Äåÿ™ÿ±€åŸÜ Ÿàÿßÿ≠ÿØ ÿµŸÜÿπÿ™€å ÿÆÿµŸàÿµ€å ÿØÿ± ÿß€åÿ±ÿßŸÜ Ÿà ÿ®ÿ≤ÿ±⁄Ø‚Äåÿ™ÿ±€åŸÜ ŸÖÿ¨ÿ™ŸÖÿπ ÿ™ŸàŸÑ€åÿØ ŸÅŸàŸÑÿßÿØ ÿØÿ± ⁄©ÿ¥Ÿàÿ± ÿß€åÿ±ÿßŸÜ ÿßÿ≥ÿ™ÿå ⁄©Ÿá ÿØÿ± ÿ¥ÿ±ŸÇ ÿ¥Ÿáÿ± ŸÖÿ®ÿßÿ±⁄©Ÿá ŸÇÿ±ÿßÿ± ÿØÿßÿ±ÿØ. ŸÅŸàŸÑÿßÿØ ŸÖÿ®ÿßÿ±⁄©Ÿá ŸáŸÖ‚Äåÿß⁄©ŸÜŸàŸÜ ŸÖÿ≠ÿ±⁄© ÿ®ÿ≥€åÿßÿ±€å ÿßÿ≤ ÿµŸÜÿß€åÿπ ÿ®ÿßŸÑÿßÿØÿ≥ÿ™€å Ÿà Ÿæÿß€å€åŸÜ‚ÄåÿØÿ≥ÿ™€å ÿßÿ≥ÿ™. ŸÅŸàŸÑÿßÿØ ŸÖÿ®ÿßÿ±⁄©Ÿá ÿØÿ± €±€± ÿØŸàÿ±Ÿá ÿ¨ÿß€åÿ≤€Ä ŸÖŸÑ€å ÿ™ÿπÿßŸÑ€å ÿ≥ÿßÿ≤ŸÖÿßŸÜ€å Ÿà €∂ ÿØŸàÿ±Ÿá ÿ¨ÿß€åÿ≤€Ä ÿ¥ÿ±⁄©ÿ™ ÿØÿßŸÜÿ¥€å ÿØÿ± ⁄©ÿ¥Ÿàÿ± ÿ±ÿ™ÿ®€Ä ŸÜÿÆÿ≥ÿ™ ÿ±ÿß ÿ®ÿØÿ≥ÿ™ ÿ¢Ÿàÿ±ÿØŸá‚Äåÿßÿ≥ÿ™ Ÿà ŸáŸÖ⁄ÜŸÜ€åŸÜ ÿß€åŸÜ ÿ¥ÿ±⁄©ÿ™ ÿØÿ± ÿ≥ÿßŸÑ €±€≥€π€± ÿ®ÿ±ÿß€å ŸÜÿÆÿ≥ÿ™€åŸÜ‚Äåÿ®ÿßÿ± ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ÿ™ŸÜŸáÿß ÿ¥ÿ±⁄©ÿ™ ÿß€åÿ±ÿßŸÜ€å ÿ®ÿß ⁄©ÿ≥ÿ® ÿßŸÖÿ™€åÿßÿ≤ €∂€µ€¥ ÿ™ŸÜÿØ€åÿ≥ ÿ≤ÿ±€åŸÜ ÿ¨ÿß€åÿ≤€Ä ŸÖŸÑ€å ÿ™ÿπÿßŸÑ€å ÿ≥ÿßÿ≤ŸÖÿßŸÜ€å ÿ±ÿß ÿßÿ≤ ÿ¢ŸÜ ÿÆŸàÿØ ⁄©ŸÜÿØ. ÿ¥ÿ±⁄©ÿ™ ŸÅŸàŸÑÿßÿØ ŸÖÿ®ÿßÿ±⁄©€Ä ÿßÿµŸÅŸáÿßŸÜ ÿØÿ± €≤€≥ ÿØ€å ŸÖÿßŸá €±€≥€∑€± ÿßÿ≠ÿØÿßÿ´ ÿ¥ÿØ Ÿà ÿß⁄©ŸÜŸàŸÜ ÿ®ÿ≤ÿ±⁄Ø‚Äåÿ™ÿ±€åŸÜ Ÿàÿßÿ≠ÿØŸáÿß€å ÿµŸÜÿπÿ™€å Ÿà ÿ®ÿ≤ÿ±⁄Øÿ™ÿ±€åŸÜ ŸÖÿ¨ÿ™ŸÖÿπ ÿ™ŸàŸÑ€åÿØ ŸÅŸàŸÑÿßÿØ ÿØÿ± ÿß€åÿ±ÿßŸÜ ÿßÿ≥ÿ™. ÿß€åŸÜ ÿ¥ÿ±⁄©ÿ™ ÿØÿ± ÿ≤ŸÖ€åŸÜ€å ÿ®Ÿá ŸÖÿ≥ÿßÿ≠ÿ™ €≥€µ ⁄©€åŸÑŸàŸÖÿ™ÿ± ŸÖÿ±ÿ®ÿπ ÿØÿ± ŸÜÿ≤ÿØ€å⁄©€å ÿ¥Ÿáÿ± ŸÖÿ®ÿßÿ±⁄©Ÿá Ÿà ÿØÿ± €∑€µ ⁄©€åŸÑŸàŸÖÿ™ÿ±€å ÿ¨ŸÜŸàÿ® ÿ∫ÿ±ÿ®€å ÿ¥Ÿáÿ± ÿßÿµŸÅŸáÿßŸÜ ŸàÿßŸÇÿπ ÿ¥ÿØŸá‚Äåÿßÿ≥ÿ™. ŸÖÿµÿ±ŸÅ ÿ¢ÿ® ÿß€åŸÜ ⁄©ÿßÿ±ÿÆÿßŸÜŸá ÿØÿ± ⁄©ŸÖÿ™ÿ±€åŸÜ ŸÖ€åÿ≤ÿßŸÜ ÿÆŸàÿØÿå €±Ÿ´€µŸ™ ÿßÿ≤ ÿØÿ®€å ÿ≤ÿß€åŸÜÿØŸá‚Äåÿ±ŸàÿØ ÿ®ÿ±ÿßÿ®ÿ± ÿ≥ÿßŸÑÿßŸÜŸá €≤€≥ ŸÖ€åŸÑ€åŸàŸÜ ŸÖÿ™ÿ± ŸÖ⁄©ÿπÿ® ÿØÿ± ÿ≥ÿßŸÑ ÿßÿ≥ÿ™ Ÿà ÿÆŸàÿØ €å⁄©€å ÿßÿ≤ ÿπŸàÿßŸÖŸÑ ⁄©ŸÖ‚Äåÿ¢ÿ®€å ÿ≤ÿß€åŸÜÿØŸá‚Äåÿ±ŸàÿØ ÿ¥ŸÜÿßÿÆÿ™Ÿá ŸÖ€å‚Äåÿ¥ŸàÿØ.\nŸÅŸàŸÑÿßÿØ ŸÖÿ®ÿßÿ±⁄©Ÿá ⁄ÜŸÜÿØ ÿ®ÿßÿ± ÿ®ÿ±ŸÜÿØŸá ÿ¨ÿß€åÿ≤Ÿá ÿ¥ÿ±⁄©ÿ™ ÿØÿßŸÜÿ¥€å ÿ±ÿß ⁄©ÿ≥ÿ® ⁄©ÿ±ÿØŸá ÿßÿ≥ÿ™ÿü\nÿ¥ÿ±⁄©ÿ™ ŸÅŸàŸÑÿßÿØ ŸÖÿ®ÿßÿ±⁄©€Ä ÿßÿµŸÅŸáÿßŸÜÿå ÿ®ÿ≤ÿ±⁄Ø‚Äåÿ™ÿ±€åŸÜ Ÿàÿßÿ≠ÿØ ÿµŸÜÿπÿ™€å ÿÆÿµŸàÿµ€å ÿØÿ± ÿß€åÿ±ÿßŸÜ Ÿà ÿ®ÿ≤ÿ±⁄Ø‚Äåÿ™ÿ±€åŸÜ ŸÖÿ¨ÿ™ŸÖÿπ ÿ™ŸàŸÑ€åÿØ ŸÅŸàŸÑÿßÿØ ÿØÿ± ⁄©ÿ¥Ÿàÿ± ÿß€åÿ±ÿßŸÜ ÿßÿ≥ÿ™ÿå ⁄©Ÿá ÿØÿ± ÿ¥ÿ±ŸÇ ÿ¥Ÿáÿ± ŸÖÿ®ÿßÿ±⁄©Ÿá ŸÇÿ±ÿßÿ± ÿØÿßÿ±ÿØ. ŸÅŸàŸÑÿßÿØ ŸÖÿ®ÿßÿ±⁄©Ÿá ŸáŸÖ‚Äåÿß⁄©ŸÜŸàŸÜ ŸÖÿ≠ÿ±⁄© ÿ®ÿ≥€åÿßÿ±€å ÿßÿ≤ ÿµŸÜÿß€åÿπ ÿ®ÿßŸÑÿßÿØÿ≥ÿ™€å Ÿà Ÿæÿß€å€åŸÜ‚ÄåÿØÿ≥ÿ™€å ÿßÿ≥ÿ™. ŸÅŸàŸÑÿßÿØ ŸÖÿ®ÿßÿ±⁄©Ÿá ÿØÿ± €±€± ÿØŸàÿ±Ÿá ÿ¨ÿß€åÿ≤€Ä ŸÖŸÑ€å ÿ™ÿπÿßŸÑ€å ÿ≥ÿßÿ≤ŸÖÿßŸÜ€å Ÿà €∂ ÿØŸàÿ±Ÿá ÿ¨ÿß€åÿ≤€Ä ÿ¥ÿ±⁄©ÿ™ ÿØÿßŸÜÿ¥€å ÿØÿ± ⁄©ÿ¥Ÿàÿ± ÿ±ÿ™ÿ®€Ä ŸÜÿÆÿ≥ÿ™ ÿ±ÿß ÿ®ÿØÿ≥ÿ™ ÿ¢Ÿàÿ±ÿØŸá‚Äåÿßÿ≥ÿ™ Ÿà ŸáŸÖ⁄ÜŸÜ€åŸÜ ÿß€åŸÜ ÿ¥ÿ±⁄©ÿ™ ÿØÿ± ÿ≥ÿßŸÑ €±€≥€π€± ÿ®ÿ±ÿß€å ŸÜÿÆÿ≥ÿ™€åŸÜ‚Äåÿ®ÿßÿ± ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ÿ™ŸÜŸáÿß ÿ¥ÿ±⁄©ÿ™ ÿß€åÿ±ÿßŸÜ€å ÿ®ÿß ⁄©ÿ≥ÿ® ÿßŸÖÿ™€åÿßÿ≤ €∂€µ€¥ ÿ™ŸÜÿØ€åÿ≥ ÿ≤ÿ±€åŸÜ ÿ¨ÿß€åÿ≤€Ä ŸÖŸÑ€å ÿ™ÿπÿßŸÑ€å ÿ≥ÿßÿ≤ŸÖÿßŸÜ€å ÿ±ÿß ÿßÿ≤ ÿ¢ŸÜ ÿÆŸàÿØ ⁄©ŸÜÿØ. ÿ¥ÿ±⁄©ÿ™ ŸÅŸàŸÑÿßÿØ ŸÖÿ®ÿßÿ±⁄©€Ä ÿßÿµŸÅŸáÿßŸÜ ÿØÿ± €≤€≥ ÿØ€å ŸÖÿßŸá €±€≥€∑€± ÿßÿ≠ÿØÿßÿ´ ÿ¥ÿØ Ÿà ÿß⁄©ŸÜŸàŸÜ ÿ®ÿ≤ÿ±⁄Ø‚Äåÿ™ÿ±€åŸÜ Ÿàÿßÿ≠ÿØŸáÿß€å ÿµŸÜÿπÿ™€å Ÿà ÿ®ÿ≤ÿ±⁄Øÿ™ÿ±€åŸÜ ŸÖÿ¨ÿ™ŸÖÿπ ÿ™ŸàŸÑ€åÿØ ŸÅŸàŸÑÿßÿØ ÿØÿ± ÿß€åÿ±ÿßŸÜ ÿßÿ≥ÿ™. ÿß€åŸÜ ÿ¥ÿ±⁄©ÿ™ ÿØÿ± ÿ≤ŸÖ€åŸÜ€å ÿ®Ÿá ŸÖÿ≥ÿßÿ≠ÿ™ €≥€µ ⁄©€åŸÑŸàŸÖÿ™ÿ± ŸÖÿ±ÿ®ÿπ ÿØÿ± ŸÜÿ≤ÿØ€å⁄©€å ÿ¥Ÿáÿ± ŸÖÿ®ÿßÿ±⁄©Ÿá Ÿà ÿØÿ± €∑€µ ⁄©€åŸÑŸàŸÖÿ™ÿ±€å ÿ¨ŸÜŸàÿ® ÿ∫ÿ±ÿ®€å ÿ¥Ÿáÿ± ÿßÿµŸÅŸáÿßŸÜ ŸàÿßŸÇÿπ ÿ¥ÿØŸá‚Äåÿßÿ≥ÿ™. ŸÖÿµÿ±ŸÅ ÿ¢ÿ® ÿß€åŸÜ ⁄©ÿßÿ±ÿÆÿßŸÜŸá ÿØÿ± ⁄©ŸÖÿ™ÿ±€åŸÜ ŸÖ€åÿ≤ÿßŸÜ ÿÆŸàÿØÿå €±Ÿ´€µŸ™ ÿßÿ≤ ÿØÿ®€å ÿ≤ÿß€åŸÜÿØŸá‚Äåÿ±ŸàÿØ ÿ®ÿ±ÿßÿ®ÿ± ÿ≥ÿßŸÑÿßŸÜŸá €≤€≥ ŸÖ€åŸÑ€åŸàŸÜ ŸÖÿ™ÿ± ŸÖ⁄©ÿπÿ® ÿØÿ± ÿ≥ÿßŸÑ ÿßÿ≥ÿ™ Ÿà ÿÆŸàÿØ €å⁄©€å ÿßÿ≤ ÿπŸàÿßŸÖŸÑ ⁄©ŸÖ‚Äåÿ¢ÿ®€å ÿ≤ÿß€åŸÜÿØŸá‚Äåÿ±ŸàÿØ ÿ¥ŸÜÿßÿÆÿ™Ÿá ŸÖ€å‚Äåÿ¥ŸàÿØ.\nÿ¥ÿ±⁄©ÿ™ ŸÅŸàŸÑÿßÿØ ŸÖÿ®ÿßÿ±⁄©Ÿá ÿØÿ± ÿ≥ÿßŸÑ €±€≥€π€± ⁄ÜŸá ÿ¨ÿß€åÿ≤Ÿá ÿß€å ÿ®ÿ±ÿØÿü\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nghaemiyeh_pair\nDataset: ghaemiyeh_pair\nSize: 1,444 training samples\nColumns: anchor and positive\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\ntype\nstring\nstring\ndetails\nmin: 6 tokensmean: 19.4 tokensmax: 69 tokens\nmin: 4 tokensmean: 82.05 tokensmax: 442 tokens\nSamples:\nanchor\npositive\nÿßŸÖ ÿ≠ÿ®€åÿ®Ÿá ⁄ÜŸá ŸÜŸÇÿ¥€å ÿØÿ± ÿ±Ÿàÿß€åÿ™ ÿ≠ÿØ€åÿ´ ÿØÿßÿ¥ÿ™Ÿá ÿßÿ≥ÿ™ÿü\nÿßŸÖ ÿ≠ÿ®€åÿ®Ÿá ŸáŸÖÿ≥ÿ± ⁄Øÿ±ÿßŸÖ€å ÿ±ÿ≥ŸàŸÑ ÿÆÿØÿß ÿ®ŸàÿØŸá Ÿà ŸÜŸÇÿ¥ ŸÖŸáŸÖ€å ÿØÿ± ÿ±Ÿàÿß€åÿ™ ÿ≠ÿØ€åÿ´ ÿØÿßÿ¥ÿ™Ÿá ÿßÿ≥ÿ™. ÿßŸà ÿßÿ≤ ÿ≤ŸÜÿßŸÜ ÿßÿ≥ŸàŸá ŸÖÿπÿ™ÿ®ÿ± ÿ®Ÿá ÿ¥ŸÖÿßÿ± ŸÖ€å‚Äåÿ±ŸÅÿ™Ÿá Ÿà ÿØÿ± ŸÖŸÇÿØŸÖŸá ⁄©ÿ™ÿßÿ® 'ÿ≤ŸÜÿßŸÜ ÿßÿ≥ŸàŸá' ÿ∞⁄©ÿ± ÿ¥ÿØŸá ÿßÿ≥ÿ™ ⁄©Ÿá ÿßŸÖ ÿ≠ÿ®€åÿ®Ÿá ŸáŸÖÿ≥ÿ± ÿ±ÿ≥ŸàŸÑ ÿÆÿØÿß ÿ®ÿ≠ÿ≥ÿ® ÿ±Ÿàÿß€åÿßÿ™ ÿßŸáŸÑ ÿ≥ŸÜÿ™ Ÿà ÿ¥€åÿπŸá ÿ®ŸàÿØŸá ÿßÿ≥ÿ™. ÿß€åŸÜ ŸÜŸÇÿ¥ ÿßŸà ŸÜÿ¥ÿßŸÜ ÿßÿ≤ ÿßŸáŸÖ€åÿ™ Ÿà ÿ™ÿ£ÿ´€åÿ±⁄Øÿ∞ÿßÿ±€å ÿßŸÖ ÿ≠ÿ®€åÿ®Ÿá ÿØÿ± ÿ¨ÿßŸÖÿπŸá ÿßÿ≥ŸÑÿßŸÖ€å ÿØÿßÿ±ÿØ.\n⁄ÜŸá ⁄©ÿ≥€å ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ŸÖÿ™ŸÇŸÜ ÿ™ÿ±€åŸÜ ÿ¢ŸÖŸàÿ≤Ÿá ŸáÿßŸä ÿ™ÿ±ÿ®€åÿ™€å Ÿà ÿßÿ≥ÿßÿ≥ŸÄ€å ÿ™ÿ±€åŸÜ ŸÖÿ§ŸÑŸÅŸá ÿØÿ± ŸÅÿ±ŸáŸÜ⁄Ø ÿßÿ≥ŸÄŸÑÿßŸÖ€å ÿ¥ŸÜÿßÿÆÿ™Ÿá ŸÖ€å ÿ¥ŸàÿØÿü\nÿßŸáŸÑ ÿ®€åÿ™ ÿÆÿßŸÜŸÄÿØÿßŸÜ Ÿàÿ≠€å (ÿπŸÑ€åŸáŸÖ ÿßŸÑÿ≥ŸÄŸÑÿßŸÖ)\nÿßŸÖŸë ÿ≠ÿ®€åÿ®Ÿá ⁄ÜŸá ÿ¨ÿß€å⁄ØÿßŸá Ÿà€å⁄òŸá‚Äåÿß€å ÿØÿ± ÿ≤ŸÜÿØ⁄Ø€å Ÿæ€åÿßŸÖÿ®ÿ± ÿßÿ≥ŸÑÿßŸÖ (ÿµ) ÿØÿßÿ¥ÿ™Ÿá ÿßÿ≥ÿ™ÿü\nÿßŸÖŸë ÿ≠ÿ®€åÿ®Ÿá €å⁄©€å ÿßÿ≤ ŸáŸÖÿ≥ÿ±ÿßŸÜ ÿ±ÿ≥ŸàŸÑ ÿÆÿØÿß (ÿµ) ÿ®ŸàÿØŸá Ÿà ÿßÿ≤ ÿµÿßÿØŸÇ‚Äåÿ™ÿ±€åŸÜ ŸáŸÖÿ≥ÿ±ÿßŸÜ ÿß€åÿ¥ÿßŸÜ ŸÖÿ≠ÿ≥Ÿàÿ® ŸÖ€å‚Äåÿ¥ŸàÿØ. ÿßŸà ÿ™ŸÜŸáÿß ŸáŸÖÿ≥ÿ±€å ÿ®ŸàÿØ ⁄©Ÿá ÿ®ÿß Ÿæ€åÿßŸÖÿ®ÿ± ÿßÿ≥ŸÑÿßŸÖ (ÿµ) ÿ±ÿßÿ®ÿ∑Ÿá ŸÜÿ≥ÿ®€å ÿØÿßÿ¥ÿ™Ÿá Ÿà ÿßÿ≤ ŸÖ€åÿßŸÜ ŸáŸÖÿ≥ÿ±ÿßŸÜ ÿß€åÿ¥ÿßŸÜ ÿ®ÿ±ÿ™ÿ±€å ÿØÿßÿ¥ÿ™Ÿá ÿßÿ≥ÿ™. ÿßŸÖŸë ÿ≠ÿ®€åÿ®Ÿá Ÿæ€åÿ¥ ÿßÿ≤ ÿßÿ≤ÿØŸàÿßÿ¨ ÿ®ÿß Ÿæ€åÿßŸÖÿ®ÿ± (ÿµ) ÿ®ÿß ÿπŸÖŸá ÿ≤ÿßÿØŸá ÿß€åÿ¥ÿßŸÜ ÿßÿ≤ÿØŸàÿßÿ¨ ⁄©ÿ±ÿØŸá ÿ®ŸàÿØ Ÿà ŸÜÿßŸÖ ÿßŸà ÿ®Ÿá ÿØŸÑ€åŸÑ ŸÜÿßŸÖ ŸÅÿ±ÿ≤ŸÜÿØÿ¥ (ÿ≠ÿ®€åÿ®Ÿá ÿ®ŸÜÿ™ ÿπÿ®€åŸÄÿØÿßÔ∑≤ ÿ®ŸÜ ÿ¨ÿ≠ÿ¥)ÿå ÿßŸÖ ÿ≠ÿ®€åÿ®Ÿá ÿßÿ≥ÿ™.\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nfeghehi_QA_QQ_pair\nDataset: feghehi_QA_QQ_pair\nSize: 13,631 training samples\nColumns: positive and anchor\nApproximate statistics based on the first 1000 samples:\npositive\nanchor\ntype\nstring\nstring\ndetails\nmin: 12 tokensmean: 129.47 tokensmax: 512 tokens\nmin: 46 tokensmean: 92.32 tokensmax: 179 tokens\nSamples:\npositive\nanchor\nÿ±Ÿàÿ≤Ÿá ÿ¢ŸÜ ÿßÿ≥ÿ™ ⁄©Ÿá ÿßŸÜÿ≥ÿßŸÜ ÿ®ÿ±ÿß€å ÿ™ÿ∞ŸÑŸÑ Ÿàÿßÿ∏Ÿáÿßÿ±ÿ®ŸÜÿØ⁄Ø€å ÿØÿ± Ÿæ€åÿ¥⁄ØÿßŸá ÿÆÿØÿßŸàŸÜÿØ ŸÖÿ™ÿπÿßŸÑÿå ÿßÿ≤ ÿßÿ∞ÿßŸÜ ÿµÿ®ÿ≠ ÿ™ÿß ŸÖÿ∫ÿ±ÿ® ÿå ÿßÿ≤ ŸÜŸá ŸÖŸàÿ±ÿØ€å ⁄©Ÿá ÿ®ÿπÿØÿß ⁄ØŸÅÿ™Ÿá ŸÖ€å ÿ¥ŸàÿØÿå ÿÆŸàÿØÿØÿßÿ±€å ŸÜŸÖÿß€åÿØ. (3) ÿ¥ÿß€åÿßŸÜ ÿ∞⁄©ÿ± ÿßÿ≥ÿ™ÿå ÿØÿ± ŸÖŸàÿ±ÿØ ÿ™ÿ¥ÿÆ€åÿµ ŸÖÿ∫ÿ±ÿ® (ÿ®ÿ±ÿß€å Ÿæÿß€åÿßŸÜ ÿ±Ÿàÿ≤Ÿá)ÿå ÿ™Ÿàÿ∂€åÿ≠€å ⁄©Ÿá ÿØÿ± ŸàŸÇÿ™ ŸÖÿ∫ÿ±ÿ® ÿØÿ± ŸÖŸàÿ±ÿØ ŸÜŸÖÿßÿ≤ ŸÖÿ∫ÿ±ÿ® ÿØÿ± ÿ¨ŸÑÿØ ÿßŸàŸÑ ŸÖÿ≥ÿ£ŸÑŸá ¬´957¬ª ÿ∞⁄©ÿ±ÿ¥ÿØÿå ÿ¨ÿßÿ±€å ŸÖ€å ÿ®ÿßÿ¥ÿØ.\nÿØÿ± ÿ±Ÿàÿ≤Ÿáÿß€å ⁄Øÿ±ŸÖ ÿ™ÿßÿ®ÿ≥ÿ™ÿßŸÜÿå ŸàŸÇÿ™€å ⁄©Ÿá ÿ±Ÿàÿ≤Ÿá ŸÖ€å‚Äå⁄Ø€åÿ±ŸÖÿå ŸáŸÖ€åÿ¥Ÿá ŸÜ⁄Øÿ±ÿßŸÜ ÿß€åŸÜ Ÿáÿ≥ÿ™ŸÖ ⁄©Ÿá ÿ¢€åÿß ŸÖ€å‚Äåÿ™ŸàÿßŸÜŸÖ ÿ®Ÿá ÿØÿ±ÿ≥ÿ™€å ÿ≤ŸÖÿßŸÜ ÿßŸÅÿ∑ÿßÿ± ÿ±ÿß ÿ™ÿ¥ÿÆ€åÿµ ÿØŸáŸÖ €åÿß ŸÜŸá. ⁄ØÿßŸá€å ÿßŸàŸÇÿßÿ™ ÿßÿ≠ÿ≥ÿßÿ≥ ŸÖ€å‚Äå⁄©ŸÜŸÖ ⁄©Ÿá ŸÖŸÖ⁄©ŸÜ ÿßÿ≥ÿ™ ÿØÿ± ÿ™ÿ¥ÿÆ€åÿµ ÿ≤ŸÖÿßŸÜ ŸÖÿ∫ÿ±ÿ® ÿØ⁄Üÿßÿ± ÿßÿ¥ÿ™ÿ®ÿßŸá ÿ¥ŸàŸÖ. ÿ¢€åÿß ÿ±ÿßŸá€å Ÿàÿ¨ŸàÿØ ÿØÿßÿ±ÿØ ⁄©Ÿá ÿ®ÿ™ŸàÿßŸÜŸÖ ŸÖÿ∑ŸÖÿ¶ŸÜ ÿ¥ŸàŸÖ ⁄©Ÿá ÿ≤ŸÖÿßŸÜ ÿßŸÅÿ∑ÿßÿ± ŸÅÿ±ÿß ÿ±ÿ≥€åÿØŸá Ÿà ÿ±Ÿàÿ≤Ÿá‚ÄåÿßŸÖ ÿ±ÿß ÿ®Ÿá ÿØÿ±ÿ≥ÿ™€å ÿßŸÅÿ∑ÿßÿ± ⁄©ŸÜŸÖÿü ÿ¢€åÿß ŸÜÿ¥ÿßŸÜŸá ÿÆÿßÿµ€å Ÿàÿ¨ŸàÿØ ÿØÿßÿ±ÿØ ⁄©Ÿá ÿ®ÿ™ŸàÿßŸÜŸÖ ÿ®Ÿá ÿ¢ŸÜ ÿ™Ÿàÿ¨Ÿá ⁄©ŸÜŸÖ ÿ™ÿß ÿßÿ≤ ÿßÿ¥ÿ™ÿ®ÿßŸá ÿØÿ± ÿß€åŸÜ ÿ≤ŸÖ€åŸÜŸá ÿ¨ŸÑŸà⁄Ø€åÿ±€å ⁄©ŸÜŸÖÿü\nÿ®ÿ± Ÿáÿ± ÿßŸÜÿ≥ÿßŸÜ€å Ÿàÿßÿ¨ÿ® ÿßÿ≥ÿ™ ÿØÿ± ÿµŸàÿ±ÿ™€å ⁄©Ÿá ÿ¥ÿ±ÿß€åÿ∑ ÿ∞€åŸÑ Ÿàÿ¨ŸàÿØ ÿØÿßÿ¥ÿ™Ÿá ÿ®ÿßÿ¥ÿØÿå ŸÖÿßŸá ŸÖÿ®ÿßÿ±⁄© ÿ±ŸÖÿ∂ÿßŸÜ ÿ±ÿß ÿ±Ÿàÿ≤Ÿá ÿ®⁄Ø€åÿ±ÿØ.\nÿØÿ± ÿ¥ÿ±ÿß€åÿ∑€å ⁄©Ÿá ÿ±Ÿàÿ≤Ÿá ⁄Øÿ±ŸÅÿ™ŸÜ ÿØÿ± ŸÖÿßŸá ŸÖÿ®ÿßÿ±⁄© ÿ±ŸÖÿ∂ÿßŸÜ ÿ®ÿ± Ÿáÿ± ÿßŸÜÿ≥ÿßŸÜ€å Ÿàÿßÿ¨ÿ® ÿßÿ≥ÿ™ÿå ÿß⁄Øÿ± ŸÅÿ±ÿØ€å ÿ®Ÿá ÿØŸÑ€åŸÑ ÿ®€åŸÖÿßÿ±€å €åÿß ÿ¥ÿ±ÿß€åÿ∑ ÿÆÿßÿµ€å ŸÜÿ™ŸàÿßŸÜÿØ ÿ±Ÿàÿ≤Ÿá ÿ®⁄Ø€åÿ±ÿØÿå ÿ¢€åÿß ÿ®ÿß€åÿØ ŸÇÿ∂ÿß ⁄©ŸÜÿØ €åÿß ⁄©ŸÅÿßÿ±Ÿá ÿ®Ÿæÿ±ÿØÿßÿ≤ÿØÿü Ÿà ÿß⁄Øÿ± ÿ®€åŸÖÿßÿ±€å ŸÖŸàŸÇÿ™€å ÿ®ÿßÿ¥ÿØÿå ÿ¢€åÿß ÿ®ÿß€åÿØ ÿ±Ÿàÿ≤Ÿá‚ÄåŸáÿß€å ŸÇÿ∂ÿß ÿ¥ÿØŸá ÿ±ÿß ÿØÿ± ŸáŸÖÿßŸÜ ÿ≥ÿßŸÑ ÿ®⁄Ø€åÿ±ÿØ €åÿß ŸÖ€å‚Äåÿ™ŸàÿßŸÜÿØ ÿ™ÿß ÿ≥ÿßŸÑ‚ÄåŸáÿß€å ÿ®ÿπÿØ ÿ®Ÿá ÿ™ÿ£ÿÆ€åÿ± ÿ®€åŸÜÿØÿßÿ≤ÿØÿü\nÿ±Ÿàÿ≤Ÿá ÿ®ÿ± ŸÅÿ±ÿØ€å Ÿàÿßÿ¨ÿ® ÿßÿ≥ÿ™ ⁄©Ÿá ÿ®ÿßŸÑÿ∫ ÿ®ÿßÿ¥ÿØ Ÿà ÿß⁄Øÿ± ÿ®⁄ÜŸá ŸÜÿßÿ®ÿßŸÑÿ∫ ŸÇÿ®ŸÑ ÿßÿ≤ ÿßÿ∞ÿßŸÜ ÿµÿ®ÿ≠ ŸÖÿßŸá ÿ±ŸÖÿ∂ÿßŸÜÿå ÿ®ÿßŸÑÿ∫ ÿ¥ŸàÿØÿå ÿ®ÿß€åÿØ ÿ±Ÿàÿ≤Ÿá ÿ®⁄Ø€åÿ±ÿØ Ÿà ÿß⁄Øÿ± ÿ®ÿπÿØ ÿßÿ≤ ÿßÿ∞ÿßŸÜ ÿµÿ®ÿ≠ ÿ®ÿßŸÑÿ∫ ÿ¥ŸàÿØÿå ÿ±Ÿàÿ≤Ÿá ÿ¢ŸÜ ÿ±Ÿàÿ≤ ÿ®ÿ± ÿßŸà Ÿàÿßÿ¨ÿ® ŸÜ€åÿ≥ÿ™ÿõ ŸàŸÑ€å ÿß⁄Øÿ± ŸÇÿµÿØ ÿ±Ÿàÿ≤Ÿá ŸÖÿßŸá ÿ±ŸÖÿ∂ÿßŸÜ ⁄©ÿ±ÿØŸá ÿ®ÿßÿ¥ÿØÿå ÿßÿ≠ÿ™€åÿßÿ∑ ŸÖÿ≥ÿ™ÿ≠ÿ® ÿß€åŸÜ ÿßÿ≥ÿ™ ⁄©Ÿá ÿ¢ŸÜ ÿ±ÿß ÿ™ŸÖÿßŸÖ ⁄©ŸÜÿØ.\nÿØÿ± ÿ±Ÿàÿ≤Ÿáÿß€å ŸÖÿßŸá ÿ±ŸÖÿ∂ÿßŸÜÿå ŸÅÿ±ÿ≤ŸÜÿØŸÖ ⁄©Ÿá ŸáŸÜŸàÿ≤ ÿ®Ÿá ÿ≥ŸÜ ÿ®ŸÑŸàÿ∫ ŸÜÿ±ÿ≥€åÿØŸá ÿßÿ≥ÿ™ÿå ÿØÿ± ÿ≠ÿßŸÑ ŸÜÿ≤ÿØ€å⁄© ÿ¥ÿØŸÜ ÿ®Ÿá ÿ≥ŸÜ ÿ®ŸÑŸàÿ∫ ÿßÿ≥ÿ™. ÿß⁄Øÿ± ÿßŸà ŸÇÿ®ŸÑ ÿßÿ≤ ÿßÿ∞ÿßŸÜ ÿµÿ®ÿ≠ ÿ®ÿßŸÑÿ∫ ÿ¥ŸàÿØÿå ÿ¢€åÿß ŸÑÿßÿ≤ŸÖ ÿßÿ≥ÿ™ ÿ±Ÿàÿ≤Ÿá ÿ®⁄Ø€åÿ±ÿØÿü Ÿà ÿß⁄Øÿ± ÿß€åŸÜ ÿßÿ™ŸÅÿßŸÇ ÿ®ÿπÿØ ÿßÿ≤ ÿßÿ∞ÿßŸÜ ÿµÿ®ÿ≠ ÿ®€åŸÅÿ™ÿØÿå ÿ¢€åÿß ÿ®ÿß€åÿØ ÿ±Ÿàÿ≤Ÿá ÿ±ÿß ÿßÿØÿßŸÖŸá ÿØŸáÿØ ÿß⁄Øÿ± ŸÇÿ®ŸÑÿßŸã ŸÜ€åÿ™ ÿ±Ÿàÿ≤Ÿá ⁄©ÿ±ÿØŸá ÿ®ÿßÿ¥ÿØÿü\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\neducation_books_pair\nDataset: education_books_pair\nSize: 14,730 training samples\nColumns: context and instruction\nApproximate statistics based on the first 1000 samples:\ncontext\ninstruction\ntype\nstring\nstring\ndetails\nmin: 512 tokensmean: 512.0 tokensmax: 512 tokens\nmin: 9 tokensmean: 24.8 tokensmax: 79 tokens\nSamples:\ncontext\ninstruction\n'ÿ¢ÿ≥ŸÖÿßŸÜ€å ÿ®ÿß ÿ™⁄©€åŸá ÿ®ÿ± ÿß€åŸÖÿßŸÜ  ⁄©Ÿá ÿØÿ± ÿßÿπŸÖÿßŸÇ Ÿàÿ¨ŸàÿØ ÿßŸÜÿ≥ÿßŸÜ ÿ¢ÿ¥€åÿßŸÜ ÿØÿßÿ±ÿØ  ÿßŸà ÿ±ÿß ÿØÿ± Ÿæ€å⁄©ÿßÿ± ÿ®ÿß ÿØÿ±ŸàŸÜ ÿ®Ÿá Ÿæ€åÿ±Ÿàÿ≤€åŸÖ€å ÿ±ÿ≥ÿßŸÜŸÜÿØ Ÿà ÿ≤ŸÖ€åŸÜŸá ÿ±ÿß ÿ®ÿ±ÿß€å ÿ∫ŸÑÿ®Ÿá ÿ®ÿ± ÿØÿ¥ŸÖŸÜÿßŸÜ ÿ≠ŸÇ Ÿà ÿßŸÜÿ≥ÿßŸÜ€åÿ™ ŸÅÿ±ÿßŸáŸÖ ŸÖ€å ÿ¢Ÿàÿ±ŸÜÿØ.ÿß€å ÿ¥ŸáÿßŸÜ ⁄©ÿ¥ÿ™€åŸÖ ŸÖÿß ÿÆÿµŸÖ ÿ®ÿ±ŸàŸÜ ŸÖÿßŸÜÿØ ÿÆÿµŸÖ€å ÿ≤ÿßŸÜ ÿ®ÿ™ÿ±ÿßŸÜÿØÿ± ÿØÿ±ŸàŸÜ⁄ÜŸàŸÜ ⁄©Ÿá Ÿàÿßÿ±ÿ≥ÿ™ŸÖ ÿ≤Ÿæ€å⁄©ÿßÿ± ÿ®ÿ±ŸàŸÜ ÿ®ÿßÿ≤ ⁄Øÿ¥ÿ™ŸÖ ÿ≥Ÿà€å Ÿæ€å⁄©ÿßÿ± ÿØÿ±ŸàŸÜÿßÿπÿ™ŸÇÿßÿØÿå ÿßÿÆŸÑÿßŸÇ ŸàÿπŸÖŸÑÿ®ÿß ÿ™Ÿàÿ¨Ÿá ÿ®Ÿá ÿ¢ŸÜ ⁄ÜŸá ÿØÿ±ÿ®ÿßÿ±Ÿá €å ÿ¥ÿÆÿµ€åÿ™ ÿßŸÑŸá€å ÿßŸÜÿ≥ÿßŸÜ ⁄ØŸÅÿ™€åŸÖÿå ŸÖ€å ÿ™ŸàÿßŸÜ ÿØÿ±€åÿßŸÅÿ™ ⁄©Ÿá ŸÜŸàÿπ€å ŸáŸÖÿßŸáŸÜ⁄Ø€å ŸàŸæ€åŸàÿ≥ÿ™⁄Ø€å ŸÖ€åÿßŸÜ ŸÖÿπÿ™ŸÇÿØÿßÿ™ ÿßŸÜÿ≥ÿßŸÜ Ÿà ÿ±Ÿàÿ≠€åÿßÿ™ Ÿà ÿßÿπŸÖÿßŸÑ ÿßŸà Ÿàÿ¨ŸàÿØ ÿØÿßÿ±ÿØ ⁄©Ÿá Ÿáÿ±⁄Øÿ≤ ŸÜŸÖ€å ÿ™ŸàÿßŸÜ ÿ¢ŸÜ Ÿáÿß ÿ±ÿß ÿßÿ≤ ŸáŸÖÿØ€å⁄Øÿ±ÿ¨ÿØÿß ÿ≥ÿßÿÆÿ™. ÿ™ÿπÿßŸÑ€åŸÖ ÿ¢ÿ≥ŸÖÿßŸÜ€å ÿ®ÿß ÿØŸÇÿ™ Ÿáÿ± ⁄ÜŸá ÿ™ŸÖÿßŸÖ ÿ™ÿ± ÿß€åŸÜ ŸáŸÖÿßŸáŸÜ⁄Ø€å ÿ±ÿß ÿß€åÿ¨ÿßÿØ ŸÜŸÖŸàÿØŸá Ÿà ÿ¥ÿÆÿµ€åÿ™ ÿßŸÜÿ≥ÿßŸÜ ÿ±ÿß ÿ®ÿ±Ÿæÿß€åŸá €å ÿß€åŸÖÿßŸÜ ÿ®Ÿá ÿÆÿØÿß ŸÇŸàÿßŸÖ ŸÖ€å ÿ®ÿÆÿ¥ÿØ.ÿ®ŸÜÿßÿ®ÿ±ÿß€åŸÜÿå ÿ™ŸÖÿßŸÖ€å ŸÅÿ∂ÿß€åŸÑ ÿßÿÆŸÑÿßŸÇ€å ÿßÿ≤ ŸÇÿ®€åŸÑ Ÿæÿ±Ÿá€åÿ≤⁄Øÿßÿ±€åÿå ÿµÿ®ÿ± Ÿà ÿ¥ÿ¨ÿßÿπÿ™ÿå ŸÅÿØÿß⁄©ÿßÿ±€åÿå ⁄Øÿ∞ÿ¥ÿ™ÿå ÿπÿØÿßŸÑÿ™Ÿà‚Ä¶ ÿßÿ≤ ÿß€åŸÖÿßŸÜ ÿ®Ÿá ÿÆÿØÿß ÿ≥ÿ±⁄Üÿ¥ŸÖŸá ⁄Øÿ±ŸÅÿ™Ÿá Ÿà ŸÑÿßÿ≤ŸÖŸá €å ÿ¢ŸÜ Ÿáÿ≥ÿ™ŸÜÿØ Ÿà Ÿáÿ±⁄Øÿ≤ ŸÜŸÖ€å ÿ™ŸàÿßŸÜ ÿß€åŸÖÿßŸÜ Ÿà ÿ±ÿ∞ÿß€åŸÑ ÿßÿÆŸÑÿßŸÇ€å ÿßÿ≤ŸÇÿ®€åŸÑ ÿ≥ÿ™ŸÖÿå ÿ∫ÿ±Ÿàÿ±ÿå ŸÜŸÅÿßŸÇÿå Ÿà ÿ®ÿØ⁄ØŸà€å€å Ÿà ÿ∫€åÿ±Ÿá ÿ±ÿß €å⁄© ÿ¨ÿß ÿ¨ŸÖÿπ ŸÜŸÖŸàÿØ ÿ≤€åÿ±ÿß ÿ±€åÿ¥ Ÿá€å ŸáŸÖ Ÿá€å ÿß€åŸÜ ÿ±ÿ∞ÿß€åŸÑ ÿØÿ±ÿ±Ÿà⁄Øÿ±ÿØÿßŸÜ€å ÿßÿ≤ ÿÆÿØÿß Ÿà ÿ∫ŸÅŸÑÿ™ ÿßÿ≤ €åÿßÿØ ÿßŸàÿ≥ÿ™.ÿßŸÜÿ≥ÿßŸÜ ŸáŸÜ⁄ØÿßŸÖ€å ŸÖ€å ÿ™ŸàÿßŸÜÿØ ÿØÿ± ÿ¨Ÿáÿ™ ÿ≥ÿßÿ≤ŸÜÿØ⁄Ø€å ÿ¥ÿÆÿµ€åÿ™ ÿÆŸàÿØ ⁄ØÿßŸÖ ÿ®ÿ±ÿØÿßÿ±ÿØ Ÿà ÿ®Ÿá ÿ∑ÿ±ŸÅ ⁄©ŸÖÿßŸÑ ÿ≠ÿ±⁄©ÿ™ ⁄©ŸÜÿØ⁄©Ÿá ÿß€åŸÜ ŸáŸÖÿßŸáŸÜ⁄Ø€å ÿ±ÿß ŸÖ€åÿßŸÜ ŸÖÿπÿ™ŸÇÿØÿßÿ™ ÿÆŸàÿØ ...\n'⁄Üÿ∑Ÿàÿ± ŸÖ€å‚Äåÿ™ŸàŸÜ€åŸÖ ÿß€åŸÖÿßŸÜ ÿÆŸàÿØŸÖŸàŸÜ Ÿà ÿØ€å⁄Øÿ±ÿßŸÜ ÿ±Ÿà ÿ®ÿ≥ŸÜÿ¨€åŸÖ Ÿà ŸÖŸàŸÖŸÜ€åŸÜ ŸàÿßŸÇÿπ€å ÿ±Ÿà ÿßÿ≤ ŸÖŸÜÿßŸÅŸÇ€åŸÜ Ÿà ŸÅÿ±€åÿ®⁄©ÿßÿ±ÿßŸÜ ÿ™ÿ¥ÿÆ€åÿµ ÿ®ÿØ€åŸÖÿü'\n'ÿ¢ÿ≥ŸÖÿßŸÜ€å ÿ®ÿß ÿ™⁄©€åŸá ÿ®ÿ± ÿß€åŸÖÿßŸÜ  ⁄©Ÿá ÿØÿ± ÿßÿπŸÖÿßŸÇ Ÿàÿ¨ŸàÿØ ÿßŸÜÿ≥ÿßŸÜ ÿ¢ÿ¥€åÿßŸÜ ÿØÿßÿ±ÿØ  ÿßŸà ÿ±ÿß ÿØÿ± Ÿæ€å⁄©ÿßÿ± ÿ®ÿß ÿØÿ±ŸàŸÜ ÿ®Ÿá Ÿæ€åÿ±Ÿàÿ≤€åŸÖ€å ÿ±ÿ≥ÿßŸÜŸÜÿØ Ÿà ÿ≤ŸÖ€åŸÜŸá ÿ±ÿß ÿ®ÿ±ÿß€å ÿ∫ŸÑÿ®Ÿá ÿ®ÿ± ÿØÿ¥ŸÖŸÜÿßŸÜ ÿ≠ŸÇ Ÿà ÿßŸÜÿ≥ÿßŸÜ€åÿ™ ŸÅÿ±ÿßŸáŸÖ ŸÖ€å ÿ¢Ÿàÿ±ŸÜÿØ.ÿß€å ÿ¥ŸáÿßŸÜ ⁄©ÿ¥ÿ™€åŸÖ ŸÖÿß ÿÆÿµŸÖ ÿ®ÿ±ŸàŸÜ ŸÖÿßŸÜÿØ ÿÆÿµŸÖ€å ÿ≤ÿßŸÜ ÿ®ÿ™ÿ±ÿßŸÜÿØÿ± ÿØÿ±ŸàŸÜ⁄ÜŸàŸÜ ⁄©Ÿá Ÿàÿßÿ±ÿ≥ÿ™ŸÖ ÿ≤Ÿæ€å⁄©ÿßÿ± ÿ®ÿ±ŸàŸÜ ÿ®ÿßÿ≤ ⁄Øÿ¥ÿ™ŸÖ ÿ≥Ÿà€å Ÿæ€å⁄©ÿßÿ± ÿØÿ±ŸàŸÜÿßÿπÿ™ŸÇÿßÿØÿå ÿßÿÆŸÑÿßŸÇ ŸàÿπŸÖŸÑÿ®ÿß ÿ™Ÿàÿ¨Ÿá ÿ®Ÿá ÿ¢ŸÜ ⁄ÜŸá ÿØÿ±ÿ®ÿßÿ±Ÿá €å ÿ¥ÿÆÿµ€åÿ™ ÿßŸÑŸá€å ÿßŸÜÿ≥ÿßŸÜ ⁄ØŸÅÿ™€åŸÖÿå ŸÖ€å ÿ™ŸàÿßŸÜ ÿØÿ±€åÿßŸÅÿ™ ⁄©Ÿá ŸÜŸàÿπ€å ŸáŸÖÿßŸáŸÜ⁄Ø€å ŸàŸæ€åŸàÿ≥ÿ™⁄Ø€å ŸÖ€åÿßŸÜ ŸÖÿπÿ™ŸÇÿØÿßÿ™ ÿßŸÜÿ≥ÿßŸÜ Ÿà ÿ±Ÿàÿ≠€åÿßÿ™ Ÿà ÿßÿπŸÖÿßŸÑ ÿßŸà Ÿàÿ¨ŸàÿØ ÿØÿßÿ±ÿØ ⁄©Ÿá Ÿáÿ±⁄Øÿ≤ ŸÜŸÖ€å ÿ™ŸàÿßŸÜ ÿ¢ŸÜ Ÿáÿß ÿ±ÿß ÿßÿ≤ ŸáŸÖÿØ€å⁄Øÿ±ÿ¨ÿØÿß ÿ≥ÿßÿÆÿ™. ÿ™ÿπÿßŸÑ€åŸÖ ÿ¢ÿ≥ŸÖÿßŸÜ€å ÿ®ÿß ÿØŸÇÿ™ Ÿáÿ± ⁄ÜŸá ÿ™ŸÖÿßŸÖ ÿ™ÿ± ÿß€åŸÜ ŸáŸÖÿßŸáŸÜ⁄Ø€å ÿ±ÿß ÿß€åÿ¨ÿßÿØ ŸÜŸÖŸàÿØŸá Ÿà ÿ¥ÿÆÿµ€åÿ™ ÿßŸÜÿ≥ÿßŸÜ ÿ±ÿß ÿ®ÿ±Ÿæÿß€åŸá €å ÿß€åŸÖÿßŸÜ ÿ®Ÿá ÿÆÿØÿß ŸÇŸàÿßŸÖ ŸÖ€å ÿ®ÿÆÿ¥ÿØ.ÿ®ŸÜÿßÿ®ÿ±ÿß€åŸÜÿå ÿ™ŸÖÿßŸÖ€å ŸÅÿ∂ÿß€åŸÑ ÿßÿÆŸÑÿßŸÇ€å ÿßÿ≤ ŸÇÿ®€åŸÑ Ÿæÿ±Ÿá€åÿ≤⁄Øÿßÿ±€åÿå ÿµÿ®ÿ± Ÿà ÿ¥ÿ¨ÿßÿπÿ™ÿå ŸÅÿØÿß⁄©ÿßÿ±€åÿå ⁄Øÿ∞ÿ¥ÿ™ÿå ÿπÿØÿßŸÑÿ™Ÿà‚Ä¶ ÿßÿ≤ ÿß€åŸÖÿßŸÜ ÿ®Ÿá ÿÆÿØÿß ÿ≥ÿ±⁄Üÿ¥ŸÖŸá ⁄Øÿ±ŸÅÿ™Ÿá Ÿà ŸÑÿßÿ≤ŸÖŸá €å ÿ¢ŸÜ Ÿáÿ≥ÿ™ŸÜÿØ Ÿà Ÿáÿ±⁄Øÿ≤ ŸÜŸÖ€å ÿ™ŸàÿßŸÜ ÿß€åŸÖÿßŸÜ Ÿà ÿ±ÿ∞ÿß€åŸÑ ÿßÿÆŸÑÿßŸÇ€å ÿßÿ≤ŸÇÿ®€åŸÑ ÿ≥ÿ™ŸÖÿå ÿ∫ÿ±Ÿàÿ±ÿå ŸÜŸÅÿßŸÇÿå Ÿà ÿ®ÿØ⁄ØŸà€å€å Ÿà ÿ∫€åÿ±Ÿá ÿ±ÿß €å⁄© ÿ¨ÿß ÿ¨ŸÖÿπ ŸÜŸÖŸàÿØ ÿ≤€åÿ±ÿß ÿ±€åÿ¥ Ÿá€å ŸáŸÖ Ÿá€å ÿß€åŸÜ ÿ±ÿ∞ÿß€åŸÑ ÿØÿ±ÿ±Ÿà⁄Øÿ±ÿØÿßŸÜ€å ÿßÿ≤ ÿÆÿØÿß Ÿà ÿ∫ŸÅŸÑÿ™ ÿßÿ≤ €åÿßÿØ ÿßŸàÿ≥ÿ™.ÿßŸÜÿ≥ÿßŸÜ ŸáŸÜ⁄ØÿßŸÖ€å ŸÖ€å ÿ™ŸàÿßŸÜÿØ ÿØÿ± ÿ¨Ÿáÿ™ ÿ≥ÿßÿ≤ŸÜÿØ⁄Ø€å ÿ¥ÿÆÿµ€åÿ™ ÿÆŸàÿØ ⁄ØÿßŸÖ ÿ®ÿ±ÿØÿßÿ±ÿØ Ÿà ÿ®Ÿá ÿ∑ÿ±ŸÅ ⁄©ŸÖÿßŸÑ ÿ≠ÿ±⁄©ÿ™ ⁄©ŸÜÿØ⁄©Ÿá ÿß€åŸÜ ŸáŸÖÿßŸáŸÜ⁄Ø€å ÿ±ÿß ŸÖ€åÿßŸÜ ŸÖÿπÿ™ŸÇÿØÿßÿ™ ÿÆŸàÿØ ...\n'€åÿßÿØ ÿÆÿØÿß ⁄ÜŸá ÿ™ÿ£ÿ´€åÿ±€å ÿ®ÿ± ÿß€åŸÖÿßŸÜ Ÿà ÿ¥ÿÆÿµ€åÿ™ ÿßŸÜÿ≥ÿßŸÜ ÿØÿßÿ±ÿØ Ÿà ⁄Ü⁄ØŸàŸÜŸá ŸÖ€å‚Äåÿ™ŸàÿßŸÜÿØ ÿØÿ± ÿ≤ŸÜÿØ⁄Ø€å ÿ±Ÿàÿ≤ŸÖÿ±Ÿá ŸÅÿ±ÿØ ŸÖÿ§ÿ´ÿ± ÿ®ÿßÿ¥ÿØÿü'\n'ÿ¢ÿ≥ŸÖÿßŸÜ€å ÿ®ÿß ÿ™⁄©€åŸá ÿ®ÿ± ÿß€åŸÖÿßŸÜ  ⁄©Ÿá ÿØÿ± ÿßÿπŸÖÿßŸÇ Ÿàÿ¨ŸàÿØ ÿßŸÜÿ≥ÿßŸÜ ÿ¢ÿ¥€åÿßŸÜ ÿØÿßÿ±ÿØ  ÿßŸà ÿ±ÿß ÿØÿ± Ÿæ€å⁄©ÿßÿ± ÿ®ÿß ÿØÿ±ŸàŸÜ ÿ®Ÿá Ÿæ€åÿ±Ÿàÿ≤€åŸÖ€å ÿ±ÿ≥ÿßŸÜŸÜÿØ Ÿà ÿ≤ŸÖ€åŸÜŸá ÿ±ÿß ÿ®ÿ±ÿß€å ÿ∫ŸÑÿ®Ÿá ÿ®ÿ± ÿØÿ¥ŸÖŸÜÿßŸÜ ÿ≠ŸÇ Ÿà ÿßŸÜÿ≥ÿßŸÜ€åÿ™ ŸÅÿ±ÿßŸáŸÖ ŸÖ€å ÿ¢Ÿàÿ±ŸÜÿØ.ÿß€å ÿ¥ŸáÿßŸÜ ⁄©ÿ¥ÿ™€åŸÖ ŸÖÿß ÿÆÿµŸÖ ÿ®ÿ±ŸàŸÜ ŸÖÿßŸÜÿØ ÿÆÿµŸÖ€å ÿ≤ÿßŸÜ ÿ®ÿ™ÿ±ÿßŸÜÿØÿ± ÿØÿ±ŸàŸÜ⁄ÜŸàŸÜ ⁄©Ÿá Ÿàÿßÿ±ÿ≥ÿ™ŸÖ ÿ≤Ÿæ€å⁄©ÿßÿ± ÿ®ÿ±ŸàŸÜ ÿ®ÿßÿ≤ ⁄Øÿ¥ÿ™ŸÖ ÿ≥Ÿà€å Ÿæ€å⁄©ÿßÿ± ÿØÿ±ŸàŸÜÿßÿπÿ™ŸÇÿßÿØÿå ÿßÿÆŸÑÿßŸÇ ŸàÿπŸÖŸÑÿ®ÿß ÿ™Ÿàÿ¨Ÿá ÿ®Ÿá ÿ¢ŸÜ ⁄ÜŸá ÿØÿ±ÿ®ÿßÿ±Ÿá €å ÿ¥ÿÆÿµ€åÿ™ ÿßŸÑŸá€å ÿßŸÜÿ≥ÿßŸÜ ⁄ØŸÅÿ™€åŸÖÿå ŸÖ€å ÿ™ŸàÿßŸÜ ÿØÿ±€åÿßŸÅÿ™ ⁄©Ÿá ŸÜŸàÿπ€å ŸáŸÖÿßŸáŸÜ⁄Ø€å ŸàŸæ€åŸàÿ≥ÿ™⁄Ø€å ŸÖ€åÿßŸÜ ŸÖÿπÿ™ŸÇÿØÿßÿ™ ÿßŸÜÿ≥ÿßŸÜ Ÿà ÿ±Ÿàÿ≠€åÿßÿ™ Ÿà ÿßÿπŸÖÿßŸÑ ÿßŸà Ÿàÿ¨ŸàÿØ ÿØÿßÿ±ÿØ ⁄©Ÿá Ÿáÿ±⁄Øÿ≤ ŸÜŸÖ€å ÿ™ŸàÿßŸÜ ÿ¢ŸÜ Ÿáÿß ÿ±ÿß ÿßÿ≤ ŸáŸÖÿØ€å⁄Øÿ±ÿ¨ÿØÿß ÿ≥ÿßÿÆÿ™. ÿ™ÿπÿßŸÑ€åŸÖ ÿ¢ÿ≥ŸÖÿßŸÜ€å ÿ®ÿß ÿØŸÇÿ™ Ÿáÿ± ⁄ÜŸá ÿ™ŸÖÿßŸÖ ÿ™ÿ± ÿß€åŸÜ ŸáŸÖÿßŸáŸÜ⁄Ø€å ÿ±ÿß ÿß€åÿ¨ÿßÿØ ŸÜŸÖŸàÿØŸá Ÿà ÿ¥ÿÆÿµ€åÿ™ ÿßŸÜÿ≥ÿßŸÜ ÿ±ÿß ÿ®ÿ±Ÿæÿß€åŸá €å ÿß€åŸÖÿßŸÜ ÿ®Ÿá ÿÆÿØÿß ŸÇŸàÿßŸÖ ŸÖ€å ÿ®ÿÆÿ¥ÿØ.ÿ®ŸÜÿßÿ®ÿ±ÿß€åŸÜÿå ÿ™ŸÖÿßŸÖ€å ŸÅÿ∂ÿß€åŸÑ ÿßÿÆŸÑÿßŸÇ€å ÿßÿ≤ ŸÇÿ®€åŸÑ Ÿæÿ±Ÿá€åÿ≤⁄Øÿßÿ±€åÿå ÿµÿ®ÿ± Ÿà ÿ¥ÿ¨ÿßÿπÿ™ÿå ŸÅÿØÿß⁄©ÿßÿ±€åÿå ⁄Øÿ∞ÿ¥ÿ™ÿå ÿπÿØÿßŸÑÿ™Ÿà‚Ä¶ ÿßÿ≤ ÿß€åŸÖÿßŸÜ ÿ®Ÿá ÿÆÿØÿß ÿ≥ÿ±⁄Üÿ¥ŸÖŸá ⁄Øÿ±ŸÅÿ™Ÿá Ÿà ŸÑÿßÿ≤ŸÖŸá €å ÿ¢ŸÜ Ÿáÿ≥ÿ™ŸÜÿØ Ÿà Ÿáÿ±⁄Øÿ≤ ŸÜŸÖ€å ÿ™ŸàÿßŸÜ ÿß€åŸÖÿßŸÜ Ÿà ÿ±ÿ∞ÿß€åŸÑ ÿßÿÆŸÑÿßŸÇ€å ÿßÿ≤ŸÇÿ®€åŸÑ ÿ≥ÿ™ŸÖÿå ÿ∫ÿ±Ÿàÿ±ÿå ŸÜŸÅÿßŸÇÿå Ÿà ÿ®ÿØ⁄ØŸà€å€å Ÿà ÿ∫€åÿ±Ÿá ÿ±ÿß €å⁄© ÿ¨ÿß ÿ¨ŸÖÿπ ŸÜŸÖŸàÿØ ÿ≤€åÿ±ÿß ÿ±€åÿ¥ Ÿá€å ŸáŸÖ Ÿá€å ÿß€åŸÜ ÿ±ÿ∞ÿß€åŸÑ ÿØÿ±ÿ±Ÿà⁄Øÿ±ÿØÿßŸÜ€å ÿßÿ≤ ÿÆÿØÿß Ÿà ÿ∫ŸÅŸÑÿ™ ÿßÿ≤ €åÿßÿØ ÿßŸàÿ≥ÿ™.ÿßŸÜÿ≥ÿßŸÜ ŸáŸÜ⁄ØÿßŸÖ€å ŸÖ€å ÿ™ŸàÿßŸÜÿØ ÿØÿ± ÿ¨Ÿáÿ™ ÿ≥ÿßÿ≤ŸÜÿØ⁄Ø€å ÿ¥ÿÆÿµ€åÿ™ ÿÆŸàÿØ ⁄ØÿßŸÖ ÿ®ÿ±ÿØÿßÿ±ÿØ Ÿà ÿ®Ÿá ÿ∑ÿ±ŸÅ ⁄©ŸÖÿßŸÑ ÿ≠ÿ±⁄©ÿ™ ⁄©ŸÜÿØ⁄©Ÿá ÿß€åŸÜ ŸáŸÖÿßŸáŸÜ⁄Ø€å ÿ±ÿß ŸÖ€åÿßŸÜ ŸÖÿπÿ™ŸÇÿØÿßÿ™ ÿÆŸàÿØ ...\n'⁄Üÿ±ÿß ŸÅŸÇÿ∑ ÿ¢⁄ØÿßŸá€å ⁄©ÿßŸÅ€å ŸÜ€åÿ≥ÿ™ Ÿà ⁄ÜŸá ⁄Ü€åÿ≤€å ÿ®ÿß€åÿØ ÿ®Ÿá ÿπŸÜŸàÿßŸÜ Ÿæÿ¥ÿ™ŸàÿßŸÜŸá ÿ¢⁄ØÿßŸá€å Ÿàÿ¨ŸàÿØ ÿØÿßÿ¥ÿ™Ÿá ÿ®ÿßÿ¥ÿØ ÿ™ÿß ÿ®Ÿá ÿπŸÖŸÑ ŸÖŸÜÿ¨ÿ± ÿ¥ŸàÿØÿü'\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nself_instruct_persian_pairs\nDataset: self_instruct_persian_pairs\nSize: 14,456 training samples\nColumns: anchor and positive\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\ntype\nstring\nstring\ndetails\nmin: 8 tokensmean: 16.33 tokensmax: 54 tokens\nmin: 4 tokensmean: 100.43 tokensmax: 512 tokens\nSamples:\nanchor\npositive\nÿ™ŸÅÿßŸàÿ™ ÿ®€åŸÜ ÿ¨ÿ±ŸÖ ŸÖÿ≥ÿ™ŸÖÿ± Ÿà ÿ¨ÿ±ŸÖ ÿ∫€åÿ±ŸÖÿ≥ÿ™ŸÖÿ± ÿØÿ± ŸÇÿßŸÜŸàŸÜ ÿß€åÿ±ÿßŸÜ ⁄Ü€åÿ≥ÿ™ÿü\nÿ¨ÿ±ŸÖ ŸÖÿ≥ÿ™ŸÖÿ± ÿ¨ÿ±ŸÖ€å ÿßÿ≥ÿ™ ⁄©Ÿá ŸàŸÇŸàÿπ ÿ¢ŸÜ ÿ®Ÿá ÿµŸàÿ±ÿ™ Ÿæ€åŸàÿ≥ÿ™Ÿá Ÿà ŸÖŸÖÿ™ÿØ ÿµŸàÿ±ÿ™ ŸÖ€å‚Äå⁄Ø€åÿ±ÿØÿå ŸÖÿ´ŸÑ ÿØÿßÿ¥ÿ™ŸÜ ŸÖŸàÿßÿØ ŸÖÿÆÿØÿ±. ÿØÿ± ŸÖŸÇÿßÿ®ŸÑÿå ÿ¨ÿ±ŸÖ ÿ∫€åÿ±ŸÖÿ≥ÿ™ŸÖÿ± ÿ¨ÿ±ŸÖ€å ÿßÿ≥ÿ™ ⁄©Ÿá ÿØÿ± €å⁄© ŸÑÿ≠ÿ∏Ÿá €åÿß €å⁄© ÿ®ÿßÿ≤Ÿá ÿ≤ŸÖÿßŸÜ€å ⁄©Ÿàÿ™ÿßŸá ÿßÿ™ŸÅÿßŸÇ ŸÖ€å‚ÄåÿßŸÅÿ™ÿØ Ÿà ÿ®Ÿá Ÿæÿß€åÿßŸÜ ŸÖ€å‚Äåÿ±ÿ≥ÿØÿå ŸÖÿ´ŸÑ ŸÇÿ™ŸÑ €åÿß ÿ≥ÿ±ŸÇÿ™.\nŸÖŸÜÿßÿ®ÿπ ÿßÿµŸÑ€å ÿßÿ¨ÿ™ŸáÿßÿØ ÿØÿ± ŸÅŸÇŸá ÿ¥€åÿπŸá ⁄©ÿØÿßŸÖ‚ÄåŸáÿß Ÿáÿ≥ÿ™ŸÜÿØÿü\nŸÖŸÜÿßÿ®ÿπ ÿßÿµŸÑ€å ÿßÿ¨ÿ™ŸáÿßÿØ ÿØÿ± ŸÅŸÇŸá ÿ¥€åÿπŸá ÿπÿ®ÿßÿ±ÿ™ŸÜÿØ ÿßÿ≤ ŸÇÿ±ÿ¢ŸÜÿå ÿ≥ŸÜÿ™ (ÿ≠ÿØ€åÿ´)ÿå ÿßÿ¨ŸÖÿßÿπ Ÿà ÿπŸÇŸÑ. ŸÇÿ±ÿ¢ŸÜ ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ⁄©ÿ™ÿßÿ® ŸÖŸÇÿØÿ≥ Ÿà ÿßÿµŸÑ€å‚Äåÿ™ÿ±€åŸÜ ŸÖŸÜÿ®ÿπÿå ÿ≠ÿØ€åÿ´ ÿ¥ÿßŸÖŸÑ ÿ≥ŸÜÿ™ Ÿæ€åÿßŸÖÿ®ÿ± Ÿà ÿßŸáŸÑ ÿ®€åÿ™ÿå ÿßÿ¨ŸÖÿßÿπ ÿ®Ÿá ŸÖŸàÿßŸÅŸÇÿ™ ŸÜÿ∏ÿ± ÿπÿßŸÑŸÖÿßŸÜ ÿ¥€åÿπŸá Ÿà ÿπŸÇŸÑ ÿ®Ÿá ÿπŸÜŸàÿßŸÜ ÿßÿ®ÿ≤ÿßÿ± ÿ™ÿ≠ŸÑ€åŸÑ Ÿà ÿßÿ≥ÿ™ÿØŸÑÿßŸÑ ÿØÿ± ŸÖÿ≥ÿßÿ¶ŸÑ ŸÅŸÇŸá€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€å‚Äåÿ¥ŸàŸÜÿØ.\n⁄Ü⁄ØŸàŸÜŸá ÿ¥ÿßÿÆÿµŸá‚ÄåŸáÿß€å ÿßÿµŸÑ€å ŸáŸÜÿ± ÿÆŸàÿ¥ŸÜŸà€åÿ≥€å ÿØÿ± ÿß€åÿ±ÿßŸÜ ÿ¥⁄©ŸÑ ⁄Øÿ±ŸÅÿ™ÿü\nÿ¥ÿßÿÆÿµŸá‚ÄåŸáÿß€å ÿßÿµŸÑ€å ŸáŸÜÿ± ÿÆŸàÿ¥ŸÜŸà€åÿ≥€å ÿØÿ± ÿß€åÿ±ÿßŸÜ ÿ™ÿ≠ÿ™ ÿ™ÿ£ÿ´€åÿ± ÿ™ÿßÿ±€åÿÆ Ÿà ŸÅÿ±ŸáŸÜ⁄Ø ÿ∫ŸÜ€å ÿß€åŸÜ ⁄©ÿ¥Ÿàÿ± ÿ¥⁄©ŸÑ ⁄Øÿ±ŸÅÿ™. ÿ¢ÿ∫ÿßÿ≤ ÿß€åŸÜ ŸáŸÜÿ± ÿØÿ± ÿß€åÿ±ÿßŸÜ ÿ®Ÿá ÿØŸàÿ±ÿßŸÜ ŸÇÿ®ŸÑ ÿßÿ≤ ÿßÿ≥ŸÑÿßŸÖ ÿ®ÿ±ŸÖ€å‚Äå⁄Øÿ±ÿØÿØÿå ÿßŸÖÿß ÿ®ÿß Ÿàÿ±ŸàÿØ ÿßÿ≥ŸÑÿßŸÖ Ÿà ŸÜ€åÿßÿ≤ ÿ®Ÿá ŸÜŸàÿ¥ÿ™ŸÜ ŸÇÿ±ÿ¢ŸÜ ÿ®ÿß ÿÆÿ∑Ÿàÿ∑ ÿ≤€åÿ®ÿß Ÿà ŸÖŸÇÿ®ŸàŸÑÿå ÿß€åŸÜ ŸáŸÜÿ± ÿ®Ÿá ÿ¥ÿØÿ™ ÿ™ÿ±ŸÇ€å ⁄©ÿ±ÿØ. ÿ™Ÿàÿ≥ÿπŸá Ÿà ÿ™⁄©ŸÖ€åŸÑ ÿßŸÜŸàÿßÿπ ŸÖÿÆÿ™ŸÑŸÅ ÿÆÿ∑‚ÄåŸáÿß€å ŸÜÿ≥ÿ™ÿπŸÑ€åŸÇÿå ÿ´ŸÑÿ´ Ÿà ŸÜÿ≥ÿÆ ÿßÿ≤ ŸÖÿ´ÿßŸÑ€å‚ÄåŸáÿß€å ÿ®ÿßÿ±ÿ≤ ÿß€åŸÜ ŸáŸÜÿ± ÿØÿ± ÿß€åÿ±ÿßŸÜ ÿßÿ≥ÿ™. ÿßÿ≥ÿ™ÿßÿØÿßŸÜ ÿ®ÿ≤ÿ±⁄Ø€å ŸÖÿßŸÜŸÜÿØ ŸÖ€åÿ±ÿπŸÖÿßÿØ Ÿà €åÿßŸÇŸàÿ™ ŸÖÿ≥ÿ™ÿπÿµŸÖ€å ŸÜŸÇÿ¥ ÿ®ÿ≥€åÿßÿ± ÿ≤€åÿßÿØ€å ÿØÿ± ⁄Øÿ≥ÿ™ÿ±ÿ¥ Ÿà ÿ≤€åÿ®ÿß€å€å‚Äåÿ®ÿÆÿ¥€å ÿ®Ÿá ÿÆŸàÿ¥ŸÜŸà€åÿ≥€å ÿß€åÿ±ÿßŸÜ€å ÿØÿßÿ¥ÿ™ŸÜÿØ.\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\ncooking_tourism_pair\nDataset: cooking_tourism_pair\nSize: 42,396 training samples\nColumns: anchor and positive\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\ntype\nstring\nstring\ndetails\nmin: 17 tokensmean: 41.58 tokensmax: 269 tokens\nmin: 27 tokensmean: 297.12 tokensmax: 512 tokens\nSamples:\nanchor\npositive\n⁄ÜŸÜÿØÿ™ÿß ÿØÿ≥ÿ± ÿß€åÿ±ÿßŸÜ€å Ÿáÿ≥ÿ™ ⁄©Ÿá ÿ®ÿπÿØ ÿßÿ≤ €åŸá ÿ∫ÿ∞ÿß€å ÿ≥ŸÜ⁄Ø€åŸÜ ŸÖÿ´ŸÑ ⁄ÜŸÑŸà⁄©ÿ®ÿßÿ® ÿÆŸàÿ® ŸÖ€å‚Äå⁄Üÿ≥ÿ®Ÿá Ÿà ÿ∑ÿπŸÖ Ÿà ÿ®ÿßŸÅÿ™ÿ¥ ŸáŸÖ ŸÖÿ™ÿπÿßÿØŸÑ Ÿáÿ≥ÿ™ÿü\nÿ®ÿ±ÿß€å Ÿæÿ≥ ÿßÿ≤ €å⁄© ŸàÿπÿØŸá ÿ∫ÿ∞ÿß€å€å ÿ≥ŸÜ⁄Ø€åŸÜ ŸÖÿßŸÜŸÜÿØ ⁄ÜŸÑŸà⁄©ÿ®ÿßÿ®ÿå ÿØÿ≥ÿ±Ÿáÿß€å€å ÿ®ÿß ÿ∑ÿπŸÖ Ÿà ÿ®ÿßŸÅÿ™ ŸÖÿ™ÿπÿßÿØŸÑ ÿßŸÜÿ™ÿÆÿßÿ® ŸÖ€å‚Äåÿ¥ŸàŸÜÿØ ⁄©Ÿá ÿßÿ≠ÿ≥ÿßÿ≥ ÿ≥ŸÜ⁄Ø€åŸÜ€å ÿ±ÿß ⁄©ÿßŸáÿ¥ ÿØŸáŸÜÿØ Ÿà ÿ≠ÿ≥ ÿ™ÿßÿ≤Ÿá‚Äåÿß€å ÿ®Ÿá ÿ®ÿØŸÜ ÿ®ÿØŸáŸÜÿØ. ⁄ÜŸÜÿØ ŸÜŸÖŸàŸÜŸá ÿßÿ≤ ÿØÿ≥ÿ±Ÿáÿß€å ÿß€åÿ±ÿßŸÜ€å ŸÖŸÜÿßÿ≥ÿ® ÿ®ÿ±ÿß€å ÿß€åŸÜ ŸÖŸàŸÇÿπ€åÿ™ ÿ¥ÿßŸÖŸÑ:1. ŸÅÿ±ŸÜ€å: ÿß€åŸÜ ÿØÿ≥ÿ± ÿ≥ÿ®⁄© Ÿà ŸÑÿ∑€åŸÅ ÿßÿ≥ÿ™ Ÿà ÿ®ÿß ÿ™Ÿàÿ¨Ÿá ÿ®Ÿá ŸÖŸàÿßÿØ ŸÖÿ∫ÿ∞€å‚Äåÿßÿ¥ÿå ÿ®ÿπÿØ ÿßÿ≤ ÿ∫ÿ∞ÿß€å ÿ≥ŸÜ⁄Ø€åŸÜ ÿßÿ≠ÿ≥ÿßÿ≥ ÿÆŸàÿ®€å ÿß€åÿ¨ÿßÿØ ŸÖ€å‚Äå⁄©ŸÜÿØ.2. ÿ®ÿ≥ÿ™ŸÜ€å ÿ≥ŸÜÿ™€å: ÿ®ÿ≥ÿ™ŸÜ€å‚ÄåŸáÿß€å ÿ≤ÿπŸÅÿ±ÿßŸÜ€å €åÿß ÿ®ÿ≥ÿ™ŸÜ€å ÿ®ÿß ÿ∑ÿπŸÖ‚ÄåŸáÿß€å ÿØ€å⁄Øÿ±ÿå ÿÆŸÜ⁄© Ÿà ÿÆŸàÿ¥ŸÖÿ≤Ÿá‚ÄåÿßŸÜÿØ Ÿà ŸÖ€å‚Äåÿ™ŸàÿßŸÜŸÜÿØ ÿ®Ÿá Ÿáÿ∂ŸÖ ÿ®Ÿáÿ™ÿ± ⁄©ŸÖ⁄© ⁄©ŸÜŸÜÿØ.3. ÿ¥ŸÑŸá ÿ≤ÿ±ÿØ: ÿß€åŸÜ ÿØÿ≥ÿ± ⁄©Ÿá ÿ®ÿ± Ÿæÿß€åŸá ÿ®ÿ±ŸÜÿ¨ Ÿà ÿ≤ÿπŸÅÿ±ÿßŸÜ ÿ™Ÿá€åŸá ŸÖ€å‚Äåÿ¥ŸàÿØÿå ÿ∑ÿπŸÖ€å ÿØŸÑŸæÿ∞€åÿ± Ÿà ÿ¥€åÿ±€åŸÜ ÿØÿßÿ±ÿØ Ÿà ŸÖ€å‚Äåÿ™ŸàÿßŸÜÿØ ÿ®Ÿá ÿπŸÜŸàÿßŸÜ €å⁄© ÿØÿ≥ÿ± ŸÖÿ∑ÿ®Ÿàÿπ ÿ®ÿπÿØ ÿßÿ≤ ÿ∫ÿ∞ÿß ŸÖÿµÿ±ŸÅ ÿ¥ŸàÿØ.4. ÿ®ÿßŸÇŸÑŸàÿß: ÿß€åŸÜ ÿØÿ≥ÿ± ÿ®ÿßŸÅÿ™ ŸÜÿ±ŸÖ Ÿà ÿ∑ÿπŸÖ€å ÿ¥€åÿ±€åŸÜ Ÿà ŸÖÿ∫ÿ∞€å ÿØÿßÿ±ÿØ ⁄©Ÿá Ÿæÿ≥ ÿßÿ≤ €å⁄© ŸàÿπÿØŸá ÿ∫ÿ∞ÿß€å€å ÿ≥ŸÜ⁄Ø€åŸÜ ÿ≠ÿ≥ ÿÆŸàÿ¥ÿß€åŸÜÿØ€å ÿ®Ÿá ŸÅÿ±ÿØ ŸÖ€å‚ÄåÿØŸáÿØ.ÿß€åŸÜ ÿØÿ≥ÿ±Ÿáÿß ŸÖÿπŸÖŸàŸÑÿßŸã ÿ®ÿß ÿ∑ÿπŸÖ ŸÖŸÑÿß€åŸÖ Ÿà ŸÖÿ™ÿπÿßÿØŸÑ ÿÆŸàÿØÿå ŸÖ⁄©ŸÖŸÑ ÿÆŸàÿ®€å ÿ®ÿ±ÿß€å ÿ∫ÿ∞ÿß€å€å ÿ≥ŸÜ⁄Ø€åŸÜ ŸÖÿßŸÜŸÜÿØ ⁄ÜŸÑŸà⁄©ÿ®ÿßÿ® Ÿáÿ≥ÿ™ŸÜÿØ.\nŸÖŸÜ ÿÆ€åŸÑ€å ŸæŸÑŸà ÿ®ÿß ÿ™Ÿá‚ÄåÿØ€å⁄Ø ÿ≥€åÿ®‚Äåÿ≤ŸÖ€åŸÜ€å ÿ±Ÿà ÿØŸàÿ≥ÿ™ ÿØÿßÿ±ŸÖ! ŸÖ€å‚Äåÿ™ŸàŸÜ€å €åŸá ÿ±Ÿàÿ¥ ÿ¨ÿØ€åÿØ Ÿà ÿ¨ÿßŸÑÿ® ÿ®ÿ±ÿß€å ÿØÿ±ÿ≥ÿ™ ⁄©ÿ±ÿØŸÜÿ¥ ÿ®ŸáŸÖ ÿ®⁄Ø€å ⁄©Ÿá ÿ®ÿß ÿ∑ÿπŸÖ‚ÄåŸáÿß€å ÿÆÿßÿµ ŸÖÿ´ŸÑ ÿ≤ÿπŸÅÿ±ÿßŸÜ Ÿà ÿ≤ÿ±ÿØ⁄ÜŸàÿ®Ÿá ŸáŸÖ ÿ™ÿ±⁄©€åÿ® ÿ¥ÿØŸá ÿ®ÿßÿ¥Ÿáÿü\nÿßŸÑÿ®ÿ™Ÿá! ÿ®ÿ±ÿß€å ÿ™Ÿá€åŸá ŸæŸÑŸà ÿ®ÿß ÿ™Ÿá‚ÄåÿØ€å⁄Ø ÿ≥€åÿ®‚Äåÿ≤ŸÖ€åŸÜ€å ÿ®Ÿá ŸáŸÖÿ±ÿßŸá ÿ∑ÿπŸÖ‚ÄåŸáÿß€å ÿÆÿßÿµ ŸÖÿ´ŸÑ ÿ≤ÿπŸÅÿ±ÿßŸÜ Ÿà ÿ≤ÿ±ÿØ⁄ÜŸàÿ®Ÿáÿå ŸÖ€å‚Äåÿ™ŸàÿßŸÜ€åÿØ ÿ®Ÿá ÿ±Ÿàÿ¥ ÿ≤€åÿ± ÿπŸÖŸÑ ⁄©ŸÜ€åÿØ:### ŸÖŸàÿßÿØ ŸÑÿßÿ≤ŸÖ:- ÿ®ÿ±ŸÜÿ¨: €≤ Ÿæ€åŸÖÿßŸÜŸá- ÿ≥€åÿ®‚Äåÿ≤ŸÖ€åŸÜ€å: €≤ ÿπÿØÿØ ÿ®ÿ≤ÿ±⁄Ø- ÿ≤ÿπŸÅÿ±ÿßŸÜ: €± ŸÇÿßÿ¥ŸÇ ⁄Üÿß€å‚ÄåÿÆŸàÿ±€å (ÿØŸÖ‚Äå⁄©ÿ±ÿØŸá)- ÿ≤ÿ±ÿØ⁄ÜŸàÿ®Ÿá: €± ŸÇÿßÿ¥ŸÇ ⁄Üÿß€å‚ÄåÿÆŸàÿ±€å- ŸÜŸÖ⁄©: ÿ®Ÿá ŸÖ€åÿ≤ÿßŸÜ ŸÑÿßÿ≤ŸÖ- ÿ±Ÿàÿ∫ŸÜ: ÿ®Ÿá ŸÖ€åÿ≤ÿßŸÜ ŸÑÿßÿ≤ŸÖ- ÿ¢ÿ®: €¥ Ÿæ€åŸÖÿßŸÜŸá### ÿ∑ÿ±ÿ≤ ÿ™Ÿá€åŸá:1. ÿ¥ÿ≥ÿ™ŸÜ ÿ®ÿ±ŸÜÿ¨: ÿßÿ®ÿ™ÿØÿß ÿ®ÿ±ŸÜÿ¨ ÿ±ÿß ÿ®Ÿá ÿÆŸàÿ®€å ÿ®ÿ¥Ÿà€å€åÿØ Ÿà ⁄ÜŸÜÿØ ÿ≥ÿßÿπÿ™ ŸÇÿ®ŸÑ ÿßÿ≤ ŸæÿÆÿ™ ÿØÿ± ÿ¢ÿ® ÿ®ÿÆ€åÿ≥ÿßŸÜ€åÿØ ÿ™ÿß ŸÜÿ±ŸÖ ÿ¥ŸàÿØ.2. ÿ®ÿ±ÿ¥ ÿ≥€åÿ®‚Äåÿ≤ŸÖ€åŸÜ€å: ÿ≥€åÿ®‚Äåÿ≤ŸÖ€åŸÜ€å‚ÄåŸáÿß ÿ±ÿß ŸæŸàÿ≥ÿ™ ⁄©ŸÜÿØŸá Ÿà ÿ®Ÿá ÿ¥⁄©ŸÑ ÿÆŸÑÿßŸÑ€å ÿ®ÿ±ÿ¥ ÿ®ÿ≤ŸÜ€åÿØ. ÿ≥Ÿæÿ≥ ÿ¢ŸÜŸáÿß ÿ±ÿß ÿØÿ± ÿ¢ÿ® ÿ≥ÿ±ÿØ ÿÆ€åÿ≥ ⁄©ŸÜ€åÿØ ÿ™ÿß ŸÜÿ¥ÿßÿ≥ÿ™Ÿá ÿßÿ∂ÿßŸÅ€å ÿ¢ŸÜŸáÿß ⁄Øÿ±ŸÅÿ™Ÿá ÿ¥ŸàÿØ.3. ÿ≥ÿ±ÿÆ ⁄©ÿ±ÿØŸÜ ÿ≥€åÿ®‚Äåÿ≤ŸÖ€åŸÜ€å: ÿØÿ± €å⁄© ÿ™ÿßÿ®Ÿá ŸÖŸÇÿØÿßÿ±€å ÿ±Ÿàÿ∫ŸÜ ÿ®ÿ±€åÿ≤€åÿØ Ÿà ÿ≥€åÿ®‚Äåÿ≤ŸÖ€åŸÜ€å‚ÄåŸáÿß€å ÿÆŸÑÿßŸÑ€å ÿ±ÿß ÿØÿ± ÿ¢ŸÜ ÿ≥ÿ±ÿÆ ⁄©ŸÜ€åÿØ ÿ™ÿß ÿ∑ŸÑÿß€å€å Ÿà ÿ™ÿ±ÿØ ÿ¥ŸàŸÜÿØ. ÿ≥Ÿæÿ≥ ÿ¢ŸÜŸáÿß ÿ±ÿß ÿßÿ≤ ÿ±Ÿàÿ∫ŸÜ ÿÆÿßÿ±ÿ¨ ⁄©ÿ±ÿØŸá Ÿà ÿ±Ÿà€å ÿØÿ≥ÿ™ŸÖÿßŸÑ ⁄©ÿßÿ∫ÿ∞€å ÿ®⁄Øÿ∞ÿßÿ±€åÿØ ÿ™ÿß ÿ±Ÿàÿ∫ŸÜ ÿßÿ∂ÿßŸÅ€å ⁄Øÿ±ŸÅÿ™Ÿá ÿ¥ŸàÿØ.4. ŸæÿÆÿ™ ÿ®ÿ±ŸÜÿ¨: ÿØÿ± €å⁄© ŸÇÿßÿ®ŸÑŸÖŸá ÿ®ÿ≤ÿ±⁄Øÿå ÿ¢ÿ® ÿ±ÿß ÿ®Ÿá ÿ¨Ÿàÿ¥ ÿ®€åÿßŸàÿ±€åÿØ Ÿà ŸÖŸÇÿØÿßÿ±€å ŸÜŸÖ⁄© ÿ®Ÿá ÿ¢ŸÜ ÿßÿ∂ÿßŸÅŸá ⁄©ŸÜ€åÿØ. ÿ®ÿ±ŸÜÿ¨ ÿ±ÿß ÿ®Ÿá ÿ¢ÿ® ÿ¨Ÿàÿ¥ ÿßÿ∂ÿßŸÅŸá ⁄©ÿ±ÿØŸá Ÿà ÿ®⁄Øÿ∞ÿßÿ±€åÿØ ⁄©ŸÖ€å ÿ®Ÿæÿ≤ÿØ. ÿ®ÿπÿØ ÿßÿ≤ €µ ÿ™ÿß €∑ ÿØŸÇ€åŸÇŸáÿå ÿ®ÿ±ŸÜÿ¨ ÿ±ÿß ÿ¢ÿ®⁄©ÿ¥ ⁄©ŸÜ€åÿØ.5. ÿ™Ÿá€åŸá ŸæŸÑŸà: ÿØÿ± ÿ™Ÿá ŸÇÿßÿ®ŸÑŸÖŸáÿå ŸÖŸÇÿØÿßÿ±€å ÿ±Ÿàÿ∫ŸÜ ÿ®ÿ±€åÿ≤€åÿØ Ÿà Ÿæÿ≥ ÿßÿ≤ ÿ¢ŸÜÿå ÿ≥€åÿ®‚Äåÿ≤ŸÖ€åŸÜ€å‚ÄåŸáÿß€å ÿ≥ÿ±ÿÆ ÿ¥ÿØ...\nÿ®ÿ±ÿß€å €å⁄© ÿ≥ŸÅÿ± ÿ®Ÿá ÿ¨ŸÜ⁄ØŸÑÿå ⁄ÜŸá ÿ∫ÿ∞ÿß€å€å ÿ±Ÿà Ÿæ€åÿ¥ŸÜŸáÿßÿØ ŸÖ€å‚Äå⁄©ŸÜ€å ⁄©Ÿá ŸáŸÖ ÿ≥ÿ®⁄© Ÿà ⁄©ŸÖ ÿ≠ÿ¨ŸÖ ÿ®ÿßÿ¥Ÿáÿå ŸáŸÖ ÿßÿ≤ ŸÜÿ∏ÿ± ÿßÿ±ÿ≤ÿ¥ ÿ∫ÿ∞ÿß€å€å ÿÆŸàÿ® ÿ®ÿßÿ¥Ÿáÿü ŸÖ€å‚ÄåÿÆŸàÿßŸÖ ÿ™Ÿà€å ŸÅÿπÿßŸÑ€åÿ™‚ÄåŸáÿß€å ÿ®€åÿ±ŸàŸÜÿå ÿßŸÜÿ±⁄ò€å ⁄©ÿßŸÅ€å ÿØÿßÿ¥ÿ™Ÿá ÿ®ÿßÿ¥ŸÖ!\nÿ®ÿ±ÿß€å ÿ≥ŸÅÿ± ÿ®Ÿá ÿ¨ŸÜ⁄ØŸÑ Ÿà ŸÅÿπÿßŸÑ€åÿ™‚ÄåŸáÿß€å ÿÆÿßÿ±ÿ¨ ÿßÿ≤ ÿÆÿßŸÜŸáÿå Ÿæ€åÿ¥ŸÜŸáÿßÿØ ŸÖ€å‚Äå⁄©ŸÜŸÖ ÿßÿ≤ ⁄Øÿ≤€åŸÜŸá‚ÄåŸáÿß€å€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜ€åÿØ ⁄©Ÿá ÿ≥ÿ®⁄©ÿå ⁄©ŸÖ‚Äåÿ≠ÿ¨ŸÖ Ÿà ŸÖÿ∫ÿ∞€å ÿ®ÿßÿ¥ŸÜÿØ. ÿ®ÿ±ÿÆ€å ÿßÿ≤ ÿ®Ÿáÿ™ÿ±€åŸÜ ÿßŸÜÿ™ÿÆÿßÿ®‚ÄåŸáÿß ÿ¥ÿßŸÖŸÑ:1. ÿ¢ÿ¨€åŸÑ: ÿßŸÜŸàÿßÿπ ÿ¢ÿ¨€åŸÑ ŸÖÿßŸÜŸÜÿØ ÿ®ÿßÿØÿßŸÖÿå Ÿæÿ≥ÿ™Ÿá Ÿà ⁄Øÿ±ÿØŸà ⁄©Ÿá ŸÖŸÜÿ®ÿπ ÿÆŸàÿ®€å ÿßÿ≤ Ÿæÿ±Ÿàÿ™ÿ¶€åŸÜ Ÿà ⁄Üÿ±ÿ®€å‚ÄåŸáÿß€å ÿ≥ÿßŸÑŸÖ Ÿáÿ≥ÿ™ŸÜÿØ Ÿà ÿßŸÜÿ±⁄ò€å ÿ®ÿßŸÑÿß€å€å ÿ±ÿß ÿ™ÿßŸÖ€åŸÜ ŸÖ€å‚Äå⁄©ŸÜŸÜÿØ.2. ŸÖ€åŸàŸá‚ÄåŸáÿß€å ÿÆÿ¥⁄©: ŸÖ€åŸàŸá‚ÄåŸáÿß€å ÿÆÿ¥⁄© ŸÖÿßŸÜŸÜÿØ ÿÆÿ±ŸÖÿßÿå ⁄©ÿ¥ŸÖÿ¥ Ÿà ÿ≤ÿ±ÿØÿ¢ŸÑŸà ÿÆÿ¥⁄© ⁄©Ÿá ŸÇŸÜÿØ ÿ∑ÿ®€åÿπ€å Ÿà ÿßŸÜÿ±⁄ò€å ÿ≥ÿ±€åÿπ ŸÅÿ±ÿßŸáŸÖ ŸÖ€å‚Äå⁄©ŸÜŸÜÿØ.3. ⁄Øÿ±ÿßŸÜŸàŸÑÿß ÿ®ÿßÿ±: ÿ®ÿßÿ±Ÿáÿß€å ⁄Øÿ±ÿßŸÜŸàŸÑÿß ⁄©Ÿá ÿ¥ÿßŸÖŸÑ ÿ∫ŸÑÿßÿ™ ⁄©ÿßŸÖŸÑ Ÿà ÿ¢ÿ¨€åŸÑ Ÿáÿ≥ÿ™ŸÜÿØÿå ⁄Øÿ≤€åŸÜŸá‚ÄåŸáÿß€å ⁄©ŸÖ‚Äåÿ≠ÿ¨ŸÖ Ÿà ŸÖÿ∫ÿ∞€å ÿ®ÿ±ÿß€å ŸÖ€åÿßŸÜ‚ÄåŸàÿπÿØŸá ŸÖÿ≠ÿ≥Ÿàÿ® ŸÖ€å‚Äåÿ¥ŸàŸÜÿØ.4. Ÿæÿ±Ÿàÿ™ÿ¶€åŸÜ ÿ®ÿßÿ±: ÿ®ÿßÿ±Ÿáÿß€å€å ⁄©Ÿá ÿØÿßÿ±ÿß€å Ÿæÿ±Ÿàÿ™ÿ¶€åŸÜ ÿ®ÿßŸÑÿß Ÿáÿ≥ÿ™ŸÜÿØ Ÿà ŸÖ€å‚Äåÿ™ŸàÿßŸÜŸÜÿØ ÿßŸÜÿ±⁄ò€å ŸÑÿßÿ≤ŸÖ ÿ®ÿ±ÿß€å ŸÅÿπÿßŸÑ€åÿ™‚ÄåŸáÿß€å ÿ®ÿØŸÜ€å ÿ±ÿß ŸÅÿ±ÿßŸáŸÖ ⁄©ŸÜŸÜÿØ.5. ÿ≥ÿßŸÜÿØŸà€å⁄Ü‚ÄåŸáÿß€å ÿ≥ÿ®⁄©: ÿ≥ÿßŸÜÿØŸà€å⁄Ü‚ÄåŸáÿß€å€å ÿ®ÿß ŸÜÿßŸÜ ÿ≥ÿ®Ÿàÿ≥‚ÄåÿØÿßÿ±ÿå Ÿæÿ±Ÿàÿ™ÿ¶€åŸÜ ŸÖÿßŸÜŸÜÿØ ÿ™ŸÜ ŸÖÿßŸá€å €åÿß ÿ≥€åŸÜŸá ŸÖÿ±ÿ∫ Ÿà ÿ≥ÿ®ÿ≤€åÿ¨ÿßÿ™ ÿ™ÿßÿ≤Ÿá ⁄©Ÿá ÿ≥€åÿ±⁄©ŸÜŸÜÿØŸá Ÿà ŸÖÿ∫ÿ∞€å Ÿáÿ≥ÿ™ŸÜÿØ.ÿß€åŸÜ ⁄Øÿ≤€åŸÜŸá‚ÄåŸáÿß ÿ®Ÿá ÿ¥ŸÖÿß ⁄©ŸÖ⁄© ŸÖ€å‚Äå⁄©ŸÜŸÜÿØ ÿ™ÿß ÿßŸÜÿ±⁄ò€å ŸÑÿßÿ≤ŸÖ ÿ®ÿ±ÿß€å ŸÅÿπÿßŸÑ€åÿ™‚ÄåŸáÿß€å ÿÆÿßÿ±ÿ¨ ÿßÿ≤ ŸÖŸÜÿ≤ŸÑ ÿ±ÿß ÿØÿßÿ¥ÿ™Ÿá ÿ®ÿßÿ¥€åÿØ Ÿà ÿØÿ± ÿπ€åŸÜ ÿ≠ÿßŸÑ ⁄©ŸÖ‚Äåÿ≠ÿ¨ŸÖ Ÿà ÿ≥ÿ®⁄© ÿ®ÿßÿ¥ŸÜÿØ.\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\ntranslation_pair\nDataset: translation_pair\nSize: 60,691 training samples\nColumns: anchor and positive\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\ntype\nstring\nstring\ndetails\nmin: 9 tokensmean: 195.28 tokensmax: 512 tokens\nmin: 10 tokensmean: 203.28 tokensmax: 512 tokens\nSamples:\nanchor\npositive\nSure! Here are some ideas for using fabric and thread in your mixed media piece exploring consumerism:1. Use fabric and thread to create a shopping bag or purse. This could be decorated with logos or images of products from popular brands or messages about the cost of overconsumption.2. Use fabrics with patterns that relate to consumerism, like plaid or gingham, as a background. This could represent the idea of buying more than you need or the pattern of consumption within a society.3. Incorporate denim fabric, which is traditionally used for jeans, to represent mass production and consumption.4. Use thread to sew outlines around logos or images of products from popular brands. This creates a sense of detaching what the brand represents to the individual‚Äôs perception.5. Incorporate scraps of paper and fabric that mimic product packaging to create dimension within your artwork. 6. Use contrasting colors and textures of fabric to create a sense of disruption or chaos within y...\nÿ≠ÿ™ŸÖÿßŸã! ÿØÿ± ÿß€åŸÜÿ¨ÿß ÿß€åÿØŸá‚ÄåŸáÿß€å€å ÿ®ÿ±ÿß€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ Ÿæÿßÿ±⁄ÜŸá Ÿà ŸÜÿÆ ÿØÿ± ÿßÿ´ÿ± ŸáŸÜÿ± ÿ™ÿ±⁄©€åÿ®€å ÿ¥ŸÖÿß ÿØÿ± ŸÖŸàÿ±ÿØ ŸÖÿµÿ±ŸÅ‚Äå⁄Øÿ±ÿß€å€å ÿßÿ±ÿßÿ¶Ÿá ÿ¥ÿØŸá ÿßÿ≥ÿ™:1. ÿßÿ≤ Ÿæÿßÿ±⁄ÜŸá Ÿà ŸÜÿÆ ÿ®ÿ±ÿß€å ÿ≥ÿßÿÆÿ™ €å⁄© ⁄©€åŸÅ ÿÆÿ±€åÿØ €åÿß ⁄©€åŸÅ ÿØÿ≥ÿ™€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜ€åÿØ. ÿß€åŸÜ ŸÖ€å‚Äåÿ™ŸàÿßŸÜÿØ ÿ®ÿß ŸÑŸà⁄ØŸàŸáÿß €åÿß ÿ™ÿµÿßŸà€åÿ± ŸÖÿ≠ÿµŸàŸÑÿßÿ™ ÿ®ÿ±ŸÜÿØŸá‚ÄåŸáÿß€å ŸÖÿπÿ±ŸàŸÅ €åÿß Ÿæ€åÿßŸÖ‚ÄåŸáÿß€å€å ÿØÿ±ÿ®ÿßÿ±Ÿá Ÿáÿ≤€åŸÜŸá‚ÄåŸáÿß€å ŸÖÿµÿ±ŸÅ ÿ®€åÿ¥ ÿßÿ≤ ÿ≠ÿØ ÿ™ÿ≤ÿ¶€åŸÜ ÿ¥ŸàÿØ.2. ÿßÿ≤ Ÿæÿßÿ±⁄ÜŸá‚ÄåŸáÿß€å€å ÿ®ÿß ÿßŸÑ⁄ØŸàŸáÿß€å ŸÖÿ±ÿ™ÿ®ÿ∑ ÿ®ÿß ŸÖÿµÿ±ŸÅ‚Äå⁄Øÿ±ÿß€å€åÿå ŸÖÿßŸÜŸÜÿØ ⁄ÜŸáÿßÿ±ÿÆÿßŸÜŸá €åÿß ⁄Ø€åŸÜ⁄ØŸáÿßŸÖÿå ÿ®Ÿá ÿπŸÜŸàÿßŸÜ Ÿæÿ≥‚Äåÿ≤ŸÖ€åŸÜŸá ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜ€åÿØ. ÿß€åŸÜ ŸÖ€å‚Äåÿ™ŸàÿßŸÜÿØ ÿß€åÿØŸá ÿÆÿ±€åÿØ ÿ®€åÿ¥ ÿßÿ≤ ŸÜ€åÿßÿ≤ ÿÆŸàÿØ €åÿß ÿßŸÑ⁄ØŸà€å ŸÖÿµÿ±ŸÅ ÿØÿ± €å⁄© ÿ¨ÿßŸÖÿπŸá ÿ±ÿß ŸÜÿ¥ÿßŸÜ ÿØŸáÿØ.3. Ÿæÿßÿ±⁄ÜŸá ÿ¨€åŸÜÿå ⁄©Ÿá ÿ®Ÿá‚Äåÿ∑Ÿàÿ± ÿ≥ŸÜÿ™€å ÿ®ÿ±ÿß€å ÿ¥ŸÑŸàÿßÿ± ÿ¨€åŸÜ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ŸÖ€å‚Äåÿ¥ŸàÿØÿå ÿ±ÿß ÿ®ÿ±ÿß€å ŸÜÿ¥ÿßŸÜ ÿØÿßÿØŸÜ ÿ™ŸàŸÑ€åÿØ Ÿà ŸÖÿµÿ±ŸÅ ÿßŸÜÿ®ŸàŸáÿå ÿ®⁄ØŸÜÿ¨ÿßŸÜ€åÿØ.4. ÿßÿ≤ ŸÜÿÆ ÿ®ÿ±ÿß€å ÿØŸàÿÆÿ™ŸÜ ÿØŸàÿ± ŸÑŸà⁄ØŸàŸáÿß €åÿß ÿ™ÿµÿßŸà€åÿ± ŸÖÿ≠ÿµŸàŸÑÿßÿ™ ÿ®ÿ±ŸÜÿØŸá‚ÄåŸáÿß€å ŸÖÿπÿ±ŸàŸÅ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜ€åÿØ. ÿß€åŸÜ ÿ≠ÿ≥ ÿ¨ÿØÿß ⁄©ÿ±ÿØŸÜ ÿ¢ŸÜ⁄ÜŸá ÿ®ÿ±ŸÜÿØ ŸÜŸÖÿß€åŸÜÿØ⁄Ø€å ŸÖ€å‚Äå⁄©ŸÜÿØ ÿßÿ≤ ÿØÿ±⁄© ŸÅÿ±ÿØ ÿ±ÿß ÿß€åÿ¨ÿßÿØ ŸÖ€å‚Äå⁄©ŸÜÿØ.5. ÿ™⁄©Ÿá‚ÄåŸáÿß€å ⁄©ÿßÿ∫ÿ∞ Ÿà Ÿæÿßÿ±⁄ÜŸá‚Äåÿß€å ⁄©Ÿá ÿ®ÿ≥ÿ™Ÿá‚Äåÿ®ŸÜÿØ€å ŸÖÿ≠ÿµŸàŸÑÿßÿ™ ÿ±ÿß ÿ™ÿØÿßÿπ€å ŸÖ€å‚Äå⁄©ŸÜŸÜÿØÿå ÿ®Ÿá‚Äå⁄©ÿßÿ± ÿ®ÿ±€åÿØ ÿ™ÿß ÿØÿ± ÿßÿ´ÿ± ÿÆŸàÿØ ÿ®ŸèÿπÿØ ÿß€åÿ¨ÿßÿØ ⁄©ŸÜ€åÿØ.6. ÿßÿ≤ ÿ±ŸÜ⁄Ø‚ÄåŸáÿß Ÿà ÿ®ÿßŸÅÿ™‚ÄåŸáÿß€å ŸÖÿ™ÿ∂ÿßÿØ Ÿæÿßÿ±⁄ÜŸá ÿ®ÿ±ÿß€å ÿß€åÿ¨ÿßÿØ ÿ≠ÿ≥ ÿßÿÆÿ™ŸÑÿßŸÑ €åÿß ÿ¢ÿ¥ŸÅÿ™⁄Ø€å ÿØÿ± ÿßÿ´ÿ± ÿÆŸàÿØ ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜ€åÿØ. ÿß€åŸÜ ŸÖ€å‚Äåÿ™ŸàÿßŸÜÿØ ŸÜŸÖÿß€åÿ¥‚ÄåÿØŸáŸÜÿØŸá ÿ∑ÿ®€åÿπÿ™ ÿ∫ÿßŸÅŸÑ⁄Ø€åÿ±⁄©ŸÜŸÜÿØŸá ŸÖÿµÿ±ŸÅ‚Äå⁄Øÿ±ÿß€å€å ÿ®ÿßÿ¥ÿØ.7...\nStudents from both traditional and continuing education programs have opportunities to participate in college governance through open meetings and the Student Government Association (SGA). They can propose changes to governance structures through amendment processes and request funds from activity fees. The student body president and some executive board members attend board of trustee committee meetings, and the president serves on hiring committees for administrative positions. Student representatives are chosen through popular voting in the spring and are part of committees addressing different categories. Staff are represented through the AASSA, while non-supervisory staff serve on committees that make decisions or recommendations on various issues. Faculty members, including adjuncts, can participate in committees like the Budget, Strategic Planning, and Curriculum committees that influence college policies. The Clerk of Faculty position oversees communications between the adminis...\nÿØÿßŸÜÿ¥ÿ¨Ÿà€åÿßŸÜ Ÿáÿ± ÿØŸà ÿ®ÿ±ŸÜÿßŸÖŸá ÿ≥ŸÜÿ™€å Ÿà ÿ¢ŸÖŸàÿ≤ÿ¥ ŸÖÿØÿßŸàŸÖ ŸÅÿ±ÿµÿ™‚ÄåŸáÿß€å€å ÿ®ÿ±ÿß€å ŸÖÿ¥ÿßÿ±⁄©ÿ™ ÿØÿ± ÿ≠ÿß⁄©ŸÖ€åÿ™ ÿØÿßŸÜÿ¥⁄ØÿßŸá ÿßÿ≤ ÿ∑ÿ±€åŸÇ ÿ¨ŸÑÿ≥ÿßÿ™ ÿπŸÖŸàŸÖ€å Ÿà ÿßŸÜÿ¨ŸÖŸÜ ÿØŸàŸÑÿ™ ÿØÿßŸÜÿ¥ÿ¨Ÿà€å€å (SGA) ÿØÿßÿ±ŸÜÿØ. ÿ¢ŸÜŸáÿß ŸÖ€å‚Äåÿ™ŸàÿßŸÜŸÜÿØ ÿßÿ≤ ÿ∑ÿ±€åŸÇ ŸÅÿ±ÿ¢€åŸÜÿØŸáÿß€å ÿßÿµŸÑÿßÿ≠€å ÿ™ÿ∫€å€åÿ±ÿßÿ™€å ÿ±ÿß ÿØÿ± ÿ≥ÿßÿÆÿ™ÿßÿ±Ÿáÿß€å ÿ≠ÿß⁄©ŸÖ€åÿ™€å Ÿæ€åÿ¥ŸÜŸáÿßÿØ ÿØŸáŸÜÿØ Ÿà ÿ®ŸàÿØÿ¨Ÿá‚Äåÿß€å ÿßÿ≤ Ÿáÿ≤€åŸÜŸá‚ÄåŸáÿß€å ŸÅÿπÿßŸÑ€åÿ™ ÿØÿ±ÿÆŸàÿßÿ≥ÿ™ ⁄©ŸÜŸÜÿØ. ÿ±ÿ¶€åÿ≥ ÿ®ÿØŸÜ€Ä ÿØÿßŸÜÿ¥ÿ¨Ÿà€å€å Ÿà ÿ™ÿπÿØÿßÿØ€å ÿßÿ≤ ÿßÿπÿ∂ÿß€å Ÿá€åÿ¶ÿ™ ÿßÿ¨ÿ±ÿß€å€å ÿØÿ± ÿ¨ŸÑÿ≥ÿßÿ™ ⁄©ŸÖ€åÿ™Ÿá‚ÄåŸáÿß€å Ÿá€åÿ¶ÿ™ ÿßŸÖŸÜÿß ÿ¥ÿ±⁄©ÿ™ ŸÖ€å‚Äå⁄©ŸÜŸÜÿØ Ÿà ÿ±ÿ¶€åÿ≥ ÿØÿ± ⁄©ŸÖ€åÿ™Ÿá‚ÄåŸáÿß€å ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ€å ÿ®ÿ±ÿß€å ÿ≥ŸÖÿ™‚ÄåŸáÿß€å ÿßÿØÿßÿ±€å ÿÆÿØŸÖÿ™ ŸÖ€å‚Äå⁄©ŸÜÿØ. ŸÜŸÖÿß€åŸÜÿØ⁄ØÿßŸÜ ÿØÿßŸÜÿ¥ÿ¨Ÿà€å€å ÿßÿ≤ ÿ∑ÿ±€åŸÇ ÿ±ÿ£€å‚Äå⁄Ø€åÿ±€å ÿπŸÖŸàŸÖ€å ÿØÿ± ÿ®Ÿáÿßÿ± ÿßŸÜÿ™ÿÆÿßÿ® ŸÖ€å‚Äåÿ¥ŸàŸÜÿØ Ÿà ÿØÿ± ⁄©ŸÖ€åÿ™Ÿá‚ÄåŸáÿß€å€å ⁄©Ÿá ÿ®Ÿá ÿØÿ≥ÿ™Ÿá‚Äåÿ®ŸÜÿØ€å‚ÄåŸáÿß€å ŸÖÿÆÿ™ŸÑŸÅ ŸÖ€å‚ÄåŸæÿ±ÿØÿßÿ≤ŸÜÿØÿå ÿ¥ÿ±⁄©ÿ™ ÿØÿßÿ±ŸÜÿØ. ⁄©ÿßÿ±⁄©ŸÜÿßŸÜ ÿßÿ≤ ÿ∑ÿ±€åŸÇ AASSA ŸÜŸÖÿß€åŸÜÿØ⁄Ø€å ŸÖ€å‚Äåÿ¥ŸàŸÜÿØÿå ÿØÿ± ÿ≠ÿßŸÑ€å ⁄©Ÿá ⁄©ÿßÿ±⁄©ŸÜÿßŸÜ ÿ∫€åÿ± ŸÖÿØ€åÿ±€åÿ™€å ÿØÿ± ⁄©ŸÖ€åÿ™Ÿá‚ÄåŸáÿß€å€å ⁄©Ÿá ÿ™ÿµŸÖ€åŸÖÿßÿ™ €åÿß ÿ™Ÿàÿµ€åŸá‚ÄåŸáÿß€å€å ÿØÿ±ÿ®ÿßÿ±Ÿá ŸÖŸàÿ∂Ÿàÿπÿßÿ™ ŸÖÿÆÿ™ŸÑŸÅ ÿØÿßÿ±ŸÜÿØÿå ÿÆÿØŸÖÿ™ ŸÖ€å‚Äå⁄©ŸÜŸÜÿØ. ÿßÿπÿ∂ÿß€å Ÿá€åÿ¶ÿ™ ÿπŸÑŸÖ€å ÿßÿ≤ ÿ¨ŸÖŸÑŸá ÿßÿπÿ∂ÿß€å Ÿæÿßÿ±Ÿá‚ÄåŸàŸÇÿ™ ŸÖ€å‚Äåÿ™ŸàÿßŸÜŸÜÿØ ÿØÿ± ⁄©ŸÖ€åÿ™Ÿá‚ÄåŸáÿß€å€å ŸÖÿßŸÜŸÜÿØ ÿ®ŸàÿØÿ¨Ÿáÿå ÿ®ÿ±ŸÜÿßŸÖŸá‚Äåÿ±€åÿ≤€å ÿßÿ≥ÿ™ÿ±ÿßÿ™⁄ò€å⁄© Ÿà ÿ®ÿ±ŸÜÿßŸÖŸá ÿØÿ±ÿ≥€å ÿ¥ÿ±⁄©ÿ™ ⁄©ŸÜŸÜÿØ ⁄©Ÿá ÿ≥€åÿßÿ≥ÿ™‚ÄåŸáÿß€å ÿØÿßŸÜÿ¥⁄ØÿßŸá ÿ±ÿß ÿ™ÿ≠ÿ™ ÿ™ÿ£ÿ´€åÿ± ŸÇÿ±ÿßÿ± ŸÖ€å‚ÄåÿØŸáŸÜÿØ. ŸÖŸàŸÇÿπ€åÿ™ ÿØÿ®€åÿ± Ÿá€åÿ¶ÿ™ ÿπŸÑŸÖ€å ÿßÿ±ÿ™ÿ®ÿßÿ∑ÿßÿ™ ÿ®€åŸÜ ŸÖÿØ€åÿ±€åÿ™ Ÿà Ÿá€åÿ¶ÿ™ ÿπŸÑŸÖ€å ÿ±ÿß ŸÜÿ∏ÿßÿ±ÿ™ ŸÖ€å‚Äå⁄©ŸÜÿØ Ÿà ŸÖÿ¨ÿßŸÖÿπ ÿßÿ¨ÿ™ŸÖÿßÿπ€å ÿ±ÿß ÿ™ÿ≥Ÿá€åŸÑ ŸÖ€å‚Äå⁄©ŸÜÿØÿå ŸáŸÖ⁄ÜŸÜ€åŸÜ ÿØÿ±ÿ®ÿßÿ±Ÿá Ÿæ€åÿßŸÖ‚ÄåŸáÿß€å ÿÆ...\nWrite a poetry collection consisting of at least 10 poems that explore the theme of hope as a powerful force in life. Your collection should feature different styles of poetry and include imagery, metaphors, and other literary devices to convey the impact of hope in personal, social, and global contexts. Consider incorporating different perspectives, symbols, or cultural references to illustrate the universality of hope and its transformative potential. Make sure to revise and edit your poems to ensure they are well-crafted and impactful for readers.\nŸÖÿ¨ŸÖŸàÿπŸá‚Äåÿß€å ÿßÿ≤ ÿßÿ¥ÿπÿßÿ± ÿ®ŸÜŸà€åÿ≥ ⁄©Ÿá ÿ¥ÿßŸÖŸÑ ÿ≠ÿØÿßŸÇŸÑ €±€∞ ÿ¥ÿπÿ± ÿ®ÿßÿ¥ÿØ Ÿà ŸÖŸàÿ∂Ÿàÿπ ÿßŸÖ€åÿØ ÿ±ÿß ÿ®Ÿá ÿπŸÜŸàÿßŸÜ €å⁄© ŸÜ€åÿ±Ÿà€å ŸÇÿØÿ±ÿ™ŸÖŸÜÿØ ÿØÿ± ÿ≤ŸÜÿØ⁄Ø€å ÿ®ÿ±ÿ±ÿ≥€å ⁄©ŸÜÿØ. ŸÖÿ¨ŸÖŸàÿπŸá ÿ¥ŸÖÿß ÿ®ÿß€åÿØ ÿ≥ÿ®⁄©‚ÄåŸáÿß€å ŸÖÿÆÿ™ŸÑŸÅ ÿ¥ÿπÿ± ÿ±ÿß ÿ¥ÿßŸÖŸÑ ÿ¥ŸàÿØ Ÿà ÿßÿ≤ ÿ™ÿµÿßŸà€åÿ±€åÿå ÿßÿ≥ÿ™ÿπÿßÿ±Ÿá‚ÄåŸáÿß Ÿà ÿØ€å⁄Øÿ± ÿßÿ®ÿ≤ÿßÿ±Ÿáÿß€å ÿßÿØÿ®€å ÿ®ÿ±ÿß€å ÿßŸÜÿ™ŸÇÿßŸÑ ÿ™ÿ£ÿ´€åÿ± ÿßŸÖ€åÿØ ÿØÿ± ÿ≤ŸÖ€åŸÜŸá‚ÄåŸáÿß€å ÿ¥ÿÆÿµ€åÿå ÿßÿ¨ÿ™ŸÖÿßÿπ€å Ÿà ÿ¨ŸáÿßŸÜ€å ÿßÿ≥ÿ™ŸÅÿßÿØŸá ⁄©ŸÜÿØ. ÿ®Ÿá ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ ÿØ€åÿØ⁄ØÿßŸá‚ÄåŸáÿß€å ŸÖÿÆÿ™ŸÑŸÅÿå ŸÜŸÖÿßÿØŸáÿß €åÿß ÿßÿ±ÿ¨ÿßÿπÿßÿ™ ŸÅÿ±ŸáŸÜ⁄Ø€å ŸÅ⁄©ÿ± ⁄©ŸÜ ÿ™ÿß ÿ¨ŸáÿßŸÜ€å ÿ®ŸàÿØŸÜ ÿßŸÖ€åÿØ Ÿà Ÿæÿ™ÿßŸÜÿ≥€åŸÑ ÿ™ÿ≠ŸàŸÑ‚Äåÿ¢ŸÅÿ±€åŸÜ ÿ¢ŸÜ ÿ±ÿß ŸÜÿ¥ÿßŸÜ ÿØŸá€å. ÿ≠ÿ™ŸÖÿßŸã ÿßÿ¥ÿπÿßÿ± ÿÆŸàÿØ ÿ±ÿß Ÿà€åÿ±ÿß€åÿ¥ Ÿà ÿßÿµŸÑÿßÿ≠ ⁄©ŸÜ ÿ™ÿß ÿ®ÿ±ÿß€å ÿÆŸàÿßŸÜŸÜÿØ⁄ØÿßŸÜ ÿ®Ÿá ÿÆŸàÿ®€å ÿ≥ÿßÿÆÿ™Ÿá Ÿà ÿ™ÿ£ÿ´€åÿ±⁄Øÿ∞ÿßÿ± ÿ®ÿßÿ¥ŸÜÿØ.\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nTraining Hyperparameters\nNon-Default Hyperparameters\nper_device_train_batch_size: 30\nper_device_eval_batch_size: 20\ngradient_accumulation_steps: 2\ntorch_empty_cache_steps: 400\nweight_decay: 0.01\nnum_train_epochs: 4\nlr_scheduler_type: polynomial\nwarmup_ratio: 0.4\nseed: 2024\ndata_seed: 2024\nfp16: True\ngroup_by_length: True\nbatch_sampler: no_duplicates\nFramework Versions\nPython: 3.12.3\nSentence Transformers: 3.3.1\nTransformers: 4.47.0.dev0\nPyTorch: 2.4.1+cu121\nAccelerate: 1.1.1\nDatasets: 3.1.0\nTokenizers: 0.20.1\nCitation\nIf you find this model helpful, please ensure to cite the following paper.\nBibTeX:\n@misc{hosseinbeigi2025advancingretrievalaugmentedgenerationpersian,\ntitle={Advancing Retrieval-Augmented Generation for Persian: Development of Language Models, Comprehensive Benchmarks, and Best Practices for Optimization},\nauthor={Sara Bourbour Hosseinbeigi and Sina Asghari and Mohammad Ali Seif Kashani and Mohammad Hossein Shalchian and Mohammad Amin Abbasi},\nyear={2025},\neprint={2501.04858},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.04858},\n}",
    "TheDrummer/Behemoth-123B-v2.1": "Join our Discord! https://discord.gg/Nbv9pQ88Xb\nNearly 2500 members strong üí™\nNow with more channels! A hub for creatives and makers alike!\nBehemoth 123B v2.1 ü¶£\nLinks\nDescription\nUsage\nVersions\nSpecial Thanks\nJoin our Discord! https://discord.gg/Nbv9pQ88Xb\nNearly 2500 members strong üí™\nNow with more channels! A hub for creatives and makers alike!\nBeaverAI proudly presents...\nBehemoth 123B v2.1 ü¶£\nNothing in the void is foreign to us. The place we go is the place we belong.\nLinks\nOriginal: https://huggingface.co/TheDrummer/Behemoth-123B-v2.1\nGGUF: https://huggingface.co/TheDrummer/Behemoth-123B-v2.1-GGUF\niMatrix: https://huggingface.co/bartowski/Behemoth-123B-v2.1-GGUF (recommended for smaller quants)\nDescription\nBehemoth v2.x is a finetune of the new Largestral 2411 with system prompt support. Testers have noted that everything felt improved.\nUsage\nTesters say this frankenformat maximizes the model's potential: Metharme with Mistral's new system tokens\n[SYSTEM_PROMPT] <|system|>{{system_message}}[/SYSTEM_PROMPT]<|user|>{{user_message}}<|model|>{{assistant_message}}\n<|system|>[SYSTEM_PROMPT] {{system_message}}[/SYSTEM_PROMPT]<|user|>{{user_message}}<|model|>{{assistant_message}}\nTake note that the opening system tag SHOULD ALWAYS have a leading whitespace after it.\nComplete SillyTavern Settings in BeaverAI Club: https://discord.com/channels/1238219753324281886/1309968730301792370/1309968730301792370\nVersions\nv2.0 is equivalent to Behemoth v1.0 (Classic)\nv2.1 is equivalent to Behemoth v1.1 (Creative Boost)\nv2.2 is an improvement of Behemoth v2.1 (Creative++)\nSpecial Thanks\nThank you to each and everyone who donated/subscribed in Ko-Fi üôá I hope to never disappoint!\nToasty Pigeon\ntheguywhogamesalot\nGrozi\nF\nMarinara\nKo-fi Supporter\nGrozi\nPhaelon\nONTHEREDTEAM\nEvarinSharath'fe(USM-Valor)\nSilva\nDakkidaze\nAlexTheVP\nPseudo\nKistara\nDr. Fjut\nGrozi ü•à\nKinjiHakari777\ndustywintr\nSyd\nHumbleConsumer\nSyd\nKo-fi Supporter\nArkamist\njoe ü•á\nToad\nLied\nKonnect\nKistara\nGrozi ü•â\nSleepDeprived3\nLuigi\nNestor\nhttps://ko-fi.com/thedrummer/leaderboard\nFinetuned by yours truly,\nDrummer",
    "OuteAI/OuteTTS-0.2-500M": "A newer version of this model is available:\nOuteAI/Llama-OuteTTS-1.0-1B\nModel Description\nKey Improvements\nSpeech Demo\nInstallation\nUsage\nQuick Start: Basic Full Example\nBackend-Specific Configuration\nHugging Face Transformers\nGGUF (llama-cpp-python)\nExLlamaV2\nSpeaker Creation and Management\nCreating a Speaker\nSaving and Loading Speaker Profiles\nDefault Speaker Initialization\nText-to-Speech Generation\nCustom Backend Configuration\nExample with Flash Attention for Hugging Face Transformers\nSpeaker Profile Recommendations\nModel Specifications\nTraining Datasets\nCredits & References\nOuteAI\nüåê OuteAI.com\nüí¨ Join our Discord\nùïè @OuteAI\nü§ó Hugging Face - OuteTTS 0.2 500M\nü§ó Hugging Face - OuteTTS 0.2 500M GGUF\nü§ó Hugging Face - Demo Space\nGitHub - OuteTTS\nModel Description\nOuteTTS-0.2-500M is our improved successor to the v0.1 release.\nThe model maintains the same approach of using audio prompts without architectural changes to the foundation model itself.\nBuilt upon the Qwen-2.5-0.5B, this version was trained on larger and more diverse datasets, resulting in significant improvements across all aspects of performance.\nSpecial thanks to Hugging Face for providing GPU grant that supported the training of this model!\nKey Improvements\nEnhanced Accuracy: Significantly improved prompt following and output coherence compared to the previous version\nNatural Speech: Produces more natural and fluid speech synthesis\nExpanded Vocabulary: Trained on over 5 billion audio prompt tokens\nVoice Cloning: Improved voice cloning capabilities with greater diversity and accuracy\nMultilingual Support: New experimental support for Chinese, Japanese, and Korean languages\nSpeech Demo\nYour browser does not support the video tag.\nInstallation\npip install outetts --upgrade\nImportant:\nFor GGUF support, install llama-cpp-python manually. Installation Guide\nFor EXL2 support, install exllamav2 manually. Installation Guide\nUsage\nQuick Start: Basic Full Example\nimport outetts\n# Configure the model\nmodel_config = outetts.HFModelConfig_v1(\nmodel_path=\"OuteAI/OuteTTS-0.2-500M\",\nlanguage=\"en\",  # Supported languages in v0.2: en, zh, ja, ko\n)\n# Initialize the interface\ninterface = outetts.InterfaceHF(model_version=\"0.2\", cfg=model_config)\n# Print available default speakers\ninterface.print_default_speakers()\n# Load a default speaker\nspeaker = interface.load_default_speaker(name=\"male_1\")\n# Generate speech\noutput = interface.generate(\ntext=\"Speech synthesis is the artificial production of human speech.\",\ntemperature=0.1,\nrepetition_penalty=1.1,\nmax_length=4096,\n# Optional: Use a speaker profile for consistent voice characteristics\n# Without a speaker profile, the model will generate a voice with random characteristics\nspeaker=speaker,\n)\n# Save the generated speech to a file\noutput.save(\"output.wav\")\n# Optional: Play the generated audio\n# output.play()\nBackend-Specific Configuration\nHugging Face Transformers\nimport outetts\nmodel_config = outetts.HFModelConfig_v1(\nmodel_path=\"OuteAI/OuteTTS-0.2-500M\",\nlanguage=\"en\",  # Supported languages in v0.2: en, zh, ja, ko\n)\ninterface = outetts.InterfaceHF(model_version=\"0.2\", cfg=model_config)\nGGUF (llama-cpp-python)\nimport outetts\nmodel_config = outetts.GGUFModelConfig_v1(\nmodel_path=\"local/path/to/model.gguf\",\nlanguage=\"en\", # Supported languages in v0.2: en, zh, ja, ko\nn_gpu_layers=0,\n)\ninterface = outetts.InterfaceGGUF(model_version=\"0.2\", cfg=model_config)\nExLlamaV2\nimport outetts\nmodel_config = outetts.EXL2ModelConfig_v1(\nmodel_path=\"local/path/to/model\",\nlanguage=\"en\", # Supported languages in v0.2: en, zh, ja, ko\n)\ninterface = outetts.InterfaceEXL2(model_version=\"0.2\", cfg=model_config)\nSpeaker Creation and Management\nCreating a Speaker\nYou can create a speaker profile for voice cloning, which is compatible across all backends.\nspeaker = interface.create_speaker(\naudio_path=\"path/to/audio/file.wav\",\n# If transcript is not provided, it will be automatically transcribed using Whisper\ntranscript=None,            # Set to None to use Whisper for transcription\nwhisper_model=\"turbo\",      # Optional: specify Whisper model (default: \"turbo\")\nwhisper_device=None,        # Optional: specify device for Whisper (default: None)\n)\nSaving and Loading Speaker Profiles\nSpeaker profiles can be saved and loaded across all supported backends.\n# Save speaker profile\ninterface.save_speaker(speaker, \"speaker.json\")\n# Load speaker profile\nspeaker = interface.load_speaker(\"speaker.json\")\nDefault Speaker Initialization\nOuteTTS includes a set of default speaker profiles. Use them directly:\n# Print available default speakers\ninterface.print_default_speakers()\n# Load a default speaker\nspeaker = interface.load_default_speaker(name=\"male_1\")\nText-to-Speech Generation\nThe generation process is consistent across all backends.\noutput = interface.generate(\ntext=\"Speech synthesis is the artificial production of human speech.\",\ntemperature=0.1,\nrepetition_penalty=1.1,\nmax_length=4096,\nspeaker=speaker, # Optional: speaker profile\n)\noutput.save(\"output.wav\")\n# Optional: Play the audio\n# output.play()\nCustom Backend Configuration\nYou can initialize custom backend configurations for specific needs.\nExample with Flash Attention for Hugging Face Transformers\nmodel_config = outetts.HFModelConfig_v1(\nmodel_path=\"OuteAI/OuteTTS-0.2-500M\",\nlanguage=\"en\",\ndtype=torch.bfloat16,\nadditional_model_config={\n'attn_implementation': \"flash_attention_2\"\n}\n)\nSpeaker Profile Recommendations\nTo achieve the best results when creating a speaker profile, consider the following recommendations:\nAudio Clip Duration:\nUse an audio clip of around 10-15 seconds.\nThis duration provides sufficient data for the model to learn the speaker's characteristics while keeping the input manageable. The model's context length is 4096 tokens, allowing it to generate around 54 seconds of audio in total. However, when a speaker profile is included, this capacity is reduced proportionally to the length of the speaker's audio clip.\nAudio Quality:\nEnsure the audio is clear and noise-free. Background noise or distortions can reduce the model's ability to extract accurate voice features.\nAccurate Transcription:\nProvide a highly accurate transcription of the audio clip. Mismatches between the audio and transcription can lead to suboptimal results.\nSpeaker Familiarity:\nThe model performs best with voices that are similar to those seen during training. Using a voice that is significantly different from typical training samples (e.g., unique accents, rare vocal characteristics) might result in inaccurate replication.\nIn such cases, you may need to fine-tune the model specifically on your target speaker's voice to achieve a better representation.\nParameter Adjustments:\nAdjust parameters like temperature in the generate function to refine the expressive quality and consistency of the synthesized voice.\nModel Specifications\nBase Model: Qwen-2.5-0.5B\nParameter Count: 500M\nLanguage Support:\nPrimary: English\nExperimental: Chinese, Japanese, Korean\nLicense: CC BY NC 4.0\nTraining Datasets\nEmilia-Dataset (CC BY NC 4.0)\nLibriTTS-R (CC BY 4.0)\nMultilingual LibriSpeech (MLS) (CC BY 4.0)\nCredits & References\nWavTokenizer\nCTC Forced Alignment\nQwen-2.5-0.5B",
    "chestnutlzj/WF-VAE-L-16Chn": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model\nIf you like our project, please give us a star ‚≠ê on GitHub for latest update.",
    "amazon/chronos-bolt-mini": "Chronos-Bolt‚ö° (Mini)\nPerformance\nUsage\nUsage with AutoGluon\nDeploying a Chronos-Bolt endpoint to SageMaker\nUsage with inference library\nCitation\nLicense\nChronos-Bolt‚ö° (Mini)\nüöÄ Update Feb 14, 2025: Chronos-Bolt models are now available on Amazon SageMaker JumpStart! Check out the tutorial notebook to learn how to deploy Chronos endpoints for production use in a few lines of code.\nChronos-Bolt is a family of pretrained time series forecasting models which can be used for zero-shot forecasting. It is based on the T5 encoder-decoder architecture and has been trained on nearly 100 billion time series observations. It chunks the historical time series context into patches of multiple observations, which are then input into the encoder. The decoder then uses these representations to directly generate quantile forecasts across multiple future steps‚Äîa method known as direct multi-step forecasting. Chronos-Bolt models are up to 250 times faster and 20 times more memory-efficient than the original Chronos models of the same size.\nPerformance\nThe following plot compares the inference time of Chronos-Bolt against the original Chronos models for forecasting 1024 time series with a context length of 512 observations and a prediction horizon of 64 steps.\nChronos-Bolt models are not only significantly faster but also more accurate than the original Chronos models. The following plot reports the probabilistic and point forecasting performance of Chronos-Bolt in terms of the Weighted Quantile Loss (WQL) and the Mean Absolute Scaled Error (MASE), respectively, aggregated over 27 datasets (see the Chronos paper for details on this benchmark). Remarkably, despite having no prior exposure to these datasets during training, the zero-shot Chronos-Bolt models outperform commonly used statistical models and deep learning models that have been trained on these datasets (highlighted by *). Furthermore, they also perform better than other FMs, denoted by a +, which indicates that these models were pretrained on certain datasets in our benchmark and are not entirely zero-shot. Notably, Chronos-Bolt (Base) also surpasses the original Chronos (Large) model in terms of the forecasting accuracy while being over 600 times faster.\nChronos-Bolt models are available in the following sizes.\nModel\nParameters\nBased on\nchronos-bolt-tiny\n9M\nt5-efficient-tiny\nchronos-bolt-mini\n21M\nt5-efficient-mini\nchronos-bolt-small\n48M\nt5-efficient-small\nchronos-bolt-base\n205M\nt5-efficient-base\nUsage\nUsage with AutoGluon\nThe recommended way of using Chronos for production use cases is through AutoGluon.\nAutoGluon offers effortless fine-tuning of Chronos models, incorporating covariates into the forecast through covariate regressors, and ensembling with other statistical and machine learning models for maximum accuracy.\nCheck out the AutoGluon Chronos tutorial for more details.\nA minimal example showing how to perform zero-shot inference using Chronos-Bolt with AutoGluon:\nInstall the required dependencies.\npip install autogluon\nForecast with the Chronos-Bolt model.\nfrom autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\ndf = TimeSeriesDataFrame(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly/train.csv\")\npredictor = TimeSeriesPredictor(prediction_length=48).fit(\ndf,\nhyperparameters={\n\"Chronos\": {\"model_path\": \"amazon/chronos-bolt-mini\"},\n},\n)\npredictions = predictor.predict(df)\nDeploying a Chronos-Bolt endpoint to SageMaker\nSageMaker JumpStart makes it easy to deploy Chronos endpoints for production use with just a few lines of code.\nChronos-Bolt endpoints can be deployed to both CPU and GPU instances, as well as support forecasting with covariates.\nMore details are available in this example notebook.\nA minimal example showing how to deploy a Chronos-Bolt (Base) endpoint to SageMaker:\nUpdate the SageMaker SDK to make sure that all the latest models are available.\npip install -U sagemaker\nDeploy an inference endpoint to SageMaker.\nfrom sagemaker.jumpstart.model import JumpStartModel\nmodel = JumpStartModel(\nmodel_id=\"autogluon-forecasting-chronos-bolt-base\",\ninstance_type=\"ml.c5.2xlarge\",\n)\npredictor = model.deploy()\nNow you can send time series data to the endpoint in JSON format.\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\")\npayload = {\n\"inputs\": [\n{\"target\": df[\"#Passengers\"].tolist()}\n],\n\"parameters\": {\n\"prediction_length\": 12,\n}\n}\nforecast = predictor.predict(payload)[\"predictions\"]\nUsage with inference library\nAlternatively, you can install the package in the GitHub companion repo.\nThis is intended for research purposes and provides a minimal interface to Chronos models.\nInstall the library by running:\npip install chronos-forecasting\nA minimal example showing how to perform inference using Chronos-Bolt models:\nimport pandas as pd  # requires: pip install pandas\nimport torch\nfrom chronos import BaseChronosPipeline\npipeline = BaseChronosPipeline.from_pretrained(\n\"amazon/chronos-bolt-mini\",\ndevice_map=\"cuda\",  # use \"cpu\" for CPU inference and \"mps\" for Apple Silicon\ntorch_dtype=torch.bfloat16,\n)\ndf = pd.read_csv(\n\"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\"\n)\n# context must be either a 1D tensor, a list of 1D tensors,\n# or a left-padded 2D tensor with batch as the first dimension\n# Chronos-Bolt models generate quantile forecasts, so forecast has shape\n# [num_series, num_quantiles, prediction_length].\nforecast = pipeline.predict(\ncontext=torch.tensor(df[\"#Passengers\"]), prediction_length=12\n)\nCitation\nIf you find Chronos or Chronos-Bolt models useful for your research, please consider citing the associated paper:\n@article{ansari2024chronos,\ntitle={Chronos: Learning the Language of Time Series},\nauthor={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\njournal={Transactions on Machine Learning Research},\nissn={2835-8856},\nyear={2024},\nurl={https://openreview.net/forum?id=gerNCVqqtR}\n}\nLicense\nThis project is licensed under the Apache-2.0 License.",
    "amazon/chronos-bolt-small": "Chronos-Bolt‚ö° (Small)\nPerformance\nUsage\nUsage with AutoGluon\nDeploying a Chronos-Bolt endpoint to SageMaker\nUsage with inference library\nCitation\nLicense\nChronos-Bolt‚ö° (Small)\nüöÄ Update Feb 14, 2025: Chronos-Bolt models are now available on Amazon SageMaker JumpStart! Check out the tutorial notebook to learn how to deploy Chronos endpoints for production use in a few lines of code.\nChronos-Bolt is a family of pretrained time series forecasting models which can be used for zero-shot forecasting. It is based on the T5 encoder-decoder architecture and has been trained on nearly 100 billion time series observations. It chunks the historical time series context into patches of multiple observations, which are then input into the encoder. The decoder then uses these representations to directly generate quantile forecasts across multiple future steps‚Äîa method known as direct multi-step forecasting. Chronos-Bolt models are up to 250 times faster and 20 times more memory-efficient than the original Chronos models of the same size.\nPerformance\nThe following plot compares the inference time of Chronos-Bolt against the original Chronos models for forecasting 1024 time series with a context length of 512 observations and a prediction horizon of 64 steps.\nChronos-Bolt models are not only significantly faster but also more accurate than the original Chronos models. The following plot reports the probabilistic and point forecasting performance of Chronos-Bolt in terms of the Weighted Quantile Loss (WQL) and the Mean Absolute Scaled Error (MASE), respectively, aggregated over 27 datasets (see the Chronos paper for details on this benchmark). Remarkably, despite having no prior exposure to these datasets during training, the zero-shot Chronos-Bolt models outperform commonly used statistical models and deep learning models that have been trained on these datasets (highlighted by *). Furthermore, they also perform better than other FMs, denoted by a +, which indicates that these models were pretrained on certain datasets in our benchmark and are not entirely zero-shot. Notably, Chronos-Bolt (Base) also surpasses the original Chronos (Large) model in terms of the forecasting accuracy while being over 600 times faster.\nChronos-Bolt models are available in the following sizes.\nModel\nParameters\nBased on\nchronos-bolt-tiny\n9M\nt5-efficient-tiny\nchronos-bolt-mini\n21M\nt5-efficient-mini\nchronos-bolt-small\n48M\nt5-efficient-small\nchronos-bolt-base\n205M\nt5-efficient-base\nUsage\nUsage with AutoGluon\nThe recommended way of using Chronos for production use cases is through AutoGluon.\nAutoGluon offers effortless fine-tuning of Chronos models, incorporating covariates into the forecast through covariate regressors, and ensembling with other statistical and machine learning models for maximum accuracy.\nCheck out the AutoGluon Chronos tutorial for more details.\nA minimal example showing how to perform zero-shot inference using Chronos-Bolt with AutoGluon:\nInstall the required dependencies.\npip install autogluon\nForecast with the Chronos-Bolt model.\nfrom autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\ndf = TimeSeriesDataFrame(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly/train.csv\")\npredictor = TimeSeriesPredictor(prediction_length=48).fit(\ndf,\nhyperparameters={\n\"Chronos\": {\"model_path\": \"amazon/chronos-bolt-small\"},\n},\n)\npredictions = predictor.predict(df)\nDeploying a Chronos-Bolt endpoint to SageMaker\nSageMaker JumpStart makes it easy to deploy Chronos endpoints for production use with just a few lines of code.\nChronos-Bolt endpoints can be deployed to both CPU and GPU instances, as well as support forecasting with covariates.\nMore details are available in this example notebook.\nA minimal example showing how to deploy a Chronos-Bolt (Base) endpoint to SageMaker:\nUpdate the SageMaker SDK to make sure that all the latest models are available.\npip install -U sagemaker\nDeploy an inference endpoint to SageMaker.\nfrom sagemaker.jumpstart.model import JumpStartModel\nmodel = JumpStartModel(\nmodel_id=\"autogluon-forecasting-chronos-bolt-base\",\ninstance_type=\"ml.c5.2xlarge\",\n)\npredictor = model.deploy()\nNow you can send time series data to the endpoint in JSON format.\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\")\npayload = {\n\"inputs\": [\n{\"target\": df[\"#Passengers\"].tolist()}\n],\n\"parameters\": {\n\"prediction_length\": 12,\n}\n}\nforecast = predictor.predict(payload)[\"predictions\"]\nUsage with inference library\nAlternatively, you can install the package in the GitHub companion repo.\nThis is intended for research purposes and provides a minimal interface to Chronos models.\nInstall the library by running:\npip install chronos-forecasting\nA minimal example showing how to perform inference using Chronos-Bolt models:\nimport pandas as pd  # requires: pip install pandas\nimport torch\nfrom chronos import BaseChronosPipeline\npipeline = BaseChronosPipeline.from_pretrained(\n\"amazon/chronos-bolt-small\",\ndevice_map=\"cuda\",  # use \"cpu\" for CPU inference and \"mps\" for Apple Silicon\ntorch_dtype=torch.bfloat16,\n)\ndf = pd.read_csv(\n\"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\"\n)\n# context must be either a 1D tensor, a list of 1D tensors,\n# or a left-padded 2D tensor with batch as the first dimension\n# Chronos-Bolt models generate quantile forecasts, so forecast has shape\n# [num_series, num_quantiles, prediction_length].\nforecast = pipeline.predict(\ncontext=torch.tensor(df[\"#Passengers\"]), prediction_length=12\n)\nCitation\nIf you find Chronos or Chronos-Bolt models useful for your research, please consider citing the associated paper:\n@article{ansari2024chronos,\ntitle={Chronos: Learning the Language of Time Series},\nauthor={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\njournal={Transactions on Machine Learning Research},\nissn={2835-8856},\nyear={2024},\nurl={https://openreview.net/forum?id=gerNCVqqtR}\n}\nLicense\nThis project is licensed under the Apache-2.0 License.",
    "SMARTICT/bge-small-en-v1.5-tr-rag-v1": "bge-small-en-v1.5-tr-rag-v1\nModel Details\nModel Description\nModel Sources\nFull Model Architecture\nUsage\nDirect Usage (Sentence Transformers)\nEvaluation\nMetrics\nTraining Details\nTraining Dataset\nTraining Hyperparameters\nTraining Logs\nFramework Versions\nCitation\nBibTeX\nbge-small-en-v1.5-tr-rag-v1\nThis is a sentence-transformers model finetuned from BAAI/bge-small-en-v1.5 on the json dataset. It maps sentences & paragraphs to a 384-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.\nModel Details\nModel Description\nModel Type: Sentence Transformer\nBase model: BAAI/bge-small-en-v1.5\nMaximum Sequence Length: 512 tokens\nOutput Dimensionality: 384 dimensions\nSimilarity Function: Cosine Similarity\nTraining Dataset:\njson\nLanguage: en\nLicense: apache-2.0\nModel Sources\nDocumentation: Sentence Transformers Documentation\nRepository: Sentence Transformers on GitHub\nHugging Face: Sentence Transformers on Hugging Face\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel\n(1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n(2): Normalize()\n)\nUsage\nDirect Usage (Sentence Transformers)\nFirst install the Sentence Transformers library:\npip install -U sentence-transformers\nThen you can load this model and run inference.\nfrom sentence_transformers import SentenceTransformer\n# Download from the ü§ó Hub\nmodel = SentenceTransformer(\"bge-small-en-v1.5-tr-rag-v1\")\n# Run inference\nsentences = [\n'veya \\'\\'\\'Afrika insansƒ±larƒ±\\'\\'\\', ilk kez John Edward Gray tarafƒ±ndan 1825 yƒ±lƒ±nda tanƒ±mlanmƒ±≈ü bir Hominidae alt familyasƒ±dƒ±r. A√ßƒ±klama (insansƒ±) aile aƒüacƒ± sol Mevcut (5 t√ºr) ve soyu t√ºkenmi≈ü t√ºrleriyle birlikte iki oymak i√ßerir: \\'\\'\\'Hominini\\'\\'\\' oymaƒüƒ± ve \\'\\'\\'Gorillini\\'\\'\\' oymaƒüƒ±. Kimi yazarlar ise, \\'\\'Pan\\'\\' cinsinin bazen kendi √º√ß√ºnc√º oymaƒüƒ± Panini\\'ye ait olduƒüunu d√º≈ü√ºn√ºr. Homininae, orangutanlarƒ±n (Ponginae alt familyasƒ±) hominid soyundan ayrƒ±lmasƒ±ndan (yakla≈üƒ±k 16 my√∂) sonra ortaya √ßƒ±kan, insanlarla orangutanlara g√∂re daha yakƒ±n akraba olan t√ºm hominidleri i√ßerir. Bu alt familyadaki canlƒ±lar, \\'\\'hominine\\'\\' veya \\'\\'hominineler\\'\\' olarak tanƒ±mlanƒ±r. Evrim Homininae alt familyasƒ±nƒ±n ya≈üƒ± son ortak atasƒ±) tahminlere g√∂re 14 ila 12.5 milyon yƒ±ldƒ±r Gorillini ve Hominini oymaklarƒ±na ayrƒ±lmasƒ±nƒ±n (\"goril insan son ortak atasƒ±\", GHLCA) ge√ß Miyosen\\'de, nakayamai\\'\\'nin ya≈üadƒ±ƒüƒ± d√∂neme yakƒ±n bir zamanda, ila 10 milyon yƒ±l √∂nce ger√ßekle≈ütiƒüi tahmin edilmi≈ütir (TGHLCA). \\'\\'Pan-Homo\\'\\' b√∂l√ºnmesine kadar (5-7 my√∂) gorillerin ve \\'\\'Pan-Homo\\'\\' atalarƒ±nƒ±n melezlendiƒüine dair kanƒ±tlar vardƒ±r. Filogeni Parins-Fukuchi \\'\\'ve 2019\\'daki √ßalƒ±≈ümasƒ±na g√∂re olu≈üturulmu≈ü, soyu t√ºkenmi≈ü homininleri i√ßeren bir Homininae kladogramƒ±: Ayrƒ±ca bakƒ±nƒ±z son ortak ata Ponginae Notlar Kaynak√ßa Dƒ±≈ü baƒülantƒ±lar Kategori:John Edward Gray tarafƒ±ndan adlandƒ±rƒ±lmƒ±≈ü taksonlar tanƒ±mlanan taksonlar',\n'Homininae alt familyasƒ± ilk kez ne zaman ve kim tarafƒ±ndan tanƒ±mlandƒ±?',\n'Amr Hassan Zaki hangi takƒ±mlarda forma giymi≈ütir?',\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, 384]\n# Get the similarity scores for the embeddings\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities.shape)\n# [3, 3]\nEvaluation\nMetrics\nInformation Retrieval\nDataset: dim_384\nEvaluated with InformationRetrievalEvaluator\nMetric\nValue\ncosine_accuracy@1\n0.6088\ncosine_accuracy@3\n0.6851\ncosine_accuracy@5\n0.7172\ncosine_accuracy@10\n0.7482\ncosine_precision@1\n0.6088\ncosine_precision@3\n0.2284\ncosine_precision@5\n0.1434\ncosine_precision@10\n0.0748\ncosine_recall@1\n0.6088\ncosine_recall@3\n0.6851\ncosine_recall@5\n0.7172\ncosine_recall@10\n0.7482\ncosine_ndcg@10\n0.6771\ncosine_mrr@10\n0.6545\ncosine_map@100\n0.6583\nTraining Details\nTraining Dataset\njson\nDataset: json\nSize: 8,970 training samples\nColumns: positive and anchor\nApproximate statistics based on the first 1000 samples:\npositive\nanchor\ntype\nstring\nstring\ndetails\nmin: 92 tokensmean: 387.75 tokensmax: 512 tokens\nmin: 2 tokensmean: 22.76 tokensmax: 58 tokens\nSamples:\npositive\nanchor\nDiyarbakƒ±r ilinin Bismil il√ßesine baƒülƒ± bir mahalledir. Tarih√ße Mahallenin adƒ±, 1928 yƒ±lƒ± kayƒ±tlarƒ±nda olarak ge√ßmektedir. Coƒürafya Diyarbakƒ±r il merkezine 57 km, Bismil il√ße merkezine 22 km uzaklƒ±ktadƒ±r. N√ºfus Yƒ±llara g√∂re mahalle n√ºfus verileri 2007 2000 185 1997 165 Kaynak√ßa Dƒ±≈ü baƒülantƒ±lar Yerelnet mahalleleri\nMahallenin adƒ± ne zaman kaydedilmi≈ütir?\n'''karma≈üƒ±k neden''', '''nedensel a≈üƒ±rƒ± '''nedensel veya '''indirgeme safsatasƒ±''', bir sonucun birka√ß nedenden kaynaklanmasƒ± m√ºmk√ºnken; bir tek nedeni olduƒüu varsayƒ±ldƒ±ƒüƒ±nda ortaya √ßƒ±kan ku≈ükulu neden safsatasƒ±dƒ±r. Mantƒ±ksal olarak ≈üu ≈üekilde a√ßƒ±klanabilir: \"X, Y'ye neden oldu; bu nedenle, X, Y'nin tek nedeniydi\" Nedensel a≈üƒ±rƒ± basitle≈ütirme, birle≈üik olasƒ±lƒ±klarƒ±n g√∂z ardƒ± edildiƒüi belirli bir t√ºr yanlƒ±≈ü ikilemdir. Diƒüer bir deyi≈üle, \"A ve ve C\" veya \"A ve ama deƒüil\" ≈üeklindeki √∂nc√ºller dikkate alƒ±nmadƒ±ƒüƒ±nda olasƒ± nedenlerin \"A veya veya C\" olduƒüu varsayƒ±lƒ±r. Kaynak√ßa\nKarma≈üƒ±k neden safsatasƒ± nedir ve nasƒ±l olu≈üur?\nAkyazƒ± Sakarya ili il√ßesi Akyazƒ±, Adƒ±yaman Adƒ±yaman ili merkez il√ßesine baƒülƒ± k√∂y Akyazƒ±, Besni Adƒ±yaman ili Besni il√ßesine baƒülƒ± k√∂y Akyazƒ±, Amasya Amasya ili merkez il√ßesine baƒülƒ± k√∂y Akyazƒ±, Adilcevaz Bitlis ili Adilcevaz il√ßesine baƒülƒ± k√∂y Akyazƒ±, D√ºzce D√ºzce ili merkez il√ßesine baƒülƒ± k√∂y Akyazƒ±, √áorum √áorum ili merkez il√ßesine baƒülƒ± k√∂y Akyazƒ±, Aziziye Erzurum ili Aziziye il√ßesine baƒülƒ± mahalle Akyazƒ±, Kƒ±zƒ±ltepe Mardin ili Kƒ±zƒ±ltepe il√ßesine baƒülƒ± mahalle Akyazƒ±, Asarcƒ±k Samsun ili Asarcƒ±k il√ßesine baƒülƒ± mahalle Akyazƒ±, Ortahisar Trabzon ili Ortahisar il√ßesine baƒülƒ± mahalle\nAkyazƒ± adƒ±nda ka√ß k√∂y vardƒ±r?\nLoss: MultipleNegativesRankingLoss with these parameters:{\n\"scale\": 20.0,\n\"similarity_fct\": \"cos_sim\"\n}\nTraining Hyperparameters\nNon-Default Hyperparameters\neval_strategy: epoch\nper_device_train_batch_size: 32\nper_device_eval_batch_size: 16\ngradient_accumulation_steps: 16\nlearning_rate: 2e-05\nnum_train_epochs: 5\nlr_scheduler_type: cosine\nwarmup_ratio: 0.1\ntf32: False\nload_best_model_at_end: True\noptim: adamw_torch_fused\nbatch_sampler: no_duplicates\nAll Hyperparameters\nClick to expand\noverwrite_output_dir: False\ndo_predict: False\neval_strategy: epoch\nprediction_loss_only: True\nper_device_train_batch_size: 32\nper_device_eval_batch_size: 16\nper_gpu_train_batch_size: None\nper_gpu_eval_batch_size: None\ngradient_accumulation_steps: 16\neval_accumulation_steps: None\nlearning_rate: 2e-05\nweight_decay: 0.0\nadam_beta1: 0.9\nadam_beta2: 0.999\nadam_epsilon: 1e-08\nmax_grad_norm: 1.0\nnum_train_epochs: 5\nmax_steps: -1\nlr_scheduler_type: cosine\nlr_scheduler_kwargs: {}\nwarmup_ratio: 0.1\nwarmup_steps: 0\nlog_level: passive\nlog_level_replica: warning\nlog_on_each_node: True\nlogging_nan_inf_filter: True\nsave_safetensors: True\nsave_on_each_node: False\nsave_only_model: False\nrestore_callback_states_from_checkpoint: False\nno_cuda: False\nuse_cpu: False\nuse_mps_device: False\nseed: 42\ndata_seed: None\njit_mode_eval: False\nuse_ipex: False\nbf16: False\nfp16: False\nfp16_opt_level: O1\nhalf_precision_backend: auto\nbf16_full_eval: False\nfp16_full_eval: False\ntf32: False\nlocal_rank: 0\nddp_backend: None\ntpu_num_cores: None\ntpu_metrics_debug: False\ndebug: []\ndataloader_drop_last: False\ndataloader_num_workers: 0\ndataloader_prefetch_factor: None\npast_index: -1\ndisable_tqdm: False\nremove_unused_columns: True\nlabel_names: None\nload_best_model_at_end: True\nignore_data_skip: False\nfsdp: []\nfsdp_min_num_params: 0\nfsdp_config: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}\nfsdp_transformer_layer_cls_to_wrap: None\naccelerator_config: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}\ndeepspeed: None\nlabel_smoothing_factor: 0.0\noptim: adamw_torch_fused\noptim_args: None\nadafactor: False\ngroup_by_length: False\nlength_column_name: length\nddp_find_unused_parameters: None\nddp_bucket_cap_mb: None\nddp_broadcast_buffers: False\ndataloader_pin_memory: True\ndataloader_persistent_workers: False\nskip_memory_metrics: True\nuse_legacy_prediction_loop: False\npush_to_hub: False\nresume_from_checkpoint: None\nhub_model_id: None\nhub_strategy: every_save\nhub_private_repo: False\nhub_always_push: False\ngradient_checkpointing: False\ngradient_checkpointing_kwargs: None\ninclude_inputs_for_metrics: False\neval_do_concat_batches: True\nfp16_backend: auto\npush_to_hub_model_id: None\npush_to_hub_organization: None\nmp_parameters:\nauto_find_batch_size: False\nfull_determinism: False\ntorchdynamo: None\nray_scope: last\nddp_timeout: 1800\ntorch_compile: False\ntorch_compile_backend: None\ntorch_compile_mode: None\ndispatch_batches: None\nsplit_batches: None\ninclude_tokens_per_second: False\ninclude_num_input_tokens_seen: False\nneftune_noise_alpha: None\noptim_target_modules: None\nbatch_eval_metrics: False\nprompts: None\nbatch_sampler: no_duplicates\nmulti_dataset_batch_sampler: proportional\nTraining Logs\nEpoch\nStep\nTraining Loss\ndim_384_cosine_ndcg@10\n0.5694\n10\n1.8837\n-\n0.9680\n17\n-\n0.6095\n1.1388\n20\n1.1104\n-\n1.7082\n30\n0.8451\n-\n1.9929\n35\n-\n0.6585\n2.2776\n40\n0.7245\n-\n2.8470\n50\n0.6472\n-\n2.9609\n52\n-\n0.6751\n3.4164\n60\n0.6274\n-\n3.9858\n70\n0.5872\n0.6764\n4.5552\n80\n0.5975\n-\n4.8399\n85\n-\n0.6771\nThe bold row denotes the saved checkpoint.\nFramework Versions\nPython: 3.12.7\nSentence Transformers: 3.3.1\nTransformers: 4.41.2\nPyTorch: 2.5.1+cu124\nAccelerate: 1.1.1\nDatasets: 2.19.1\nTokenizers: 0.19.1\nCitation\nBibTeX\nSentence Transformers\n@inproceedings{reimers-2019-sentence-bert,\ntitle = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\nauthor = \"Reimers, Nils and Gurevych, Iryna\",\nbooktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\nmonth = \"11\",\nyear = \"2019\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://arxiv.org/abs/1908.10084\",\n}\nMultipleNegativesRankingLoss\n@misc{henderson2017efficient,\ntitle={Efficient Natural Language Response Suggestion for Smart Reply},\nauthor={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},\nyear={2017},\neprint={1705.00652},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "apple/DepthPro-hf": "DepthPro: Monocular Depth Estimation\nTable of Contents\nModel Details\nModel Sources\nHow to Get Started with the Model\nTraining Details\nTraining Data\nPreprocessing\nTraining Hyperparameters\nEvaluation\nModel Architecture and Objective\nCitation\nModel Card Authors\nDepthPro: Monocular Depth Estimation\nThis is the transformers version of DepthPro, a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. For the checkpoint compatible with the original codebase, please check this repo.\nTable of Contents\nDepthPro: Monocular Depth Estimation\nTable of Contents\nModel Details\nModel Sources\nHow to Get Started with the Model\nTraining Details\nTraining Data\nPreprocessing\nTraining Hyperparameters\nEvaluation\nModel Architecture and Objective\nCitation\nModel Card Authors\nModel Details\nDepthPro is a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. It employs a multi-scale Vision Transformer (ViT)-based architecture, where images are downsampled, divided into patches, and processed using a shared Dinov2 encoder. The extracted patch-level features are merged, upsampled, and refined using a DPT-like fusion stage, enabling precise depth estimation.\nThe abstract from the paper is the following:\nWe present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions.\nThis is the model card of a ü§ó transformers model that has been pushed on the Hub.\nDeveloped by: Aleksei Bochkovskii, Ama√´l Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, Vladlen Koltun.\nModel type: DepthPro\nLicense: Apple-ASCL\nModel Sources\nHF Docs: DepthPro\nRepository: https://github.com/apple/ml-depth-pro\nPaper: https://arxiv.org/abs/2410.02073\nHow to Get Started with the Model\nUse the code below to get started with the model.\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import DepthProImageProcessorFast, DepthProForDepthEstimation\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nurl = 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nimage_processor = DepthProImageProcessorFast.from_pretrained(\"apple/DepthPro-hf\")\nmodel = DepthProForDepthEstimation.from_pretrained(\"apple/DepthPro-hf\").to(device)\ninputs = image_processor(images=image, return_tensors=\"pt\").to(device)\nwith torch.no_grad():\noutputs = model(**inputs)\npost_processed_output = image_processor.post_process_depth_estimation(\noutputs, target_sizes=[(image.height, image.width)],\n)\nfield_of_view = post_processed_output[0][\"field_of_view\"]\nfocal_length = post_processed_output[0][\"focal_length\"]\ndepth = post_processed_output[0][\"predicted_depth\"]\ndepth = (depth - depth.min()) / (depth.max() - depth.min())\ndepth = depth * 255.\ndepth = depth.detach().cpu().numpy()\ndepth = Image.fromarray(depth.astype(\"uint8\"))\nTraining Details\nTraining Data\nThe DepthPro model was trained on the following datasets:\nPreprocessing\nImages go through the following preprocessing steps:\nrescaled by 1/225.\nnormalized with mean=[0.5, 0.5, 0.5] and std=[0.5, 0.5, 0.5]\nresized to 1536x1536 pixels\nTraining Hyperparameters\nEvaluation\nModel Architecture and Objective\nThe DepthProForDepthEstimation model uses a DepthProEncoder, for encoding the input image and a FeatureFusionStage for fusing the output features from encoder.\nThe DepthProEncoder further uses two encoders:\npatch_encoder\nInput image is scaled with multiple ratios, as specified in the scaled_images_ratios configuration.\nEach scaled image is split into smaller patches of size patch_size with overlapping areas determined by scaled_images_overlap_ratios.\nThese patches are processed by the patch_encoder\nimage_encoder\nInput image is also rescaled to patch_size and processed by the image_encoder\nBoth these encoders can be configured via patch_model_config and image_model_config respectively, both of which are separate Dinov2Model by default.\nOutputs from both encoders (last_hidden_state) and selected intermediate states (hidden_states) from patch_encoder are fused by a DPT-based FeatureFusionStage for depth estimation.\nThe network is supplemented with a focal length estimation head. A small convolutional head ingests frozen features from the depth estimation network and task-specific features from a separate ViT image encoder to predict the horizontal angular field-of-view.\nCitation\nBibTeX:\n@misc{bochkovskii2024depthprosharpmonocular,\ntitle={Depth Pro: Sharp Monocular Metric Depth in Less Than a Second},\nauthor={Aleksei Bochkovskii and Ama√´l Delaunoy and Hugo Germain and Marcel Santos and Yichao Zhou and Stephan R. Richter and Vladlen Koltun},\nyear={2024},\neprint={2410.02073},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2410.02073},\n}\nModel Card Authors\nArmaghan Shakir",
    "Respair/Tsukasa_Speech": "Tsukasa Âè∏ Speech: Engineering the Naturalness and Rich Expressiveness\nWhat is this?\nWhy does it matter?\nHow to do ...\nPre-requisites\nInference:\nTraining:\nsome ideas for future\nPre-requisites\nTraining details\nImportant Notes\nSome cool projects:\nReferences\nTsukasa Âè∏ Speech: Engineering the Naturalness and Rich Expressiveness\ntl;dr : I made a very cool japanese speech generation model.\nif the demo didn't work and you just want to listen to some samples, take a look at this  notebook. (ps. this belongs to a much earlier checkpoint, not representative of the model at its best.)\nTry chatting with Aira, a mini-project I did by using various Tech, including Tsukasa. (maybe not very optimized, but hey, it works!)\nÊó•Êú¨Ë™û„ÅÆ„É¢„Éá„É´„Ç´„Éº„Éâ„ÅØ„Åì„Å°„Çâ„ÄÇ\nPart of a personal project, focusing on further advancing Japanese speech field.\nUse the HuggingFace Space for Tsukasa (24khz):\nHuggingFace Space for Tsumugi (48khz):\nJoin Shoukan lab's discord server, a comfy place I frequently visit ->\nGithub's repo:\nWhat is this?\nNote: This model only supports the Japanese language; but you can feed it Romaji if you use the Gradio demo. (no longer, due to resource constraints, but the Tech is there.)\nThis is a speech generation network, aimed at maximizing the expressiveness and Controllability of the generated speech. at its core it uses StyleTTS 2's architecture with the following changes:\nIncorporating mLSTM Layers instead of regular PyTorch LSTM layers, and increasing the capacity of the text and prosody encoder by using a higher number of parameters\nRetrained PL-Bert, Pitch Extractor, Text Aligner from scratch\nWhisper's Encoder instead of WavLM for the SLM\n48khz Config\nimproved Performance on non-verbal sounds and cues. such as sigh, pauses, etc. and also very slightly on laughter (depends on the speaker)\na new way of sampling the Style Vectors.\nPromptable Speech Synthesizing.\na Smart Phonemization algorithm that can handle Romaji inputs or a mixture of Japanese and Romaji.\nFixed DDP and BF16 Training (mostly!)\nThere are two checkpoints you can use. Tsukasa & Tsumugi 48khz (placeholder).\nTsukasa was trained on ~800 hours of studio grade, high quality data. sourced mainly from games and novels, part of it from a private dataset.\nSo the Japanese is going to be the \"anime japanese\" (it's different than what people usually speak in real-life.)\nBrought to you by:\nSoshyant (me)\nAuto Meta\nCryptowooser\nButtercream\nSpecial thanks to Yinghao Aaron Li, the Author of StyleTTS which this work is based on top of that.  He is one of the most talented Engineers I've ever seen in this field.\nAlso Karesto and Raven(a.k.a hexgrad) for their help in debugging some of the scripts. wonderful people.\nWhy does it matter?\nRecently, there's a big trend towards larger models, increasing the scale. We're going the opposite way, trying to see how far we can push the limits by utilizing existing tools.\nMaybe, just maybe, scale is not necessarily the answer.\nThere's also a few things that's related to Japanese (but can have a wider impact on languages that face a similar issue like Arabic). such as how we can improve the intonations for this language. what can be done to accurately annotate a text that can have various spellings depending on the context, etc.\nHow to do ...\nPre-requisites\nPython >= 3.11\nClone this repository:\ngit clone https://huggingface.co/Respair/Tsukasa_Speech\ncd Tsukasa_Speech\nInstall python requirements:\npip install -r requirements.txt\nInference:\nGradio demo:\npython app_tsuka.py\nor check the inference notebook. before that, make sure you read the Important Notes section down below.\nTraining:\nBefore starting remove lines 985 and 986 from models.py also remove \"KotoDama_Prompt, KotoDama_Text\" from the \"build_model\" function's parameters.\nFirst stage training:\naccelerate launch train_first.py --config_path ./Configs/config.yml\nSecond stage training:\naccelerate launch accelerate_train_second.py --config_path ./Configs/config.yml\nSLM Joint-Training doesn't work on multigpu. (you don't need it, i didn't use it too.)\nor:\nlaunch train_first.py --config_path ./Configs/config.yml\nThird stage training (Kotodama, prompt encoding, etc.):\nnot planned right now, due to some constraints, but feel free to replicate.\nsome ideas for future\nI can think of a few things that can be improved, not nessarily by me, treat it as some sorts of suggestions:\n[o] changing the decoder (fregrad looks promising)\n[o] retraining the Pitch Extractor using a different algorithm\n[o] while the quality of non-speech sounds have been improved, it cannot generate an entirely non-speech output, perhaps because of the hard alignement.\n[o] using the Style encoder as another modality in LLMs, since they have a detailed representation of the tone and expression of a speech (similar to Style-Talker).\nPre-requisites\nPython >= 3.11\nClone this repository:\ngit clone https://huggingface.co/Respair/Tsukasa_Speech\ncd Tsukasa_Speech\nInstall python requirements:\npip install -r requirements.txt\nTraining details\n8x A40s + 2x V100s(32gb each)\n750 ~ 800 hours of data\nBfloat16\nApproximately 3 weeks of training, overall 3 months including the work spent on the data pipeline.\nRoughly 66.6 kg of CO2eq. of Carbon emitted if we base it on Google Cloud. (I didn't use Google, but the cluster is located in US, please treat it as a very rough approximation.)\nImportant Notes\nCheck here\nAny questions?\nsaoshiant@protonmail.com\nor simply DM me on discord.\nSome cool projects:\nKokoro - a very nice and light weight TTS, based on StyleTTS. supports Japanese and English.\nVoPho - a meta phonemizer to rule them all. it will automatically handle any languages with hand-picked high quality phonemizers.\nReferences\nyl4579/StyleTTS2\nNX-AI/xlstm\narchinetai/audio-diffusion-pytorch\njik876/hifi-gan\nrishikksh20/iSTFTNet-pytorch\nnii-yamagishilab/project-NN-Pytorch-scripts/project/01-nsf\nlitain's Moe Speech a very cool dataset you can use in case i couldn't release mine\n@article{xlstm,\ntitle={xLSTM: Extended Long Short-Term Memory},\nauthor={Beck, Maximilian and P{\\\"o}ppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, G{\\\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},\njournal={arXiv preprint arXiv:2405.04517},\nyear={2024}\n}",
    "neuronbit/voice-clone-large-finetune-final": "voice-clone-large-finetune-final\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nvoice-clone-large-finetune-final\nThis model is a fine-tuned version of openai/whisper-large-v3 on an unknown dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.4377\nWer: 15.3572\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 1e-05\ntrain_batch_size: 8\neval_batch_size: 8\nseed: 42\ngradient_accumulation_steps: 2\ntotal_train_batch_size: 16\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nlr_scheduler_warmup_steps: 500\ntraining_steps: 5000\nmixed_precision_training: Native AMP\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nWer\n0.1607\n0.8460\n250\n0.5163\n25.9413\n0.0598\n1.6920\n500\n0.4849\n24.8444\n0.0257\n2.5381\n750\n0.4450\n30.4180\n0.0141\n3.3841\n1000\n0.4369\n19.3003\n0.0029\n4.2301\n1250\n0.4267\n16.0095\n0.0015\n5.0761\n1500\n0.4209\n18.4109\n0.0063\n5.9222\n1750\n0.4259\n19.3300\n0.0016\n6.7682\n2000\n0.4341\n17.7587\n0.0009\n7.6142\n2250\n0.4121\n17.0471\n0.0013\n8.4602\n2500\n0.4199\n16.3653\n0.0009\n9.3063\n2750\n0.4233\n16.5135\n0.001\n10.1523\n3000\n0.4237\n16.0688\n0.0019\n10.9983\n3250\n0.4230\n16.4542\n0.0014\n11.8443\n3500\n0.4292\n15.8316\n0.0007\n12.6904\n3750\n0.4291\n15.8316\n0.0005\n13.5364\n4000\n0.4321\n15.3869\n0.0009\n14.3824\n4250\n0.4334\n15.2980\n0.001\n15.2284\n4500\n0.4344\n15.2980\n0.0\n16.0745\n4750\n0.4372\n15.3572\n0.0\n16.9205\n5000\n0.4377\n15.3572\nFramework versions\nTransformers 4.45.2\nPytorch 2.5.1+cu124\nDatasets 3.1.0\nTokenizers 0.20.3",
    "ModelSpace/GemmaX2-28-9B-v0.1": "Model Description\nModel Performance\nRun the model\nCitation\nLimitations\nModel Description\nGemmaX2-28-9B-v0.1 is an LLM-based translation model. It has been fintuned on GemmaX2-28-9B-Pretrain, which is a language model developed through continual pretraining of Gemma2-9B using a mix of 56 billion tokens from both monolingual and parallel data across 28 different languages. Please find more details in our paper: Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study.\nDeveloped by: Xiaomi\nModel type: GemmaX2-28-9B-Pretrain is obtained by continually pretraining Gemma2-9B on a large amount of monolingual and parallel data. Subsequently, GemmaX2-28-9B-v0.1 is derived through supervised finetuning on a small set of high-quality translation instruction data.\nLanguages: Arabic, Bengali, Czech, German, English, Spanish, Persian, French, Hebrew, Hindi, Indonesian, Italian, Japanese, Khmer, Korean, Lao, Malay, Burmese, Dutch, Polish, Portuguese, Russian, Thai, Tagalog, Turkish, Urdu, Vietnamese, Chinese.\nGithub: Please find more details in our Github repository.\nModel Performance\nRun the model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"ModelSpace/GemmaX2-28-9B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\ntext = \"Translate this from Chinese to English:\\nChinese: ÊàëÁà±Êú∫Âô®ÁøªËØë\\nEnglish:\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=512)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nCitation\n@misc{cui2025multilingualmachinetranslationopen,\ntitle={Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study},\nauthor={Menglong Cui and Pengzhi Gao and Wei Liu and Jian Luan and Bin Wang},\nyear={2025},\neprint={2502.02481},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2502.02481},\n}\nLimitations\nGemmaX2-28-9B-v0.1 only supports the 28 languages listed above and does not guarantee strong translation performance for other languages. We will continue to enhance the translation performance of GemmaX2-28-9B, and future models will be released in due course.",
    "ModelSpace/GemmaX2-28-2B-v0.1": "Model Description\nModel Performance\nRun the model\nCitation\nLimitations\nModel Description\nGemmaX2-28-2B-v0.1 is an LLM-based translation model. It has been fintuned on GemmaX2-28-2B-Pretrain, which is a language model developed through continual pretraining of Gemma2-2B using a mix of 56 billion tokens from both monolingual and parallel data across 28 different languages. Please find more details in our paper: Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study.\nDeveloped by: Xiaomi\nModel type: GemmaX2-28-2B-Pretrain is obtained by continually pretraining Gemma2-2B on a large amount of monolingual and parallel data. Subsequently, GemmaX2-28-2B-v0.1 is derived through supervised finetuning on a small set of high-quality translation instruction data.\nLanguages: Arabic, Bengali, Czech, German, English, Spanish, Persian, French, Hebrew, Hindi, Indonesian, Italian, Japanese, Khmer, Korean, Lao, Malay, Burmese, Dutch, Polish, Portuguese, Russian, Thai, Tagalog, Turkish, Urdu, Vietnamese, Chinese.\nGithub: Please find more details in our Github repository.\nModel Performance\nRun the model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"ModelSpace/GemmaX2-28-2B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\ntext = \"Translate this from Chinese to English:\\nChinese: ÊàëÁà±Êú∫Âô®ÁøªËØë\\nEnglish:\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nCitation\n@misc{cui2025multilingualmachinetranslationopen,\ntitle={Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study},\nauthor={Menglong Cui and Pengzhi Gao and Wei Liu and Jian Luan and Bin Wang},\nyear={2025},\neprint={2502.02481},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2502.02481},\n}\nLimitations\nGemmaX2-28-2B-v0.1 only supports the 28 languages listed above and does not guarantee strong translation performance for other languages. We will continue to enhance the translation performance of GemmaX2-28-2B, and future models will be released in due course.",
    "strangerzonehf/Flux-3DXL-Partfile-0004": "Model description for 3DXL Partfile 0004\nBest Dimensions & Inference\nInference Range\nSetting Up\nTrigger words\nDownload model\nPrompt\n3DXLP4, a medium-sized African-American man stands in front of a backdrop of a cityscape. The mans upper torso is adorned with tattoos, including a cross, a gold chain, and a gold bracelet. He is wearing a gold ring, a silver watch, and blue jeans with a black belt. His hair is short and dark, and he is wearing a white bandana tied around his head. The backdrop is a deep blue, with a few buildings visible in the distance.\nPrompt\n3DXLP4, A close-up portrait of a man in a black suit with a black collared button down shirt and a black and white striped tie. The man has dark brown hair and blue eyes. The background is completely dark, creating a stark contrast to the mans face.\nPrompt\n3DXLP4, a blonde woman, wearing a pink bikini, adorned with a gold necklace and matching earrings, stands in front of a dark gray backdrop. She is striking a seductive pose, with her right hand on her hip, adding a pop of color to the scene. The womans lips are pursed, her lips are painted a deep pink, and her hair is a light blonde, her bangs are a darker shade of black, while her left hand is a gold ring.\nPrompt\n3DXLP4, Captured in a close-up, eye-level shot, a woman with long, straight red hair, wearing a red strapless bra, is striking a pose with a lollipop in her right hand. Her lips are painted a vibrant red, adding a pop of color to her face. Her eyes are a piercing blue, while her lips are a darker red. The backdrop is a stark gray, with a statue of liberty in the upper right corner.\nModel description for 3DXL Partfile 0004\nImage Processing Parameters\nParameter\nValue\nParameter\nValue\nLR Scheduler\nconstant\nNoise Offset\n0.03\nOptimizer\nAdamW\nMultires Noise Discount\n0.1\nNetwork Dim\n64\nMultires Noise Iterations\n10\nNetwork Alpha\n32\nRepeat & Steps\n17 & 2550\nEpoch\n20\nSave Every N Epochs\n1\nLabeling: florence2-en(natural language & English)\nTotal Images Used for Training : 14\nBest Dimensions & Inference\nDimensions\nAspect Ratio\nRecommendation\n1280 x 832\n3:2\nBest\n1024 x 1024\n1:1\nDefault\nInference Range\nRecommended Inference Steps: 30‚Äì35\nSetting Up\nimport torch\nfrom pipelines import DiffusionPipeline\nbase_model = \"black-forest-labs/FLUX.1-dev\"\npipe = DiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.bfloat16)\nlora_repo = \"strangerzonehf/Flux-3DXL-Partfile-0004\"\ntrigger_word = \"3DXLP4\"\npipe.load_lora_weights(lora_repo)\ndevice = torch.device(\"cuda\")\npipe.to(device)\nTrigger words\nYou should use 3DXLP4 to trigger the image generation.\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.",
    "mradermacher/SAINEMO-reMIX-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/Moraliane/SAINEMO-reMIX\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/SAINEMO-reMIX-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n4.9\nGGUF\nQ3_K_S\n5.6\nGGUF\nQ3_K_M\n6.2\nlower quality\nGGUF\nQ3_K_L\n6.7\nGGUF\nIQ4_XS\n6.9\nGGUF\nQ4_0_4_4\n7.2\nfast on arm, low quality\nGGUF\nQ4_K_S\n7.2\nfast, recommended\nGGUF\nQ4_K_M\n7.6\nfast, recommended\nGGUF\nQ5_K_S\n8.6\nGGUF\nQ5_K_M\n8.8\nGGUF\nQ6_K\n10.2\nvery good quality\nGGUF\nQ8_0\n13.1\nfast, best quality\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time. Additional thanks to @nicoboss for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.",
    "oxyapi/oxy-1-small": "Open LLM Leaderboard Evaluation Results\nIntroduction\nOxy 1 Small is a fine-tuned version of the Qwen/Qwen2.5-14B-Instruct language model, specialized for role-play scenarios. Despite its small size, it delivers impressive performance in generating engaging dialogues and interactive storytelling.\nDeveloped by Oxygen (oxyapi), with contributions from TornadoSoftwares, Oxy 1 Small aims to provide an accessible and efficient language model for creative and immersive role-play experiences.\nModel Details\nModel Name: Oxy 1 Small\nModel ID: oxyapi/oxy-1-small\nBase Model: Qwen/Qwen2.5-14B-Instruct\nModel Type: Chat Completions\nPrompt Format: ChatML\nLicense: Apache-2.0\nLanguage: English\nTokenizer: Qwen/Qwen2.5-14B-Instruct\nMax Input Tokens: 32,768\nMax Output Tokens: 8,192\nFeatures\nFine-tuned for Role-Play: Specially trained to generate dynamic and contextually rich role-play dialogues.\nEfficient: Compact model size allows for faster inference and reduced computational resources.\nParameter Support:\ntemperature\ntop_p\ntop_k\nfrequency_penalty\npresence_penalty\nmax_tokens\nMetadata\nOwned by: Oxygen (oxyapi)\nContributors: TornadoSoftwares\nDescription: A Qwen/Qwen2.5-14B-Instruct fine-tune for role-play trained on custom datasets\nUsage\nTo utilize Oxy 1 Small for text generation in role-play scenarios, you can load the model using the Hugging Face Transformers library:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"oxyapi/oxy-1-small\")\nmodel = AutoModelForCausalLM.from_pretrained(\"oxyapi/oxy-1-small\")\nprompt = \"You are a wise old wizard in a mystical land. A traveler approaches you seeking advice.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_length=500)\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\nPerformance\nPerformance benchmarks for Oxy 1 Small are not available at this time. Future updates may include detailed evaluations on relevant datasets.\nLicense\nThis model is licensed under the Apache 2.0 License.\nCitation\nIf you find Oxy 1 Small useful in your research or applications, please cite it as:\n@misc{oxy1small2024,\ntitle={Oxy 1 Small: A Fine-Tuned Qwen2.5-14B-Instruct Model for Role-Play},\nauthor={Oxygen (oxyapi)},\nyear={2024},\nhowpublished={\\url{https://huggingface.co/oxyapi/oxy-1-small}},\n}\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n33.14\nIFEval (0-Shot)\n62.45\nBBH (3-Shot)\n41.18\nMATH Lvl 5 (4-Shot)\n18.28\nGPQA (0-shot)\n16.22\nMuSR (0-shot)\n16.28\nMMLU-PRO (5-shot)\n44.45",
    "PleIAs/Pleias-3b-Preview": "Description\nRecommended use\nTraining\nEthical Considerations\nAcknowledgements\nUpdate\nPleias-3b-Preview is an early preview of a 3 billion parameters base model trained by Pleias on Common Corpus. Pleias-3b-Preview was pretrained at Jean Zay (compute grant n¬∞GC011015451) with support from Etalab\nLike all the base and specialized models from Pleias, Pleias-3b-Preview has only been trained on open data out of copyright (public domain) or under a permissible license.\nDescription\nPleias-3b-Preview is a transformer base model, entirely pretrained from scratch, using an architecture similar to Llama/GPT-Neox for easier deployment/inference.\nIt includes the following features, that would apply to any responsibly trained variant:\nOnly trained on open data under a permissible license and in compliance with the European AI Act. By design, all Pleias model are unable to output copyrighted content.\nExtensive multilingual support for main European languages.\nA new tokenizer designed for enhanced document processing tasks and better multilingual support.\nExtremely low level of toxicity and problematic content.\nFully supported languages include English, French, Spanish, German, Italian, Dutch, Latin and Portuguese.\nRecommended use\nAs a base model, Pleias-3b-Preview is only able to run continuation prompts.\nText generation is currently able to support a range of creative writing tasks in multiple European languages. For more consistent results we recommend using a low or null temperature with a slight repetition penalty (1.1-1.2).\nPleias-3b-Preview has been successfully adapted for continuous pretraining and full-fine-tuning on document processing tasks such as RAG, translation or OCR correction. Given the small size of the model we do not recommend fine-tuning methods based on LORA.\nTraining\nPleias-3b-Preview was fully pretrained at Jean Zay on 192 h100s for about 20 days (compute grant n¬∞GC011015451) with support from Etalab. Training code relied on Nanotron, the open source library from HuggingFace. We provide the complete settings as a yaml file as part of our release.\nTraining schedule includes 518,000 steps (batch size 1,024) on a filtered and enhanced version of Common Corpus (1,086,324,736,000 tokens).\nTraining Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 16 tons CO2eq for training.\nEthical Considerations\npleias-3B-Preview model, like all large language models, carries inherent ethical risks that require careful consideration. Our approach to mitigating these risks begins at the data level, where we exclusively use vetted sources, deliberately excluding CommonCrawl. The primary challenge comes from our public domain dataset component, which contains historical texts that may reflect outdated social norms and potentially harmful language, particularly regarding minoritized groups.\nTo address this, we implemented a systematic ethical filtering process using toxicity classifiers to identify extremely harmful content. We also employed synthetic rewriting techniques to transform mildly problematic passages while preserving the underlying informational value. This process significantly reduced potential societal harm without compromising the dataset's size or textual quality, resulting in notably low toxicity scores in benchmarks compared to other models.\nDespite these preventive measures, users should be aware that the model has not undergone additional safety alignment procedures and may still produce problematic outputs. The model's capabilities in generative AI tasks must be balanced against the risks of bias, misinformation propagation, and autonomous decision-making challenges. We explicitly prohibit any malicious utilization and emphasize the responsibility of users to implement appropriate safeguards.\nAt Pleias, we continue to research and develop improved methods for creating safer and more equitable models and datasets. This includes ongoing work in toxicity reduction, bias mitigation, and the development of more sophisticated ethical filtering techniques.\nAcknowledgements\nThis work would not have been possible without the substantial support from √©talab.\nThe training was conducted as part of the Grand Challenge of GENCI, aligned with the European strategy for establishing AI factories through the EuroHPC Joint Undertaking, aimed at supporting European startups and providing open-source models to the community.\nWe express our gratitude to GENCI's Jean Zay supercomputer, France's AI flagship facility, which was instrumental in our model's training. The project benefited from the new NVIDIA H100 partition specifically dedicated to the French AI community. We appreciate the generous allocation of compute hours over five months and the invaluable technical expertise provided by IDRIS, EVIDEN, and NVIDIA (as well as its Inception program).\nWe are deeply grateful to the Mozilla Foundation Local AI Program for their generous support.\nFinally, we acknowledge the significant contributions from the open science LLM community, particularly HuggingFace, Eleuther AI and Allen AI whose insights and cooperation have been invaluable to our work.\nUpdate\nPleias-3b-Preview is currently released as an early preview.\nThe model will undergo several more round of post-training to enhance reasoning capacities and fine-tunability as well as in anticipation of a generalist instruct version.",
    "beethogedeon/gte-Qwen2-7B-instruct-Q4_K_M-GGUF": "beethogedeon/gte-Qwen2-7B-instruct-Q4_K_M-GGUF\nUse with llama.cpp\nCLI:\nServer:\nbeethogedeon/gte-Qwen2-7B-instruct-Q4_K_M-GGUF\nThis model was converted to GGUF format from Alibaba-NLP/gte-Qwen2-7B-instruct using llama.cpp via the ggml.ai's GGUF-my-repo space.\nRefer to the original model card for more details on the model.\nUse with llama.cpp\nInstall llama.cpp through brew (works on Mac and Linux)\nbrew install llama.cpp\nInvoke the llama.cpp server or the CLI.\nCLI:\nllama-cli --hf-repo beethogedeon/gte-Qwen2-7B-instruct-Q4_K_M-GGUF --hf-file gte-qwen2-7b-instruct-q4_k_m.gguf -p \"The meaning to life and the universe is\"\nServer:\nllama-server --hf-repo beethogedeon/gte-Qwen2-7B-instruct-Q4_K_M-GGUF --hf-file gte-qwen2-7b-instruct-q4_k_m.gguf -c 2048\nNote: You can also use this checkpoint directly through the usage steps listed in the Llama.cpp repo as well.\nStep 1: Clone llama.cpp from GitHub.\ngit clone https://github.com/ggerganov/llama.cpp\nStep 2: Move into the llama.cpp folder and build it with LLAMA_CURL=1 flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).\ncd llama.cpp && LLAMA_CURL=1 make\nStep 3: Run inference through the main binary.\n./llama-cli --hf-repo beethogedeon/gte-Qwen2-7B-instruct-Q4_K_M-GGUF --hf-file gte-qwen2-7b-instruct-q4_k_m.gguf -p \"The meaning to life and the universe is\"\nor\n./llama-server --hf-repo beethogedeon/gte-Qwen2-7B-instruct-Q4_K_M-GGUF --hf-file gte-qwen2-7b-instruct-q4_k_m.gguf -c 2048"
}