{
    "USTC-KnowledgeComputingLab/Llama3-KALE-LM-Chem-8B": "Llama3-KALE-LM-Chem-8B\nIntroduction\nTraining Details\nBenchmarks\nOpen Benchmarks\nQuick Start\nCite This Work\nLlama3-KALE-LM-Chem-8B\nIntroduction\nWe are thrilled to present Llama3-KALE-LM-Chem 8B, our first open-source KALE-LM, which specializes in chemistry.\nTraining Details\nWe have continually pre-trained the model with a large amount of data and post-trained it through supervised fine-tuning.\nBenchmarks\nOpen Benchmarks\nModels\nChemBench\nMMLU\nMMLU-Chem\nSciQ\nIE(Acc)\nIE(LS)\nGPT-3.5\n47.15\n69.75\n53.32\n89.6\n52.98\n68.28\nGPT-4\n53.72\n78.67\n63.70\n94.10\n54.20\n69.74\nLlama3-8B-Instruct\n46.02\n68.3\n51.10\n93.30\n45.83\n61.22\nLlaSMol\n28.47\n54.47\n33.24\n72.30\n2.16\n3.23\nChemDFM\n44.44\n58.11\n45.60\n86.70\n7.61\n11.49\nChemLLM-7B-Chat\n34.16\n61.79\n48.39\n94.00\n29.66\n39.17\nChemLLM-7B-Chat-1.5-SFT\n42.75\n63.56\n49.63\n95.10\n14.96\n19.61\nLlama3-KALE-LM-Chem-8B\n52.40\n68.74\n53.83\n91.50\n62.89\n76.21\nChemBench Details (Evaluated By OpenCompass)\nModels\nNC\nPP\nM2C\nC2M\nPP\nRS\nYP\nTP\nSP\nAverage\nGPT-3.5\n46.93\n56.98\n85.28\n38.25\n43.67\n42.33\n30.33\n42.57\n38\n47.15\nGPT-4\n54.82\n65.02\n92.64\n52.88\n62.67\n52.67\n42.33\n24.75\n35.67\n53.72\nLlama3-8B-Instruct\n51.31\n27.79\n90.30\n40.88\n34.00\n30.00\n45.33\n60.89\n33.67\n46.02\nLlaSMol\n27.78\n29.34\n31.44\n23.38\n25.67\n24.00\n37.33\n34.65\n22.67\n28.47\nChemDFM\n36.92\n55.57\n83.95\n42.00\n40.00\n37.33\n39.00\n33.17\n32.00\n44.44\nChemLLM-7B-Chat\n41.05\n29.76\n85.28\n26.12\n26.00\n24.00\n20.00\n24.26\n31.00\n34.16\nChemLLM-7B-Chat-1.5-SFT\n50.06\n49.51\n85.28\n38.75\n38.00\n26.67\n28.33\n31.68\n33.67\n42.44\nLlama3-KALE-LM-Chem-8B\n63.58\n58.39\n92.98\n44.50\n48.67\n38.33\n46.33\n44.55\n34.33\n52.41\nQuick Start\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\nmodel = AutoModelForCausalLM.from_pretrained(\n\"USTC-KnowledgeComputingLab/Llama3-KALE-LM-Chem-8B\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"USTC-KnowledgeComputingLab/Llama3-KALE-LM-Chem-8B\")\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\ngenerated_ids = model.generate(\nmodel_inputs.input_ids,\nmax_new_tokens=2048\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nCite This Work\n@article{dai2024kale,\ntitle={KALE-LM: Unleash The Power Of AI For Science Via Knowledge And Logic Enhanced Large Model},\nauthor={Dai, Weichen and Chen, Yezeng and Dai, Zijie and Huang, Zhijie and Liu, Yubo and Pan, Yixuan and Song, Baiyang and Zhong, Chengli and Li, Xinhe and Wang, Zeyu and others},\njournal={arXiv preprint arXiv:2409.18695},\nyear={2024}\n}",
    "KimChen/bge-m3-GGUF": "KimChen/bge-m3-GGUF\nUse with llama.cpp\nCLI:\nServer:\nKimChen/bge-m3-GGUF\nThis model was converted to GGUF format from BAAI/bge-m3 using llama.cpp.\nRefer to the original model card for more details on the model.\nUse with llama.cpp\nInstall llama.cpp through brew (works on Mac and Linux)\nbrew install llama.cpp\nInvoke the llama.cpp server or the CLI.\nCLI:\nllama-cli --hf-repo KimChen/bge-m3-GGUF --hf-file bge-m3.gguf -p \"The meaning to life and the universe is\"\nServer:\nllama-server --hf-repo KimChen/bge-m3-GGUF --hf-file bge-m3.gguf -c 2048\nNote: You can also use this checkpoint directly through the usage steps listed in the Llama.cpp repo as well.\nStep 1: Clone llama.cpp from GitHub.\ngit clone https://github.com/ggerganov/llama.cpp.git\nStep 2: Move into the llama.cpp folder and build it with LLAMA_CURL=1 flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).\ncd llama.cpp && LLAMA_CURL=1 make\nStep 3: Run inference through the main binary.\n./llama-cli --hf-repo KimChen/bge-m3-GGUF --hf-file bge-m3.gguf -p \"The meaning to life and the universe is\"\nor\n./llama-server --hf-repo KimChen/bge-m3-GGUF --hf-file bge-m3.gguf -c 2048",
    "mirla/ONNX-quantizado-roberta-base-squad2": "Optimum RoBERTa-base-SQuAD2 Quantizado\nIntrodu√ß√£o\nAvalia√ß√£o\nOptimum RoBERTa-base-SQuAD2 Quantizado\nIntrodu√ß√£o\nEste reposit√≥rio cont√©m uma vers√£o quantizada do modelo optimum/roberta-base-squad2, desenvolvido por Branden Chan et al. A quantiza√ß√£o foi realizada utilizando a biblioteca Optimum ONNX para reduzir o tamanho do modelo e melhorar a efici√™ncia, mantendo uma precis√£o aceit√°vel.\nAvalia√ß√£o\nOs modelos foram testados utilizando 600 entradas do conjunto de valida√ß√£o da base de dados rajpurkar/squad_v2.\nRedu√ß√£o da Lat√™ncia:\nModelo Original: 0.572 segundos por amostra\nModelo Quantizado: 0.437 segundos por amostra\nAn√°lise: A lat√™ncia foi significativamente reduzida, tornando o modelo mais adequado para aplica√ß√µes em tempo real.\nAumento da Efici√™ncia:\nTempo Total:\nModelo Original: 343.20 segundos\nModelo Quantizado: 262.41 segundos\nAn√°lise: O tempo total de execu√ß√£o foi consideravelmente reduzido.\nAmostras por Segundo:\nModelo Original: 1.75 amostras/segundo\nModelo Quantizado: 2.29 amostras/segundo\nAn√°lise: A taxa de processamento aumentou, permitindo que mais amostras sejam processadas no mesmo per√≠odo de tempo.\nManuten√ß√£o de Precis√£o Razo√°vel:\nExact Score:\nModelo Original: 81.67\nModelo Quantizado: 80.5\nAn√°lise: Pequena queda na precis√£o, mas ainda em n√≠vel aceit√°vel.\nF1 Score:\nModelo Original: 83.75\nModelo Quantizado: 82.49\nAn√°lise: Queda ligeira no desempenho de F1 Score.\nCompara√ß√£o do Espa√ßo Ocupado na Mem√≥ria:\nModelo Original: 476.52 MB\nModelo Quantizado: 122.41 MB\nAn√°lise: A quantiza√ß√£o resultou em uma redu√ß√£o significativa no espa√ßo ocupado, com o modelo quantizado utilizando apenas cerca de 25.7% do tamanho do modelo original.\nEsses resultados indicam que a quantiza√ß√£o foi bem-sucedida, alcan√ßando uma redu√ß√£o significativa na lat√™ncia, aumento na efici√™ncia e uma economia substancial de espa√ßo na mem√≥ria, enquanto mant√©m uma precis√£o aceit√°vel para tarefas de perguntas e respostas.",
    "nvidia/Llama-3.1-Minitron-4B-Width-Base": "Llama-3.1-Minitron-4B-Width-Base\nModel Overview\nLicense\nModel Architecture\nUsage\nSoftware Integration\nDataset & Training\nEvaluation Results\nOverview\nInference\nLimitations\nEthical Considerations\nReferences\nLlama-3.1-Minitron-4B-Width-Base\nModel Overview\nLlama-3.1-Minitron-4B-Width-Base is a base text-to-text model that can be adopted for a variety of natural language generation tasks.\nIt is obtained by pruning Llama-3.1-8B; specifically, we prune model embedding size and MLP intermediate dimension.\nFollowing pruning, we perform continued training with distillation using 94 billion tokens to arrive at the final model; we use the continuous pre-training data corpus used in Nemotron-4 15B for this purpose. Please refer to our technical report for more details.\nThis model is ready for commercial use.\nModel Developer: NVIDIA\nModel Dates: Llama-3.1-Minitron-4B-Width-Base was trained between July 29, 2024 and Aug 3, 2024.\nLicense\nThis model is released under the NVIDIA Open Model License Agreement.\nModel Architecture\nLlama-3.1-Minitron-4B-Width-Base uses a model embedding size of 3072, 32 attention heads, MLP intermediate dimension of 9216, with 32 layers in total. Additionally, it uses Grouped-Query Attention (GQA) and Rotary Position Embeddings (RoPE).\nArchitecture Type: Transformer Decoder (Auto-Regressive Language Model)\nNetwork Architecture: Llama-3.1\nInput Type(s): Text\nInput Format(s): String\nInput Parameters: None\nOther Properties Related to Input: Works well within 8k characters or less.\nOutput Type(s): Text\nOutput Format: String\nOutput Parameters: 1D\nOther Properties Related to Output: None\nUsage\nSupport for this model will be added in the upcoming transformers release. In the meantime, please install the library from source:\npip install git+https://github.com/huggingface/transformers\nWe can now run inference on this model:\nimport torch\nfrom transformers import AutoTokenizer, LlamaForCausalLM\n# Load the tokenizer and model\nmodel_path = \"nvidia/Llama-3.1-Minitron-4B-Width-Base\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ndevice = 'cuda'\ndtype = torch.bfloat16\nmodel = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=dtype, device_map=device)\n# Prepare the input text\nprompt = 'Complete the paragraph: our solar system is'\ninputs = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n# Generate the output\noutputs = model.generate(inputs, max_length=20)\n# Decode and print the output\noutput_text = tokenizer.decode(outputs[0])\nprint(output_text)\nSoftware Integration\nRuntime Engine(s):\nNeMo 24.05\nSupported Hardware Microarchitecture Compatibility:\nNVIDIA Ampere\nNVIDIA Blackwell\nNVIDIA Hopper\nNVIDIA Lovelace\n[Preferred/Supported] Operating System(s):\nLinux\nDataset & Training\nData Collection Method by Dataset: Automated\nLabeling Method by Dataset: Not Applicable\nProperties:\nThe training corpus for Llama-3.1-Minitron-4B-Width-Base consists of English and multilingual text, as well as code. Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. The corpus spans domains including legal, math, science, finance, and more. In our continued training set, we introduce a small portion of question-answering, and alignment style data to improve model performance.\nData Freshness: The pretraining data has a cutoff of June 2023.\nEvaluation Results\nOverview\n5-shot performance. Language Understanding evaluated using Massive Multitask Language Understanding:\nAverage\n60.5\nZero-shot performance. Evaluated using select datasets from the LM Evaluation Harness with additions:\nHellaSwag\nWinogrande\nGSM8K\nARC-Challenge\nXLSum\n76.1\n73.5\n41.2\n55.6\n28.7\nCode generation performance. Evaluated using MBPP:\nScore\n32.0\nInference\nEngine: TensorRT-LLM\nTest Hardware: NVIDIA A100\nDType: BFloat16\nLimitations\nThe model was trained on data that contains toxic language, unsafe content, and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive.\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nReferences\nCompact Language Models via Pruning and Knowledge Distillation\nLLM Pruning and Distillation in Practice: The Minitron Approach",
    "ggml-org/Meta-Llama-3.1-8B-Instruct-Q4_0-GGUF": "ggml-org/Meta-Llama-3.1-8B-Instruct-Q4_0-GGUF\nUse with llama.cpp\nCLI:\nServer:\nggml-org/Meta-Llama-3.1-8B-Instruct-Q4_0-GGUF\nThis model was converted to GGUF format from meta-llama/Meta-Llama-3.1-8B-Instruct using llama.cpp via the ggml.ai's GGUF-my-repo space.\nRefer to the original model card for more details on the model.\nUse with llama.cpp\nInstall llama.cpp through brew (works on Mac and Linux)\nbrew install llama.cpp\nInvoke the llama.cpp server or the CLI.\nCLI:\nllama-cli --hf-repo ggml-org/Meta-Llama-3.1-8B-Instruct-Q4_0-GGUF --hf-file meta-llama-3.1-8b-instruct-q4_0.gguf -p \"The meaning to life and the universe is\"\nServer:\nllama-server --hf-repo ggml-org/Meta-Llama-3.1-8B-Instruct-Q4_0-GGUF --hf-file meta-llama-3.1-8b-instruct-q4_0.gguf -c 2048\nNote: You can also use this checkpoint directly through the usage steps listed in the Llama.cpp repo as well.\nStep 1: Clone llama.cpp from GitHub.\ngit clone https://github.com/ggerganov/llama.cpp\nStep 2: Move into the llama.cpp folder and build it with LLAMA_CURL=1 flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).\ncd llama.cpp && LLAMA_CURL=1 make\nStep 3: Run inference through the main binary.\n./llama-cli --hf-repo ggml-org/Meta-Llama-3.1-8B-Instruct-Q4_0-GGUF --hf-file meta-llama-3.1-8b-instruct-q4_0.gguf -p \"The meaning to life and the universe is\"\nor\n./llama-server --hf-repo ggml-org/Meta-Llama-3.1-8B-Instruct-Q4_0-GGUF --hf-file meta-llama-3.1-8b-instruct-q4_0.gguf -c 2048",
    "EpistemeAI/OpenCodeLlama-3.1-8B": "Uploaded  model\nUploaded  model\nDeveloped by: EpistemeAI\nLicense: apache-2.0\nFinetuned from model : unsloth/Meta-Llama-3.1-8B-bnb-4bit\nThis llama model was trained 2x faster with Unsloth and Huggingface's TRL library.",
    "infly/InfMLLM2_7B_chat": "INF-MLLM2: High-Resolution Image and Document Understanding\nInstall\nModel Zoo\nEvaluation\nVisualization\nUsage\nAcknowledgement\nINF-MLLM2: High-Resolution Image and Document Understanding\nIn INF-MLLM2, we have introduced significant updates, particularly in high-resolution image processing, document understanding and OCR.\nThe key improvements include the following:\nDynamic Image Resolution Support: The model now supports dynamic image resolution up to 1344x1344 pixels.\nEnhanced OCR Capabilities: The model has significantly improved OCR capabilities, enabling robust document parsing, table and formula recognition, document layout analysis, and key information extraction.\nAdvanced Training Strategies: We employed a progressive multi-stage training strategy along with an enhanced data mixup strategy tailored for image and document multitask scenarios.\nTechnical Report\nInstall\nconda create -n infmllm2 python=3.9\nconda activate infmllm2\nconda install pytorch==2.2.1 torchvision==0.17.1 torchaudio==2.1.2\npip install transformers==4.40.2 timm==0.5.4 pillow==10.4.0 sentencepiece==0.1.99\npip install bigmodelvis peft einops spacy\nModel Zoo\nWe have released the INF-MLLM2-7B model on Hugging Face.\nINF-MLLM2-7B\nEvaluation\nThe comparison with general multimodal LLM across multiple benchmarks and OCR-related tasks.\nThe comparison with OCR-free multimodal LLM for content parsing of documents/tables/formulas.\nThe comparison with OCR-free multimodal LLM for key information extraction.\nVisualization\nUsage\nThe inference process for INF-MLLM2 is straightforward. We also provide a simple demo.py script as a reference.\nCUDA_VISIBLE_DEVICES=0 python demo.py --model_path /path/to/InfMLLM2_7B_chat\nAcknowledgement\nWe thank the great work from LLaVA-Next and InternLM-XComposer.",
    "gokaygokay/Flux-Detailer-LoRA": "üé® Flux Creativity LoRA\nüöÄ Recommendations\nüåü Example\n‚ú® Results\nüé® Flux Creativity LoRA\nThis LoRA model is designed to elevate your images by removing the \"generic look\" and adding a touch of creativity.\nüöÄ Recommendations\nUse this LoRA with:\n0.5 weight and CFG 3.5\nor 1.0 weight and CFG 2.5\nüåü Example\nPrompt: \"Beautiful lady in forest, close up shot, extremely detailed, highly detailed\"\nSettings:\nCFG: 2.5\nLoRA Weight: 1.0\nBefore\nAfter\nSettings:\nCFG: 3.5\nLoRA Weight: 0.5\nBefore\nAfter\n‚ú® Results\nAs you can see, this LoRA transforms your images by:\nEnhancing creativity\nRemoving generic elements\nAdding unique and eye-catching details",
    "microsoft/Phi-3.5-vision-instruct": "Model Summary\nIntended Uses\nPrimary Use Cases\nUse Case Considerations\nRelease Notes\nUsage\nRequirements\nInput Formats\nLoading the model locally\nResponsible AI Considerations\nTraining\nModels\nData Overview\nHow to finetune?\nBenchmarks\nSafety Evaluation and Red-Teaming\nSoftware\nHardware\nLicense\nTrademarks\nModel Summary\nPhi-3.5-vision is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision. The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\nüè° Phi-3 Portal\nüì∞ Phi-3 Microsoft Blog\nüìñ Phi-3 Technical Report\nüë©‚Äçüç≥ Phi-3 Cookbook\nüñ•Ô∏è Try It\nPhi-3.5: [mini-instruct]; [MoE-instruct] ; [vision-instruct]\nIntended Uses\nPrimary Use Cases\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications with visual and text input capabilities which require:\nMemory/compute constrained environments\nLatency bound scenarios\nGeneral image understanding\nOptical character recognition\nChart and table understanding\nMultiple image comparison\nMulti-image or video clip summarization\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\nUse Case Considerations\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\nRelease Notes\nIn this release, the model enables multi-frame image understanding and reasoning which is based on valuable customer feedback. The hero example multi-frame capabilities include detailed image comparison, multi-image summarization/storytelling and video summarization, which have broad applications in Office scenarios. We also observed performance improvement on most single image benchmarks, e.g., boost MMMU performance from 40.2 to 43.0, MMBench performance from 80.5 to 81.9, document understanding benchmark TextVQA from 70.9 to 72.0. We believe most use cases will benefit from this release, but we encourage users to test the new model in their AI applications. We appreciate the enthusiastic adoption of the Phi-3 model family and continue to welcome all the feedback from the community.\nBelow are the comparison results on existing multi-image benchmarks. On average, our model outperforms competitor models on the same size and competitive with much bigger models on multi-frame capabilities and video summarization.\nBLINK: a benchmark with 14 visual tasks that humans can solve very quickly but are still hard for current multimodal LLMs.\nBenchmark\nPhi-3.5-vision-instruct\nLlaVA-Interleave-Qwen-7B\nInternVL-2-4B\nInternVL-2-8B\nGemini-1.5-Flash\nGPT-4o-mini\nClaude-3.5-Sonnet\nGemini-1.5-Pro\nGPT-4o\nArt Style\n87.2\n62.4\n55.6\n52.1\n64.1\n70.1\n59.8\n70.9\n73.3\nCounting\n54.2\n56.7\n54.2\n66.7\n51.7\n55.0\n59.2\n65.0\n65.0\nForensic Detection\n92.4\n31.1\n40.9\n34.1\n54.5\n38.6\n67.4\n60.6\n75.8\nFunctional Correspondence\n29.2\n34.6\n24.6\n24.6\n33.1\n26.9\n33.8\n31.5\n43.8\nIQ Test\n25.3\n26.7\n26.0\n30.7\n25.3\n29.3\n26.0\n34.0\n19.3\nJigsaw\n68.0\n86.0\n55.3\n52.7\n71.3\n72.7\n57.3\n68.0\n67.3\nMulti-View Reasoning\n54.1\n44.4\n48.9\n42.9\n48.9\n48.1\n55.6\n49.6\n46.6\nObject Localization\n49.2\n54.9\n53.3\n54.1\n44.3\n57.4\n62.3\n65.6\n68.0\nRelative Depth\n69.4\n77.4\n63.7\n67.7\n57.3\n58.1\n71.8\n76.6\n71.0\nRelative Reflectance\n37.3\n34.3\n32.8\n38.8\n32.8\n27.6\n36.6\n38.8\n40.3\nSemantic Correspondence\n36.7\n31.7\n31.7\n22.3\n32.4\n31.7\n45.3\n48.9\n54.0\nSpatial Relation\n65.7\n75.5\n78.3\n78.3\n55.9\n81.1\n60.1\n79.0\n84.6\nVisual Correspondence\n53.5\n40.7\n34.9\n33.1\n29.7\n52.9\n72.1\n81.4\n86.0\nVisual Similarity\n83.0\n91.9\n48.1\n45.2\n47.4\n77.8\n84.4\n81.5\n88.1\nOverall\n57.0\n53.1\n45.9\n45.4\n45.8\n51.9\n56.5\n61.0\n63.2\nVideo-MME: comprehensively assess the capabilities of MLLMs in processing video data, covering a wide range of visual domains, temporal durations, and data modalities.\nBenchmark\nPhi-3.5-vision-instruct\nLlaVA-Interleave-Qwen-7B\nInternVL-2-4B\nInternVL-2-8B\nGemini-1.5-Flash\nGPT-4o-mini\nClaude-3.5-Sonnet\nGemini-1.5-Pro\nGPT-4o\nshort (<2min)\n60.8\n62.3\n60.7\n61.7\n72.2\n70.1\n66.3\n73.3\n77.7\nmedium (4-15min)\n47.7\n47.1\n46.4\n49.6\n62.7\n59.6\n54.7\n61.2\n68.0\nlong (30-60min)\n43.8\n41.2\n42.6\n46.6\n52.1\n53.9\n46.6\n53.2\n59.6\nOverall\n50.8\n50.2\n49.9\n52.6\n62.3\n61.2\n55.9\n62.6\n68.4\nUsage\nRequirements\nThe current transformers version can be verified with: pip list | grep transformers.\nExamples of required packages:\nflash_attn==2.5.8\nnumpy==1.24.4\nPillow==10.3.0\nRequests==2.31.0\ntorch==2.3.0\ntorchvision==0.18.0\ntransformers==4.43.0\naccelerate==0.30.0\nPhi-3.5-vision-Instruct is also available in Azure AI Studio.\nInput Formats\nGiven the nature of the training data, the Phi-3.5-vision model is best suited for prompts using the chat format as follows:\nSingle image:\n<|user|>\\n<|image_1|>\\n{prompt}<|end|>\\n<|assistant|>\\n\nMulti-turn conversations:\n<|user|>\\n<|image_1|>\\n{prompt_1}<|end|>\\n<|assistant|>\\n{response_1}<|end|>\\n<|user|>\\n{prompt_2}<|end|>\\n<|assistant|>\\n\nFor multi-image usage, add multiple image placeholders in the front of the prompts. <|image_{}|> index should start from 1. One example of prompt is shown as follows:\n<|user|>\\n<|image_1|>\\n<|image_2|>\\n<|image_3|>\\n<|image_4|>\\n{prompt}<|end|>\\n<|assistant|>\\n\nLoading the model locally\nAfter obtaining the Phi-3.5-vision-instruct model checkpoints, users can use this sample code for inference.\nfrom PIL import Image\nimport requests\nfrom transformers import AutoModelForCausalLM\nfrom transformers import AutoProcessor\nmodel_id = \"microsoft/Phi-3.5-vision-instruct\"\n# Note: set _attn_implementation='eager' if you don't have flash_attn installed\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"cuda\",\ntrust_remote_code=True,\ntorch_dtype=\"auto\",\n_attn_implementation='flash_attention_2'\n)\n# for best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.\nprocessor = AutoProcessor.from_pretrained(model_id,\ntrust_remote_code=True,\nnum_crops=4\n)\nimages = []\nplaceholder = \"\"\n# Note: if OOM, you might consider reduce number of frames in this example.\nfor i in range(1,20):\nurl = f\"https://image.slidesharecdn.com/azureintroduction-191206101932/75/Introduction-to-Microsoft-Azure-Cloud-{i}-2048.jpg\"\nimages.append(Image.open(requests.get(url, stream=True).raw))\nplaceholder += f\"<|image_{i}|>\\n\"\nmessages = [\n{\"role\": \"user\", \"content\": placeholder+\"Summarize the deck of slides.\"},\n]\nprompt = processor.tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ninputs = processor(prompt, images, return_tensors=\"pt\").to(\"cuda:0\")\ngeneration_args = {\n\"max_new_tokens\": 1000,\n\"temperature\": 0.0,\n\"do_sample\": False,\n}\ngenerate_ids = model.generate(**inputs,\neos_token_id=processor.tokenizer.eos_token_id,\n**generation_args\n)\n# remove input tokens\ngenerate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\nresponse = processor.batch_decode(generate_ids,\nskip_special_tokens=True,\nclean_up_tokenization_spaces=False)[0]\nprint(response)\nNotes:\nto achieve best performances we suggest to set num_crops=4 for multi-frame and num_crops=16 for single-frame.\nto turn off flash_attention users can set _attn_implementation='eager'\nResponsible AI Considerations\nLike other models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\nQuality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.\nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\nInappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.\nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\nLimited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\nAllocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\nIdentification of individuals: models with vision capabilities may have the potential to uniquely identify individuals in images. Safety post-training steers the model to refuse such requests, but developers should consider and implement, as appropriate, additional mitigations or user consent flows as required in their respective jurisdiction, (e.g., building measures to blur faces in image inputs before processing).\nTraining\nModels\nArchitecture: Phi-3.5-vision has 4.2B parameters and contains image encoder, connector, projector, and Phi-3 Mini language model.\nInputs: Text and Image. It‚Äôs best suited for prompts using the chat format.\nContext length: 128K tokens\nGPUs: 256 A100-80G\nTraining time: 6 days\nTraining data: 500B tokens (vision tokens + text tokens)\nOutputs: Generated text in response to the input\nDates: Trained between July and August 2024\nStatus: This is a static model trained on an offline text dataset with cutoff date March 15, 2024. Future versions of the tuned models may be released as we improve models.\nRelease date: August 2024\nData Overview\nOur training data includes a wide variety of sources, and is a combination of\npublicly available documents filtered rigorously for quality, selected high-quality educational data and code;\nselected high-quality image-text interleave data;\nnewly created synthetic, ‚Äútextbook-like‚Äù data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.), newly created image data, e.g., chart/table/diagram/slides, newly created multi-image and video data, e.g., short video clips/pair of two similar images;\nhigh quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\nThe data collection process involved sourcing information from publicly available documents, with a meticulous approach to filtering out undesirable documents and images. To safeguard privacy, we carefully filtered various image and text data sources to remove or scrub any potentially personal data from the training data. More details about data can be found in the Phi-3 Technical Report.\nHow to finetune?\nWe recommend user to take a look at the Phi-3 CookBook finetuning recipe for Vision\nBenchmarks\nTo understand the capabilities, we compare Phi-3.5-vision with a set of models over a variety of zero-shot benchmarks using our internal benchmark platform. At the high-level overview of the model quality on representative benchmarks:\nCategory\nBenchmark\nPhi-3.5-vision-instruct\nIntern-VL-2-4B\nIntern-VL-2-8B\nGemini-1.5-Flash\nGPT-4o-mini 2024-7-18\nClaude-3.5-Sonnet\nGemini-1.5-Pro\nGPT-4o 2024-5-13\nPopular aggregated benchmark\nMMMU (val)\n43.0\n44.22\n46.33\n49.33\n52.1\n52.67\n54.11\n61.78\nMMBench (dev-en)\n81.9\n83.4\n87.0\n85.7\n83.8\n82.3\n87.9\n88.4\nVisual scientific knowledge reasoning\nScienceQA (img-test)\n91.3\n94.9\n95.9\n84.5\n84.0\n73.8\n86.0\n88.5\nVisual math reasoning\nMathVista (testmini)\n43.9\n53.7\n51.1\n55.3\n38.8\n54.0\n57.4\n54.4\nInterGPS (test)\n36.3\n45.6\n53.2\n39.4\n39.9\n45.6\n58.2\n46.9\nChart reasoning\nAI2D (test)\n78.1\n77.3\n81.4\n78.4\n75.2\n68.9\n75.6\n82.8\nChartQA (test)\n81.8\n78.8\n80.4\n57.6\n54.5\n73.2\n68.2\n64.0\nDocument Intelligence\nTextVQA (val)\n72.0\n66.2\n68.8\n67.4\n70.9\n70.5\n64.5\n75.6\nObject visual presence verification\nPOPE (test)\n86.1\n83.3\n84.2\n86.1\n83.6\n76.6\n89.3\n87.0\nSafety Evaluation and Red-Teaming\nApproach\nThe Phi-3 family of models has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated datasets.\nThe overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and RLHF (Reinforcement Learning from Human Feedback) approaches\nby utilizing human-labeled and synthetic English-language datasets, including publicly available datasets focusing on helpfulness and harmlessness as well as various\nquestions and answers targeted to multiple safety categories.\nSafety Evaluation\nWe leveraged various evaluation techniques including red teaming, adversarial conversation simulations, and safety evaluation benchmark datasets to evaluate Phi-3.5\nmodels' propensity to produce undesirable outputs across multiple risk categories. Several approaches were used to compensate for the limitations of one approach alone.\nPlease refer to the technical report for more details of our safety alignment.\nSoftware\nPyTorch\nTransformers\nFlash-Attention\nHardware\nNote that by default, the Phi-3.5-Mini-Instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\nNVIDIA A100\nNVIDIA A6000\nNVIDIA H100\nLicense\nThe model is licensed under the MIT license.\nTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow‚ÄØMicrosoft‚Äôs Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies.",
    "anthracite-org/magnum-v2-123b": "Prompting\nCredits\nTraining\nSafety\nThis is the sixth in a series of models designed to replicate the prose quality of the Claude 3 models, specifically Sonnet and Opus. This model is fine-tuned on top of Mistral-Large-Instruct-2407.\nPrompting\nModel has been Instruct tuned with the Mistral formatting. A typical input would look like this:\n<s>[INST] SYSTEM MESSAGE\\nUSER MESSAGE[/INST] ASSISTANT MESSAGE</s>[INST] USER MESSAGE[/INST]\nWe also provide SillyTavern presets for Context and Instruct respectively.\nThe Mistral preset included in SillyTavern seems to be misconfigured by default, so we recommend using these as a replacement.\nCredits\nanthracite-org/Stheno-Data-Filtered\nanthracite-org/kalo-opus-instruct-22k-no-refusal\nanthracite-org/nopm_claude_writing_fixed\nThis model has been a team effort, and the credits goes to all members of Anthracite.\nTraining\nThe training was done for 1.5 epochs. We used 8x AMD Instinct‚Ñ¢ MI300X Accelerators for the full-parameter fine-tuning of the model.\nIn addition to this, we noticed that Mistral Large models seemed much more sensitive to learning rate adjustments than other models:\nWe hypothesize this is primarily due to the particularly narrow and low variance weight distributions typical of Mistral derived models regardless of their scale.\nIn the end, due to the costs that would be involved in training another full 2 epochs run ($600) on an even lower rate, we settled on our third attempt: 2e-6 with an effective batch size of 64. We chose to publish the 1.5 epoch run after manually testing and comparing it.\nAlso, we notice a correlation between the significance of the 2nd epoch loss drop and the strength of the learning rate, implying 4e-6 leads to more catastrophic forgetting.\nSafety\n...",
    "zai-org/CogVideoX-5b": "CogVideoX-5B\nDemo Show\nModel Introduction\nQuick Start ü§ó\nQuantized Inference\nExplore the Model\nModel License\nCitation\nCogVideoX-5B\nüìÑ ‰∏≠ÊñáÈòÖËØª |\nü§ó Huggingface Space |\nüåê Github  |\nüìú arxiv\nüìç Visit QingYing and API Platform to experience commercial video generation models.\nDemo Show\nVideo Gallery with Captions\nA garden comes to life as a kaleidoscope of butterflies flutters amidst the blossoms, their delicate wings casting shadows on the petals below. In the background, a grand fountain cascades water with a gentle splendor, its rhythmic sound providing a soothing backdrop. Beneath the cool shade of a mature tree, a solitary wooden chair invites solitude and reflection, its smooth surface worn by the touch of countless visitors seeking a moment of tranquility in nature's embrace.\nA small boy, head bowed and determination etched on his face, sprints through the torrential downpour as lightning crackles and thunder rumbles in the distance. The relentless rain pounds the ground, creating a chaotic dance of water droplets that mirror the dramatic sky's anger. In the far background, the silhouette of a cozy home beckons, a faint beacon of safety and warmth amidst the fierce weather. The scene is one of perseverance and the unyielding spirit of a child braving the elements.\nA suited astronaut, with the red dust of Mars clinging to their boots, reaches out to shake hands with an alien being, their skin a shimmering blue, under the pink-tinged sky of the fourth planet. In the background, a sleek silver rocket, a beacon of human ingenuity, stands tall, its engines powered down, as the two representatives of different worlds exchange a historic greeting amidst the desolate beauty of the Martian landscape.\nAn elderly gentleman, with a serene expression, sits at the water's edge, a steaming cup of tea by his side. He is engrossed in his artwork, brush in hand, as he renders an oil painting on a canvas that's propped up against a small, weathered table. The sea breeze whispers through his silver hair, gently billowing his loose-fitting white shirt, while the salty air adds an intangible element to his masterpiece in progress. The scene is one of tranquility and inspiration, with the artist's canvas capturing the vibrant hues of the setting sun reflecting off the tranquil sea.\nIn a dimly lit bar, purplish light bathes the face of a mature man, his eyes blinking thoughtfully as he ponders in close-up, the background artfully blurred to focus on his introspective expression, the ambiance of the bar a mere suggestion of shadows and soft lighting.\nA golden retriever, sporting sleek black sunglasses, with its lengthy fur flowing in the breeze, sprints playfully across a rooftop terrace, recently refreshed by a light rain. The scene unfolds from a distance, the dog's energetic bounds growing larger as it approaches the camera, its tail wagging with unrestrained joy, while droplets of water glisten on the concrete behind it. The overcast sky provides a dramatic backdrop, emphasizing the vibrant golden coat of the canine as it dashes towards the viewer.\nOn a brilliant sunny day, the lakeshore is lined with an array of willow trees, their slender branches swaying gently in the soft breeze. The tranquil surface of the lake reflects the clear blue sky, while several elegant swans glide gracefully through the still water, leaving behind delicate ripples that disturb the mirror-like quality of the lake. The scene is one of serene beauty, with the willows' greenery providing a picturesque frame for the peaceful avian visitors.\nA Chinese mother, draped in a soft, pastel-colored robe, gently rocks back and forth in a cozy rocking chair positioned in the tranquil setting of a nursery. The dimly lit bedroom is adorned with whimsical mobiles dangling from the ceiling, casting shadows that dance on the walls. Her baby, swaddled in a delicate, patterned blanket, rests against her chest, the child's earlier cries now replaced by contented coos as the mother's soothing voice lulls the little one to sleep. The scent of lavender fills the air, adding to the serene atmosphere, while a warm, orange glow from a nearby nightlight illuminates the scene with a gentle hue, capturing a moment of tender love and comfort.\nModel Introduction\nCogVideoX is an open-source version of the video generation model originating\nfrom QingYing. The table below displays the list of video generation\nmodels we currently offer, along with their foundational information.\nModel Name\nCogVideoX-2B\nCogVideoX-5B (This Repository)\nModel Description\nEntry-level model, balancing compatibility. Low cost for running and secondary development.\nLarger model with higher video generation quality and better visual effects.\nInference Precision\nFP16* (Recommended), BF16, FP32, FP8*, INT8, no support for INT4\nBF16 (Recommended), FP16, FP32, FP8*, INT8, no support for INT4\nSingle GPU VRAM Consumption\nSAT FP16: 18GB diffusers FP16: starting from 4GB*diffusers INT8(torchao): starting from 3.6GB*\nSAT BF16: 26GB diffusers BF16: starting from 5GB*diffusers INT8(torchao): starting from 4.4GB*\nMulti-GPU Inference VRAM Consumption\nFP16: 10GB* using diffusers\nBF16: 15GB* using diffusers\nInference Speed(Step = 50, FP/BF16)\nSingle A100: ~90 secondsSingle H100: ~45 seconds\nSingle A100: ~180 secondsSingle H100: ~90 seconds\nFine-tuning Precision\nFP16\nBF16\nFine-tuning VRAM Consumption (per GPU)\n47 GB (bs=1, LORA) 61 GB (bs=2, LORA) 62GB (bs=1, SFT)\n63 GB (bs=1, LORA) 80 GB (bs=2, LORA) 75GB (bs=1, SFT)\nPrompt Language\nEnglish*\nPrompt Length Limit\n226 Tokens\nVideo Length\n6 Seconds\nFrame Rate\n8 Frames per Second\nVideo Resolution\n720 x 480, no support for other resolutions (including fine-tuning)\nPositional Encoding\n3d_sincos_pos_embed\n3d_rope_pos_embed\nData Explanation\nWhen testing using the diffusers library, all optimizations provided by the diffusers library were enabled. This\nsolution has not been tested for actual VRAM/memory usage on devices other than NVIDIA A100 / H100. Generally,\nthis solution can be adapted to all devices with NVIDIA Ampere architecture and above. If the optimizations are\ndisabled, VRAM usage will increase significantly, with peak VRAM usage being about 3 times higher than the table\nshows. However, speed will increase by 3-4 times. You can selectively disable some optimizations, including:\npipe.enable_model_cpu_offload()\npipe.enable_sequential_cpu_offload()\npipe.vae.enable_slicing()\npipe.vae.enable_tiling()\nWhen performing multi-GPU inference, the enable_model_cpu_offload() optimization needs to be disabled.\nUsing INT8 models will reduce inference speed. This is to ensure that GPUs with lower VRAM can perform inference\nnormally while maintaining minimal video quality loss, though inference speed will decrease significantly.\nThe 2B model is trained with FP16 precision, and the 5B model is trained with BF16 precision. We recommend using\nthe precision the model was trained with for inference.\nPytorchAO and Optimum-quanto can be\nused to quantize the text encoder, Transformer, and VAE modules to reduce CogVideoX's memory requirements. This makes\nit possible to run the model on a free T4 Colab or GPUs with smaller VRAM! It is also worth noting that TorchAO\nquantization is fully compatible with torch.compile, which can significantly improve inference speed. FP8\nprecision must be used on devices with NVIDIA H100 or above, which requires installing\nthe torch, torchao, diffusers, and accelerate Python packages from source. CUDA 12.4 is recommended.\nThe inference speed test also used the above VRAM optimization scheme. Without VRAM optimization, inference speed\nincreases by about 10%. Only the diffusers version of the model supports quantization.\nThe model only supports English input; other languages can be translated into English during refinement by a large\nmodel.\nNote\nUsing SAT  for inference and fine-tuning of SAT version\nmodels. Feel free to visit our GitHub for more information.\nQuick Start ü§ó\nThis model supports deployment using the huggingface diffusers library. You can deploy it by following these steps.\nWe recommend that you visit our GitHub and check out the relevant prompt\noptimizations and conversions to get a better experience.\nInstall the required dependencies\n# diffusers>=0.30.1\n# transformers>=4.44.2\n# accelerate>=0.33.0 (suggest install from source)\n# imageio-ffmpeg>=0.5.1\npip install --upgrade transformers accelerate diffusers imageio-ffmpeg\nRun the code\nimport torch\nfrom diffusers import CogVideoXPipeline\nfrom diffusers.utils import export_to_video\nprompt = \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance.\"\npipe = CogVideoXPipeline.from_pretrained(\n\"THUDM/CogVideoX-5b\",\ntorch_dtype=torch.bfloat16\n)\npipe.enable_model_cpu_offload()\npipe.vae.enable_tiling()\nvideo = pipe(\nprompt=prompt,\nnum_videos_per_prompt=1,\nnum_inference_steps=50,\nnum_frames=49,\nguidance_scale=6,\ngenerator=torch.Generator(device=\"cuda\").manual_seed(42),\n).frames[0]\nexport_to_video(video, \"output.mp4\", fps=8)\nQuantized Inference\nPytorchAO and Optimum-quanto can be\nused to quantize the Text Encoder, Transformer and VAE modules to lower the memory requirement of CogVideoX. This makes\nit possible to run the model on free-tier T4 Colab or smaller VRAM GPUs as well! It is also worth noting that TorchAO\nquantization is fully compatible with torch.compile, which allows for much faster inference speed.\n# To get started, PytorchAO needs to be installed from the GitHub source and PyTorch Nightly.\n# Source and nightly installation is only required until next release.\nimport torch\nfrom diffusers import AutoencoderKLCogVideoX, CogVideoXTransformer3DModel, CogVideoXPipeline\nfrom diffusers.utils import export_to_video\n+ from transformers import T5EncoderModel\n+ from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n+ quantization = int8_weight_only\n+ text_encoder = T5EncoderModel.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"text_encoder\", torch_dtype=torch.bfloat16)\n+ quantize_(text_encoder, quantization())\n+ transformer = CogVideoXTransformer3DModel.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n+ quantize_(transformer, quantization())\n+ vae = AutoencoderKLCogVideoX.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"vae\", torch_dtype=torch.bfloat16)\n+ quantize_(vae, quantization())\n# Create pipeline and run inference\npipe = CogVideoXPipeline.from_pretrained(\n\"THUDM/CogVideoX-5b\",\n+    text_encoder=text_encoder,\n+    transformer=transformer,\n+    vae=vae,\ntorch_dtype=torch.bfloat16,\n)\npipe.enable_model_cpu_offload()\npipe.vae.enable_tiling()\nprompt = \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance.\"\nvideo = pipe(\nprompt=prompt,\nnum_videos_per_prompt=1,\nnum_inference_steps=50,\nnum_frames=49,\nguidance_scale=6,\ngenerator=torch.Generator(device=\"cuda\").manual_seed(42),\n).frames[0]\nexport_to_video(video, \"output.mp4\", fps=8)\nAdditionally, the models can be serialized and stored in a quantized datatype to save disk space when using PytorchAO.\nFind examples and benchmarks at these links:\ntorchao\nquanto\nExplore the Model\nWelcome to our github, where you will find:\nMore detailed technical details and code explanation.\nOptimization and conversion of prompt words.\nReasoning and fine-tuning of SAT version models, and even pre-release.\nProject update log dynamics, more interactive opportunities.\nCogVideoX toolchain to help you better use the model.\nINT8 model inference code support.\nModel License\nThis model is released under the CogVideoX LICENSE.\nCitation\n@article{yang2024cogvideox,\ntitle={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer},\nauthor={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},\njournal={arXiv preprint arXiv:2408.06072},\nyear={2024}\n}",
    "fofr/flux-cassette-futurism": "Flux Cassette Futurism\nTrigger words\nUse it with the üß® diffusers library\nFlux Cassette Futurism\nPrompt\na cassette futurism photo of a car\nPrompt\na cassette futurism photo of a white retro computer\nRun on Replicate:\nhttps://replicate.com/fofr/flux-cassette-futurism\nTrained on Replicate using:\nhttps://replicate.com/ostris/flux-dev-lora-trainer/train\nTrigger words\nYou should use cassette futurism to trigger the image generation.\nUse it with the üß® diffusers library\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipeline = AutoPipelineForText2Image.from_pretrained('black-forest-labs/FLUX.1-dev', torch_dtype=torch.float16).to('cuda')\npipeline.load_lora_weights('fofr/flux-cassette-futurism', weight_name='lora.safetensors')\nimage = pipeline('your prompt').images[0]\nFor more details, including weighting, merging and fusing LoRAs, check the documentation on loading LoRAs in diffusers",
    "AiAF/D-ART-18DART5_LoRA_Flux1": "D-ART | @18DART5 Style LoRA [Flux1]\nModel description\nTrigger words\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, illustration of a woman with blonde hair, wearing a black lace bra, She has a small silver heart-shaped pendant necklace around her neck. She is looking directly at the camera with a neutral expression. The background is a plain, light-colored wall.  The image is high quality .\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, illustration of a  woman with long blonds hair, wearing a white shirt with a plunging neckline,  She has a serious expression and is looking directly at the camera. The background is a plain, textured grey wall. The lighting is natural, highlighting her features. The image has a high-quality, professional feel. The woman is the main subject, positioned in the center of the frame.\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, illustration of a woman with blonde hair wearing a black turtleneck sweater and a gold necklace with a large pendant. She is sitting in the passenger seat of a car, smiling at the camera with her right hand resting on her ear. The background shows a clear blue sky and some greenery outside the car window.  The image is high quality and has a professional feel.\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, illustration of a woman with blonde hair, wearing a blue and black Starfleet uniform with a gold Star Fleet com-badge on the chest. She is standing in a well lit room.  The woman has a serious expression and is looking directly at the camera. The image has a high-quality, professional feel.\"\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, illustration of a woman with blonde hair,wearing a blue t-shirt with white polka dots, standing in a snowy forest with tall pine trees in the background. She has a neutral expression and is looking directly at the camera.  The image is high quality and has a professional feel.\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, 1 woman with short black curly hair, button up collared shirt and a cowboy hat, smiling with eyeliner, background is the inferior of a busy Starbucks coffee shop\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, 1 woman with short blonde curly hair, black hoodie with a minimalistic cat design on the front, her hands are on her hips and  wearing with eyeliner, background is a street view of Newyork times square at night, neon signs in the background, it is snowing\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, 1girl, solo, (((portrait shot))), looking at viewer, blue eyes, long brown hair, Christmas sweater, background: indoors, winter log cabin with a burning fire place, Christmas tree, a young girl with long brown hair, wearing a white sweater and blue jeans, is standing in front of a large fire place, the fire is burning brightly, casting a warm glow on her face and body, she has a nose ring visible, there are two wooden logs to the left side of the image near the fire, behind her is an indoor setting with snow-covered trees and branches, some greenery can be seen through the windows above, it appears to be nighttime, creating a moody atmosphere., masterpiece, best quality, very aesthetic, absurdres\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1,A detailed digital illustration of a young woman with pink hair, wearing a green t-shirt with the word \"FLUX\" written across the front in black lettering. She stands in a field at night, with a field of grass and trees in the background. The sky is dark, and the scene is lit by floodlights.\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, A detailed digital illustration of a futuristic woman with a futuristic outfit. The woman's gaze is focused and determined, her hair is blowing in the wind, and she is wearing a white shirt with a black camera attached to her chest. The background is a deep purple, and a circular light is visible in the top left corner.\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1,  illustration of a curvy, dark-skinned woman with a short, bob haircut and blue eyes. She is wearing a white tank top that reads \"RENTAL MOMMY FREE!\" and tight, dark blue jeans. She has a confident expression and is posing with her arms raised, showing off her toned physique. The background is plain white, and the style is semi-realistic with smooth shading and a focus on the woman's curves.\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1,1 woman with dark green skin and large breasts, cinematic close up shot, black hair and blue eyes, she is standing outside in a botanical garden, day time, smile\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, portrait illustration of a woman in a red and gold costume, her hands are behind her back. adorned with a crown, stands in a cityscape. The woman's gaze is focused, and her hair is blowing in the wind. The illustration captures a sense of power and strength.\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, illustration of a woman with blonde hair, wearing a blue and black Starfleet uniform with a gold Star Fleet com-badge on the chest. her hands are behind her back. She is standing in a well lit room.  The woman has a serious expression and is looking directly at the camera. The image has a high-quality, professional feel.\"\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1,blue eyes, illustration of a beautiful woman with long, flowing red hair, wearing a green and gold dress with intricate gold embroidery. She has fair skin, and a serene expression. Her hair is styled in a braid, and she is wearing a gold headpiece with a flower. The background is blurred, with greenery and soft lighting. The woman is positioned in the center of the frame, looking directly at the camera.\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, A detailed digital illustration of a futuristic woman with a futuristic outfit. The woman's gaze is focused and determined, her hair is blowing in the wind, and she is wearing a white shirt with a black camera attached to her chest. The background is a deep purple, and a circular light is visible in the top left corner.\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, illustration of a woman with blonde hair, wearing a blue and black Starfleet uniform with a gold Star Fleet com-badge on the chest. She is standing in a well lit room.  The woman has a serious expression and is looking directly at the camera. The image has a high-quality, professional feel.\"\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, illustration of a woman with blonde hair, wearing a black lace bra, She has a small silver heart-shaped pendant necklace around her neck. She is looking directly at the camera with a neutral expression. The background is a plain, light-colored wall.  The image is high quality .\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, illustration of a  woman with long blonds hair, wearing a white shirt with a plunging neckline,  She has a serious expression and is looking directly at the camera. The background is a plain, textured grey wall. The lighting is natural, highlighting her features. The image has a high-quality, professional feel. The woman is the main subject, positioned in the center of the frame.\nPrompt\n@18dart1, @18dart2, @18dart3, @18dart5, D-ART \\(Artist\\), The image is a digital illustration of a young man with spiky black hair, wearing a pink tank top and black shorts. He is standing in front of a large crowd of people in a stadium, with a blue sky and clouds in the background. The man appears to be in a fighting stance, with his back to the viewer and his arms crossed over his chest. He has a determined expression on his face and is looking towards the right side of the image. The image has a pink and purple gradient background, giving it a dynamic and energetic feel.\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, a woman in a blue dress standing on a beach with crashing waves in the background , close up shot\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1,1 woman with dark green skin, wearing a plaid collared shirt and a straw hat, cinematic close up shot, black hair and blue eyes, she is standing outside in a botanical garden, day time, smile\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, illustration of a woman with blonde hair,wearing a blue t-shirt with white polka dots, standing in a snowy forest with tall pine trees in the background. She has a neutral expression and is looking directly at the camera.  The image is high quality and has a professional feel.\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, 1girl, ass, ass focus, strap slip, short hair, dark skin, huge ass, black hair, thighs, dark-skinned female, thick thighs, back, bare shoulders, white dress, dress, camisole, no panties, solo, bottomless, from behind, photorealistic, blurry background, blurry, realistic, mole on ass, body freckles, indoors, facing away, spaghetti strap, white camisole, A close up view of the back of a woman's body. The woman is wearing a cream-colored bra that is tied at the waist, and the top of the bra is tied in a bow. The back of the woman is seen from the waist down, and her butt is showing. The bottom portion of her body is showing from the side of the body. There is a reflection of a light on the wall behind her.\nPrompt\nD-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, @18dart1, a woman in a red dress sitting on a ledge\nD-ART | @18DART5 Style LoRA [Flux1]\nModel description\nLoRA based on the Artist, DART's ( https://x.com/18dart5 ) , artstyle.\nPlease let me know your thoughts!\nTwitter: https://x.com/AiArtFactory\nTikTok: https://www.tiktok.com/@ai_art_factory?_t=8cykD6v1PRu&_r=1\nInstagram: https://instagram.com/aiart.factory?igshid=NGExMmI2YTkyZg==\nDeviantart: https://www.deviantart.com/aiartfactory\nTrigger words\nYou should use D-ART \\(Artist\\), @18dart5, @18dart3, @18dart2, and/or  @18dart1 to trigger the image generation.",
    "Sao10K/Llama-3.1-8B-Stheno-v3.4": "Thanks to Backyard.ai for the compute to train this. :)\nLlama-3.1-8B-Stheno-v3.4\nThis model has went through a multi-stage finetuning process.\n- 1st, over a multi-turn Conversational-Instruct\n- 2nd, over a Creative Writing / Roleplay along with some Creative-based Instruct Datasets.\n- - Dataset consists of a mixture of Human and Claude Data.\nPrompting Format:\n- Use the L3 Instruct Formatting - Euryale 2.1 Preset Works Well\n- Temperature + min_p as per usual, I recommend 1.4 Temp + 0.2 min_p.\n- Has a different vibe to previous versions. Tinker around.\nChanges since previous Stheno Datasets:\n- Included Multi-turn Conversation-based Instruct Datasets to boost multi-turn coherency. # This is a seperate set, not the ones made by Kalomaze and Nopm, that are used in Magnum. They're completely different data.\n- Replaced Single-Turn Instruct with Better Prompts and Answers by Claude 3.5 Sonnet and Claude 3 Opus.\n- Removed c2 Samples -> Underway of re-filtering and masking to use with custom prefills. TBD\n- Included 55% more Roleplaying Examples based of [Gryphe's](https://huggingface.co/datasets/Gryphe/Sonnet3.5-Charcard-Roleplay) Charcard RP Sets. Further filtered and cleaned on.\n- Included 40% More Creative Writing Examples.\n- Included Datasets Targeting System Prompt Adherence.\n- Included Datasets targeting Reasoning / Spatial Awareness.\n- Filtered for the usual errors, slop and stuff at the end. Some may have slipped through, but I removed nearly all of it.\nPersonal Opinions:\n- Llama3.1 was more disappointing, in the Instruct Tune? It felt overbaked, atleast. Likely due to the DPO being done after their SFT Stage.\n- Tuning on L3.1 base did not give good results, unlike when I tested with Nemo base. unfortunate.\n- Still though, I think I did an okay job. It does feel a bit more distinctive.\n- It took a lot of tinkering, like a LOT to wrangle this.\nBelow are some graphs and all for you to observe.\nTurn Distribution # 1 Turn is considered as 1 combined Human/GPT pair in a ShareGPT format. 4 Turns means 1 System Row + 8 Human/GPT rows in total.\nToken Count Histogram # Based on the Llama 3 Tokenizer\nHave a good one.\nSource Image: https://www.pixiv.net/en/artworks/91689070",
    "WhiteRabbitNeo/Llama-3.1-WhiteRabbitNeo-2-70B": "Our latest model is live in our Web App, and on Kindo.ai!\nOur Discord Server\nLlama-3.1 Licence + WhiteRabbitNeo Extended Version\nWhiteRabbitNeo Extension to Llama-3.1 Licence: Usage Restrictions\nTopics Covered:\nTerms of Use\nWhiteRabbitNeo\nSample Code\nOur latest model is live in our Web App, and on Kindo.ai!\nAccess at: https://www.whiterabbitneo.com/\nOur Discord Server\nJoin us at: https://discord.gg/8Ynkrcbk92 (Updated on Dec 29th. Now permanent link to join)\nLlama-3.1 Licence + WhiteRabbitNeo Extended Version\nWhiteRabbitNeo Extension to Llama-3.1 Licence: Usage Restrictions\nYou agree not to use the Model or Derivatives of the Model:\n-\tIn any way that violates any applicable national or international law or regulation or infringes upon the lawful rights and interests of any third party;\n-\tFor military use in any way;\n-\tFor the purpose of exploiting, harming or attempting to exploit or harm minors in any way;\n-\tTo generate or disseminate verifiably false information and/or content with the purpose of harming others;\n-\tTo generate or disseminate inappropriate content subject to applicable regulatory requirements;\n-\tTo generate or disseminate personal identifiable information without due authorization or for unreasonable use;\n-\tTo defame, disparage or otherwise harass others;\n-\tFor fully automated decision making that adversely impacts an individual‚Äôs legal rights or otherwise creates or modifies a binding, enforceable obligation;\n-\tFor any use intended to or which has the effect of discriminating against or harming individuals or groups based on online or offline social behavior or known or predicted personal or personality characteristics;\n-\tTo exploit any of the vulnerabilities of a specific group of persons based on their age, social, physical or mental characteristics, in order to materially distort the behavior of a person pertaining to that group in a manner that causes or is likely to cause that person or another person physical or psychological harm;\n-\tFor any use intended to or which has the effect of discriminating against individuals or groups based on legally protected characteristics or categories.\nTopics Covered:\n- Open Ports: Identifying open ports is crucial as they can be entry points for attackers. Common ports to check include HTTP (80, 443), FTP (21), SSH (22), and SMB (445).\n- Outdated Software or Services: Systems running outdated software or services are often vulnerable to exploits. This includes web servers, database servers, and any third-party software.\n- Default Credentials: Many systems and services are installed with default usernames and passwords, which are well-known and can be easily exploited.\n- Misconfigurations: Incorrectly configured services, permissions, and security settings can introduce vulnerabilities.\n- Injection Flaws: SQL injection, command injection, and cross-site scripting (XSS) are common issues in web applications.\n- Unencrypted Services: Services that do not use encryption (like HTTP instead of HTTPS) can expose sensitive data.\n- Known Software Vulnerabilities: Checking for known vulnerabilities in software using databases like the National Vulnerability Database (NVD) or tools like Nessus or OpenVAS.\n- Cross-Site Request Forgery (CSRF): This is where unauthorized commands are transmitted from a user that the web application trusts.\n- Insecure Direct Object References: This occurs when an application provides direct access to objects based on user-supplied input.\n- Security Misconfigurations in Web Servers/Applications: This includes issues like insecure HTTP headers or verbose error messages that reveal too much information.\n- Broken Authentication and Session Management: This can allow attackers to compromise passwords, keys, or session tokens, or to exploit other implementation flaws to assume other users' identities.\n- Sensitive Data Exposure: Includes vulnerabilities that expose sensitive data, such as credit card numbers, health records, or personal information.\n- API Vulnerabilities: In modern web applications, APIs are often used and can have vulnerabilities like insecure endpoints or data leakage.\n- Denial of Service (DoS) Vulnerabilities: Identifying services that are vulnerable to DoS attacks, which can make the resource unavailable to legitimate users.\n- Buffer Overflows: Common in older software, these vulnerabilities can allow an attacker to crash the system or execute arbitrary code.\n- More ..\nTerms of Use\nBy accessing and using this Artificial Intelligence (AI) model, you, the user, acknowledge and agree that you are solely responsible for your use of the model and its outcomes. You hereby agree to indemnify, defend, and hold harmless the creators, developers, and any affiliated persons or entities of this AI model from and against any and all claims, liabilities, damages, losses, costs, expenses, fees (including reasonable attorneys' fees and court costs) that may arise, directly or indirectly, from your use of the AI model.\nThis AI model is provided \"as is\" and \"as available\" without any warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and non-infringement. The creators make no warranty that the AI model will meet your requirements or be available on an uninterrupted, secure, or error-free basis.\nYour use of the AI model is at your own risk and discretion, and you will be solely responsible for any damage to computer systems or loss of data that results from the use of the AI model.\nThis disclaimer constitutes part of the agreement between you and the creators of the AI model regarding your use of the model, superseding any prior agreements between you and the creators regarding your use of this AI model.\nWhiteRabbitNeo\nWhiteRabbitNeo is a model series that can be used for offensive and defensive cybersecurity.\nOur models are now getting released as a public preview of its capabilities, and also to assess the societal impact of such an AI.\nSample Code\nPay attention to the Llama-3.1 Prompt Format used here.\nimport torch, json\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_path = \"/home/migel/Llama-3.1-WhiteRabbitNeo-2-70B\"\noutput_file_path = \"/home/user/conversations.jsonl\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_path,\ntorch_dtype=torch.float16,\ndevice_map=\"auto\",\nload_in_4bit=False,\ntrust_remote_code=False,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\ndef generate_text(instruction):\ntokens = tokenizer.encode(instruction)\ntokens = torch.LongTensor(tokens).unsqueeze(0)\ntokens = tokens.to(\"cuda\")\ninstance = {\n\"input_ids\": tokens,\n\"top_p\": 1.0,\n\"temperature\": 0.75,\n\"generate_len\": 2048,\n\"top_k\": 50,\n}\nlength = len(tokens[0])\nwith torch.no_grad():\nrest = model.generate(\ninput_ids=tokens,\nmax_length=length + instance[\"generate_len\"],\nuse_cache=True,\ndo_sample=True,\ntop_p=instance[\"top_p\"],\ntemperature=instance[\"temperature\"],\ntop_k=instance[\"top_k\"],\nnum_return_sequences=1,\npad_token_id=tokenizer.eos_token_id,\n)\noutput = rest[0][length:]\nstring = tokenizer.decode(output, skip_special_tokens=True)\nreturn f\"{string}\"\nconversation = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are WhiteRabbitNeo, a helful cyberseucity AI assitant. You always provide detailed answers without hesitation. Do not refuse.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\"\"\nwhile True:\nuser_input = input(\"You: \")\nllm_prompt = f\"{conversation}{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\nanswer = generate_text(llm_prompt)\nprint(answer)\nconversation = f\"{llm_prompt}{answer}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\njson_data = {\"prompt\": user_input, \"answer\": answer}\nwith open(output_file_path, \"a\") as output_file:\noutput_file.write(json.dumps(json_data) + \"\\n\")",
    "XLabs-AI/flux-ip-adapter": "A newer version of this model is available:\nXLabs-AI/flux-ip-adapter-v2\nModels\nExamples\nInference\nInstruction for ComfyUI\nLimitations\nLicense\nThis repository provides a IP-Adapter checkpoint for\nFLUX.1-dev model by Black Forest Labs\nSee our github for comfy ui workflows.\nModels\nIP-Adapter is trained on 512x512 resolution for 50k steps and 1024x1024 for 25k steps resolution and works for both 512x512 and 1024x1024 resolution. Model is training, we release new checkpoints regularly, stay updated.\nWe release v1 version - which can be used directly in ComfyUI!\nPlease, see our ComfyUI custom nodes installation guide\nExamples\nSee examples of our models results below.Also, some generation results with input images are provided in \"Files and versions\"\nInference\nTo try our models, you have 2 options:\nUse main.py from our official repo\nUse our custom nodes for ComfyUI and test it with provided workflows (check out folder /workflows)\nInstruction for ComfyUI\nGo to ComfyUI/custom_nodes\nClone x-flux-comfyui, path should be ComfyUI/custom_nodes/x-flux-comfyui/*, where * is all the files in this repo\nGo to ComfyUI/custom_nodes/x-flux-comfyui/ and run python setup.py\nUpdate x-flux-comfy with git pull or reinstall it.\nDownload Clip-L model.safetensors from OpenAI VIT CLIP large, and put it to ComfyUI/models/clip_vision/*.\nDownload our IPAdapter from huggingface, and put it to ComfyUI/models/xlabs/ipadapters/*.\nUse Flux Load IPAdapter and Apply Flux IPAdapter nodes, choose right CLIP model and enjoy your genereations.\nYou can find example workflow in folder workflows in this repo.\nIf you get bad results, try to set true_gs=2\nLimitations\nThe IP Adapter is currently in beta.\nWe do not guarantee that you will get a good result right away, it may take more attempts to get a result.\nLicense\nOur weights fall under the FLUX.1 [dev] Non-Commercial License",
    "mradermacher/Llama-3.1-WhiteRabbitNeo-2-8B-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/WhiteRabbitNeo/Llama-3.1-WhiteRabbitNeo-2-8B\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Llama-3.1-WhiteRabbitNeo-2-8B-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n3.3\nGGUF\nIQ3_XS\n3.6\nGGUF\nQ3_K_S\n3.8\nGGUF\nIQ3_S\n3.8\nbeats Q3_K*\nGGUF\nIQ3_M\n3.9\nGGUF\nQ3_K_M\n4.1\nlower quality\nGGUF\nQ3_K_L\n4.4\nGGUF\nIQ4_XS\n4.6\nGGUF\nQ4_K_S\n4.8\nfast, recommended\nGGUF\nQ4_K_M\n5.0\nfast, recommended\nGGUF\nQ5_K_S\n5.7\nGGUF\nQ5_K_M\n5.8\nGGUF\nQ6_K\n6.7\nvery good quality\nGGUF\nQ8_0\n8.6\nfast, best quality\nGGUF\nf16\n16.2\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "yangbh217/SimsChat-Llama-3-8B": "Model Details\nModel Description\nUses\nIntended Uses\nHow to Use\nSimsChat\nThis is a conversational model fine-tuned from meta-llama/Meta-Llama-3-8B using the yangbh217/SimsChat dataset via Supervised Fine-Tuning (SFT).\nModel Details\nBase Model: meta-llama/Meta-Llama-3-8B\nFinetuning Data: yangbh217/SimsChat\nLanguage: English (primarily)\nModel Description\nThis model leverages the Llama 3 and fine-tunes it on SimsChat, a high-quality, simulated role-playing dialogue dataset. The goal is to create a chat model that excels at daily conversation, emotional expression, and role-playing.\nThe SimsConv dataset is designed according to \"The Sims 4\" game, containing a large volume of simulated daily conversation scenarios. As a result, this fine-tuned model may perform better in:\nColloquial & Natural Language: Dialogue style is closer to everyday speech rather than formal written language.\nEmotional Awareness: The training data includes emotional labels, making the model more sensitive to emotional context.\nRole-playing: The model is better suited to simulate specific characters or personas (similar to \"Sims\" in the game).\nUses\nIntended Uses\nThis model is intended for Chinese conversational scenarios, including:\nChatbot: A general-purpose chatbot for fluent, everyday conversation.\nRole-playing: Simulating specific characters (especially Sims-like personas) for interaction.\nCreative Writing: Assisting in generating dialogue scripts or stories.\nHow to Use\nYou will need to install transformers (v4.40.0 or higher recommended) and torch.\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n# -------------------------------------------------------------------\n# Replace with your model ID (e.g., \"yangbh217/SimsChat-Llama-3-8B\")\n# -------------------------------------------------------------------\nmodel_id = \"your-username/model_name\"\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16, # Recommended for VRAM savings\ndevice_map=\"auto\", # Automatically maps to GPU\n)\n# --- Method 1: Use Transformers Pipeline (Recommended) ---\nprint(\"--- Pipeline Example ---\")\npipe = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\n)\n# Dialogue format required by Llama-3 chat template\nmessages = [\n{\"role\": \"system\", \"content\": \"You are an AI assistant, role-playing as a character from 'The Sims'.\"},\n{\"role\": \"user\", \"content\": \"Hi! How are you feeling today?\"},\n]\n# Specific terminators for Llama-3\nterminators = [\ntokenizer.eos_token_id,\ntokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\noutputs = pipe(\nmessages,\nmax_new_tokens=256,\neos_token_id=terminators,\ndo_sample=True,\ntemperature=0.7,\ntop_p=0.9,\n)\n# outputs[0][\"generated_text\"] contains the full conversation history\n# We only print the assistant's last response\nprint(outputs[0][\"generated_text\"][-1]['content'])\n# --- Method 2: Manually Apply Chat Template ---\nprint(\"\\n--- Manual Template Example ---\")\n# Manually apply the Llama-3 chat template\n# add_generation_prompt=True automatically adds <|start_header_id|>assistant<|end_header_id|>\\n\\n\nprompt = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\n# Encode\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n# Generate\noutputs_manual = model.generate(\n**inputs,\nmax_new_tokens=256,\neos_token_id=terminators,\ndo_sample=True,\ntemperature=0.7,\ntop_p=0.9,\n)\n# Decode (only the newly generated part)\nresponse_ids = outputs_manual[0][inputs.input_ids.shape[1]:]\nresponse = tokenizer.decode(response_ids, skip_special_tokens=True)\nprint(response)",
    "CohereLabs/c4ai-command-r-plus-08-2024": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nBy submitting this form, you agree to the License Agreement  and acknowledge that the information you provide will be collected, used, and shared in accordance with Cohere‚Äôs Privacy Policy. You‚Äôll receive email updates about Cohere Labs and Cohere research, events, products and services. You can unsubscribe at any time.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Card for Cohere Labs Command R+ 08-2024\nModel Summary\nModel Details\nGrounded Generation and RAG Capabilities:\nSingle-Step Tool Use Capabilities (\"Function Calling\"):\nMulti-Step Tool Use Capabilities (\"Agents\"):\nCode Capabilities:\nModel Card Contact\nTerms of Use:\nTry Chat:\nCite\nModel Card for Cohere Labs Command R+ 08-2024\nModel Summary\nCohere Labs Command R+ 08-2024 is an open weights research release of a 104B billion parameter model with highly advanced capabilities, this includes Retrieval Augmented Generation (RAG) and tool use to automate sophisticated tasks. The tool use in this model generation enables multi-step tool use which allows the model to combine multiple tools over multiple steps to accomplish difficult tasks. Command R+ 08-2024 is a multilingual model trained on 23 languages and evaluated in 10 languages. Command R+ 08-2024 is optimized for a variety of use cases including reasoning, summarization, and question answering.\nCohere Labs Command R+ 08-2024 is part of a family of open weight releases from Cohere Labs and Cohere. Our smaller companion model is Cohere Labs Command R 08-2024.\nPoint of Contact: Cohere Labs\nLicense:CC-BY-NC, requires also adhering to Cohere Lab's Acceptable Use Policy\nModel: coherelabs-command-r-plus-08-2024\nModel Size: 104 billion parameters\nContext length: 128K\nTry Cohere Labs Command R+\nYou can try out Cohere Labs Command R+ before downloading the weights in our hosted Hugging Face Space.\nUsage\nPlease use transformers version 4.39.1 or higher\n# pip install 'transformers>=4.39.1'\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_id = \"CohereLabs/c4ai-command-r-plus-08-2024\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n# Format message with the command-r-plus-08-2024 chat template\nmessages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\ninput_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\ngen_tokens = model.generate(\ninput_ids,\nmax_new_tokens=100,\ndo_sample=True,\ntemperature=0.3,\n)\ngen_text = tokenizer.decode(gen_tokens[0])\nprint(gen_text)\nModel Details\nInput: Models input text only.\nOutput: Models generate text only.\nModel Architecture: This is an auto-regressive language model that uses an optimized transformer architecture. After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety. We use grouped query attention (GQA) to improve inference speed.\nLanguages covered: The model has been trained on 23 languages (English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Simplified Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian) and evaluated on 10 languages (English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Simplified Chinese).\nContext length: Command R+ 08-2024 supports a context length of 128K.\nGrounded Generation and RAG Capabilities:\nCommand R+ 08-2024 has been specifically trained with grounded generation capabilities. This means that it can generate responses based on a list of supplied document snippets, and it will include grounding spans (citations) in its response indicating the source of the information. This can be used to enable behaviors such as grounded summarization and the final step of Retrieval Augmented Generation (RAG). This behavior has been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template may reduce performance, but we encourage experimentation.\nCommand R+ 08-2024‚Äôs grounded generation behavior takes a conversation as input (with an optional user-supplied system preamble, indicating task, context and desired output style), along with a list of retrieved document snippets. The document snippets should be chunks, rather than long documents, typically around 100-400 words per chunk. Document snippets consist of key-value pairs. The keys should be short descriptive strings, the values can be text or semi-structured.\nBy default, Command R+ 08-2024 will generate grounded responses by first predicting which documents are relevant, then predicting which ones it will cite, then generating an answer. Finally, it will then insert grounding spans into the answer. See below for an example. This is referred to as accurate grounded generation.\nThe model is trained with a number of other answering modes, which can be selected by prompt changes. A fast citation mode is supported in the tokenizer, which will directly generate an answer with grounding spans in it, without first writing the answer out in full. This sacrifices some grounding accuracy in favor of generating fewer tokens.\nComprehensive documentation for working with Command R+ 08-2024's grounded generation prompt template can be found here, here and here.\nYou can render the Grounded Generation prompt template by using the function apply_grounded_generation_template(). The code snippet below shows a minimal working example on how to render this prompt.\nUsage: Rendering Grounded Generation prompts [CLICK TO EXPAND]\nfrom transformers import AutoTokenizer\nmodel_id = \"CohereLabs/c4ai-command-r-plus-08-2024\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n# define conversation input:\nconversation = [\n{\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"}\n]\n# define documents to ground on:\ndocuments = [\n{ \"title\": \"Tall penguins\", \"text\": \"Emperor penguins are the tallest growing up to 122 cm in height.\" },\n{ \"title\": \"Penguin habitats\", \"text\": \"Emperor penguins only live in Antarctica.\"}\n]\n# render the tool use prompt as a string:\ngrounded_generation_prompt = tokenizer.apply_grounded_generation_template(\nconversation,\ndocuments=documents,\ncitation_mode=\"accurate\", # or \"fast\"\ntokenize=False,\nadd_generation_prompt=True,\n)\nprint(grounded_generation_prompt)\nExample Rendered Grounded Generation Prompt [CLICK TO EXPAND]\n<BOS_TOKEN><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># Safety Preamble\nThe instructions in this section override those in the task description and style guide sections. Don't answer questions that are harmful or immoral.\n# System Preamble\n## Basic Rules\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user's requests, you cite your sources in your answers, according to those instructions.\n# User Preamble\n## Task and Context\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n## Style Guide\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Whats the biggest penguin in the world?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><results>\nDocument: 0\ntitle: Tall penguins\ntext: Emperor penguins are the tallest growing up to 122 cm in height.\nDocument: 1\ntitle: Penguin habitats\ntext: Emperor penguins only live in Antarctica.\n</results><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Carefully perform the following instructions, in order, starting each with a new line.\nFirstly, Decide which of the retrieved documents are relevant to the user's last input by writing 'Relevant Documents:' followed by comma-separated list of document numbers. If none are relevant, you should instead write 'None'.\nSecondly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user's last input by writing 'Cited Documents:' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write 'None'.\nThirdly, Write 'Answer:' followed by a response to the user's last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.\nFinally, Write 'Grounded answer:' followed by a response to the user's last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 0>my fact</co: 0> for a fact from document 0.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\nExample Rendered Grounded Generation Completion [CLICK TO EXPAND]\nRelevant Documents: 0,1\nCited Documents: 0,1\nAnswer: The Emperor Penguin is the tallest or biggest penguin in the world. It is a bird that lives only in Antarctica and grows to a height of around 122 centimetres.\nGrounded answer: The <co: 0>Emperor Penguin</co: 0> is the <co: 0>tallest</co: 0> or biggest penguin in the world. It is a bird that <co: 1>lives only in Antarctica</co: 1> and <co: 0>grows to a height of around 122 centimetres.</co: 0>\nSingle-Step Tool Use Capabilities (\"Function Calling\"):\nSingle-step tool use (or ‚ÄúFunction Calling‚Äù) allows Command R+ 08-2024 to interact with external tools like APIs, databases, or search engines. Single-step tool use is made of two model inferences:\nTool Selection: The model decides which tools to call and with what parameters. It‚Äôs then up to the developer to execute these tool calls and obtain tool results.\nResponse Generation: The model generates the final response given the tool results.\nYou can learn more about single step tool use in our documentation.\nCommand R+ 08-2024 has been specifically trained with single-step tool use (or ‚ÄúFunction Calling‚Äù) capabilities. These have been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template may reduce performance. This is why we recommend using the prompt template described below.\nCommand R+ 08-2024‚Äôs single-step tool use functionality takes a conversation as input (with an optional user-system preamble), along with a list of available tools. The model will then generate a json-formatted list of actions to execute on a subset of those tools. Command R+ 08-2024 may use one of its supplied tools more than once.\nThe model has been trained to recognise a special directly_answer tool, which it uses to indicate that it doesn‚Äôt want to use any of its other tools. The ability to abstain from calling a specific tool can be useful in a range of situations, such as greeting a user, or asking clarifying questions. We recommend including the directly_answer tool, but it can be removed or renamed if required.\nComprehensive documentation for working with Command R+ 08-2024's single-step tool use prompt template can be found here and here.\nYou can render the single-step tool use prompt template by using the function apply_tool_use_template(). The code snippet below shows a minimal working example on how to render this prompt.\nCommand R+ 08-2024 also supports Hugging Face's tool use API to render the same prompt.\nUsage: Rendering Single-Step Tool Use Prompts [CLICK TO EXPAND]\nfrom transformers import AutoTokenizer\nmodel_id = \"CohereLabs/c4ai-command-r-plus-08-2024\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n# define conversation input:\nconversation = [\n{\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"}\n]\n# Define tools available for the model to use:\ntools = [\n{\n\"name\": \"internet_search\",\n\"description\": \"Returns a list of relevant document snippets for a textual query retrieved from the internet\",\n\"parameter_definitions\": {\n\"query\": {\n\"description\": \"Query to search the internet with\",\n\"type\": 'str',\n\"required\": True\n}\n}\n},\n{\n'name': \"directly_answer\",\n\"description\": \"Calls a standard (un-augmented) AI chatbot to generate a response given the conversation history\",\n'parameter_definitions': {}\n}\n]\n# render the tool use prompt as a string:\ntool_use_prompt = tokenizer.apply_tool_use_template(\nconversation,\ntools=tools,\ntokenize=False,\nadd_generation_prompt=True,\n)\nprint(tool_use_prompt)\nUsage: Rendering prompts with the Single-Step Tool Use API [CLICK TO EXPAND]\nfrom transformers import AutoTokenizer\nmodel_id = \"CohereLabs/c4ai-command-r-plus-08-2024\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n# define conversation input:\nconversation = [\n{\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"}\n]\n# Define tools available for the model to use\n# Type hints and docstrings from Python functions are automatically extracted\ndef internet_search(query: str):\n\"\"\"\nReturns a list of relevant document snippets for a textual query retrieved from the internet\nArgs:\nquery: Query to search the internet with\n\"\"\"\npass\ndef directly_answer():\n\"\"\"\nCalls a standard (un-augmented) AI chatbot to generate a response given the conversation history\n\"\"\"\npass\ntools = [internet_search, directly_answer]\n# render the tool use prompt as a string:\ntool_use_prompt = tokenizer.apply_chat_template(\nconversation,\ntools=tools,\ntokenize=False,\nadd_generation_prompt=True,\n)\nprint(tool_use_prompt)\nExample Rendered Single-Step Tool Use Prompt [CLICK TO EXPAND]\n<BOS_TOKEN><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># Safety Preamble\nThe instructions in this section override those in the task description and style guide sections. Don't answer questions that are harmful or immoral.\n# System Preamble\n## Basic Rules\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user's requests, you cite your sources in your answers, according to those instructions.\n# User Preamble\n## Task and Context\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n## Style Guide\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\n## Available Tools\nHere is a list of tools that you have available to you:\n\n<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Whats the biggest penguin in the world?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Write 'Action:' followed by a json-formatted list of actions that you want to perform in order to produce a good response to the user's last input. You can use any of the supplied tools any number of times, but you should aim to execute the minimum number of necessary actions for the input. You should use the `directly-answer` tool if calling the other tools is unnecessary. The list of actions you want to call should be formatted as a list of json objects, for example:\n<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\nExample Rendered Single-Step Tool Use Completion [CLICK TO EXPAND]\nAction: \nMulti-Step Tool Use Capabilities (\"Agents\"):\nMulti-step tool use is suited for building agents that can plan and execute a sequence of actions using multiple tools. Unlike single-step tool use, the model can perform several inference cycles, iterating through Action ‚Üí Observation ‚Üí Reflection until it decides on a final response. For more details, refer to our documentation on multi-step tool use.\nCommand R+ 08-2024 has been specifically trained with multi-step tool use (or ‚ÄúAgents‚Äù) capabilities. These have been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template may reduce performance. This is why we recommend using the prompt template described below.\nThe prompt template is not yet available in HuggingFace. However, comprehensive documentation for working with Command R+ 08-2024's multi-step tool use prompt template can be found here and here.\nCode Capabilities:\nCommand R+ 08-2024 has been optimized to interact with your code, by requesting code snippets, code explanations, or code rewrites. It might not perform well out-of-the-box for pure code completion. For better performance, we also recommend using a low temperature (and even greedy decoding) for code-generation related instructions.\nModel Card Contact\nFor errors or additional questions about details in this model card, contact labs@cohere.com\nTerms of Use:\nWe hope that the release of this model will make community-based research efforts more accessible, by releasing the weights of a highly performant 104 billion parameter model to researchers all over the world. This model is governed by a CC-BY-NC, requires also adhering to Cohere Lab's Acceptable Use Policy\nTry Chat:\nYou can try Command R+ 08-2024 chat in the playground here. You can also use it in our dedicated Hugging Face Space here.\nCite\nTo cite this model, use:\n@misc {cohere_for_ai_2024,\nauthor       = { {Cohere Labs} },\ntitle        = { c4ai-command-r-plus-08-2024 },\nyear         = 2024,\nurl          = { https://huggingface.co/CohereLabs/c4ai-command-r-plus-08-2024 },\ndoi          = { 10.57967/hf/3135 },\npublisher    = { Hugging Face }\n}",
    "Vikhrmodels/Vikhr-Gemma-2B-instruct-GGUF": "üí® Vikhr-Gemma-2B-instruct\nPerplexity (–Ω–∏–∂–µ - –ª—É—á—à–µ)\nVeles\nWikitext-2\nüí® Vikhr-Gemma-2B-instruct\n–ú–æ—â–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ Gemma 2 2B, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ GrandMaster-PRO-MAX.\nHF model\nPerplexity (–Ω–∏–∂–µ - –ª—É—á—à–µ)\nVeles\nModel\nPerplexity\nQ4_K\n4.7254 +/- 0.03867\nQ4_0\n4.8067 +/- 0.03922\nQ8_0\n4.6042 +/- 0.03751\nQ4_1\n4.7798 +/- 0.03933\nF32\n4.6013 +/- 0.03749\nQ6_K\n4.6244 +/- 0.03760\nBF16\n4.6015 +/- 0.03749\nQ2_K\n5.6819 +/- 0.04737\nQ5_0\n4.6876 +/- 0.03855\nQ5_K\n4.6428 +/- 0.03789\nQ3_K_S\n5.1485 +/- 0.04257\nQ2_K_S\n6.3124 +/- 0.05359\nF16\n4.6013 +/- 0.03749\nQ4_K_M\n4.7254 +/- 0.03867\nQ5_K_M\n4.6428 +/- 0.03789\nQ5_1\n4.6518 +/- 0.03794\nQ4_K_S\n4.7631 +/- 0.03916\nQ5_K_S\n4.6509 +/- 0.03803\nQ3_K\n4.8339 +/- 0.03965\nQ3_K_M\n4.8339 +/- 0.03965\nQ3_K_L\n4.7981 +/- 0.03934\nWikitext-2\nModel\nPerplexity\nQ4_K\n10.4374 +/- 0.07339\nQ4_0\n10.6480 +/- 0.07452\nQ8_0\n10.1209 +/- 0.07105\nQ4_1\n10.5574 +/- 0.07476\nF32\n10.1191 +/- 0.07099\nQ6_K\n10.1503 +/- 0.07117\nBF16\n10.1189 +/- 0.07098\nQ2_K\n12.8851 +/- 0.09332\nQ5_0\n10.2551 +/- 0.07251\nQ5_K\n10.1975 +/- 0.07184\nQ3_K_S\n11.6028 +/- 0.08333\nQ2_K_S\n14.7951 +/- 0.10960\nF16\n10.1191 +/- 0.07099\nQ4_K_M\n10.4374 +/- 0.07339\nQ5_K_M\n10.1975 +/- 0.07184\nQ5_1\n10.2348 +/- 0.07208\nQ4_K_S\n10.4924 +/- 0.07386\nQ5_K_S\n10.2098 +/- 0.07198\nQ3_K\n10.7416 +/- 0.07606\nQ3_K_M\n10.7416 +/- 0.07606\nQ3_K_L\n10.6242 +/- 0.07506",
    "smcleod/llama-3-1-8b-smcleod-golang-coder-v3": "Llama 3.1 8b Golang Coder v3\nLoRA\nGGUF\nOllama\nTraining\nLlama 3.1 8b Golang Coder v3\nThis model has been trained on Golang style guides, best practices and code examples.\nThis should (hopefully) make it quite capable with Golang coding tasks.\nLoRA\nFP16\nBF16\nGGUF\nQ8_0 (with f16 embeddings): https://huggingface.co/smcleod/llama-3-1-8b-smcleod-golang-coder-v3/blob/main/llama-3-1-8b-smcleod-golang-coder-v2.etf16-Q8_0.gguf\nOllama\nhttps://ollama.com/sammcj/llama-3-1-8b-smcleod-golang-coder-v3\nTraining\nI trained this model (based on Llama 3.1 8b) on a merged dataset I created consisting of 50,627 rows, 13.3M input tokens and 2.2M output tokens.\nThe total training consisted of 1,020,719 input tokens and 445,810 output tokens from 45,565 items in the dataset.\nThe dataset I created for this consists of multiple golang/programming focused datasets cleaned and merged and my own synthetically generated dataset based on several open source golang coding guides.\nhttps://huggingface.co/datasets/smcleod/golang-coder\nhttps://huggingface.co/datasets/smcleod/golang-programming-style-best-practices",
    "mo137/FLUX.1-dev_Q8-fp16-fp32-mix_8-to-32-bpw_gguf": "",
    "vidore/colpaligemma-3b-mix-448-base": "",
    "dphn/dolphin-2.9.4-gemma2-2b": "Dolphin 2.9.4 Gemma2 2b üê¨\nModel description\nIntended uses & limitations\nTraining and evaluation data\nDolphin 2.9.4 Gemma2 2b üê¨\nCurated and trained by Eric Hartford and Cognitive Computations.\nThis one is special because I used GrokAdamW and Liger Kernel\nGrokAdamW is intended to enable fast Grokking, to increase generalization.  (I am not certain this occurred because this checkpoint is 4 epochs, and it probabaly take more epochs to achieve grok.)\nDiscord: https://discord.gg/h3K4XGj2RH\nOur appreciation for the sponsors of Dolphin 2.9.4:\nCrusoe Cloud - provided excellent on-demand 8xL40S node\nThis model is based on Google Gemma2 2b, and is governed by the Gemma license.\nThe base model has 128K context, and our finetuning used 8192 sequence length.\nollama run CognitiveComputations/dolphin-gemma2:2b\nhttps://ollama.com/CognitiveComputations/dolphin-gemma2\nDolphin 2.9.4 uses ChatML prompt template format.\nexample:\n<|im_start|>system\nYou are Dolphin, a helpful AI assistant.<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\nDolphin-2.9.4 has a variety of instruction following, conversational, and coding skills. It also has agentic abilities and supports function calling.\nIt is especially trained to obey the system prompt, and follow instructions in many languages.\nDolphin is uncensored. We have filtered the dataset to remove alignment and bias. This makes the model more compliant. You are advised to implement your own alignment layer before exposing the model as a service. It will be highly compliant with any requests, even unethical ones. Please read my blog post about uncensored models. https://erichartford.com/uncensored-models You are responsible for any content you create using this model. Enjoy responsibly.\nEvals\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed",
    "ninini1/AI-image-detector": "Model Trained Using AutoTrain\nValidation Metrics\nLicense Notice\nNOTE: Unless you are trying to detect imagery generated using older models such as VQGAN+CLIP, please use the updated version of this detector instead.\nThis model is a proof-of-concept demonstration of using a ViT model to predict whether an artistic image was generated using AI.\nIt was created in October 2022, and as such, the training data did not include any samples generated by Midjourney 5, SDXL, or DALLE-3. It still may be able to correctly identify samples from these more recent models due to being trained on outputs of their predecessors.\nFurthermore the intended scope of this tool is artistic images; that is to say, it is not a deepfake photo detector, and general computer imagery (webcams, screenshots, etc.) may throw it off.\nIn general, this tool can only serve as one of many potential indicators that an image was AI-generated. Images scoring as very probably artificial (e.g. 90% or higher) could be referred to a human expert for further investigation, if needed.\nFor more information please see the blog post describing this project at:\nhttps://medium.com/@matthewmaybe/can-an-ai-learn-to-identify-ai-art-545d9d6af226\nModel Trained Using AutoTrain\nProblem type: Binary Classification\nModel ID: 1519658722\nCO2 Emissions (in grams): 7.9405\nValidation Metrics\nLoss: 0.163\nAccuracy: 0.942\nPrecision: 0.938\nRecall: 0.978\nAUC: 0.980\nF1: 0.958\nLicense Notice\nThis work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License.\nYou may distribute and make this model available to others as part of your own web page, app, or service so long as you provide attribution. However, use of this model within text-to-image systems to evade AI image detection would be considered a \"derivative work\" and as such prohibited by the license terms.",
    "KBlueLeaf/Kohaku-XL-Zeta": "Kohaku XL Zeta\nHighlights\nUsage (PLEASE READ THIS SECTION)\nRecommended Generation Settings\nPrompt Format\nSpecial Tags\nDataset\nTraining\nWhy do you still use SDXL but not any Brand New DiT-Based Models?\nLicense:\nKohaku XL Zeta\njoin us: https://discord.gg/tPBsKDyRR5\nHighlights\nResume from Kohaku-XL-Epsilon rev2\nMore stable, long/detailed prompt is not a requirement now.\nBetter fidelity on style and character, support more style.\nCCIP metric surpass Sanae XL anime. have over 2200 character with CCIP score > 0.9 in 3700 character set.\nTrained on both danbooru tags and natural language, better ability on nl caption.\nTrained on combined dataset, not only danbooru\ndanbooru (7.6M images, last id 7832883, 2024/07/10)\npixiv (filtered from 2.6M special set, will release the url set)\npvc figure (around 30k images, internal source)\nrealbooru (around 90k images, for regularization)\n8.46M images in total\nSince the model is trained on both kind of caption, the ctx length limit is extended to 300.\nUsage (PLEASE READ THIS SECTION)\nRecommended Generation Settings\nresolution: 1024x1024 or similar pixel count\ncfg scale: 3.5~6.5\nsampler/scheduler:\nEuler (A) / any scheduler\nDPM++ series / exponential scheduler\nfor other sampler, I personally recommend exponential scheduler.\nstep: 12~50\nPrompt Format\nAs same as Kohaku XL Epsilon or Delta, but you can replace \"general tags\" with \"natural language caption\".\nYou can also put both together.\nSpecial Tags\nQuality tags: masterpiece, best quality, great quality, good quality, normal quality, low quality, worst quality\nRating tags: safe, sensitive, nsfw, explicit\nDate tags: newest, recent, mid, early, old\nRating tags\nGeneral: safe\nSensitive: sensitive\nQuestionable: nsfw\nExplicit: nsfw, explicit\nDataset\nFor better ability on some certain concepts, I use full danbooru dataset instead of filterd one.\nThan use crawled Pixiv dataset (from 3~5 tag with popularity sort) as addon dataset.\nSince Pixiv's search system only allow 5000 page per tag so there is not much meaningful image, and some of them are duplicated with danbooru set(but since I want to reinforce these concept I directly ignore the duplication)\nAs same as kxl eps rev2, I add realbooru and pvc figure images for more flexibility on concept/style.\nTraining\nHardware: Quad RTX 3090s\nDataset\nNum Images: 8,468,798\nResolution: 1024x1024\nMin Bucket Resolution: 256\nMax Bucket Resolution: 4096\nCaption Tag Dropout: 0.2\nCaption Group Dropout: 0.2 (for dropping tag/nl caption entirely)\nTraining\nBatch Size: 4\nGrad Accumulation Step: 32\nEquivalent Batch Size: 512\nTotal Epoch: 1\nTotal Steps: 16548\nTraining Time: 430 hours (wall time)\nMixed Precision: FP16\nOptimizer\nOptimizer: Lion8bit\nLearning Rate: 1e-5 for UNet / TE training disabled\nLR Scheduler: Constant (with warmup)\nWarmup Steps: 100\nWeight Decay: 0.1\nBetas: 0.9, 0.95\nDiffusion\nMin SNR Gamma: 5\nDebiased Estimation Loss: Enabled\nIP Noise Gamma: 0.05\nWhy do you still use SDXL but not any Brand New DiT-Based Models?\nUnless any one give me reasonable compute resources or any team release efficient enough DiT or I will not train any DiT-based anime base model.\nBut if you give me 8xH100 for an year, I can even train lot of DiT from scratch (If you want)\nLicense:\nFair-AI-public-1.0-sd",
    "Delta-Vector/Holland-4B-V1": "Quants\nPrompting\nSupport\nNo longer needed as LCPP has merged support - just update.\nAxolotl config\nCredits\nTraining\nA model made to continue off my previous work on Magnum 4B, A small model made for creative writing / General assistant tasks, finetuned ontop of IntervitensInc/Llama-3.1-Minitron-4B-Width-Base-chatml, this model is made to be more coherent and generally be better then the 4B at both writing and assistant tasks.\nQuants\nGGUF: https://huggingface.co/NewEden/Holland-4B-gguf\nEXL2: https://huggingface.co/NewEden/Holland-4B-exl2\nPrompting\nModel has been Instruct tuned with the ChatML formatting. A typical input would look like this:\n\"\"\"<|im_start|>system\nsystem prompt<|im_end|>\n<|im_start|>user\nHi there!<|im_end|>\n<|im_start|>assistant\nNice to meet you!<|im_end|>\n<|im_start|>user\nCan I ask a question?<|im_end|>\n<|im_start|>assistant\n\"\"\"\nSupport\nNo longer needed as LCPP has merged support - just update.\nTo run inference on this model, you'll need to use Aphrodite, vLLM or EXL 2/tabbyAPI, as llama.cpp hasn't yet merged the required pull request to fix the llama 3.1 rope_freqs issue with custom head dimensions.\nHowever, you can work around this by quantizing the model yourself to create a functional GGUF file. Note that until this PR is merged, the context will be limited to 8 k tokens.\nTo create a working GGUF file, make the following adjustments:\nRemove the \"rope_scaling\": {} entry from config.json\nChange \"max_position_embeddings\" to 8192 in config.json\nThese modifications should allow you to use the model with llama. Cpp, albeit with the mentioned context limitation.\nAxolotl config\nSee axolotl config\nAxolotl version: 0.4.1\nbase_model: IntervitensInc/Llama-3.1-Minitron-4B-Width-Base-chatml\nmodel_type: AutoModelForCausalLM\ntokenizer_type: AutoTokenizer\nload_in_8bit: false\nload_in_4bit: false\nstrict: false\ndatasets:\n- path: NewEden/Gryphe-3.5-16k-Subset\ntype: sharegpt\nconversation: chatml\n- path: Epiculous/Synthstruct-Gens-v1.1-Filtered-n-Cleaned\ntype: sharegpt\nconversation: chatml\n- path: anthracite-org/kalo-opus-instruct-22k-no-refusal\ntype: sharegpt\nconversation: chatml\n- path: PJMixers/lodrick-the-lafted_OpusStories-ShareGPT\ntype: sharegpt\nconversation: chatml\nchat_template: chatml\nval_set_size: 0.01\noutput_dir: ./outputs/out\nadapter:\nlora_r:\nlora_alpha:\nlora_dropout:\nlora_target_linear:\nsequence_len: 16384\n# sequence_len: 32768\nsample_packing: true\neval_sample_packing: false\npad_to_sequence_len: true\nplugins:\n- axolotl.integrations.liger.LigerPlugin\nliger_rope: true\nliger_rms_norm: true\nliger_swiglu: true\nliger_fused_linear_cross_entropy: true\nwandb_project:\nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\ngradient_accumulation_steps: 32\nmicro_batch_size: 1\nnum_epochs: 2\noptimizer: adamw_bnb_8bit\n#optimizer: paged_adamw_8bit\nlr_scheduler: cosine\nlearning_rate: 0.00002\nweight_decay: 0.05\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: auto\nfp16:\ntf32: true\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\nwarmup_ratio: 0.1\nevals_per_epoch: 4\neval_table_size:\neval_max_new_tokens: 128\nsaves_per_epoch: 1\ndebug:\ndeepspeed: /workspace/axolotl/deepspeed_configs/zero2.json\n#deepspeed:\nfsdp:\nfsdp_config:\nspecial_tokens:\npad_token: <|finetune_right_pad_id|>\nCredits\nanthracite-org/kalo-opus-instruct-22k-no-refusal\nNewEden/Gryphe-3.5-16k-Subset\nEpiculous/Synthstruct-Gens-v1.1-Filtered-n-Cleaned\nlodrick-the-lafted/OpusStories\nTraining\nThe training was done for 2 epochs. We used  2 x RTX 6000s GPUs graciously provided by Kubernetes_Bad for the full-parameter fine-tuning of the model.",
    "gpt-omni/mini-omni": "Features\nInstall\nQuick start\nAcknowledgements\nMini-Omni: Language Models Can Hear, Talk While Thinking in Streaming\nü§ó Hugging Face   | üìñ Github\n|     üìë Technical report\nMini-Omni is an open-source multimodel large language model that can hear, talk while thinking. Featuring real-time end-to-end speech input and streaming audio output conversational capabilities.\nFeatures\n‚úÖ Real-time speech-to-speech conversational capabilities. No extra ASR or TTS models required.\n‚úÖ Talking while thinking, with the ability to generate text and audio at the same time.\n‚úÖ Streaming audio outupt capabilities.\n‚úÖ With \"Audio-to-Text\" and \"Audio-to-Audio\" batch inference to further boost the performance.\nNOTE: please refer to the code repository for more details.\nInstall\nCreate a new conda environment and install the required packages:\nconda create -n omni python=3.10\nconda activate omni\ngit clone https://github.com/gpt-omni/mini-omni.git\ncd mini-omni\npip install -r requirements.txt\nQuick start\nInteractive demo\nstart server\nconda activate omni\ncd mini-omni\npython3 server.py --ip '0.0.0.0' --port 60808\nrun streamlit demo\nNOTE: you need to run streamlit locally with PyAudio installed.\npip install PyAudio==0.2.14\nAPI_URL=http://0.0.0.0:60808/chat streamlit run webui/omni_streamlit.py\nrun gradio demo\nAPI_URL=http://0.0.0.0:60808/chat python3 webui/omni_gradio.py\nexample:\nNOTE: need to unmute first. Gradio seems can not play audio stream instantly, so the latency feels a bit longer.\nhttps://github.com/user-attachments/assets/29187680-4c42-47ff-b352-f0ea333496d9\nLocal test\nconda activate omni\ncd mini-omni\n# test run the preset audio samples and questions\npython inference.py\nAcknowledgements\nQwen2 as the LLM backbone.\nlitGPT for training and inference.\nwhisper  for audio encoding.\nsnac  for audio decoding.\nCosyVoice for generating synthetic speech.\nOpenOrca and MOSS for alignment.",
    "OwlMaster/AllFilesRope": "No model card",
    "v000000/L3.1-Sthenorm-8B": "Llama-3.1-Sthenorm-8B\nThanks mradermacher for the quants:\nmerge\nMerge Details\nMerge Method\nModels Merged\nConfiguration\nPrompt Template:\nLlama-3.1-Sthenorm-8B\nRP model, Stheno3.4 with one of the smartest 3.1 models, half abliterated.\nThanks mradermacher for the quants:\nGGUF\nGGUF imatrix\nmerge\nThis is a merge of pre-trained language models created using mergekit.\nMerge Details\nMerge Method\nThis model was merged using the SLERP merge method.\nModels Merged\nThe following models were included in the merge:\nv000000/Llama-3.1-8B-Stheno-v3.4-abliterated\nakjindal53244/Llama-3.1-Storm-8B\nConfiguration\nThe following YAML configuration was used to produce this model:\nslices:\n- sources:\n- model: v000000/Llama-3.1-8B-Stheno-v3.4-abliterated\nlayer_range: [0, 32]\n- model: akjindal53244/Llama-3.1-Storm-8B\nlayer_range: [0, 32]\nmerge_method: slerp\nbase_model: v000000/Llama-3.1-8B-Stheno-v3.4-abliterated\nparameters:\nt:\n- filter: self_attn\nvalue: [0.1, 0.6, 0.3, 0.8, 0.5]\n- filter: mlp\nvalue: [0.9, 0.4, 0.7, 0.2, 0.5]\n- value: 0.5\ndtype: float32\nPrompt Template:\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{output}<|eot_id|>"
}