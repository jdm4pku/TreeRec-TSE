{
    "MBZUAI/LaMini-GPT-124M": "LaMini-GPT-124M\nUse\nIntended use\nTraining Procedure\nTraining Hyperparameters\nEvaluation\nLimitations\nCitation\nLaMini-GPT-124M\nThis model is one of our LaMini-LM model series in paper \"LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions\".\nThis model is a fine-tuned version of gpt2 on LaMini-instruction dataset that contains 2.58M samples for instruction fine-tuning. For more information about our dataset, please refer to our project repository.You can view other models of LaMini-LM series as follows. Models with âœ© are those with the best overall performance given their size/architecture, hence we recommend using them. More details can be seen in our paper.\nBase model\nLaMini-LM series (#parameters)\nT5\nLaMini-T5-61M\nLaMini-T5-223M\nLaMini-T5-738M\nFlan-T5\nLaMini-Flan-T5-77Mâœ©\nLaMini-Flan-T5-248Mâœ©\nLaMini-Flan-T5-783Mâœ©\nCerebras-GPT\nLaMini-Cerebras-111M\nLaMini-Cerebras-256M\nLaMini-Cerebras-590M\nLaMini-Cerebras-1.3B\nGPT-2\nLaMini-GPT-124Mâœ©\nLaMini-GPT-774Mâœ©\nLaMini-GPT-1.5Bâœ©\nGPT-Neo\nLaMini-Neo-125M\nLaMini-Neo-1.3B\nGPT-J\ncoming soon\nLLaMA\ncoming soon\nUse\nIntended use\nWe recommend using the model to respond to human instructions written in natural language.\nSince this decoder-only model is fine-tuned with wrapper text, we suggest using the same wrapper text to achieve the best performance.\nSee the example on the right or the code below.\nWe now show you how to load and use our model using HuggingFace pipeline().\n# pip install -q transformers\nfrom transformers import pipeline\ncheckpoint = \"{model_name}\"\nmodel = pipeline('text-generation', model = checkpoint)\ninstruction = 'Please let me know your thoughts on the given place and why you think it deserves to be visited: \\n\"Barcelona, Spain\"'\ninput_prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\"\ngenerated_text = model(input_prompt, max_length=512, do_sample=True)[0]['generated_text']\nprint(\"Response\", generated_text)\nTraining Procedure\nWe initialize with gpt2 and fine-tune it on our LaMini-instruction dataset. Its total number of parameters is 124M.\nTraining Hyperparameters\nEvaluation\nWe conducted two sets of evaluations: automatic evaluation on downstream NLP tasks and human evaluation on user-oriented instructions. For more detail, please refer to our paper.\nLimitations\nMore information needed\nCitation\n@article{lamini-lm,\nauthor       = {Minghao Wu and\nAbdul Waheed and\nChiyu Zhang and\nMuhammad Abdul-Mageed and\nAlham Fikri Aji\n},\ntitle        = {LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions},\njournal      = {CoRR},\nvolume       = {abs/2304.14402},\nyear         = {2023},\nurl          = {https://arxiv.org/abs/2304.14402},\neprinttype   = {arXiv},\neprint       = {2304.14402}\n}",
    "lansinuote/diffsion_from_scratch.params": "No model card",
    "Leadmatic/tinyChat": "tinyChat: Instruction-Based LLM, <1% the size of GPT-3\nDataset\nBenchmark\nLimitations\nRunning the Code\ntinyChat: Instruction-Based LLM, <1% the size of GPT-3\nIntroducing tinyChat, the instruction-based Large Language Model (LLM) thatâ€™s less than 1% the size of GPT-3.5. tinyChat is an open-source model under the Apache 2.0 license and based on Googleâ€™s Flan-T5-Large, a 770m parameter model. By fine tuning on the databricks-dolly-15k dataset, tinyChat demonstrates improved outputs on a range of tasks compared to Flan-T5. Although not as performant as larger models, tinyChat can perform a variety of NLP tasks such as summarization, question answering, and sentiment analysis using instruction prompts.\ntinyChat is available on the HuggingFace model hub and the code repository is on GitHub.\nDataset\ndatabricks-dolly-15k - databricks-dolly-15k is an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\nBenchmark\nThe following are results from different models tested on the EleutherAI LLM Evaluation Harness. These results indicate that tinyChat is not as performant as the other models. It also shows that tinyChat is only slightly better than Flan-t5-large on openbookqa while performing worse on other datasets. However, tinyChat shows better outputs when provided with creative prompts compared to its base model. See blog post for examples. This shows the limitations of these benchmarks for evaluating generative models.\nmodel\nopenbookqa\narc_easy\nwinogrande\nhellaswag\narc_challenge\npiqa\nboolq\ncerebras/Cerebras-GPT-13B\n0.36\n0.598906\n0.607735\n0.593109\n0.325939\n0.749728\n0.611621\nEleutherAI/gpt-j-6B\n0.382\n0.621633\n0.651144\n0.662617\n0.363481\n0.761153\n0.655963\ndolly-v1-6b (1 epoch)\n0.428\n0.608586\n0.633781\n0.650568\n0.377133\n0.761697\n0.69633\ndolly-v1-6b (10 epochs)\n0.41\n0.62963\n0.643252\n0.676758\n0.384812\n0.773667\n0.687768\nEleutherAI/gpt-neox-20b\n0.402\n0.683923\n0.656669\n0.7142\n0.408703\n0.784004\n0.695413\ngoogle/flan-t5-large\n0.3120\n0.5724\n0.5991\n0.4871\n0.3072\n0.7220\n0.8645\nleadmatic/tinyChat\n0.3320\n0.4811\n0.5825\n0.4519\n0.2961\n0.7073\n0.8358\nLimitations\ntinyChat is prone to hallucination and displays model bias. It is under active development and is currently intended for research purposes only.\nRunning the Code\nimport transformers\nfrom peft import PeftModel\nmodel_name = \"google/flan-t5-large\"\npeft_model_id = \"Leadmatic/tinyChat\"\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\nbase_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model_name)\npeft_model = PeftModel.from_pretrained(base_model, peft_model_id)\ninputs = tokenizer(\"\"\"[INSERT INSTRUCTION HERE]\"\"\", return_tensors=\"pt\")\noutputs = peft_model.generate(**inputs, max_length=300, do_sample=True)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))",
    "MBZUAI/LaMini-Flan-T5-783M": "YAML Metadata\nWarning:\nThe pipeline tag \"text2text-generation\" is not in the official list: text-classification, token-classification, table-question-answering, question-answering, zero-shot-classification, translation, summarization, feature-extraction, text-generation, fill-mask, sentence-similarity, text-to-speech, text-to-audio, automatic-speech-recognition, audio-to-audio, audio-classification, audio-text-to-text, voice-activity-detection, depth-estimation, image-classification, object-detection, image-segmentation, text-to-image, image-to-text, image-to-image, image-to-video, unconditional-image-generation, video-classification, reinforcement-learning, robotics, tabular-classification, tabular-regression, tabular-to-text, table-to-text, multiple-choice, text-ranking, text-retrieval, time-series-forecasting, text-to-video, image-text-to-text, visual-question-answering, document-question-answering, zero-shot-image-classification, graph-ml, mask-generation, zero-shot-object-detection, text-to-3d, image-to-3d, image-feature-extraction, video-text-to-text, keypoint-detection, visual-document-retrieval, any-to-any, video-to-video, other\nLaMini-Flan-T5-783M\nUse\nIntended use\nTraining Procedure\nTraining Hyperparameters\nEvaluation\nLimitations\nCitation\nLaMini-Flan-T5-783M\nThis model is one of our LaMini-LM model series in paper \"LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions\". This model is a fine-tuned version of google/flan-t5-large on LaMini-instruction dataset that contains 2.58M samples for instruction fine-tuning. For more information about our dataset, please refer to our project repository.You can view other models of LaMini-LM series as follows. Models with âœ© are those with the best overall performance given their size/architecture, hence we recommend using them. More details can be seen in our paper.\nBase model\nLaMini-LM series (#parameters)\nT5\nLaMini-T5-61M\nLaMini-T5-223M\nLaMini-T5-738M\nFlan-T5\nLaMini-Flan-T5-77Mâœ©\nLaMini-Flan-T5-248Mâœ©\nLaMini-Flan-T5-783Mâœ©\nCerebras-GPT\nLaMini-Cerebras-111M\nLaMini-Cerebras-256M\nLaMini-Cerebras-590M\nLaMini-Cerebras-1.3B\nGPT-2\nLaMini-GPT-124Mâœ©\nLaMini-GPT-774Mâœ©\nLaMini-GPT-1.5Bâœ©\nGPT-Neo\nLaMini-Neo-125M\nLaMini-Neo-1.3B\nGPT-J\ncoming soon\nLLaMA\ncoming soon\nUse\nIntended use\nWe recommend using the model to response to human instructions written in natural language.\nWe now show you how to load and use our model using HuggingFace pipeline().\n# pip install -q transformers\nfrom transformers import pipeline\ncheckpoint = \"{model_name}\"\nmodel = pipeline('text2text-generation', model = checkpoint)\ninput_prompt = 'Please let me know your thoughts on the given place and why you think it deserves to be visited: \\n\"Barcelona, Spain\"'\ngenerated_text = model(input_prompt, max_length=512, do_sample=True)[0]['generated_text']\nprint(\"Response\", generated_text)\nTraining Procedure\nWe initialize with google/flan-t5-large and fine-tune it on our LaMini-instruction dataset. Its total number of parameters is 783M.\nTraining Hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 0.0005\ntrain_batch_size: 128\neval_batch_size: 64\nseed: 42\ngradient_accumulation_steps: 4\ntotal_train_batch_size: 512\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 5\nEvaluation\nWe conducted two sets of evaluations: automatic evaluation on downstream NLP tasks and human evaluation on user-oriented instructions. For more detail, please refer to our paper.\nLimitations\nMore information needed\nCitation\n@article{lamini-lm,\nauthor       = {Minghao Wu and\nAbdul Waheed and\nChiyu Zhang and\nMuhammad Abdul-Mageed and\nAlham Fikri Aji\n},\ntitle        = {LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions},\njournal      = {CoRR},\nvolume       = {abs/2304.14402},\nyear         = {2023},\nurl          = {https://arxiv.org/abs/2304.14402},\neprinttype   = {arXiv},\neprint       = {2304.14402}\n}",
    "fxmarty/tiny-llama-fast-tokenizer": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nNote: this model has random weights and is useful only for testing purposes.",
    "QuickWick/Music-AI-Voices": "â™« Discord: https://discord.gg/aihub | Join the community, learn to make models, chat with link-minded people and lets create music â™© â™ª\nâ™« Discord Latino: https://discord.gg/Crfqs7uB5V | Entren a nuestra comunidad, aprendan a crear modelos AI, habla con otros sobre musica y disfruta las notas musicales â™© â™ª\nIMPORTANT!!!!!!!!!: VOICES CANNOT BE COPYRIGHTED. We do not promote piracy so please do not come in with that. We do promote legal-length sample clips of vocals. We promote music & AI produced music covers (impressions). We promote machine learning & Voice AI Models. Note: This repository does NOT include ANY DATASETS. Only models are included.\nIf you want your credits/name removed, please message me on discord and I will remove it diligently.\nTools: https://vocalremover.org/ https://x-minus.pro/ai https://create.musicfy.lol/\nCreated Using: SoftVC VITS Singing Voice Conversion (so vits svc 4.0) | Retrieval based Voice Conversion (RVC)\n====================================\nAnnouncements:\nDocumentation is outdated and it is not planned to be updated. The backlog of Models is immense. Last known Read-ME update: 5/12/2023 - I apologize in advance to the model makers of our community but I have too much going on IRL. Go to files to view models rather than using the documentation.\nEdit: Figured out a way to automate 85% of documentation. This is a v2 of my document automation. I will begin to work on v3 in about 3 weeks and this will complete my document automation. Documentation/Credits will not be posted until v3 is completed. All previous models will be appropriately credited at that point.\nDo not rely on ctrl+f for the time being Models are in fact being uploaded. Check the files section in the mean time.\n====================================\nName - Amount of Steps - Creator\n21 Savage - 100k - brandy#4247 |\n21 Savage -\t50k - candy#6483\n2Pac Tupac - 50k - Makaveli AI#4517 |\n2Pac Tupac (RVC) - 150 Epoch - Makaveli AI#4517 |\n2Pac Tupac - 33k - ????\n6lack (RVC) - 700 Epoch - RomeTheDaddy#4293\nAaliyah - 33.6k - COMEHU#2094\nAitana - 75K - blaise#9999\nAlizee - 45.6k - CrimsonZockt#2221 |\nAlizee (2000-2003) - 23.2k - CrimsonZockt#2221\nAmano Pikamee (VOMS Project) - 30k - dacoolkid44#4173\nAmeer Vann - 15k - asher roth#3637\nAmelia Watson (Hololive EN) - 30k - dacoolkid44#4173\nAndrew Tate - 50k - Makaveli AI#4517\nAnt Clemons (RVC - 3150 Steps - SamV1sion#5354\nAnthony Green (Circa Survive) (RVC) - 500 Epochs - owl#1313 |\nAnthony Green (RVC) (Alpha) - 250 Epoch - philo#9160\nAnuel AA - 41.6k - Smile WRLD#9877 |\nAnuel AA (2016 Era) - 500 Steps - Raaul10#2946\nAriana Grande - 73k - ????? - [Trained using pro tools sessions so the vocals sound dry] |\nAriana Grande - 89k -\tchristy#0059 |\nAriana Grande (RVC) - 4k Epoch 28k Steps - MentosAndRice#8492\nAries of Wunderworld - 150k - lij#0001\nASAP Rocky (RVC) - 1k Epoch - Ski#5447\nAyesha Erotica - 100k -\thenry_#7065\nBaby Keem - 191k - okcool#5237\nBad Bunny - 180k - Bowl#2016 |\nBad Bunny - 1k Epoch - CJPP270#0162\nBANANIROU - 100k - ÅŸtar#7068\nBart Simpson - 22k - AnthonyFandom70100#9529 |\nBart Simpson (RVC) - 250 Epoch - AnthonyFandom70100#9529\nBENEE - 8k - rejekts#0820\nBiden - 20k - Nardicality\nBiggie Smalls - 112.8k - justinjohn-03#4897 |\nBiggie Smalls (RVC) - 20k - Makaveli AI#4517\nBillie Eilish - 8k - Vali665#9670 [7 Hours of Training] |\nBillie Eilish 2016-2018 - 1k - Vali665#9670 |\nBillie Eilish (RVC) - ???? - senzo#1502\nBillie Joe - 24k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\nBinyamin Netanyahu (Israel's PM) - 67.7K - yeatfan119#8009\nBktherula - 47k - averycj#3997\nBo Burnham (Inside) (RVC) - 250 Epoch -  analogspiderweb#7099\nBONES - 1k Epoch 110k - ðŸ’Š LÃ¼h Minion ðŸ’‰#1804\nBrandy (RVC) - 200 Epoch - fractalfantasy#2748\nBrendon Urie - Panic! at the Disco - 49k - Budman#5216 & Bowl#2016\nBrian Wilson (Modern Era) (RVC) - 200 Epoch - Jay#0152\nBritney Spears - 100k - AIVERSE#5393 |\nBritney Speaks (Young) - 17k - Frix#2580 |\nBritney Spears (RVC) - 500 Epoch - AIVERSE#5393\nBruno Mars - 124.9k - Thompson#2472 |\nBruno Mars (RVC) - 24k - Thompson#2472\nBruno Powroznik (RVC) - 250 Epochs - analogspiderweb#7099\nBryska - 45.6k - CrimsonZockt#2221\nCamila Cabello (RVC) - 600 Epoch - LMAO DEAD ðŸ˜‚ðŸ˜‚ðŸ˜‚#8206\nCanserbero - 67k - Frix#2580\nCaparezza - 200K - LollenApe#4707\nCazzu - 62k - NuokiFTW#0001\nChano (From Tan BiÃ³nica) - 24k - StarBoy#2512\nCharlie Dompler (Smiling Friends) (RVC) - 300 Epoch - analogspiderweb#7099 [Zach Hadel / psychicpebbles / Charlie Dompler]\nCharlie Puth - 36k - Crewe's Corner#4767\nCharlie Scene (From Hollywood Undead) - 14k - ThatOneDuder710#2594 [Rapping]\nChase Atlantic - 500 Epoch - rejekts#0820\nChester Bennington (Linkin Park) - 79k - Cheech#8254 |\nChester Bennington (RVC) - 1k Epoch 40k Steps - sgsavu#0733\nChief Keef - 100k - candy#6483\nChildish Gambino (RVC) - 1k Epoch - kalomaze#2983\nChris Brown - 105k - Sample.House#0737 [Sounds best using his lower register, when transposed down 1-2 semitones] |\nChris Brown (RVC) - 700 Epoch - RomeTheDaddy#4293\nChris Cornell - 7.4k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\nComethazine - 1086 Epoch 25K - sgsavu#0733 [batch size 7, 161 - 9 second samples] [trained on: open mics, interviews, live freestyles]\nComethazine [Mixed Edition] - 1000 Epoch 64.3k - sgsavu#0733 [trained on everything from PURE edition + least amount of voice processing (556, highriser, etc) + Mixed edition sounds more agressive than PURE but has more artifacts and noise in the resulting audio] |\nComethazine [Pure Edition] - 1000 Epoch 43k - sgsavu#0733 [trained on clean acapellas/vocals from: interviews, open mics, live freestyles]\nC.R.O - 42k - visarra#1117\nCupcakKe - 100k - HuntyDarling#4808\nDaBaby (RVC) - 1k Epoch 70k steps - sgsavu#0733\nDanny Ocean - 34k - matias464#2068\nDave Mustaine (Megadeth) (RVC) - 1000 Epoch - trioskosmos#8731\nDavid Bowie - 7.2k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\nDeku (Izuku Midoriya) (RVC) - 100 Epoch - Anon\nDem Jointz (RVC) - 4.6k - SamV1sion#5354\nDeuce (From Hollywood Undead) (RVC) - 1K Epoch - sgsavu#0733\nDigga D (RVC) - 1000 Epoch 5.6k Steps - arturocookinup#5078\nDillom - 12.8k - Xvalen#3936\nDio Brando (From JoJo's Bizzare Adventure) (RVC) -  10k Steps - nicegame#6990\nDiomedes Diaz (Cacique) (RVC) - 200 Epoch - [El Cacique de la Junta]\nDoja Cat  - 163.2k - á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼#7280\nDon Toliver - 88k - Alei#0950 |\nDon Toliver - 68k - Lightning McQueen#0001 [68k Cleaner/Better than 88k version]\nDrake - 100k - Snoop Dogg#8709 |\nDrake (RVC) - ???? - Snoop Dogg#8709\nDua Lipa - 72k - aimelody#5393\nDuki - 116.8k - Andres0i#4229 [si lo van a probar usen audios sin tune y sin entonaciones, de resto no les va a servir] |\nDuki - 75k - Labrador#6962 |\nDuki - 1k - 0900#9787 |\nDuki (RVC) - 250 Epoch - diegoAsdf#9942\nEd Sheeran (RVC) - 1000 Epoch - AIVERSE#5393\nEddie Vedder - 48.8k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\nEl Puto Coke - 10k - Vigo#2099\nEladio CarriÃ³n - 40k - blaise#9999\nElon Musk - 99K - Stephen5311#6349\nElton John - 14k - Frix#2580\nEminem (General Model v1) - 86k - Bowl#2016\nEminem (SLIM SHADY Edition) - 209k  - ???????? |\nEminem (Slim Shady Era) - 400 Epoch 48k Steps - SpaceCypher#6133 |\nEminem (New Era) (RVC) - 1k Epoch - Bowl#2016 & TRB Harry$#7680\nEnna Alouette (NIJISANJI EN) - 10k - dacoolkid44#4173\nEric Cartman - 10.2k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\nFase Yoda - 50k - Kyume â˜¥ (MÃ©ry)#4518\nFeid - 147k - CAMARA DE GTX#4459\nFerxxo - ???? - KHAKO#8845\nFoda C (French Rapper) - 30k - Kyume â˜¥ (MÃ©ry)#4518\nFrank Ocean - 400k  - Yurboii#8420 [30kEpoch70minDataset] |\nFrank Ocean (RVC) - 18.2k Steps, 210 Epoch - TheLosslessPlug#3202 |\nFrank Ocean (RVC) - 500 Epoch - Hubert Paul Flatt#9804\nFreddie Mercury - 300k - Bowl#2016 & Roberto89#2726 & musictrackcenter#4011 |\nFreddie Mercury - 125k  - jev217#8700 |\nFreddie Mercury (RVC) - Unknown Steps - K7#4523 [Around 1000 epochs, kinda better than sovits model]\nFuture - 45k - candy#6483 |\nFuture (RVC) - 2.7k - arturocookinup#5078\nGawr Gura (Hololive EN) - 30k  - dadcoolkid44#4173 |\nGawr Gura (RVC) - 126 Epoch - RaymondReddington#6845\nGeorge Harrison - ???? - ZGLM#6250 [batch size of 4,927 samples and 101 epochs]\nGeorge Michael (RVC) - 500 Epoch - clubbedsam#4419 [Trained on Crepe]\nGiovanna Grigio (Chiquititas 2013 Era) - 31.2k - CrimsonZockt#2221\nGoku (RVC) - ???? - nicegame#6990\nGunna - 123k - elijah#2251 [Sounds bad with high notes] |\nGunna (RVC) - 3.5k Steps - 1ski#4245\nHaachama (Hololive JP) RVC - 1000 Epoch - dacoolkid44#4173 & mochikiri-chan#0665\nHalf Life 2 (Male 07) (RVC) - 1K Epoch 28K Steps - ðŸ’Š LÃ¼h Minion ðŸ’‰#1804\nHarry Styles - 72k - Melatone#1344 |\nHarry Styles - 56k  - K7#4523\nHayley Williams (From Paramore) - 300k - Thompson#2472 |\nHayley Williams (From Paramore) (RVC) - 600 Epoch - owl#1313\nHef (RVC) - 250 Epoch 1362 Steps - arturocookinup#5078\nHomer Simpson - 22k - AnthonyFandom70100#9529 [voiced by Dan Castellaneta]\nHoshimachi Suisei (Hololive JP) (RVC) - ???? - Shiro-chan#9415\nHozier (RVC) - 270 Epoch - Jatazgo#2719\nHyunjin (From Stray Kids) - ???? - Smile WRLD#9877\nIbai - 11k - blaise#9999\nIce Spice - ???? - ayydot#7545 |\nIce Spice (RVC) - 11k - Zeuz Makes Music#6014\nIndio Solari - 60k - RedamOk#7021\nInugami Korone (Hololive JP) (RVC) Upd 5.2.23 - ???? dacoolkid44#4173 mochikiri-chan#0665\nIrene (From Red Velvet) - 4k - Smile WRLD#9877\nIsaac Kleiner (From Half-Life 2) - 500 Epoch - jakeH#5394\nIU (RVC) - 1k Epoch 99k Steps - baloneyboy#4232 |\nIU (RVC) - 800 Epoch - checkmate#2840\nJ Cole - 100k  - á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼#7280\nJaghit Singh (Indian Ghazal) (RVC) - 400 Epoch 48k Steps - SpaceCypher#6133\nJames Hetfield - 49.6k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\nJay Kay (Jamiroquai lead singer) - 40k - l3af#3435\nJay Z - 54.4k  - justinjohn-03#4987\nJamiroquai - 44k - ????\nJeff Lynne (Electric Light Orchestra) (RVC) - 325 Epoch - Jay#0152\nJennie Kim (From BLACKPINK) (RVC) -  300 Epoch -  ???? |\nJennie Kim (From BLACKPINK) - 65k - hristy#0059\nJeon So-yeon (From (G)I-DLE) - 800 Steps - Smile WRLD#9877\nJhene Aiko - 61.6k - ariscult#6164 |\nJhene Aiko (RVC) - 175 Epoch - baloneyboy#4232\nJihyo (Twice) - 1.6k - Smile WRLD#9877\nJim James (My Morning Jacket) (RVC) - 5k - Jay#0152\nJimin (From BTS) - 24K - neoculture#4390\nJisoo (From BLACKPINK) - 113k - RadmirGrande#0544 |\nJisoo (From BLACKPINK) (RVC) -  250 Epoch - Moonkissed#1774 Arithyst#3931\nJoba of BROCKHAMPTON - 15k - asher roth#3637\nJohn F. Kennedy (JFK) (RVC) - 600 Epoch 53k Steps - Disc#0287\nJohn Frusciante (RVC) - 1k Epoch - sgsavu#0733\nJohn Lennon - 78k - Vlader#7108 |\nJohn Lennon - 365k - Anon [Beatles AI Discord] |\nJohn Lennon (1970 Era) (RVC) - 5k - Jay#0152\nJoji (RVC) - 32k - MentosAndRice#8492\nJotaro Kujo (From JoJo's Bizzare Adventure) (RVC) - 15k Steps - nicegame#6990\nJoy (From Red Velvet) (RVC) - 200 Epoch - bee#0069\nJuice WRLD - 160k  - ryyyy#5003 |\nJuice WRLD (Agressive) - 28k - BigDRá—©CO$O#2129 |\nJuice WRLD - 1k Epoch 15k Steps - sgsavu#0733\nJulia Volkova (From  t.A.T.u.) - 500 Epoch - JpopKARAOKE#6331\nJung Kook (RVC) - 4k Epoch - MentosAndRice#8492 [v3 APR 25 2023] |\nJung Kook - 5k - MentosAndRice#8492 |\nJung Kook (RVC) - 200 Epoch 350 steps - rejekts#0820 [70mb version, 200 Epoch @ 20 Batch Size, 35 clips] |\nJung Kook - 60k - Moonkissed#1774 & Arithyst#3931\nJustin Bieber - 67k  - AguacateDev#4071\nK Suave (RVC) - 700 Epoch - checkmate#2840\nKai - Kim Jong-in (From Exo) - 34.4k Steps - YH#9495\nKanye West - 199.2k  - Pyeon Yeongsun #5759 - Internet Wide Release aka ye200k |\nKanye West (RVC) - ???? - Wil#7050 [ran to 1000 epochs] |\nKanye West - 112k - ???? (Author said 100k and model is called yeversiontwo) |\nKanye West (RVC) - 233.3k Steps, 1000 epoch - Wil#7050\nKaty Perry - 28k - RaulBlue#3655\nKen Carson (Only Interviews) - 52k - BigDRá—©CO$O#2129 |\nKen Carson (Rapping Vocals) -  59k - averycj#3997\nKendrick Lamar - 67.2k  - Snoop Dogg#8709 |\nKendrick Lamar (RVC) - ???? - Snoop Dogg#8709 |\nKendrick Lamar - 100.2k  - okcool#5237 [Might be overtrained]\nKhea - 20.8k - NuokiFTW#0001\nKid Mess (Alpha) - 0.8k  - Cowton#5872 & kesnomanaow#3304\nKidd Keo - 32k - NuokiFTW#0001\nKim Chaewon (From LE SSERAFIM) (Beta) - 500 Epoch - codebloodedgirl6#2315\nKim Garam (From LE SSERAFIM) (RVC) - 300 Epoch - codebloodedgirl6#2315\nKim Seokjin (From BTS) - 24k - neoculture#4390\nKim Taehyung - 24k - neoculture#4390\nKizaru - 45.6k - CrimsonZockt#2221\nKrystal Jung (RVC) - 1008 Epoch - Shabi_Chats#0606 [Works better with high notes]\nKurt Cobain - 138.6k  - á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼#7280\nKurtains (RVC) - 500 Epoch - Autumn#4768\nL-Gante - 12k - StarBoy#2512\nLa+ Darkness (Hololive JP) - 12k - dacoolkid44#4173 | La+ Darkness (Hololive JP) (RVC) - Updated 4.29.2023 - mochikiri-chan#0665 & dacoolkid44#4173\nLady Gaga - 14k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\nLalisa Manoban - ??? - Smile WRLD#9877\nLana Del Rey - 100k - K7#4523 |\nLana Del Rey (RVC) - 1k Epoch 74k Steps - sgsavu#0733\nLauryn Hill - 45k - averycj#3997\nLena Katina ( From t.A.T.u.) (RVC) - 300 Epoch- JpopKARAOKE#6331\nLiam Gallagher - 18.4k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\nLil Baby (RVC) - 500 Epoch - arturocookinup#5078 [Batch Size: 20]\nLil Dicky (RVC) - 1000 Epoch - Carson#1111\nLil Nas X - 26K - riddle#3363\nLil Tracy - ???? - Sztef#7028\nLil Peep - 33k  - Sztef#7028\nLil Uzi Vert - 80k  - ShadowTB#8205 |\nLil Uzi Vert - 1k Epoch 37k Steps - sgsavu#0733 [batch size 6]\nLil Yachty - 10k Epoch 120k - game#0102\nLily (From NMIXX) (RVC) - 250 Epoch - jisoos cat#7462 [Works better with high notes]\nLisa (From BLACKPINK) (RVC) - 900 Epoch - checkmate#2840\nLisa Simpson - 22k - AnthonyFandom70100#9529 |\nLisa Simpson (RVC) - 250 Epoch - AnthonyFandom70100#9529\nLiz (From IVE) - 800 steps - Smile WRLD#9877\nLogic (RVC) - 1k Epoch 116k Steps - sgsavu#0733\nLuis Miguel - 82.4k - jrbeat#4961\nLuther (French Rapper) - 50k - Kyume â˜¥ (MÃ©ry)#4518\nMaeve (From Paladins) - 1600 Epoch - wlrkt#2520\nMaria Becerra - 122k - dariovelaam#3542\nMariah Angeliq - 10k - remix#7551\nMarina Sena - 8.8k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\nMatt Bellamy (From Muse) (RVC) - 200 Epoch 61k Steps - Ryanz#0053\nMCParodyVoice - ???? - TheEpicRock7#9557\nMelanie Martinez - 72K - aimelody#5393 |\nMelanie Martinez (RVC) - 1000 Epoch - AIVERSE#5393\nMaria MendonÃ§a - 10.4k - hugo97#5776\nMariah Carey (RVC) - 300 Epoch - fractalfantasy#2748\nMF Doom - 45k  - Mellon#2653\nMichael Jackson - 83k  - clubbedsam#4419 |\nMichael Jackson (RVC) - 1k Epoch - premydaremy#2498 |\nMichael Jackson - 150k - Nyxel#7778 |\nMichael Jackson (RVC) - 1k Epoch - tea#6949 [Harsh Vocals]\nMikey Sawyer of Miss Fortune - 336k - mikeysawyermf#3327\nMiko - ???? - ????\nMiley Cyrus (RVC) - 750 Epoch - AIVERSE#5393\nMina Myoi (From TWICE) - 2k - â­ ð“šð“¾ð“¶ð“ª â­ Ê•ã£â€¢á´¥â€¢Ê”ã£#0001\nMona Lisa - 10k - COMEHU#2094\nMoonMan - 120k  - ????\nMon Laferte (RVC) - 600 Epoch - AnotherNoName#3807\nMora - 73.6k - NuokiFTW#0001\nMorad - 11k - blaise#9999\nMordecai (RVC) - 3.6k steps, 750 epochs - kalomaze#2983 [39 clips, 6 minutes long dataset]\nMorgenshtern - 15k - lunnaholy#0147\nMori Calliope (Hololive EN) - 8.8k  - dacoolkid44#4173\nMyke Towers - 100k - Labrador#6962\nNas (King's Disease Era) (SVC) - 171k - bola#1593\nNCT Haechan (SVC) - Unknown - à¸—à¸±à¸šà¸šà¸„#2007\nNCT Jaemin (RVC) - Unknown - à¸—à¸±à¸šà¸šà¸„#2007\nNCT Jeno (RVC) - 350 Epoch 11k Steps - à¸—à¸±à¸šà¸šà¸„#2007\nNCT Mark Lee (RVC) - Unknown - à¸—à¸±à¸šà¸šà¸„#2007\nNCT Renjun (RVC) - 250 Epoch 9k Steps - à¸—à¸±à¸šà¸šà¸„#2007\nNeyo - 80k  - subraiz#4688 & NoRappersAllowed#1186\nNicky Jam - 25k  - ????\nNicki Minaj - 64k - LMAO DEAD ðŸ˜‚ðŸ˜‚ðŸ˜‚#8206 |\nNicki Minaj - 27.2k - COMEHU#2094\nNicki Nicole - 120k - StarBoy#2512\nNinomae Ina'nis (Hololive EN) - 30k - dacoolkid44#4173\nNipsey Hussle - 100k  - justinjohn-03#4897\nNLE Choppa (RVC) - 1000 epochs 51k - sgsavu#0733 [trained on around 15 minutes of edited freestyles, open mics, interviews, and least vocal processed songs]\nNotti Osama - 60k - averycj#3997 & fr1ends#0001\nObama - 50k  - Nardicality\nOddcast Daniel (FROM MLG TTS Voice)(RVC) - 300 Epochs - analogspiderweb#7099 [Works best on lower pitch vocals.]\nOki (Oskar KamiÅ„ski) - 49.6k - CrimsonZockt#2221\nOlivia Rodrigo - 12.8k - karol jozef pelin#2129 |\nOlivia Rodrigo - 4k - tahaefe.ipekk#9926\nOmar Rudberg - 100k - reee#2204\nOptiJuegos - 100k - ÅŸtar#7068\nOzuna - 4.8k - ???? |\nOzuna - 4k - matias464#2068\nOzzy Osbourne (Young) (RVC) - 470 Epoch - ancientdeit#3609 [Black Sabbath to Sabotage Era & Blizzard Of Ozz]\noxxxymiron - 24K - Uker#8854\nP!NK (RVC) - 1000 Epoch - AIVERSE#5393\nPaloma Mami - 32k - Benja#4927\nPatrick Star - 500 Epoch - Autumn#4768\nParappa The Rapper (Video Game Character) - 59k - nicegame#6990\nPark Jimin (RVC) Demo - 16k - KaraBaby#3426\nPatrick Warburton (RVC) - 200 Epoch - Samoa Noah#5570 [AKA Kronk from Emperor's new Groove and Joe Swanson]\nPaul McCartney (SVC) - 200k - Albinator#8386 |\nPaul McCartney (Young Era) (RVC) - 1k Epoch - kalomaze#2983 & Albinator#8386 [Trained on harvest pitch inference using the same dataset as the sovits Paul from Albinator]\nPaul McCartney (1964 Era) (RVC) - 5k - Jay#0152\nPaulo Londra - 100k - Milkitos03#5076 |\nPaulo Londra - 10k - ð–ð–‰ð–Žð–Šð–Œð–”ð–™ð–Š#3978\nPekora - ???? - ????\nPeso Pluma - 40k - NRM#5257\nPeter Griffin (RVC) - 4.5k - Delik#0001\nPhil Anselmo - 25k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\nPlankton (From SpongeBob) (RVC) - 500 Epoch - Hubert Paul Flatt#9804\nPlayboi Carti - 45k - Snoop Dogg#8709 [This is probably v2 or SVC edition|\nPlayboi Carti - 42k - Molo#0001 [Whole Lotta Red Era v2] |\nPlayboi Carti (Die Lit Era) - 18k - Zeuz Makes Music#6014 |\nPlayboi Carti v3 (RVC) - ???? - Snoop Dogg#8709 |\nPlayboi Carti - 46k - BigDRá—©CO$O#2129 [New Sessions Used]\nPop Smoke - 36.8k - sable#0001\nPost Malone - 9.6k - Prod. Bad Dude#3218\nPostal Dude (From Postal Game) - 2.5k - HuggingFace link to be added |\nPostal Dude (From POSTAL 2) - 1K Epochs 25K Steps - ðŸ’Š LÃ¼h Minion ðŸ’‰#1804\nQuasimoto - 50k - Bowl#2016\nQuevedo - 28k - ALEXSZYT#0432\nRalph Kaminski - 48.8k - CrimsonZockt#2221 |\nRalph Kaminski(alt) - 25.6k - CrimsonZockt#2221\nRauw Alejandro - 4.8k - GOD_Tofer#6528\nRigby (RVC) 500 Epoch - Hubert Paul Flatt#9804\nRihanna - 200k - Seif#3218 & Provindo#4444 |\nRihanna (alt) - 75k - Seif#3218 & Provindo#4444 |\nRihanna (RVC)  - ???? - Snoop Dogg#8709\nRingo Starr (From Beatles) - Unknown Steps - ZGLM#6250 [Beatles AI Discord]\nRivers Cuomo of Weezer (RVC) - 18k Steps, 140 Epoch - rthawk#1502\nRochy RD - 90k - Styl#6247\nRodrigo BarÃ£o (BarÃµes Da Pisadinha) - 8k - Dimitri#7373 (Brazilian Portuguese)\nRosaliÃ¡ - 35k - Styl#6247 |\nRosalia (RVC) - 1k Epoch 15k Steps - Styl#6247\nRose (From BLACKPINK) (RVC)- ???? - uji#8864\nRossa (Indonesian Singer) (RVC) - 350 Epoch - Hengky Wijaya#3599 [not quite good at high notes, at certain high note it comes lowered to the lower octave.] [350 Epoch, 20 Batch, RVC, trained in filtered voice, podcast, live performance]\nRoxie Wegiel (13+5 Era) - 45.6k - CrimsonZockt#2221\nSaiko - 13k - Smile WRLD#9877|\nSaiko - 26.4k - blaise#9999 & m1n1#7342 |\nSaiko - 55k - blaise#9999\nSamuel L Jackson - 30k - Thompson#2472\nSarah Bonito (Kero Kero Bonito KKB) - 9k - Bwib#8693\nSCARLXRD (RVC) - 300 Epoch - YETI#9058\nSean Leon - 3.15k - SamV1sion#5354\nSelena Gomez (RVC) - 1000 Epoch - AIVERSE#5393\nSematary  - 122k - kala#6494 (trained from Rainbow Bridge 1)\nSeulgi Red Velvet - 3.2k - Smile WRLD#9877\nShakira (Classic Era) - 15k - Frix#2580 |\nShakira (Modern Era) (RVC) - 19.8K - kaan36875#0001\nSia (RVC) - 500 Epoch - owl#1313\nShiloh Dynasty - 3.3k - rejekts#0820\nSidhu Moosewala - 10k - Puneet#6616 |\nSidhu Moose Wala (RVC) - 220 Epoch - Sukh#0648 |\nSidhu Moose Wala - 60k - Frix#2580\nSolar (From MAMAMOO) - 1.6k - ????\nSOOBIN (From TOMORROW X TOGETHER) - 46K - neoculture#4390\nSpongebob Squarepants (RVC) - Unkown Steps - kalomaze#2983 [1k epochs, dataset of 19 clips, trained on pm pitch method]\nStevie Ray Vaughan - 6.2k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\nStevie Wonder - 31k - clubbedsam#4419\nStewie Griffin (RVC) - 4.5k - Delik#0001\nSUGA (From BTS) - 21.6k - neoculture#4390\nSugarhill Ddot (RVC) - 150 Epoch - Notti Osama#1111 & dacoolkid44#4173\nSummer Walker - 11k - ayydot#7545 |\nSummer Walker - 400 Epoch - RomeTheDaddy#4293\nSZA - 21k - ayydot#7545\nSwae Lee - 231k - joman_g#9910\nTaeyeon (RVC) - 72k - baloneyboi#4232 |\nTaeyeon (FROM SNSD) - 800 Steps - Smile WRLD#9877\nTakanashi Kiara (Hololive EN) - 10k - dacoolkid44#4173\nTay-K (RVC) - 300 Epoch - Notti Osama#1111\nTaylor swift - 152k Steps, 7.6k Epoch - JohnnyJones#8867 [7.6k epochs at around 20 steps an epoch so 152k steps] |\nTaylor Swift - 106.4k - ???? [Not the best but it does work good with dry vocals when it comes to hitting a bit higher notes] |\nTaylor Swift (RVC) - 3.3k Epoch 101k Steps- Filthycasual#5666\nTF2 Team Fortress 2 Demoman (RVC) - ???? - nicegame#6990\nTF2 Team Fortress 2 Engineer (RVC) - ???? - nicegame#6990\nTF2 Team Fortress 2 Heavy (RVC) - ???? - nicegame#6990\nTF2 Team Fortress 2 Medic (RVC) - ???? - nicegame#6990\nTF2 Team Fortress 2 Scout (RVC) - ???? - nicegame#6990\nTF2 Team Fortress 2 Spy (RVC) - ???? - nicegame#6990\nThe Kid LAROI - 342k - michaell#1404 |\nThe Kid LAROI - 170k - sable#0001\nThe Stanley Parable [Narrator] - 4k 286 Epoch - sourcelocation#0001 |\nThe Stanley Parable [Narrator] (RVC) - 500 Epoch - jakeH#5394\nThe Weeknd - 94k - Maki Ligon#6713 |\nThe Weeknd v2 - 110k - lonelystar#4813 |\nThe Weeknd - 60K - lonelystar#4813 [Alt Version]\nThom Yorke (RVC) - 75 Epochs - ????\nTiago PZK - 55k - StarBoy#2512\nTim Maia - 319.2k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\nTom Waits (Raspy Voice) (RVC) - 600 Epoch 18K Steps - Disc#0287\nTory Lanez (RVC) - 700 Epoch - Rome#2527\nTravis Scott - 100k - RoddyRogu#3360 |\nTravis Scott - 77k - Snoop Dogg#8709 |\nTravis Scott (RVC) - 6720 Epoch - Snoop Dogg#8709\nTrippie Redd - 56k - ShadowTB#8205 [Includes a clustering model for clustering]\nTroye Sivan - 36k - junjuncuti3#9962\nTrump - 68k - joman_g#9910 |\nTrump (alt) - 18.5k - Nardicality\nTyler The Creator - 60k - Snoop Dogg#8709\nVegeta (From Dragon Ball Z) (RVC) - 4.9k Steps - nicegame#6990 [DBZ]\nVergil (From Devil May Cry) - 1000 Epoch - just paps#6512\nWendy (From Red Velvet) - 800 Steps - Smile WRLD#9877\nWhitney Houston - 33.6K - COMEHU#2094\nwill.i.am (RVC) - 3250 steps - SamV1sion#5354\nWill Stenson - 210k - bruhmoment#7334\nxQc - 25k - kyle#9690\nXXXTentacion - 165k - Chakras#???? |\nXXXTentacion - 55k - Angell#4859 |\nXXXTENTACION (RVC) - 150 Epoch 14k Steps - ShadowTB#8205\nYeat - 60k - Vision#3184  [Go to https://medium.com/@vision3/yeat-2-0-model-status-19f47994385f for updates on ver 2.0!]\nYeonjun (From TXT) - 24K - neoculture#4390\nYoko Ono (RVC) - 4k - Jay#0152\nYoung Leosia - 45.6k - CrimsonZockt#2221\nYoung Thug - 279.2k - Monki#8033 |\nYoung Thug - 153k - á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼#7280\nYSY A - 40k - Raidener#3810",
    "comfyanonymous/GLIGEN_pruned_safetensors": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\ngligen_sd14_textbox_pruned.safetensors contains the fuser and positionnet weights from: https://huggingface.co/gligen/gligen-generation-text-box/tree/main\nThese can be used in ComfyUI: https://comfyanonymous.github.io/ComfyUI_examples/gligen/",
    "emilianJR/chilloutmix_NiPrunedFp32Fix": "ðŸ§¨ Diffusers\nLicense\nDiffuser model for this SD checkpoint:\nhttps://civitai.com/models/6424/chilloutmix\nemilianJR/chilloutmix_NiPrunedFp32Fix is the HuggingFace diffuser that you can use with diffusers.StableDiffusionPipeline().\nExamples\nExamples\nExamples\nðŸ§¨ Diffusers\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the Stable Diffusion.\nfrom diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = \"emilianJR/chilloutmix_NiPrunedFp32Fix\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nprompt = \"YOUR PROMPT\"\nimage = pipe(prompt).images[0]\nimage.save(\"image.png\")\nLicense\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies:\nPlease read the full license here",
    "facebook/sam-vit-base": "Model Card for Segment Anything Model (SAM) - ViT Base (ViT-B) version\nTable of Contents\nTL;DR\nModel Details\nUsage\nPrompted-Mask-Generation\nAutomatic-Mask-Generation\nCitation\nModel Card for Segment Anything Model (SAM) - ViT Base (ViT-B) version\nDetailed architecture of Segment Anything Model (SAM).\nTable of Contents\nTL;DR\nModel Details\nUsage\nCitation\nTL;DR\nLink to original repository\nThe Segment Anything Model (SAM) produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image. It has been trained on a dataset of 11 million images and 1.1 billion masks, and has strong zero-shot performance on a variety of segmentation tasks.\nThe abstract of the paper states:\nWe introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.\nDisclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the original SAM model card.\nModel Details\nThe SAM model is made up of 3 modules:\nThe VisionEncoder: a VIT based image encoder. It computes the image embeddings using attention on patches of the image. Relative Positional Embedding is used.\nThe PromptEncoder: generates embeddings for points and bounding boxes\nThe MaskDecoder: a two-ways transformer which performs cross attention between the image embedding and the point embeddings (->) and between the point embeddings and the image embeddings. The outputs are fed\nThe Neck: predicts the output masks based on the contextualized masks produced by the MaskDecoder.\nUsage\nPrompted-Mask-Generation\nfrom PIL import Image\nimport requests\nfrom transformers import SamModel, SamProcessor\nmodel = SamModel.from_pretrained(\"facebook/sam-vit-base\")\nprocessor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\nimg_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\ninput_points = [[[450, 600]]] # 2D localization of a window\ninputs = processor(raw_image, input_points=input_points, return_tensors=\"pt\").to(\"cuda\")\noutputs = model(**inputs)\nmasks = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu())\nscores = outputs.iou_scores\nAmong other arguments to generate masks, you can pass 2D locations on the approximate position of your object of interest, a bounding box wrapping the object of interest (the format should be x, y coordinate of the top right and bottom left point of the bounding box), a segmentation mask. At this time of writing, passing a text as input is not supported by the official model according to the official repository.\nFor more details, refer to this notebook, which shows a walk throught of how to use the model, with a visual example!\nAutomatic-Mask-Generation\nThe model can be used for generating segmentation masks in a \"zero-shot\" fashion, given an input image. The model is automatically prompt with a grid of 1024 points\nwhich are all fed to the model.\nThe pipeline is made for automatic mask generation. The following snippet demonstrates how easy you can run it (on any device! Simply feed the appropriate points_per_batch argument)\nfrom transformers import pipeline\ngenerator =  pipeline(\"mask-generation\", device = 0, points_per_batch = 256)\nimage_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\noutputs = generator(image_url, points_per_batch = 256)\nNow to display the image:\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\ndef show_mask(mask, ax, random_color=False):\nif random_color:\ncolor = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\nelse:\ncolor = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\nh, w = mask.shape[-2:]\nmask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\nax.imshow(mask_image)\nplt.imshow(np.array(raw_image))\nax = plt.gca()\nfor mask in outputs[\"masks\"]:\nshow_mask(mask, ax=ax, random_color=True)\nplt.axis(\"off\")\nplt.show()\nCitation\nIf you use this model, please use the following BibTeX entry.\n@article{kirillov2023segany,\ntitle={Segment Anything},\nauthor={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\\'a}r, Piotr and Girshick, Ross},\njournal={arXiv:2304.02643},\nyear={2023}\n}",
    "facebook/sam-vit-large": "Model Card for Segment Anything Model (SAM) - ViT Large (ViT-L) version\nTable of Contents\nTL;DR\nModel Details\nUsage\nPrompted-Mask-Generation\nAutomatic-Mask-Generation\nCitation\nModel Card for Segment Anything Model (SAM) - ViT Large (ViT-L) version\nDetailed architecture of Segment Anything Model (SAM).\nTable of Contents\nTL;DR\nModel Details\nUsage\nCitation\nTL;DR\nLink to original repository\nThe Segment Anything Model (SAM) produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image. It has been trained on a dataset of 11 million images and 1.1 billion masks, and has strong zero-shot performance on a variety of segmentation tasks.\nThe abstract of the paper states:\nWe introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.\nDisclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the original SAM model card.\nModel Details\nThe SAM model is made up of 3 modules:\nThe VisionEncoder: a VIT based image encoder. It computes the image embeddings using attention on patches of the image. Relative Positional Embedding is used.\nThe PromptEncoder: generates embeddings for points and bounding boxes\nThe MaskDecoder: a two-ways transformer which performs cross attention between the image embedding and the point embeddings (->) and between the point embeddings and the image embeddings. The outputs are fed\nThe Neck: predicts the output masks based on the contextualized masks produced by the MaskDecoder.\nUsage\nPrompted-Mask-Generation\nfrom PIL import Image\nimport requests\nfrom transformers import SamModel, SamProcessor\nmodel = SamModel.from_pretrained(\"facebook/sam-vit-large\")\nprocessor = SamProcessor.from_pretrained(\"facebook/sam-vit-large\")\nimg_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\ninput_points = [[[450, 600]]] # 2D localization of a window\ninputs = processor(raw_image, input_points=input_points, return_tensors=\"pt\").to(\"cuda\")\noutputs = model(**inputs)\nmasks = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu())\nscores = outputs.iou_scores\nAmong other arguments to generate masks, you can pass 2D locations on the approximate position of your object of interest, a bounding box wrapping the object of interest (the format should be x, y coordinate of the top right and bottom left point of the bounding box), a segmentation mask. At this time of writing, passing a text as input is not supported by the official model according to the official repository.\nFor more details, refer to this notebook, which shows a walk throught of how to use the model, with a visual example!\nAutomatic-Mask-Generation\nThe model can be used for generating segmentation masks in a \"zero-shot\" fashion, given an input image. The model is automatically prompt with a grid of 1024 points\nwhich are all fed to the model.\nThe pipeline is made for automatic mask generation. The following snippet demonstrates how easy you can run it (on any device! Simply feed the appropriate points_per_batch argument)\nfrom transformers import pipeline\ngenerator =  pipeline(\"mask-generation\", device = 0, points_per_batch = 256)\nimage_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\noutputs = generator(image_url, points_per_batch = 256)\nNow to display the image:\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\ndef show_mask(mask, ax, random_color=False):\nif random_color:\ncolor = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\nelse:\ncolor = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\nh, w = mask.shape[-2:]\nmask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\nax.imshow(mask_image)\nplt.imshow(np.array(raw_image))\nax = plt.gca()\nfor mask in outputs[\"masks\"]:\nshow_mask(mask, ax=ax, random_color=True)\nplt.axis(\"off\")\nplt.show()\nCitation\nIf you use this model, please use the following BibTeX entry.\n@article{kirillov2023segany,\ntitle={Segment Anything},\nauthor={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\\'a}r, Piotr and Girshick, Ross},\njournal={arXiv:2304.02643},\nyear={2023}\n}",
    "Vision-CAIR/MiniGPT-4": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nMiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models\nOnline Demo\nExamples\nIntroduction\nGetting Started\nInstallation\nLaunching Demo Locally\nTraining\nAcknowledgement\nLicense\nMiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models\nDeyao Zhu* (On Job Market!), Jun Chen* (On Job Market!), Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. *Equal Contribution\nKing Abdullah University of Science and Technology\nOnline Demo\nClick the image to chat with MiniGPT-4 around your images\nExamples\nMore examples can be found in the project page.\nIntroduction\nMiniGPT-4 aligns a frozen visual encoder from BLIP-2 with a frozen LLM, Vicuna, using just one projection layer.\nWe train MiniGPT-4 with two stages. The first traditional pretraining stage is trained using roughly 5 million aligned image-text pairs in 10 hours using 4 A100s. After the first stage, Vicuna is able to understand the image. But the generation ability of Vicuna is heavilly impacted.\nTo address this issue and improve usability, we propose a novel way to create high-quality image-text pairs by the model itself and ChatGPT together. Based on this, we then create a small (3500 pairs in total) yet high-quality dataset.\nThe second finetuning stage is trained on this dataset in a conversation template to significantly improve its generation reliability and overall usability. To our surprise, this stage is computationally efficient and takes only around 7 minutes with a single A100.\nMiniGPT-4 yields many emerging vision-language capabilities similar to those demonstrated in GPT-4.\nGetting Started\nInstallation\n1. Prepare the code and the environment\nGit clone our repository, creating a python environment and ativate it via the following command\ngit clone https://github.com/Vision-CAIR/MiniGPT-4.git\ncd MiniGPT-4\nconda env create -f environment.yml\nconda activate minigpt4\n2. Prepare the pretrained Vicuna weights\nThe current version of MiniGPT-4 is built on the v0 versoin of Vicuna-13B.\nPlease refer to our instruction here\nto prepare the Vicuna weights.\nThe final weights would be in a single folder with the following structure:\nvicuna_weights\nâ”œâ”€â”€ config.json\nâ”œâ”€â”€ generation_config.json\nâ”œâ”€â”€ pytorch_model.bin.index.json\nâ”œâ”€â”€ pytorch_model-00001-of-00003.bin\n...\nThen, set the path to the vicuna weight in the model config file\nhere at Line 16.\n3. Prepare the pretrained MiniGPT-4 checkpoint\nTo play with our pretrained model, download the pretrained checkpoint\nhere.\nThen, set the path to the pretrained checkpoint in the evaluation config file\nin eval_configs/minigpt4_eval.yaml at Line 11.\nLaunching Demo Locally\nTry out our demo demo.py on your local machine by running\npython demo.py --cfg-path eval_configs/minigpt4_eval.yaml  --gpu-id 0\nHere, we load Vicuna as 8 bit by default to save some GPU memory usage.\nBesides, the default beam search width is 1.\nUnder this setting, the demo cost about 23G GPU memory.\nIf you have a more powerful GPU with larger GPU memory, you can run the model\nin 16 bit by setting low_resource to False in the config file\nminigpt4_eval.yaml and use a larger beam search width.\nTraining\nThe training of MiniGPT-4 contains two alignment stages.\n1. First pretraining stage\nIn the first pretrained stage, the model is trained using image-text pairs from Laion and CC datasets\nto align the vision and language model. To download and prepare the datasets, please check\nour first stage dataset preparation instruction.\nAfter the first stage, the visual features are mapped and can be understood by the language\nmodel.\nTo launch the first stage training, run the following command. In our experiments, we use 4 A100.\nYou can change the save path in the config file\ntrain_configs/minigpt4_stage1_pretrain.yaml\ntorchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/minigpt4_stage1_pretrain.yaml\nA MiniGPT-4 checkpoint with only stage one training can be downloaded\nhere.\nCompared to the model after stage two, this checkpoint generate incomplete and repeated sentences frequently.\n2. Second finetuning stage\nIn the second stage, we use a small high quality image-text pair dataset created by ourselves\nand convert it to a conversation format to further align MiniGPT-4.\nTo download and prepare our second stage dataset, please check our\nsecond stage dataset preparation instruction.\nTo launch the second stage alignment,\nfirst specify the path to the checkpoint file trained in stage 1 in\ntrain_configs/minigpt4_stage1_pretrain.yaml.\nYou can also specify the output path there.\nThen, run the following command. In our experiments, we use 1 A100.\ntorchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/minigpt4_stage2_finetune.yaml\nAfter the second stage alignment, MiniGPT-4 is able to talk about the image coherently and user-friendly.\nAcknowledgement\nBLIP2 The model architecture of MiniGPT-4 follows BLIP-2. Don't forget to check this great open-source work if you don't know it before!\nLavis This repository is built upon Lavis!\nVicuna The fantastic language ability of Vicuna with only 13B parameters is just amazing. And it is open-source!\nIf you're using MiniGPT-4 in your research or applications, please cite using this BibTeX:\n@misc{zhu2022minigpt4,\ntitle={MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models},\nauthor={Deyao Zhu and Jun Chen and Xiaoqian Shen and xiang Li and Mohamed Elhoseiny},\nyear={2023},\n}\nLicense\nThis repository is under BSD 3-Clause License.\nMany codes are based on Lavis with\nBSD 3-Clause License here.",
    "Xenova/t5-small": "Usage (Transformers.js)\nhttps://huggingface.co/t5-small with ONNX weights to be compatible with Transformers.js.\nUsage (Transformers.js)\nIf you haven't already, you can install the Transformers.js JavaScript library from NPM using:\nnpm i @huggingface/transformers\nExample: Text-to-text generation.\nimport { pipeline } from '@huggingface/transformers';\nconst generator = await pipeline('text2text-generation', 'Xenova/t5-small');\nconst output = await generator('how can I become more healthy?', {\nmax_new_tokens: 100,\n});\nNote: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using ðŸ¤— Optimum and structuring your repo like this one (with ONNX weights located in a subfolder named onnx).",
    "sasha0552/pygmalion-6b-f16-ggml": "Pygmalion 6B\nPygmalion 6B\nConverted to GGML format, for koboldcpp or something similar.",
    "wangrongsheng/MiniGPT-4-LLaMA-7B": "è¿™æ˜¯MiniGPT-4çš„è½¬åŒ–æƒé‡ï¼Œåˆ©ç”¨çš„æ•™ç¨‹æ˜¯MiniGPT-4/PrepareVicuna.md ï¼Œä½¿ç”¨å®ƒï¼Œæ‚¨å¯ä»¥ä¸éœ€è¦LLAMA-7Bå’Œvicuna-7b-delta-v0è¿›è¡Œè½¬åŒ–ã€‚\nhttps://github.com/Vision-CAIR/MiniGPT-4",
    "funasr/fsmn-vad-onnx": "Introduce\nInstall funasr_onnx\nDownload the model\nInference with runtime\nVoice Activity Detection\nFSMN-VAD\nCitations\nIntroduce\nVoice activity detection (VAD) plays a important role in speech recognition systems by detecting the beginning and end of effective speech. FunASR provides an efficient VAD model based on the FSMN structure. To improve model discrimination, we use monophones as modeling units, given the relatively rich speech information. During inference, the VAD system requires post-processing for improved robustness, including operations such as threshold settings and sliding windows.\nThis repository demonstrates how to leverage FSMN-VAD in conjunction with the funasr_onnx runtime. The underlying model is derived from FunASR, which was trained on a massive 5,000-hour dataset.\nWe have relesed numerous industrial-grade models, including speech recognition, voice activity detection, punctuation restoration, speaker verification, speaker diarization, and timestamp prediction (force alignment). To learn more about these models, kindly refer to the documentation available on FunASR. If you are interested in leveraging advanced AI technology for your speech-related projects, we invite you to explore the possibilities offered by FunASR.\nInstall funasr_onnx\npip install -U funasr_onnx\n# For the users in China, you could install with the command:\n# pip install -U funasr_onnx -i https://mirror.sjtu.edu.cn/pypi/web/simple\nDownload the model\ngit lfs install\ngit clone https://huggingface.co/funasr/FSMN-VAD\nInference with runtime\nVoice Activity Detection\nFSMN-VAD\nfrom funasr_onnx import Fsmn_vad\nmodel_dir = \"./FSMN-VAD\"\nmodel = Fsmn_vad(model_dir, quantize=True)\nwav_path = \"./FSMN-VAD/asr_example.wav\"\nresult = model(wav_path)\nprint(result)\nmodel_dir: the model path, which contains model.onnx, config.yaml, am.mvn\nbatch_size: 1 (Default), the batch size duration inference\ndevice_id: -1 (Default), infer on CPU. If you want to infer with GPU, set it to gpu_id (Please make sure that you have install the onnxruntime-gpu)\nquantize: False (Default), load the model of model.onnx in model_dir. If set True, load the model of model_quant.onnx in model_dir\nintra_op_num_threads: 4 (Default), sets the number of threads used for intraop parallelism on CPU\nInput: wav formt file, support formats: str, np.ndarray, List[str]\nOutput: List[str]: recognition result\nCitations\n@inproceedings{gao2022paraformer,\ntitle={Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition},\nauthor={Gao, Zhifu and Zhang, Shiliang and McLoughlin, Ian and Yan, Zhijie},\nbooktitle={INTERSPEECH},\nyear={2022}\n}",
    "bigcode/starcoder": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nModel License Agreement\nPlease read the BigCode OpenRAIL-M license agreement before accepting it.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nStarCoder\nTable of Contents\nModel Summary\nUse\nIntended use\nGeneration\nFill-in-the-middle\nAttribution & Other Requirements\nLimitations\nTraining\nModel\nHardware\nSoftware\nLicense\nCitation\nStarCoder\nPlay with the model on the StarCoder Playground.\nTable of Contents\nModel Summary\nUse\nLimitations\nTraining\nLicense\nCitation\nModel Summary\nThe StarCoder models are 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2), with opt-out requests excluded. The model uses Multi Query Attention, a context window of 8192 tokens,  and was trained using the Fill-in-the-Middle objective on 1 trillion tokens.\nRepository: bigcode/Megatron-LM\nProject Website: bigcode-project.org\nPaper: ðŸ’«StarCoder: May the source be with you!\nPoint of Contact: contact@bigcode-project.org\nLanguages: 80+ Programming languages\nUse\nIntended use\nThe model was trained on GitHub code. As such it is not an instruction model and commands like \"Write a function that computes the square root.\" do not work well. However, by using the Tech Assistant prompt you can turn it into a capable technical assistant.\nFeel free to share your generations in the Community tab!\nGeneration\n# pip install -q transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"bigcode/starcoder\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\nFill-in-the-middle\nFill-in-the-middle uses special tokens to identify the prefix/middle/suffix part of the input and output:\ninput_text = \"<fim_prefix>def print_hello_world():\\n    <fim_suffix>\\n    print('Hello world!')<fim_middle>\"\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\nAttribution & Other Requirements\nThe pretraining dataset of the model was filtered for permissive licenses only. Nevertheless, the model can generate source code verbatim from the dataset. The code's license might require attribution and/or other specific requirements that must be respected. We provide a search index that let's you search through the pretraining data to identify where generated code came from and apply the proper attribution to your code.\nLimitations\nThe model has been trained on source code from 80+ programming languages. The predominant natural language in source code is English although other languages are also present. As such the model is capable of generating code snippets provided some context but the generated code is not guaranteed to work as intended. It can be inefficient, contain bugs or exploits. At this time there is no mechanism to detect content previously generated by the model. See the paper for an in-depth discussion of the model limitations.\nTraining\nModel\nArchitecture: GPT-2 model with multi-query attention and Fill-in-the-Middle objective\nPretraining steps: 250k\nPretraining tokens: 1 trillion\nPrecision: bfloat16\nHardware\nGPUs: 512 Tesla A100\nTraining time: 24 days (320,256 GPU hours pretraining + 11,208 GPU hours Python fine-tuning)\nTraining FLOPS: 8.46E+22\nSoftware\nOrchestration: Megatron-LM\nNeural networks: PyTorch\nBP16 if applicable: apex\nLicense\nThe model is licensed under the BigCode OpenRAIL-M v1 license agreement. You can find the full agreement here.\nEmail contact@bigcode-project.org with questions related to the license agreement and for appeals relating to use restrictions.\nCitation\n@article{li2023starcoder,\ntitle={StarCoder: may the source be with you!},\nauthor={Raymond Li and Loubna Ben Allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia Li and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Mishig Davaadorj and Joel Lamy-Poirier and JoÃ£o Monteiro and Oleh Shliazhko and Nicolas Gontier and Nicholas Meade and Armel Zebaze and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Benjamin Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and Jason Stillerman and Siva Sankalp Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and Nour Fahmy and Urvashi Bhattacharyya and Wenhao Yu and Swayam Singh and Sasha Luccioni and Paulo Villegas and Maxim Kunakov and Fedor Zhdanov and Manuel Romero and Tony Lee and Nadav Timor and Jennifer Ding and Claire Schlesinger and Hailey Schoelkopf and Jan Ebert and Tri Dao and Mayank Mishra and Alex Gu and Jennifer Robinson and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos MuÃ±oz Ferrandis and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},\nyear={2023},\neprint={2305.06161},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "Bingsu/adetailer": "YOLOv8 Detection Model\nDatasets\nFace\nHand\nPerson\ndeepfashion2\nInfo\nUsage\nUnsafe files\nYOLOv8 Detection Model\nDatasets\nFace\nAnime Face CreateML\nxml2txt\nAN\nwider face\nHand\nAnHDet\nhand-detection-fuao9\nPerson\ncoco2017 (only person)\nAniSeg\nskytnt/anime-segmentation\ndeepfashion2\ndeepfashion2\nid\nlabel\n0\nshort_sleeved_shirt\n1\nlong_sleeved_shirt\n2\nshort_sleeved_outwear\n3\nlong_sleeved_outwear\n4\nvest\n5\nsling\n6\nshorts\n7\ntrousers\n8\nskirt\n9\nshort_sleeved_dress\n10\nlong_sleeved_dress\n11\nvest_dress\n12\nsling_dress\nInfo\nModel\nTarget\nmAP 50\nmAP 50-95\nface_yolov8n.pt\n2D / realistic face\n0.660\n0.366\nface_yolov8n_v2.pt\n2D / realistic face\n0.669\n0.372\nface_yolov8s.pt\n2D / realistic face\n0.713\n0.404\nface_yolov8m.pt\n2D / realistic face\n0.737\n0.424\nface_yolov9c.pt\n2D / realistic face\n0.748\n0.433\nhand_yolov8n.pt\n2D / realistic hand\n0.767\n0.505\nhand_yolov8s.pt\n2D / realistic hand\n0.794\n0.527\nhand_yolov9c.pt\n2D / realistic hand\n0.810\n0.550\nperson_yolov8n-seg.pt\n2D / realistic person\n0.782 (bbox)0.761 (mask)\n0.555 (bbox)0.460 (mask)\nperson_yolov8s-seg.pt\n2D / realistic person\n0.824 (bbox)0.809 (mask)\n0.605 (bbox)0.508 (mask)\nperson_yolov8m-seg.pt\n2D / realistic person\n0.849 (bbox)0.831 (mask)\n0.636 (bbox)0.533 (mask)\ndeepfashion2_yolov8s-seg.pt\nrealistic clothes\n0.849 (bbox)0.840 (mask)\n0.763 (bbox)0.675 (mask)\nUsage\nfrom huggingface_hub import hf_hub_download\nfrom ultralytics import YOLO\npath = hf_hub_download(\"Bingsu/adetailer\", \"face_yolov8n.pt\")\nmodel = YOLO(path)\nimport cv2\nfrom PIL import Image\nimg = \"https://farm5.staticflickr.com/4139/4887614566_6b57ec4422_z.jpg\"\noutput = model(img)\npred = output[0].plot()\npred = cv2.cvtColor(pred, cv2.COLOR_BGR2RGB)\npred = Image.fromarray(pred)\npred\nUnsafe files\nSince getattr is classified as a dangerous pickle function, any segmentation model that uses it is classified as unsafe.\nAll models were created and saved using the official ultralytics library, so it's okay to use files downloaded from a trusted source.\nSee also: https://huggingface.co/docs/hub/security-pickle",
    "kz-transformers/kaz-roberta-conversational": "Kaz-RoBERTa (base-sized model)\nModel description\nUsage\nTraining data\nTraining procedure\nPreprocessing\nPretraining\nCitation\nBibTeX\nKaz-RoBERTa (base-sized model)\nModel description\nKaz-RoBERTa is a transformers model pretrained on a large corpus of Kazakh data in a self-supervised fashion. More precisely, it was pretrained with the Masked language modeling (MLM) objective.\nUsage\nYou can use this model directly with a pipeline for masked language modeling:\n>>> from transformers import pipeline\n>>> pipe = pipeline('fill-mask', model='kz-transformers/kaz-roberta-conversational')\n>>> pipe(\"ÐœÓ™Ñ‚ÐµÐ» Ñ‚ÑƒÑ€Ð°, Ð°ÑƒÑ‹ÑÐ¿Ð°Ð»Ñ‹, Ð°ÑÑ‚Ð°Ñ€Ð»Ñ‹ <mask> Ò›Ð¾Ð»Ð´Ð°Ð½Ñ‹Ð»Ð°Ð´Ñ‹\")\n#Out:\n# {'score': 0.8131822347640991,\n#   'token': 18749,\n#   'token_str': ' Ð¼Ð°Ò“Ñ‹Ð½Ð°Ð´Ð°',\n#   'sequence': 'ÐœÓ™Ñ‚ÐµÐ» Ñ‚ÑƒÑ€Ð°, Ð°ÑƒÑ‹ÑÐ¿Ð°Ð»Ñ‹, Ð°ÑÑ‚Ð°Ñ€Ð»Ñ‹ Ð¼Ð°Ò“Ñ‹Ð½Ð°Ð´Ð° Ò›Ð¾Ð»Ð´Ð°Ð½Ñ‹Ð»Ð°Ð´Ñ‹'},\n# ...\n# ...]\nTraining data\nThe Kaz-RoBERTa model was pretrained on the reunion of 2 datasets:\nMDBKD Multi-Domain Bilingual Kazakh Dataset is a Kazakh-language dataset containing just over 24 883 808 unique texts from multiple domains.\nConversational data Preprocessed dialogs between Customer Support Team and clients of Beeline KZ (Veon Group)\nTogether these datasets weigh 25GB of text.\nTraining procedure\nPreprocessing\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 52,000. The inputs of\nthe model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked\nwith <s> and the end of one by </s>\nPretraining\nThe model was trained on 2 V100 GPUs for 500K steps with a batch size of 128 and a sequence length of 512.  MLM probability - 15%, num_attention_heads=12,\nnum_hidden_layers=6.\nCitation\nIf you use Kaz-RoBERTa Conversational, please cite:\nCite as:\nBeksultan Sagyndyk, Sanzhar Murzakhmetov, Kirill Yakunin. Kaz-RoBERTa Conversational Technical Report. TechRxiv. October 02, 2025.DOI: 10.36227/techrxiv.175942902.25827042/v1\nBibTeX\n@misc{Sagyndyk2025KazRobertaConversational,\ntitle  = {Kaz-RoBERTa Conversational Technical Report},\nauthor = {Beksultan Sagyndyk and Sanzhar Murzakhmetov and Kirill Yakunin},\nyear   = {2025},\npublisher = {TechRxiv},\ndoi    = {10.36227/techrxiv.175942902.25827042/v1},\nurl    = {https://doi.org/10.36227/techrxiv.175942902.25827042/v1}\n}",
    "Xenova/nllb-200-distilled-600M": "Usage (Transformers.js)\nhttps://huggingface.co/facebook/nllb-200-distilled-600M with ONNX weights to be compatible with Transformers.js.\nUsage (Transformers.js)\nIf you haven't already, you can install the Transformers.js JavaScript library from NPM using:\nnpm i @huggingface/transformers\nYou can then perform multilingual translation like this:\nimport { pipeline } from '@huggingface/transformers';\n// Create a translation pipeline\nconst translator = await pipeline('translation', 'Xenova/nllb-200-distilled-600M');\n// Translate text from Hindi to French\nconst output = await translator('à¤œà¥€à¤µà¤¨ à¤à¤• à¤šà¥‰à¤•à¤²à¥‡à¤Ÿ à¤¬à¥‰à¤•à¥à¤¸ à¤•à¥€ à¤¤à¤°à¤¹ à¤¹à¥ˆà¥¤', {\nsrc_lang: 'hin_Deva', // Hindi\ntgt_lang: 'fra_Latn', // French\n});\nconsole.log(output);\n// [{ translation_text: 'La vie est comme une boÃ®te Ã  chocolat.' }]\nSee here for the full list of languages and their corresponding codes.\nNote: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using ðŸ¤— Optimum and structuring your repo like this one (with ONNX weights located in a subfolder named onnx).",
    "Xenova/all-MiniLM-L6-v2": "Usage (Transformers.js)\nhttps://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 with ONNX weights to be compatible with Transformers.js.\nUsage (Transformers.js)\nIf you haven't already, you can install the Transformers.js JavaScript library from NPM using:\nnpm i @huggingface/transformers\nYou can then use the model to compute embeddings like this:\nimport { pipeline } from '@huggingface/transformers';\n// Create a feature-extraction pipeline\nconst extractor = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');\n// Compute sentence embeddings\nconst sentences = ['This is an example sentence', 'Each sentence is converted'];\nconst output = await extractor(sentences, { pooling: 'mean', normalize: true });\nconsole.log(output);\n// Tensor {\n//   dims: [ 2, 384 ],\n//   type: 'float32',\n//   data: Float32Array(768) [ 0.04592696577310562, 0.07328180968761444, ... ],\n//   size: 768\n// }\nYou can convert this Tensor to a nested JavaScript array using .tolist():\nconsole.log(output.tolist());\n// [\n//   [ 0.04592696577310562, 0.07328180968761444, 0.05400655046105385, ... ],\n//   [ 0.08188057690858841, 0.10760223120450974, -0.013241755776107311, ... ]\n// ]\nNote: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using ðŸ¤— Optimum and structuring your repo like this one (with ONNX weights located in a subfolder named onnx).",
    "seanghay/whisper-small-khmer-v2": "whisper-small-khmer-v2\nModel description\nwhisper-small-khmer-v2\nThis model is a fine-tuned version of openai/whisper-small on the openslr, google/fleurs and km-speech-corpus dataset.\nIt achieves the following results on the evaluation set:\nLoss: 0.26\nWer: 0.6165\nModel description\nThis model is fine-tuned with Google FLEURS, OpenSLR (SLR42) and km-speech-corpus dataset.\nfrom transformers import pipeline\npipe = pipeline(\ntask=\"automatic-speech-recognition\",\nmodel=\"seanghay/whisper-small-khmer-v2\",\n)\nresult = pipe(\"audio.wav\",\ngenerate_kwargs={\n\"language\":\"<|km|>\",\n\"task\":\"transcribe\"},\nbatch_size=16\n)\nprint(result[\"text\"])",
    "ttj/sac-logos-ava1-l14-linearMSE": "model ported from https://github.com/christophschuhmann/improved-aesthetic-predictor",
    "mosaicml/mpt-7b": "MPT-7B\nModel Date\nModel License\nDocumentation\nHow to Use\nModel Description\nTraining Data\nStreaming Datasets\nData Mix\nTraining Configuration\nLimitations and Biases\nMosaicML Platform\nDisclaimer\nCitation\nMPT-7B\nMPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.\nThis model was trained by MosaicML.\nMPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.\nThese architectural changes include performance-optimized layer implementations and the elimination of context length limits by replacing\npositional embeddings with Attention with Linear Biases (ALiBi).\nThanks to these modifications, MPT models can be trained with high throughput efficiency and stable convergence.\nMPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer.\nThis model uses the MosaicML LLM codebase, which can be found in the llm-foundry repository. It was trained by MosaicMLâ€™s NLP team on the MosaicML platform for LLM pretraining, finetuning, and inference.\nHow is this model different?\nMPT-7B is\nLicensed for the possibility of commercial use (unlike LLaMA).\nTrained on a large amount of data (1T tokens like LLaMA vs. 300B for Pythia, 300B for OpenLLaMA, and 800B for StableLM).\nPrepared to handle extremely long inputs thanks to ALiBi (we finetuned MPT-7B-StoryWriter-65k+ on up to 65k inputs and can handle up to 84k vs. 2k-4k for other open source models).\nCapable of fast training and inference (via FlashAttention and FasterTransformer)\nEquipped with highly efficient open-source training code via the llm-foundry repository\nModels finetuned off MPT-7B:\nThe following models are finetuned on MPT-7B:\nMPT-7B-StoryWriter-65k+: a model designed to read and write fictional stories with super long context lengths.\nBuilt by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the books3 dataset.\nAt inference time, thanks to ALiBi, MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens.\nWe demonstrate generations as long as 80k tokens on a single A100-80GB GPU in our blogpost.\nLicense: Apache 2.0\nMPT-7B-Instruct: a model for short-form instruction following.\nBuilt by finetuning MPT-7B on a dataset we also release, derived from the Databricks Dolly-15k and the Anthropic Helpful and Harmless (HH-RLHF) datasets.\nLicense: Apache 2.0\nMPT-7B-Chat: a chatbot-like model for dialogue generation.\nBuilt by finetuning MPT-7B on the ShareGPT-Vicuna, HC3,\nAlpaca, HH-RLHF, and Evol-Instruct datasets.\nLicense: CC-By-NC-SA-4.0\nModel Date\nMay 5, 2023\nModel License\nApache-2.0\nDocumentation\nBlog post: Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs\nCodebase (mosaicml/llm-foundry repo)\nQuestions: Feel free to contact us via the MosaicML Community Slack!\nHow to Use\nThis model is best used with the MosaicML llm-foundry repository for training and finetuning.\nimport transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n'mosaicml/mpt-7b',\ntrust_remote_code=True\n)\nNote: This model requires that trust_remote_code=True be passed to the from_pretrained method.\nThis is because we use a custom MPT model architecture that is not yet part of the Hugging Face transformers package.\nMPT includes options for many training efficiency features such as FlashAttention, ALiBi, QK LayerNorm, and more.\nTo use the optimized triton implementation of FlashAttention, you can load the model on GPU (cuda:0) with attn_impl='triton' and with bfloat16 precision:\nimport torch\nimport transformers\nname = 'mosaicml/mpt-7b'\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config['attn_impl'] = 'triton'\nconfig.init_device = 'cuda:0' # For fast initialization directly on GPU!\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\nname,\nconfig=config,\ntorch_dtype=torch.bfloat16, # Load model weights in bfloat16\ntrust_remote_code=True\n)\nAlthough the model was trained with a sequence length of 2048, ALiBi enables users to increase the maximum sequence length during finetuning and/or inference. For example:\nimport transformers\nname = 'mosaicml/mpt-7b'\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\nname,\nconfig=config,\ntrust_remote_code=True\n)\nThis model was trained with the EleutherAI/gpt-neox-20b tokenizer.\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')\nThe model can then be used, for example, within a text-generation pipeline.Note: when running Torch modules in lower precision, it is best practice to use the torch.autocast context manager.\nfrom transformers import pipeline\npipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda:0')\nwith torch.autocast('cuda', dtype=torch.bfloat16):\nprint(\npipe('Here is a recipe for vegan banana bread:\\n',\nmax_new_tokens=100,\ndo_sample=True,\nuse_cache=True))\nModel Description\nThe architecture is a modification of a standard decoder-only transformer.\nThe model has been modified from a standard transformer in the following ways:\nIt uses FlashAttention\nIt uses ALiBi (Attention with Linear Biases) and does not use positional embeddings\nIt does not use biases\nHyperparameter\nValue\nn_parameters\n6.7B\nn_layers\n32\nn_heads\n32\nd_model\n4096\nvocab size\n50432\nsequence length\n2048\nTraining Data\nStreaming Datasets\nData was formatted using the MosaicML StreamingDataset library to host our data in object storage and efficiently stream it to our compute cluster during training.\nStreamingDataset obviates the need to download the whole dataset before starting training, and allows instant resumption of training from any point in the dataset.\nData Mix\nThe model was trained for 1T tokens (with batch size 1760 and sequence length 2048). It was trained on the following data mix:\nData Source\nNumber of Tokens in Source\nProportion\nEffective Number of Tokens\nEpochs\nmC4 3.1.0 - English\n417.99 B\n0.33\n330 B\n0.14\nC4 - English - SemDedup 80%\n100.42 B\n0.299\n299 B\n2.98\nRedPajama - CommonCrawl\n878.45 B\n0.1\n100 B\n0.11\nThe Stack - Selected Languages\n463.78 B\n0.1\n100 B\n0.22\nRedPajama - Wikipedia - En\n4.87 B\n0.04\n40 B\n8.21\nThe Stack - Markdown\n107.07 B\n0.035\n35 B\n0.33\nS2ORC\n48.85 B\n0.033\n33 B\n0.68\nRedPajama - Books\n26.02 B\n0.03\n30B\n1.15\nRedPajama - arXiv\n28.10 B\n0.019\n19 B\n0.68\nRedPajama - StackExchange\n20.54 B\n0.014\n14 B\n0.68\nSamples for each batch were selected from one of the datasets with the probability specified above.\nThe examples were shuffled within each dataset, and each example was constructed from as many sequences from that dataset as were necessary to fill the 2048 sequence length.\nThe data was tokenized using the EleutherAI/gpt-neox-20b tokenizer. This BPE tokenizer has a number of desirable characteristics,\nmost of which are relevant for tokenizing code:\n(1) It was trained on a diverse mix of data that includes code (The Pile)\n(2) It applies consistent space delimitation, unlike the GPT2 tokenizer which tokenizes inconsistently depending on the presence of prefix spaces\n(3) It contains tokens for repeated space characters, which allows superior compression of text with large amounts of repeated space characters.\nThe model vocabulary size of 50432 was set to be a multiple of 128 (as in MEGATRON-LM), model flop utilization (MFU) increased by up to four percentage points.\nTraining Configuration\nThis model was trained on 440 A100-40GBs for about 9.5 days using the MosaicML Platform.\nThe model was trained with sharded data parallelism using FSDP and used the LION optimizer.\nLimitations and Biases\nThe following language is modified from EleutherAI's GPT-NeoX-20B\nMPT-7B (Base) is not intended for deployment without finetuning.\nIt should not be used for human-facing interactions without further guardrails and user consent.\nMPT-7B can produce factually incorrect output, and should not be relied on to produce factually accurate information.\nMPT-7B was trained on various public datasets.\nWhile great efforts have been taken to clean the pretraining data, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\nMosaicML Platform\nIf you're interested in training and deploying your own MPT or LLMs on the MosaicML Platform, sign up here.\nDisclaimer\nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please cosult an attorney before using this model for commercial purposes.\nCitation\nPlease cite this model using the following format:\n@online{MosaicML2023Introducing,\nauthor    = {MosaicML NLP Team},\ntitle     = {Introducing MPT-7B: A New Standard for Open-Source,\nCommercially Usable LLMs},\nyear      = {2023},\nurl       = {www.mosaicml.com/blog/mpt-7b},\nnote      = {Accessed: 2023-05-05},\nurldate   = {2023-05-05}\n}",
    "Xenova/sam-vit-base": "Usage (Transformers.js)\nDemo\nhttps://huggingface.co/facebook/sam-vit-base with ONNX weights to be compatible with Transformers.js.\nUsage (Transformers.js)\nIf you haven't already, you can install the Transformers.js JavaScript library from NPM using:\nnpm i @huggingface/transformers\nExample: Perform mask generation with Xenova/sam-vit-base.\nimport { SamModel, AutoProcessor, RawImage } from \"@huggingface/transformers\";\n// Load model and processor\nconst model = await SamModel.from_pretrained(\"Xenova/sam-vit-base\");\nconst processor = await AutoProcessor.from_pretrained(\"Xenova/sam-vit-base\");\n// Prepare image and input points\nconst img_url = \"https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/corgi.jpg\";\nconst raw_image = await RawImage.read(img_url);\nconst input_points = [[[340, 250]]];\n// Process inputs and perform mask generation\nconst inputs = await processor(raw_image, { input_points });\nconst outputs = await model(inputs);\n// Post-process masks\nconst masks = await processor.post_process_masks(outputs.pred_masks, inputs.original_sizes, inputs.reshaped_input_sizes);\nconsole.log(masks);\n// [\n//   Tensor {\n//     dims: [ 1, 3, 410, 614 ],\n//     type: 'bool',\n//     data: Uint8Array(755220) [ ... ],\n//     size: 755220\n//   }\n// ]\nconst scores = outputs.iou_scores;\nconsole.log(scores);\n// Tensor {\n//   dims: [ 1, 1, 3 ],\n//   type: 'float32',\n//   data: Float32Array(3) [\n//     0.9466127157211304,\n//     0.9890615344047546,\n//     0.8316894769668579\n//   ],\n//   size: 3\n// }\nYou can then visualize the generated mask with:\nconst image = RawImage.fromTensor(masks[0][0].mul(255));\nimage.save('mask.png');\nNext, select the channel with the highest IoU score, which in this case is the second (green) channel. Intersecting this with the original image gives us an isolated version of the subject:\nDemo\nWe've also got an online demo, which you can try out here.\nNote: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using ðŸ¤— Optimum and structuring your repo like this one (with ONNX weights located in a subfolder named onnx).",
    "timm/vit_base_patch14_dinov2.lvd142m": "Model card for vit_base_patch14_dinov2.lvd142m\nModel Details\nModel Usage\nImage Classification\nImage Embeddings\nModel Comparison\nCitation\nModel card for vit_base_patch14_dinov2.lvd142m\nA Vision Transformer (ViT) image feature model. Pretrained on LVD-142M with self-supervised DINOv2 method.\nModel Details\nModel Type: Image classification / feature backbone\nModel Stats:\nParams (M): 86.6\nGMACs: 151.7\nActivations (M): 397.6\nImage size: 518 x 518\nPapers:\nDINOv2: Learning Robust Visual Features without Supervision: https://arxiv.org/abs/2304.07193\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2\nOriginal: https://github.com/facebookresearch/dinov2\nPretrain Dataset: LVD-142M\nModel Usage\nImage Classification\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model('vit_base_patch14_dinov2.lvd142m', pretrained=True)\nmodel = model.eval()\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\nImage Embeddings\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model(\n'vit_base_patch14_dinov2.lvd142m',\npretrained=True,\nnum_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n# or equivalently (without needing to set num_classes=0)\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 1370, 768) shaped tensor\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\nModel Comparison\nExplore the dataset and runtime metrics of this model in timm model results.\nCitation\n@misc{oquab2023dinov2,\ntitle={DINOv2: Learning Robust Visual Features without Supervision},\nauthor={Oquab, Maxime and Darcet, TimothÃ©e and Moutakanni, Theo and Vo, Huy V. and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Howes, Russell and Huang, Po-Yao and Xu, Hu and Sharma, Vasu and Li, Shang-Wen and Galuba, Wojciech and Rabbat, Mike and Assran, Mido and Ballas, Nicolas and Synnaeve, Gabriel and Misra, Ishan and Jegou, Herve and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},\njournal={arXiv:2304.07193},\nyear={2023}\n}\n@article{dosovitskiy2020vit,\ntitle={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\nauthor={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\njournal={ICLR},\nyear={2021}\n}\n@misc{rw2019timm,\nauthor = {Ross Wightman},\ntitle = {PyTorch Image Models},\nyear = {2019},\npublisher = {GitHub},\njournal = {GitHub repository},\ndoi = {10.5281/zenodo.4414861},\nhowpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}",
    "timm/vit_large_patch14_dinov2.lvd142m": "Model card for vit_large_patch14_dinov2.lvd142m\nModel Details\nModel Usage\nImage Classification\nImage Embeddings\nModel Comparison\nCitation\nModel card for vit_large_patch14_dinov2.lvd142m\nA Vision Transformer (ViT) image feature model. Pretrained on LVD-142M with self-supervised DINOv2 method.\nModel Details\nModel Type: Image classification / feature backbone\nModel Stats:\nParams (M): 304.4\nGMACs: 507.1\nActivations (M): 1058.8\nImage size: 518 x 518\nPapers:\nDINOv2: Learning Robust Visual Features without Supervision: https://arxiv.org/abs/2304.07193\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: https://arxiv.org/abs/2010.11929v2\nOriginal: https://github.com/facebookresearch/dinov2\nPretrain Dataset: LVD-142M\nModel Usage\nImage Classification\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model('vit_large_patch14_dinov2.lvd142m', pretrained=True)\nmodel = model.eval()\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\nImage Embeddings\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model(\n'vit_large_patch14_dinov2.lvd142m',\npretrained=True,\nnum_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n# or equivalently (without needing to set num_classes=0)\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 1370, 1024) shaped tensor\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\nModel Comparison\nExplore the dataset and runtime metrics of this model in timm model results.\nCitation\n@misc{oquab2023dinov2,\ntitle={DINOv2: Learning Robust Visual Features without Supervision},\nauthor={Oquab, Maxime and Darcet, TimothÃ©e and Moutakanni, Theo and Vo, Huy V. and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Howes, Russell and Huang, Po-Yao and Xu, Hu and Sharma, Vasu and Li, Shang-Wen and Galuba, Wojciech and Rabbat, Mike and Assran, Mido and Ballas, Nicolas and Synnaeve, Gabriel and Misra, Ishan and Jegou, Herve and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},\njournal={arXiv:2304.07193},\nyear={2023}\n}\n@article{dosovitskiy2020vit,\ntitle={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\nauthor={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\njournal={ICLR},\nyear={2021}\n}\n@misc{rw2019timm,\nauthor = {Ross Wightman},\ntitle = {PyTorch Image Models},\nyear = {2019},\npublisher = {GitHub},\njournal = {GitHub repository},\ndoi = {10.5281/zenodo.4414861},\nhowpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}",
    "coffeeee/nsfw-story-generator2": "No model card",
    "TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ": "Wizard Vicuna 13B Uncensored - GPTQ\nDescription\nRepositories available\nPrompt template: Vicuna\nProvided files and GPTQ parameters\nHow to download from branches\nHow to easily download and use this model in text-generation-webui.\nHow to use this GPTQ model from Python code\nInstall the necessary packages\nFor CodeLlama models only: you must use Transformers 4.33.0 or later.\nYou can then use the following code\nCompatibility\nDiscord\nThanks, and how to contribute\nOriginal model card: Eric Hartford's Wizard Vicuna 13B Uncensored\nChat & support: TheBloke's Discord server\nWant to contribute? TheBloke's Patreon page\nTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)\nWizard Vicuna 13B Uncensored - GPTQ\nModel creator: Eric Hartford\nOriginal model: Wizard Vicuna 13B Uncensored\nDescription\nThis repo contains GPTQ model files for Eric Hartford's Wizard Vicuna 13B Uncensored.\nMultiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.\nRepositories available\nAWQ model(s) for GPU inference.\nGPTQ models for GPU inference, with multiple quantisation parameter options.\n2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference\nEric Hartford's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions\nPrompt template: Vicuna\nA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {prompt} ASSISTANT:\nProvided files and GPTQ parameters\nMultiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.\nEach separate quant is in a different branch.  See below for instructions on fetching from different branches.\nAll recent GPTQ files are made with AutoGPTQ, and all files in non-main branches are made with AutoGPTQ. Files in the main branch which were uploaded before August 2023 were made with GPTQ-for-LLaMa.\nExplanation of GPTQ parameters\nBits: The bit size of the quantised model.\nGS: GPTQ group size. Higher numbers use less VRAM, but have lower quantisation accuracy. \"None\" is the lowest possible value.\nAct Order: True or False. Also known as desc_act. True results in better quantisation accuracy. Some GPTQ clients have had issues with models that use Act Order plus Group Size, but this is generally resolved now.\nDamp %: A GPTQ parameter that affects how samples are processed for quantisation. 0.01 is default, but 0.1 results in slightly better accuracy.\nGPTQ dataset: The dataset used for quantisation. Using a dataset more appropriate to the model's training can improve quantisation accuracy. Note that the GPTQ dataset is not the same as the dataset used to train the model - please refer to the original model repo for details of the training dataset(s).\nSequence Length: The length of the dataset sequences used for quantisation. Ideally this is the same as the model sequence length. For some very long sequence models (16+K), a lower sequence length may have to be used.  Note that a lower sequence length does not limit the sequence length of the quantised model. It only impacts the quantisation accuracy on longer inference sequences.\nExLlama Compatibility: Whether this file can be loaded with ExLlama, which currently only supports Llama models in 4-bit.\nBranch\nBits\nGS\nAct Order\nDamp %\nGPTQ Dataset\nSeq Len\nSize\nExLlama\nDesc\nlatest\n4\n128\nYes\n0.01\nwikitext\n2048\n8.11 GB\nYes\n4-bit, with Act Order and group size 128g. Uses even less VRAM than 64g, but with slightly lower accuracy.\nmodel_v1\n4\n128\nNo\n0.01\nwikitext\n2048\n8.11 GB\nYes\n4-bit, without Act Order and group size 128g.\nmain\n4\n128\nNo\n0.01\nwikitext\n2048\n8.11 GB\nYes\n4-bit, without Act Order and group size 128g.\nHow to download from branches\nIn text-generation-webui, you can add :branch to the end of the download name, eg TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ:latest\nWith Git, you can clone a branch with:\ngit clone --single-branch --branch latest https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\nIn Python Transformers code, the branch is the revision parameter; see below.\nHow to easily download and use this model in text-generation-webui.\nPlease make sure you're using the latest version of text-generation-webui.\nIt is strongly recommended to use the text-generation-webui one-click-installers unless you're sure you know how to make a manual install.\nClick the Model tab.\nUnder Download custom model or LoRA, enter TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ.\nTo download from a specific branch, enter for example TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ:latest\nsee Provided Files above for the list of branches for each option.\nClick Download.\nThe model will start downloading. Once it's finished it will say \"Done\".\nIn the top left, click the refresh icon next to Model.\nIn the Model dropdown, choose the model you just downloaded: Wizard-Vicuna-13B-Uncensored-GPTQ\nThe model will automatically load, and is now ready for use!\nIf you want any custom settings, set them and then click Save settings for this model followed by Reload the Model in the top right.\nNote that you do not need to and should not set manual GPTQ parameters any more. These are set automatically from the file quantize_config.json.\nOnce you're ready, click the Text Generation tab and enter a prompt to get started!\nHow to use this GPTQ model from Python code\nInstall the necessary packages\nRequires: Transformers 4.32.0 or later, Optimum 1.12.0 or later, and AutoGPTQ 0.4.2 or later.\npip3 install transformers>=4.32.0 optimum>=1.12.0\npip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7\nIf you have problems installing AutoGPTQ using the pre-built wheels, install it from source instead:\npip3 uninstall -y auto-gptq\ngit clone https://github.com/PanQiWei/AutoGPTQ\ncd AutoGPTQ\npip3 install .\nFor CodeLlama models only: you must use Transformers 4.33.0 or later.\nIf 4.33.0 is not yet released when you read this, you will need to install Transformers from source:\npip3 uninstall -y transformers\npip3 install git+https://github.com/huggingface/transformers.git\nYou can then use the following code\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nmodel_name_or_path = \"TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\"\n# To use a different branch, change revision\n# For example: revision=\"latest\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\ndevice_map=\"auto\",\ntrust_remote_code=False,\nrevision=\"main\")\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\nprompt = \"Tell me about AI\"\nprompt_template=f'''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {prompt} ASSISTANT:\n'''\nprint(\"\\n\\n*** Generate:\")\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n# Inference can also be done using transformers' pipeline\nprint(\"*** Pipeline:\")\npipe = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\nmax_new_tokens=512,\ndo_sample=True,\ntemperature=0.7,\ntop_p=0.95,\ntop_k=40,\nrepetition_penalty=1.1\n)\nprint(pipe(prompt_template)[0]['generated_text'])\nCompatibility\nThe files provided are tested to work with AutoGPTQ, both via Transformers and using AutoGPTQ directly. They should also work with Occ4m's GPTQ-for-LLaMa fork.\nExLlama is compatible with Llama models in 4-bit. Please see the Provided Files table above for per-file compatibility.\nHuggingface Text Generation Inference (TGI) is compatible with all GPTQ models.\nDiscord\nFor further support, and discussions on these models and AI in general, join us at:\nTheBloke AI's Discord server\nThanks, and how to contribute\nThanks to the chirper.ai team!\nThanks to Clay from gpus.llm-utils.org!\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\nPatreon: https://patreon.com/TheBlokeAI\nKo-Fi: https://ko-fi.com/TheBlokeAI\nSpecial thanks to: Aemon Algiz.\nPatreon special mentions: Alicia Loh, Stephen Murray, K, Ajan Kanaga, RoA, Magnesian, Deo Leter, Olakabola, Eugene Pentland, zynix, Deep Realms, Raymond Fosdick, Elijah Stavena, Iucharbius, Erik BjÃ¤reholt, Luis Javier Navarrete Lozano, Nicholas, theTransient, John Detwiler, alfie_i, knownsqashed, Mano Prime, Willem Michiel, Enrico Ros, LangChain4j, OG, Michael Dempsey, Pierre Kircher, Pedro Madruga, James Bentley, Thomas Belote, Luke @flexchar, Leonard Tan, Johann-Peter Hartmann, Illia Dulskyi, Fen Risland, Chadd, S_X, Jeff Scroggin, Ken Nordquist, Sean Connelly, Artur Olbinski, Swaroop Kallakuri, Jack West, Ai Maven, David Ziegler, Russ Johnson, transmissions 11, John Villwock, Alps Aficionado, Clay Pascal, Viktor Bowallius, Subspace Studios, Rainer Wilmers, Trenton Dambrowitz, vamX, Michael Levine, ì¤€êµ ê¹€, Brandon Frisco, Kalila, Trailburnt, Randy H, Talal Aujan, Nathan Dryer, Vadim, é˜¿æ˜Ž, ReadyPlayerEmma, Tiffany J. Kim, George Stoitzev, Spencer Kim, Jerry Meng, Gabriel Tamborski, Cory Kujawski, Jeffrey Morgan, Spiking Neurons AB, Edmond Seymore, Alexandros Triantafyllidis, Lone Striker, Cap'n Zoog, Nikolai Manek, danny, ya boyyy, Derek Yates, usrbinkat, Mandus, TL, Nathan LeClaire, subjectnull, Imad Khwaja, webtim, Raven Klaugh, Asp the Wyvern, Gabriel Puliatti, Caitlyn Gatomon, Joseph William Delisle, Jonathan Leane, Luke Pendergrass, SuperWojo, Sebastain Graf, Will Dee, Fred von Graf, Andrey, Dan Guido, Daniel P. Andersen, Nitin Borwankar, Elle, Vitor Caleffi, biorpg, jjj, NimbleBox.ai, Pieter, Matthew Berman, terasurfer, Michael Davis, Alex, Stanislav Ovsiannikov\nThank you to all my generous patrons and donaters!\nAnd thank you again to a16z for their generous grant.\nOriginal model card: Eric Hartford's Wizard Vicuna 13B Uncensored\nThis is wizard-vicuna-13b trained with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.\nShout out to the open source AI/ML community, and everyone who helped me out.\nNote:\nAn uncensored model has no guardrails.\nYou are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.\nPublishing anything this model generates is the same as publishing it yourself.\nYou are responsible for the content you publish, and you cannot blame the model any more than you can blame the knife, gun, lighter, or car for what you do with it.",
    "qm9/ggml-gpt4all-j-v1.3-groovy.bin": "No model card",
    "gisohi6975/nsfw-waifu-diffusion": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nwaifu-diffusion v1.4 - Diffusion for Weebs\nGradio & Colab\nModel Description\nLicense\nDownstream Uses\nExample Code\nTeam Members and Acknowledgements\nwaifu-diffusion v1.4 - Diffusion for Weebs\nwaifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\nmasterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck\nOriginal Weights\nGradio & Colab\nWe also support a Gradio Web UI and Colab with Diffusers to run Waifu Diffusion:\nModel Description\nSee here for a full model overview.\nLicense\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies:\nYou can't use the model to deliberately produce nor share illegal or harmful outputs or content\nThe authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\nYou may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\nPlease read the full license here\nDownstream Uses\nThis model can be used for entertainment purposes and as a generative art assistant.\nExample Code\nimport torch\nfrom torch import autocast\nfrom diffusers import StableDiffusionPipeline\npipe = StableDiffusionPipeline.from_pretrained(\n'hakurei/waifu-diffusion',\ntorch_dtype=torch.float32\n).to('cuda')\nprompt = \"1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\"\nwith autocast(\"cuda\"):\nimage = pipe(prompt, guidance_scale=6)[\"sample\"][0]\nimage.save(\"test.png\")\nTeam Members and Acknowledgements\nThis project would not have been possible without the incredible work by Stability AI and Novel AI.\nHaru\nSalt\nSta @ Bit192\nIn order to reach us, you can join our Discord server."
}