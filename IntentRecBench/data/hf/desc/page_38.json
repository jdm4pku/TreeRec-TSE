{
    "Goedel-LM/Goedel-Prover-V2-32B": "1. Introduction\n2. Benchmark Performance\n3. Compelling Scaling Performance\n4. Model & Dataset Downloads\n5. Quick Start\nCite\nGoedel-Prover-V2: The Strongest Open-Source Theorem Prover to Date\n1. Introduction\nWe introduce Goedel-Prover-V2, an open-source language model series that sets a new state-of-the-art in automated formal proof generation. Built on the standard expert iteration and reinforcement learning pipeline, our approach incorporates three key innovations: (1) Scaffolded data synthesis: We generate synthetic proof tasks of increasing difficulty to progressively train the model, enabling it to master increasingly complex theorems; (2) Verifier-guided self-correction: The model learns to iteratively revise its own proofs by leveraging feedback from Lean‚Äôs compiler, closely mimicking how humans refine their work; (3) Model averaging: We combine multiple model checkpoints to improve robustness and overall performance.\nOur small model, Goedel-Prover-V2-8B, reaches 84.6% on MiniF2F test set at Pass@32, matching the performance of prior state-of-the-art DeepSeek-Prover-V2-671B while being nearly 100 times smaller in model size.  Our flagship model, Goedel-Prover-V2-32B, achieves 88.0% on MiniF2F at Pass@32 on standard mode and 90.4% on self-correction mode, outperforming prior SOTA DeepSeek-Prover-V2-671B and concurrent work Kimina-Prover-72B by a large margin. Additionaly, our flagship model with self-correction solves 64 problems on PutnamBench at Pass@64, securing the 1st on the leaderboard surpassing DeepSeek-Prover-V2-671B's record of solving 47 problems by Pass@1024.\n2. Benchmark Performance\nSelf-correction mode: Our model improves proof quality by first generating an initial candidate and then using Lean compiler feedback to iteratively revise it. We perform two rounds of self-correction, which remain computationally efficient‚Äîthe total output length (including the initial proof and two revisions) increases only modestly from the standard 32K to 40K tokens.\nFigure 1: Pass@32 performance on MiniF2F, PutnamBench, and our new MathOlympiadBench containing 360 IMO-level problems.\nThe charts above demonstrate the state-of-the-art performance of Goedel-Prover-V2. We report all numbers at Pass@32: (1) Across all three datasets, our flagship 32B model, in both standard and self-correction mode, significantly outperforms prior state-of-the-art DeepSeek-Prover-V2-671B and Kimina-Prover-72B; (2) on miniF2F, our 8B model matches the performance of DeepSeek-Prover-V2-671B while being 100 times smaller in model size.\n#\nModel\nnum‚Äësolved\ncompute\n1Goedel-Prover-V2-32B (self-correction mode)86Pass@192\n1Goedel-Prover-V2-32B (self-correction mode)57Pass@32\n1Goedel-Prover-V2-32B43Pass@32\n2DeepSeek‚ÄëProver‚ÄëV2-671B47Pass@1024\n2DeepSeek‚ÄëProver‚ÄëV2-671B22Pass@32\n3DSP+23Pass@128\n4Kimina‚ÄëProver‚Äë7B‚ÄëDistill10Pass@192\n5Self-play Theorem Prover8Pass@3200\n6Goedel-Prover-V17Pass@512\nTable 1: PutnamBench leaderboard. Goedel-Prover-V2-32B secures the top rank with significantly less compute (pass number) than the previous state-of-the-art.\n3. Compelling Scaling Performance\nFigure 2: Performance on MiniF2F test set under different sample budgets.\nThe scaling curves above show that our 32B model consistently outperforms all prior state-of-the-art models across the entire range of inference-time compute budgets.\n4. Model & Dataset Downloads\nWe release our Goedel-Prover-V2 models and the new MathOlympiadBench benchmark to foster future research.\nModel\nDownload\nGoedel-Prover-V2-32B\nü§óDownload\nGoedel-Prover-V2-8B\nü§óDownload\nDataset\nDownload\nMathOlympiadBench\nü§óDownload\nMathOlympiadBench (Math Olympiad Bench) comprises human-verified formalizations of Olympiad-level mathematical competition problems, sourced from Compfiles and IMOSLLean4 repository. MathOlympiadBench contains 360 problems, including 158 IMO problems from 1959 to 2024, 131 IMO shortlist problems covering 2006 to 2023, 68 regional mathematical Olympiad problems, and 3 additional mathematical puzzles.\nThis model is being released to aid other open-source projects, including those geared towards the upcoming IMO competition. A full paper with all details will be released in the coming weeks.\n5. Quick Start\nYou can directly use Huggingface's Transformers for model inference.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntorch.manual_seed(30)\nmodel_id = \"Goedel-LM/Goedel-Prover-V2-32B\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\nformal_statement = \"\"\"\nimport Mathlib\nimport Aesop\nset_option maxHeartbeats 0\nopen BigOperators Real Nat Topology Rat\ntheorem square_equation_solution {x y : ‚Ñù} (h : x^2 + y^2 = 2*x - 4*y - 5) : x + y = -1 := by\nsorry\n\"\"\".strip()\nprompt = \"\"\"\nComplete the following Lean 4 code:\n\nBefore producing the Lean 4 code to formally prove the given theorem, provide a detailed proof plan outlining the main proof steps and strategies.\nThe plan should highlight key ideas, intermediate lemmas, and proof structures that will guide the construction of the final formal proof.\n\"\"\".strip()\nchat = [\n{\"role\": \"user\", \"content\": prompt.format(formal_statement)},\n]\ninputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\nimport time\nstart = time.time()\noutputs = model.generate(inputs, max_new_tokens=32768)\nprint(tokenizer.batch_decode(outputs))\nprint(time.time() - start)\nCite\n@article{lin2025goedelproverv2,\ntitle={Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction},\nauthor={Lin, Yong and Tang, Shange and Lyu, Bohan and Yang, Ziran and Chung, Jui-Hui and Zhao, Haoyu and Jiang, Lai and Geng, Yihan and Ge, Jiawei and Sun, Jingruo and others},\njournal={arXiv preprint arXiv:2508.03613},\nyear={2025}\n}",
    "lightx2v/Wan2.1-I2V-14B-480P-StepDistill-CfgDistill-Lightx2v": "Wan2.1-I2V-14B-480P-StepDistill-CfgDistill-Lightx2v\nOverview\nTraining\nInference\nLicense Agreement\nAcknowledgements\nWan2.1-I2V-14B-480P-StepDistill-CfgDistill-Lightx2v\nOverview\nWan2.1-I2V-14B-480P-StepDistill-CfgDistill-Lightx2v is an advanced image-to-video generation model built upon the Wan2.1-I2V-14B-480P foundation. This approach allows the model to generate videos with significantly fewer inference steps (4 steps) and without classifier-free guidance, substantially reducing video generation time while maintaining high quality outputs.\nIn this version, we added the following features:\nTrained with higher quality datasets for extended iterations.\nNew fp8 and int8 quantized distillation models have been added, which enable fast inference using lightx2v on RTX 4060.\nTraining\nOur training code is modified based on the Self-Forcing repository. We extended support for the Wan2.1-14B-I2V-480P model and performed a 4-step bidirectional distillation process. The modified code is available at Self-Forcing-Plus.\nInference\nOur inference framework utilizes lightx2v, a highly efficient inference engine that supports multiple models. This framework significantly accelerates the video generation process while maintaining high quality output.\nbash scripts/wan/run_wan_i2v_distill_4step_cfg.sh\nor using the lora version:\nbash scripts/wan/run_wan_i2v_distill_4step_cfg_lora.sh\nWe recommend using the LCM scheduler with the following settings:\nshift=5.0\nguidance_scale=1.0 (i.e., without CFG)\nLicense Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generate contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the license.\nAcknowledgements\nWe would like to thank the contributors to the Wan2.1, Self-Forcing repositories, for their open research.",
    "Anzhc/MS-LC-EQ-D-VR_VAE": "MS-LC-EQ-D-VR VAE: another reproduction of EQ-VAE on variable VAEs and then some\nIntroduction\nVisual Examples\nUsage\nTraining Setup\nEvaluation Results\nResults on small benchmark of 500 photos\nResults on a small benchmark of 434 Illustrations from Boorus\nReferences\nCite\nAcknowledgement\nMS-LC-EQ-D-VR VAE: another reproduction of EQ-VAE on variable VAEs and then some\nCurrent VAEs present:\nSDXL VAE\nFLUX VAE\nEQ-VAE paper: https://arxiv.org/abs/2502.09509\nVIVAT paper: https://arxiv.org/pdf/2506.07863v1\nThanks to Kohaku and his reproduction that made me look into this: https://huggingface.co/KBlueLeaf/EQ-SDXL-VAE\nBase model adapted to EQ VAE: https://huggingface.co/Anzhc/Noobai11-EQ\nLatent to PCA\nIMPORTANT: This VAE requires reflection padding on conv layers. It should be added both in your trainer, and your webui.You can do it with this function on VAE model:\nfor module in self.model.modules():\nif isinstance(module, nn.Conv2d):\npad_h, pad_w = module.padding if isinstance(module.padding, tuple) else (module.padding, module.padding)\nif pad_h > 0 or pad_w > 0:\nmodule.padding_mode = \"reflect\"\nIf you have trained without this - don't worry, just add this modification and do a small tune to fix up artefacts on edges.\nComfyUI/SwarmUI padding for VAEs - https://github.com/Jelosus2/comfyui-vae-reflection\nTrainer fork with optional padding (loras only) - https://github.com/Jelosus2/LoRA_Easy_Training_Scripts\n(left - padded, right - not)\nIntroduction\nRefer to https://huggingface.co/KBlueLeaf/EQ-SDXL-VAE for introduction to EQ-VAE.\nThis implementation additionally utilizes some of fixes proposed in VIVAT paper, and custom in-house regularization techniques, as well as training implementation.\nFor additional examples and more information refer to: https://arcenciel.io/articles/20 and https://arcenciel.io/models/10994\nVisual Examples\nUsage\nThis is a finetuned SDXL VAE, adapted with new regularization, and other techniques. You can use this with your existing SDXL model, but image will be quite artefacting, particularly - oversharpening and ringing.\nThis VAE is supposed to be used for finetune, after that images will become normal. But be aware, compatibility with old VAEs, that are not EQ, will be lost(They will become blurry).\nTraining Setup\nBase SDXL:\nBase Model: SDXL-VAE\nResolution: 256\nDataset: ~12.8k Illustrations from Boorus\nBatch Size: 128 (bs 8, grad acc 16)\nSamples Seen: ~75k\nLoss Weights:\nL1: 0.3\nL2: 0.5\nSSIM: 0.5\nLPIPS: 0.5\nKL: 0.000001\nConsistency Loss: 0.75\nBoth Encoder and Decoder were trained.\nTraining Time: ~8-10 hours on 4060Ti\nB2:\nBase Model: First version\nResolution: 256\nDataset: 87.8k Illustrations from Boorus\nBatch Size: 128 (bs 8, grad acc 16)\nSamples Seen: ~150k\nLoss Weights:\nL1: 0.2\nL2: 0.4\nSSIM: 0.6\nLPIPS: 0.8\nKL: 0.000001\nConsistency Loss: 0.75\nBoth Encoder and Decoder were trained.\nTraining Time: ~16 hours on 4060Ti\nB3:\nBase Model: B2\nResolution: 256\nDataset: 162.8k Illustrations from Boorus\nBatch Size: 128 (bs 8, grad acc 16)\nSamples Seen: ~225k\nLoss Weights:\nL1: 0.2\nL2: 0.4\nSSIM: 0.6\nLPIPS: 0.8\nKL: 0.000001\nConsistency Loss: 0.75\nBoth Encoder and Decoder were trained.\nTraining Time: ~24 hours on 4060Ti\nB4:\nBase Model: B3\nResolution: 320\nDataset: ~237k Illustrations from Boorus\nBatch Size: 72 (bs 6, grad acc 12)\nSamples Seen: ~300k\nLoss Weights:\nL1: 0.5\nL2: 0.9\nSSIM: 0.6\nLPIPS: 0.7\nKL: 0.000001\nConsistency Loss: 0.75\nwavelet: 0.3\nBoth Encoder and Decoder were trained.\nTotal Training Time: ~33 hours on 4060Ti\nB4:\nBase Model: B4\nResolution: 384\nDataset: ~312k Illustrations from Boorus\nBatch Size: 48 (bs 4, grad acc 12)\nSamples Seen: ~375k\nLoss Weights:\nL1: 0.5\nL2: 0.9\nSSIM: 0.6\nLPIPS: 0.7\nKL: 0.000001\nConsistency Loss: 0.75\nwavelet: 0.3\nBoth Encoder and Decoder were trained.\nTotal Training Time: ~48 hours on 4060Ti\nB2 is a direct continuation of base version, stats displayed are cumulative across multiple runs.\nI took batch of 75k images, so samples seen never repeated.\nB3 repeats B2 for another batch of data and further solidifies cleaner latents. Minor tweaks were done to training code for better regularization.\nB4 changes mixture a bit, to concentrate more on reconstruction quality. Additionally, resolution was increased to 320. Wavelet loss was added at low values(but it's effect is yet to be studied).\nB5 same as B4, but higher resolution again.\nBase FLUX:\nBase Model: FLUX-VAE\nDataset: ~12.8k Illustrations from Boorus\nBatch Size: 128 (bs 8, grad acc 16)\nSamples Seen: ~62.5k\nLoss Weights:\nL1: 0.3\nL2: 0.4\nSSIM: 0.6\nLPIPS: 0.6\nKL: 0.000001\nConsistency Loss: 0.75\nBoth Encoder and Decoder were trained.\nTraining Time: ~6 hours on 4060Ti\nEvaluation Results\nIm using small test set i have on me, separated into anime(434) and photo(500) images. Additionally, im measuring noise in latents. Sorgy for no larger test sets.\nResults on small benchmark of 500 photos\nVAE SDXL\nL1 ‚Üì\nL2 ‚Üì\nPSNR ‚Üë\nLPIPS ‚Üì\nMS-SSIM ‚Üë\nKL ‚Üì\nConsistency ‚Üì\nRFID ‚Üì\nsdxl_vae\n6.282\n10.534\n29.278\n0.063\n0.947\n31.216\n0.0086\n4.819\nKohaku EQ-VAE\n6.423\n10.428\n29.140\n0.082\n0.945\n43.236\nn/a\n6.202\nAnzhc MS-LC-EQ-D-VR VAE\n5.975\n10.096\n29.526\n0.106\n0.952\n33.176\nn/a\n5.578\nAnzhc MS-LC-EQ-D-VR VAE B2\n6.082\n10.214\n29.432\n0.103\n0.951\n33.535\nn/a\n5.509\nAnzhc MS-LC-EQ-D-VR VAE B3\n6.066\n10.151\n29.475\n0.104\n0.951\n34.341\nn/a\n5.538\nAnzhc MS-LC-EQ-D-VR VAE B4\n5.839\n9.818\n29.788\n0.112\n0.9535\n35.762\nn/a\n5.260\nAnzhc MS-LC-EQ-D-VR VAE B5\n5.8117\n9.7627\n29.8545\n0.1112\n0.9538\n36.5573\n0.0080\n4.963894\nAnzhc MS-LC-EQ-D-VR VAE B7\n5.7046\n9.5975\n30.0106\n0.0980\n0.9553\n39.4477\n0.0071\n4.017592\nVAE FLUX\nL1¬†‚Üì\nL2¬†‚Üì\nPSNR¬†‚Üë\nLPIPS¬†‚Üì\nMS-SSIM¬†‚Üë\nKL¬†‚Üì\nCONSISTENCY¬†‚Üì\nrFID¬†‚Üì\nFLUX VAE\n4.1471\n6.2940\n33.3887\n0.0209\n0.9868\n12.1461\n0.0077\n0.564150\nMS-LC-EQ-D-VR VAE FLUX\n3.799\n6.077\n33.807\n0.032\n0.986\n10.992\n‚Äî\n1.692\nFlux EQ v2 B1\n3.4560\n5.5851\n34.6641\n0.0281\n0.9884\n11.4340\n0.0040\n0.686061\nNoise in latents\nVAE SDXL\nNoise ‚Üì\nsdxl_vae\n27.508\nKohaku EQ-VAE\n17.395\nAnzhc MS-LC-EQ-D-VR VAE\n15.527\nAnzhc MS-LC-EQ-D-VR VAE B2\n13.914\nAnzhc MS-LC-EQ-D-VR VAE B3\n13.124\nAnzhc MS-LC-EQ-D-VR VAE B4\n12.354\nAnzhc MS-LC-EQ-D-VR VAE B5\n11.846\nAnzhc MS-LC-EQ-D-VR VAE B7\n12.1471\nVAE FLUX\nNoise¬†‚Üì\nFLUX VAE\n10.499\nMS‚ÄëLC‚ÄëEQ‚ÄëD‚ÄëVR VAE‚ÄØFLUX\n7.635\nFlux EQ v2 B1\n8.5019\nResults on a small benchmark of 434 Illustrations from Boorus\nVAE SDXL\nL1 ‚Üì\nL2 ‚Üì\nPSNR ‚Üë\nLPIPS ‚Üì\nMS-SSIM ‚Üë\nKL ‚Üì\nConsistency ‚Üì\nRFID ‚Üì\nsdxl_vae\n4.369\n7.905\n31.080\n0.038\n0.969\n35.057\n0.0079\n5.088\nKohaku EQ-VAE\n4.818\n8.332\n30.462\n0.048\n0.967\n50.022\nn/a\n7.264\nAnzhc MS-LC-EQ-D-VR VAE\n4.351\n7.902\n30.956\n0.062\n0.970\n36.724\nn/a\n6.239\nAnzhc MS-LC-EQ-D-VR VAE B2\n4.313\n7.935\n30.951\n0.059\n0.970\n36.963\nn/a\n6.147\nAnzhc MS-LC-EQ-D-VR VAE B3\n4.323\n7.910\n30.977\n0.058\n0.970\n37.809\nn/a\n6.075\nAnzhc MS-LC-EQ-D-VR VAE B4\n4.140\n7.617\n31.343\n0.058\n0.971\n39.057\nn/a\n5.670\nAnzhc MS-LC-EQ-D-VR VAE B5\n4.0998\n7.5481\n31.4378\n0.0569\n0.9717\n39.8600\n0.0070\n5.178428\nAnzhc MS-LC-EQ-D-VR VAE B7\n3.9949\n7.3784\n31.6544\n0.0508\n0.9731\n42.8447\n0.0063\n4.216971\nAnzhc B7 Decoer-only pass\n3.9856\n7.3349\n31.6946\n0.0505\n0.9735\n42.8447\n0.0065\n3.851861\nVAE FLUX\nL1¬†‚Üì\nL2¬†‚Üì\nPSNR¬†‚Üë\nLPIPS¬†‚Üì\nMS-SSIM¬†‚Üë\nKL¬†‚Üì\nCONSISTENCY¬†‚Üì\nrFID¬†‚Üì\nFLUX VAE\n3.0600\n4.7752\n35.4400\n0.0112\n0.9905\n12.4717\n0.0079\n0.669906\nMS-LC-EQ-D-VR VAE FLUX\n2.933\n4.856\n35.251\n0.018\n0.990\n11.225\n‚Äî\n1.561\nFlux EQ v2 B1\n2.4825\n4.2776\n36.6027\n0.0132\n0.9916\n11.6388\n0.0039\n0.744904\nNoise in latents\nVAE SDXL\nNoise ‚Üì\nsdxl_vae\n26.359\nKohaku EQ-VAE\n17.314\nAnzhc MS-LC-EQ-D-VR VAE\n14.976\nAnzhc MS-LC-EQ-D-VR VAE B2\n13.649\nAnzhc MS-LC-EQ-D-VR VAE B3\n13.247\nAnzhc MS-LC-EQ-D-VR VAE B4\n12.652\nAnzhc MS-LC-EQ-D-VR VAE B5\n12.217\nAnzhc MS-LC-EQ-D-VR VAE B7\n12.3996\nVAE FLUX\nNoise¬†‚Üì\nFLUX VAE\n9.913\nMS‚ÄëLC‚ÄëEQ‚ÄëD‚ÄëVR VAE‚ÄØFLUX\n7.723\nFlux EQ v2 B1\n8.4004\nKL loss suggests that this VAE implementation is much closer to SDXL, and likely will be a better candidate for further finetune, but that is just a theory.\nB2 further improves latent clarity, while maintaining same or better performance. Particularly improved very fine texture handling, which previously would be overcorrected into smooth surface. Performs better in such cases now.\nB3 cleans them up ever more, but at that point visually they are +- same.\nB4 Moar.\nB5 MOAR. (also benchmarked with padding added, so results are overall a tiny bit more consistent due to fixed edges)\nB6-7 Concentration on improving details. Previous runs were clearing latents up as much as possible, now target is to preserve and improve, while allowing model to still change latents to accommodate for new details in a clear way.\nReferences\n[1] [2502.09509] EQ-VAE: Equivariance Regularized Latent Space for Improved Generative Image Modeling\n[2] [2506.07863] VIVAT: VIRTUOUS IMPROVING VAE TRAINING THROUGH ARTIFACT MITIGATION\n[3] sdxl-vae\nCite\n@misc{anzhc_ms-lc-eq-d-vr_vae,\nauthor       = {Anzhc},\ntitle        = {MS-LC-EQ-D-VR VAE: another reproduction of EQ-VAE on cariable VAEs and then some},\nyear         = {2025},\nhowpublished = {Hugging Face model card},\nurl          = {https://huggingface.co/Anzhc/MS-LC-EQ-D-VR_VAE},\nnote         = {Finetuned SDXL-VAE with EQ regularization and more, for improved latent representation.}\n}\nAcknowledgement\nMy friend Bluvoll, for no particular reason.",
    "mmnga/plamo-2-translate-gguf": "plamo-2-translate-gguf\nUsage\nplamo-2-translate-gguf\npfnet„Åï„Çì„ÅåÂÖ¨Èñã„Åó„Å¶„ÅÑ„Çãplamo-2-translate„ÅÆgguf„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂ§âÊèõÁâà„Åß„Åô„ÄÇ\nimatrix„ÅÆ„Éá„Éº„Çø„ÅØTFMC/imatrix-dataset-for-japanese-llm„Çí‰ΩøÁî®„Åó„Å¶‰ΩúÊàê„Åó„Åæ„Åó„Åü„ÄÇ\nUsage\ngit clone https://github.com/ggml-org/llama.cpp.git\ncd llama.cpp\ncmake -B build -DGGML_CUDA=ON\ncmake --build build --config Release\nbuild/bin/llama-cli -m 'plamo-2-translate-gguf' -n 128 -c 128 -p '<|plamo:op|>dataset\\ntranslation\\n\\n<|plamo:op|>input lang=English\\nWrite the text to be translated here.\\n<|plamo:op|>output lang=Japanese' -no-cnv",
    "onnx-community/LFM2-1.2B-ONNX": "LFM2-1.2B\nüìÑ Model details\nüèÉ How to run LFM2\nTransformers.js\nONNXRuntime\nLiquid: Playground\nLiquid\nLiquid\nPlayground\nPlayground\nLFM2-1.2B\nLFM2 is a new generation of hybrid models developed by Liquid AI, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.\nWe're releasing the weights of three post-trained checkpoints with 350M, 700M, and 1.2B parameters. They provide the following key features to create AI-powered edge applications:\nFast training & inference ‚Äì LFM2 achieves 3x faster training compared to its previous generation. It also benefits from 2x faster decode and prefill speed on CPU compared to Qwen3.\nBest performance ‚Äì LFM2 outperforms similarly-sized models across multiple benchmark categories, including knowledge, mathematics, instruction following, and multilingual capabilities.\nNew architecture ‚Äì LFM2 is a new hybrid Liquid model with multiplicative gates and short convolutions.\nFlexible deployment ‚Äì LFM2 runs efficiently on CPU, GPU, and NPU hardware for flexible deployment on smartphones, laptops, or vehicles.\nFind more information about LFM2 in our blog post.\nüìÑ Model details\nDue to their small size, we recommend fine-tuning LFM2 models on narrow use cases to maximize performance.\nThey are particularly suited for agentic tasks, data extraction, RAG, creative writing, and multi-turn conversations.\nHowever, we do not recommend using them for tasks that are knowledge-intensive or require programming skills.\nProperty\nValue\nParameters\n1,170,340,608\nLayers\n16 (10 conv + 6 attn)\nContext length\n32,768 tokens\nVocabulary size\n65,536\nPrecision\nbfloat16\nTraining budget\n10 trillion tokens\nLicense\nLFM Open License v1.0\nSupported languages: English, Arabic, Chinese, French, German, Japanese, Korean, and Spanish.\nGeneration parameters: We recommend the following parameters:\ntemperature=0.3\nmin_p=0.15\nrepetition_penalty=1.05\nArchitecture: Hybrid model with multiplicative gates and short convolutions: 10 double-gated short-range LIV convolution blocks and 6 grouped query attention (GQA) blocks.\nPre-training mixture: Approximately 75% English, 20% multilingual, and 5% code data sourced from the web and licensed materials.\nTraining approach:\nKnowledge distillation using LFM1-7B as teacher model\nVery large-scale SFT on 50% downstream tasks, 50% general domains\nCustom DPO with length normalization and semi-online datasets\nIterative model merging\nüèÉ How to run LFM2\nTransformers.js\nIf you haven't already, you can install the Transformers.js JavaScript library from NPM using:\nnpm i @huggingface/transformers\nExample: Basic example\nimport { pipeline, TextStreamer } from \"@huggingface/transformers\";\n// Create a text generation pipeline\nconst generator = await pipeline(\n\"text-generation\",\n\"onnx-community/LFM2-1.2B-ONNX\",\n{ dtype: \"q4\" },\n);\n// Define the list of messages\nconst messages = [\n{ role: \"system\", content: \"You are a helpful assistant.\" },\n{ role: \"user\", content: \"What is the capital of France?\" },\n];\n// Generate a response\nconst output = await generator(messages, {\nmax_new_tokens: 512,\ndo_sample: false,\nstreamer: new TextStreamer(generator.tokenizer, { skip_prompt: true, skip_special_tokens: true }),\n});\nconsole.log(output[0].generated_text.at(-1).content);\n// The capital of France is Paris.\nExample: Tool calling\nimport { AutoModelForCausalLM, AutoTokenizer, TextStreamer } from \"@huggingface/transformers\";\n// Load tokenizer and model\nconst model_id = \"onnx-community/LFM2-1.2B-ONNX\";\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\nconst model = await AutoModelForCausalLM.from_pretrained(\nmodel_id, { dtype: \"q4\", device: \"webgpu\" },\n);\n// Define tools and messages\nconst tools = [\n{\nname: \"get_weather\",\ndescription: \"Get current weather information for a location\",\nparameters: {\ntype: \"object\",\nproperties: {\nlocation: {\ntype: \"string\",\ndescription: \"The city and state, e.g. San Francisco, CA\",\n},\nunit: {\ntype: \"string\",\nenum: [\"celsius\", \"fahrenheit\"],\ndescription: \"The unit of temperature to use\",\n},\n},\nrequired: [\"location\"],\n},\n},\n];\nconst messages = [\n{\nrole: \"user\",\ncontent: \"What's the weather like in New York?\"\n},\n];\n// Prepare inputs\nconst input = tokenizer.apply_chat_template(messages, {\ntools,\nadd_generation_prompt: true,\nreturn_dict: true,\n});\n// Generate output\nconst sequences = await model.generate({\n...input,\nmax_new_tokens: 512,\ndo_sample: false,\nstreamer: new TextStreamer(tokenizer, { skip_prompt: true, skip_special_tokens: false }),\n});\n// Decode and print the generated text\nconst response = tokenizer.batch_decode(\nsequences.slice(null, [input.input_ids.dims[1], null]),\n{ skip_special_tokens: true },\n);\nconsole.log(response[0]); // [get_weather(location=\"New York\", unit=\"fahrenheit\")]\nONNXRuntime\nfrom transformers import AutoConfig, AutoTokenizer\nimport onnxruntime\nimport numpy as np\nfrom huggingface_hub import hf_hub_download\n# 1. Load config, processor, and model\nmodel_id = \"onnx-community/LFM2-1.2B-ONNX\"\nconfig = AutoConfig.from_pretrained(model_id)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nfilename = \"model.onnx\" # Options: \"model.onnx\", \"model_fp16.onnx\", \"model_q4.onnx\", \"model_q4f16.onnx\"\nmodel_path = hf_hub_download(repo_id=model_id, filename=f\"onnx/{filename}\") # Download the graph\nhf_hub_download(repo_id=model_id, filename=f\"onnx/{filename}_data\") # Download the weights\nsession = onnxruntime.InferenceSession(model_path)\n## Set config values\nnum_key_value_heads = config.num_key_value_heads\nhead_dim = config.hidden_size // config.num_attention_heads\nnum_hidden_layers = config.num_hidden_layers\neos_token_id = config.eos_token_id\nhidden_size = config.hidden_size\nconv_L_cache = config.conv_L_cache\nlayer_types = config.layer_types\n# 2. Prepare inputs\nprompt = \"What is C. elegans?\"\nmessages = [{\"role\": \"user\", \"content\": prompt}]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"np\")\ninput_ids = inputs['input_ids']\nattention_mask = inputs['attention_mask']\nbatch_size = input_ids.shape[0]\nposition_ids = np.tile(np.arange(0, input_ids.shape[-1]), (batch_size, 1))\npast_cache_values = {}\nfor i in range(num_hidden_layers):\nif layer_types[i] == 'full_attention':\nfor kv in ('key', 'value'):\npast_cache_values[f'past_key_values.{i}.{kv}'] = np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)\nelif layer_types[i] == 'conv':\npast_cache_values[f'past_conv.{i}'] = np.zeros([batch_size, hidden_size, conv_L_cache], dtype=np.float32)\nelse:\nraise ValueError(f\"Unsupported layer type: {layer_types[i]}\")\n# 3. Generation loop\nmax_new_tokens = 1024\ngenerated_tokens = np.array([[]], dtype=np.int64)\nfor i in range(max_new_tokens):\nlogits, *present_cache_values = session.run(None, dict(\ninput_ids=input_ids,\nattention_mask=attention_mask,\nposition_ids=position_ids,\n**past_cache_values,\n))\n## Update values for next generation loop\ninput_ids = logits[:, -1].argmax(-1, keepdims=True)\nattention_mask = np.concatenate([attention_mask, np.ones_like(input_ids, dtype=np.int64)], axis=-1)\nposition_ids = position_ids[:, -1:] + 1\nfor j, key in enumerate(past_cache_values):\npast_cache_values[key] = present_cache_values[j]\ngenerated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)\nif (input_ids == eos_token_id).all():\nbreak\n## (Optional) Streaming\nprint(tokenizer.decode(input_ids[0]), end='', flush=True)\nprint()\n# 4. Output result\nprint(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])",
    "nunchaku-tech/nunchaku-flux.1-kontext-dev": "Model Card for nunchaku-flux.1-kontext-dev\nModel Details\nModel Description\nModel Files\nModel Sources\nUsage\nPerformance\nCitation\nAttribution Notice\nModel Card for nunchaku-flux.1-kontext-dev\nThis repository contains Nunchaku-quantized versions of FLUX.1-Kontext-dev, capable of editing images based on text instructions. It is optimized for efficient inference while maintaining minimal loss in performance.\nModel Details\nModel Description\nDeveloped by: Nunchaku Team\nModel type: image-to-image\nLicense: flux-1-dev-non-commercial-license\nQuantized from model: FLUX.1-Kontext-dev\nModel Files\nsvdq-int4_r32-flux.1-kontext-dev.safetensors: SVDQuant quantized INT4 FLUX.1-Kontext-dev model. For users with non-Blackwell GPUs (pre-50-series).\nsvdq-fp4_r32-flux.1-kontext-dev.safetensors: SVDQuant quantized NVFP4 FLUX.1-Kontext-dev model. For users with Blackwell GPUs (50-series).\nModel Sources\nInference Engine: nunchaku\nQuantization Library: deepcompressor\nPaper: SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models\nDemo: svdquant.mit.edu\nUsage\nDiffusers Usage: See flux.1-kontext-dev.py. Check our tutorial for more advanced usage.\nComfyUI Usage: See nunchaku-flux.1-kontext-dev.json.\nPerformance\nCitation\n@inproceedings{\nli2024svdquant,\ntitle={SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models},\nauthor={Li*, Muyang and Lin*, Yujun and Zhang*, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song},\nbooktitle={The Thirteenth International Conference on Learning Representations},\nyear={2025}\n}\nAttribution Notice\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs Inc. IN NO EVENT SHALL BLACK FOREST LABS INC. BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH USE OF THIS MODEL.",
    "OpenIXCLab/SeC-4B": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction\nHighlights\nSeC Performance\nCitation\nSeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction\n[üìÇ GitHub]\n[üì¶ Benchmark]\n[üåê Homepage]\n[üìÑ Paper]\nHighlights\nüî•We introduce Segment Concept (SeC), a concept-driven segmentation framework for video object segmentation that integrates Large Vision-Language Models (LVLMs) for robust, object-centric representations.\nüî•SeC dynamically balances semantic reasoning with feature matching, adaptively adjusting computational efforts based on scene complexity for optimal segmentation performance.\nüî•We propose the Semantic Complex Scenarios Video Object Segmentation (SeCVOS) benchmark, designed to evaluate segmentation in challenging scenarios.\nSeC Performance\nModel\nSA-V val\nSA-V test\nLVOS v2 val\nMOSE val\nDAVIS 2017 val\nYTVOS 2019 val\nSeCVOS\nSAM 2.1\n78.6\n79.6\n84.1\n74.5\n90.6\n88.7\n58.2\nSAMURAI\n79.8\n80.0\n84.2\n72.6\n89.9\n88.3\n62.2\nSAM2.1Long\n81.1\n81.2\n85.9\n75.2\n91.4\n88.7\n62.3\nSeC (Ours)\n82.7\n81.7\n86.5\n75.3\n91.3\n88.6\n70.0\nCitation\nIf you find this project useful in your research, please consider citing:\n@article{zhang2025sec,\ntitle     = {SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction},\nauthor    = {Zhixiong Zhang and Shuangrui Ding and Xiaoyi Dong and Songxin He and Jianfan Lin and Junsong Tang and Yuhang Zang and Yuhang Cao and Dahua Lin and Jiaqi Wang},\njournal   = {arXiv preprint arXiv:2507.15852},\nyear      = {2025}\n}",
    "lovis93/Motion-Lora-Camera-Push-In-Wan-14B-720p-I2V": "Push-in camera ‚Äî A Motion LoRA for Wan 2.1 (14B - 720p)\nüöÄ How to Use\nüß© ComfyUI Workflow\nüé¨ Examples\nüõ†Ô∏è Model Details\nFeedback\nPush-in camera ‚Äî A Motion LoRA for Wan 2.1 (14B - 720p)\nThis LoRA was trained on 100 clips to introduce realistic, high-quality push-in drone camera motion into your generations. While the Wan 2.1 I2V 720p model produces great image quality, it can sometimes lack meaningful movement. Push-in camera enhances your creations by delivering natural camera dynamics across various styles and scenes.\nThis final version is the result of over 40 iterations of tweaking and refining to achieve a truly cinematic motion.\nüöÄ How to Use\nTo use this LoRA, you need to use the trigger word Push-in camera in your prompt.\nExample prompt:\nPush-in camera, Instantly rocketing towards the heart of the lavender field, the vibrant purple blooms blurring into a hypnotic, swirling vortex of color as the rows of lavender, each individual flower a tiny point of light, rush towards the viewer, the distant cypress trees transforming into sharp, dark silhouettes against a breathtaking sunset sky, the soft golden light illuminating every minute detail of the scene, until the camera slams into the heart of the field, revealing the intricate texture of the blossoms, the delicate variations in purple hues, and the subtle golden undertones of the setting sun, in breathtaking, hyper-real clarity.\nüß© ComfyUI Workflow\nA complete ComfyUI workflow is available to help you get started instantly. This makes the LoRA directly usable without any complex setup.\nYou can download the workflow file here: workflow-pushin-v2-comfyui.json\nüé¨ Examples\nHere are some examples of the push-in effect generated with this LoRA.\nInput Image\nGenerated Video\nüõ†Ô∏è Model Details\nBase Model: Wan 2.1 I2V 720p\nTrigger Word: Push-in camera\nTraining: Trained on 100 carefully curated clips.\nCompatibility: Ideal for cinematic sequences, storytelling shots, and dynamic framing. It adapts beautifully across all styles.\nFeedback\nThis LoRA is also available on Civitai. I'm curious to hear your feedback!\nFeel free to open an issue or reach out with any questions.",
    "ibm-granite/granite-embedding-english-r2": "Granite-Embedding-English-R2\nModel Details\nUsage\nEvaluation Results\nModel Architecture and Key Features\nTraining and Optimization\nData Collection\nInfrastructure\nEthical Considerations and Limitations\nCitation\nGranite-Embedding-English-R2\nModel Summary: Granite-embedding-english-r2 is a 149M parameter dense biencoder embedding model from the Granite Embeddings collection that can be used to generate high quality text embeddings. This model produces embedding vectors of size 768 based on context length of upto 8192 tokens. Compared to most other open-source models, this model was only trained using open-source relevance-pair datasets with permissive, enterprise-friendly license, plus IBM collected and generated datasets.\nThe r2 models show strong performance across standard and IBM-built information retrieval benchmarks (BEIR, ClapNQ),\ncode retrieval (COIR), long-document search benchmarks (MLDR, LongEmbed), conversational multi-turn (MTRAG),\ntable retrieval (NQTables, OTT-QA, AIT-QA, MultiHierTT, OpenWikiTables), and on many enterprise use cases.\nThese models use a bi-encoder architecture to generate high-quality embeddings from text inputs such as queries, passages, and documents, enabling seamless comparison through cosine similarity. Built using retrieval oriented pretraining, contrastive finetuning, knowledge distillation, and model merging, granite-embedding-english-r2 is optimized to ensure strong alignment between query and passage embeddings.\nThe latest granite embedding r2 release introduces two English embedding models, both based on the ModernBERT architecture:\ngranite-embedding-english-r2 (149M parameters): with an output embedding size of 768, replacing granite-embedding-125m-english.\ngranite-embedding-small-english-r2 (47M parameters): A first-of-its-kind reduced-size model, with fewer layers and a smaller output embedding size (384), replacing granite-embedding-30m-english.\nModel Details\nDeveloped by: Granite Embedding Team, IBM\nRepository: ibm-granite/granite-embedding-models\nPaper: Granite Embedding R2 Models\nLanguage(s) (NLP): English\nRelease Date: Aug 15, 2025\nLicense: Apache 2.0\nUsage\nIntended Use: The model is designed to produce fixed length vector representations for a given text, which can be used for text similarity, retrieval, and search applications.\nFor efficient decoding, these models use Flash Attention 2. Installing it is optional, but can lead to faster inference.\npip install flash_attn==2.6.1\nUsage with Sentence Transformers:\nThe model is compatible with SentenceTransformer library and is very easy to use:\nFirst, install the sentence transformers library\npip install sentence_transformers\nThe model can then be used to encode pairs of text and find the similarity between their representations\nfrom sentence_transformers import SentenceTransformer, util\nmodel_path = \"ibm-granite/granite-embedding-english-r2\"\n# Load the Sentence Transformer model\nmodel = SentenceTransformer(model_path)\ninput_queries = [\n' Who made the song My achy breaky heart? ',\n'summit define'\n]\ninput_passages = [\n\"Achy Breaky Heart is a country song written by Don Von Tress. Originally titled Don't Tell My Heart and performed by The Marcy Brothers in 1991. \",\n\"Definition of summit for English Language Learners. : 1 the highest point of a mountain : the top of a mountain. : 2 the highest level. : 3 a meeting or series of meetings between the leaders of two or more governments.\"\n]\n# encode queries and passages. The model produces unnormalized vectors. If your task requires normalized embeddings pass normalize_embeddings=True to encode as below.\nquery_embeddings = model.encode(input_queries)\npassage_embeddings = model.encode(input_passages)\n# calculate cosine similarity\nprint(util.cos_sim(query_embeddings, passage_embeddings))\nUsage with Huggingface Transformers:\nThis is a simple example of how to use the granite-embedding-english-r2 model with the Transformers library and PyTorch.\nFirst, install the required libraries\npip install transformers torch\nThe model can then be used to encode pairs of text\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\nmodel_path = \"ibm-granite/granite-embedding-english-r2\"\n# Load the model and tokenizer\nmodel = AutoModel.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel.eval()\ninput_queries = [\n' Who made the song My achy breaky heart? ',\n'summit define'\n]\n# tokenize inputs\ntokenized_queries = tokenizer(input_queries, padding=True, truncation=True, return_tensors='pt')\n# encode queries\nwith torch.no_grad():\n# Queries\nmodel_output = model(**tokenized_queries)\n# Perform pooling. granite-embedding-278m-multilingual uses CLS Pooling\nquery_embeddings = model_output[0][:, 0]\n# normalize the embeddings\nquery_embeddings = torch.nn.functional.normalize(query_embeddings, dim=1)\nEvaluation Results\nGranite embedding r2 models show a strong performance across tasks diverse tasks.\nPerformance of the granite models on MTEB Retrieval (i.e., BEIR), MTEB-v2, code retrieval (CoIR), long-document search benchmarks (MLDR, LongEmbed), conversational multi-turn (MTRAG),\ntable retrieval (NQTables, OTT-QA, AIT-QA, MultiHierTT, OpenWikiTables),  benchmarks is reported in the below tables.\nThe r2 models demonstrates speed and efficiency, while mainintaining competitive performance. The average speed to encode documents on a single H100 GPU using a sliding window with 512 context length chunks is also reported.\nModel\nParameters (M)\nEmbedding Size\nBEIR Retrieval (15)\nMTEB-v2 (41)\nCoIR (10)\nMLDR (En)\nMTRAG (4)\nEncoding Speed (docs/sec)\ngranite-embedding-125m-english\n125\n768\n52.3\n62.1\n50.3\n35.0\n49.4\n149\ngranite-embedding-30m-english\n30\n384\n49.1\n60.2\n47.0\n32.6\n48.6\n198\ngranite-embedding-english-r2\n149\n768\n53.1\n62.8\n55.3\n40.7\n56.7\n144\ngranite-embedding-small-english-r2\n47\n384\n50.9\n61.1\n53.8\n39.8\n48.1\n199\nModel\nParameters (M)\nEmbedding Size\nAVERAGE\nMTEB-v2 Retrieval (10)\nCoIR (10)\nMLDR (En)\nLongEmbed (6)\nTable IR (5)\nMTRAG(4)\nEncoding Speed (docs/sec)\ne5-base-v2\n109\n768\n47.5\n49.7\n50.3\n32.5\n41.1\n74.09\n37.0\n115\nbge-base-en-v1.5\n109\n768\n46.9\n54.8\n46.6\n33.5\n33.9\n73.98\n38.8\n116\nsnowflake-arctic-embed-m-v2.0\n305\n768\n51.4\n58.4\n52.2\n32.4\n55.4\n80.75\n29.2\n106\ngte-base-en-v1.5\n137\n768\n52.8\n55.5\n42.4\n42.7\n59.4\n80.52\n36.0\n116\ngte-modernbert-base\n149\n768\n57.5\n57.0\n71.5\n46.2\n57.0\n76.68\n36.8\n142\nnomic-ai/modernbert-embed-base\n149\n768\n48.0\n48.7\n48.8\n31.3\n56.3\n66.69\n36.2\n141\ngranite-embedding-english-r2\n149\n768\n59.5\n56.4\n54.8\n41.6\n67.8\n78.53\n57.6\n144\ngranite-embedding-small-english-r2\n47\n384\n55.6\n53.9\n53.4\n40.1\n61.9\n75.51\n48.9\n199\nModel Architecture and Key Features\nThe latest granite embedding r2 release introduces two English embedding models, both based on the ModernBERT architecture:\ngranite-embedding-english-r2 (149M parameters): with an output embedding size of 768, replacing granite-embedding-125m-english.\ngranite-embedding-small-english-r2 (47M parameters): A first-of-its-kind reduced-size model, with fewer layers and a smaller output embedding size (384), replacing granite-embedding-30m-english.\nThe following table shows the structure of the two models:\nModel\ngranite-embedding-small-english-r2\ngranite-embedding-english-r2\nEmbedding size\n384\n768\nNumber of layers\n12\n22\nNumber of attention heads\n12\n12\nIntermediate size\n1536\n1152\nActivation Function\nGeGLU\nGeGLU\nVocabulary Size\n50368\n50368\nMax. Sequence Length\n8192\n8192\n# Parameters\n47M\n149M\nTraining and Optimization\nThe granite embedding r2 models incorporate key enhancements from the ModernBERT architecture, including:\nAlternating attention lengths to accelerate processing\nRotary position embeddings for extended sequence length\nA newly trained tokenizer optimized with code and text data\nFlash Attention 2.0 for improved efficiency\nStreamlined parameters, eliminating unnecessary bias terms\nData Collection\nGranite embedding r2 models are trained using data from four key sources:\nUnsupervised title-body paired data scraped from the web\nPublicly available paired with permissive, enterprise-friendly license\nIBM-internal paired data targetting specific technical domains\nIBM-generated synthetic data\nNotably, we do not use the popular MS-MARCO retrieval dataset in our training corpus due to its non-commercial license (many open-source models use this dataset due to its high quality).\nThe underlying encoder models using GneissWeb, an IBM-curated dataset composed exclusively of open, commercial-friendly sources.\nFor governance, all our data undergoes a data clearance process subject to technical, business, and governance review. This comprehensive process captures critical information about the data, including but not limited to their content description ownership, intended use, data classification, licensing information, usage restrictions, how the data will be acquired, as well as an assessment of sensitive information (i.e, personal information).\nInfrastructure\nWe trained the granite embedding english r2 models using IBM's computing cluster, BlueVela Cluster, which is outfitted with NVIDIA H100 80GB GPUs. This cluster provides a scalable and efficient infrastructure for training our models over multiple GPUs.\nEthical Considerations and Limitations\nGranite-embedding-english-r2 leverages both permissively licensed open-source and select proprietary data for enhanced performance. The training data for the base language model was filtered to remove text containing hate, abuse, and profanity. Granite-embedding-english-r2 is trained only for English texts, and has a context length of 8192 tokens (longer texts will be truncated to this size).\n‚≠êÔ∏è Learn about the latest updates with Granite: https://www.ibm.com/granite\nüìÑ Get started with tutorials, best practices, and prompt engineering advice: https://www.ibm.com/granite/docs/\nüí° Learn about the latest Granite learning resources: https://ibm.biz/granite-learning-resources\nCitation\n@misc{awasthy2025graniteembeddingr2models,\ntitle={Granite Embedding R2 Models},\nauthor={Parul Awasthy and Aashka Trivedi and Yulong Li and Meet Doshi and Riyaz Bhat and Vignesh P and Vishwajeet Kumar and Yushu Yang and Bhavani Iyer and Abraham Daniels and Rudra Murthy and Ken Barker and Martin Franz and Madison Lee and Todd Ward and Salim Roukos and David Cox and Luis Lastras and Jaydeep Sen and Radu Florian},\nyear={2025},\neprint={2508.21085},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2508.21085},\n}",
    "Chroma111/CivitAI-Archive-2": "No model card",
    "unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF": "Qwen3-Coder-480B-A35B-Instruct\nHighlights\nModel Overview\nQuickstart\nAgentic Coding\nBest Practices\nCitation\nSee our collection for all versions of Qwen3 including GGUF, 4-bit & 16-bit formats.\nLearn to run Qwen3-Coder correctly - Read our Guide.\nSee Unsloth Dynamic 2.0 GGUFs for our quantization benchmarks.\n‚ú® Read our Qwen3-Coder Guide here!\nFine-tune Qwen3 (14B) for free using our Google Colab notebook!\nRead our Blog about Qwen3 support: unsloth.ai/blog/qwen3\nView the rest of our notebooks in our docs here.\nRun & export your fine-tuned model to Ollama, llama.cpp or HF.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nQwen3 (14B)\n‚ñ∂Ô∏è Start on Colab\n3x faster\n70% less\nGRPO with Qwen3 (8B)\n‚ñ∂Ô∏è Start on Colab\n3x faster\n80% less\nLlama-3.2 (3B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen2.5 (7B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n60% less\nQwen3-Coder-480B-A35B-Instruct\nHighlights\nToday, we're announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we're excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct. featuring the following key enhancements:\nSignificant Performance among open models on Agentic Coding, Agentic Browser-Use, and other foundational coding tasks, achieving results comparable to Claude Sonnet.\nLong-context Capabilities with native support for 256K tokens, extendable up to 1M tokens using Yarn, optimized for repository-scale understanding.\nAgentic Coding supporting for most platforms such as Qwen Code, CLINE, featuring a specially designed function call format.\nModel Overview\nQwen3-480B-A35B-Instruct has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 480B in total and 35B activated\nNumber of Layers: 62\nNumber of Attention Heads (GQA): 96 for Q and 8 for KV\nNumber of Experts: 160\nNumber of Activated Experts: 8\nContext Length: 262,144 natively.\nNOTE: This model supports only non-thinking mode and does not generate <think></think> blocks in its output. Meanwhile, specifying enable_thinking=False is no longer required.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nQuickstart\nWe advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3_moe'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-480B-A35B-Instruct\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Write a quick sort algorithm.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=65536\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"content:\", content)\nNote: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as 32,768.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nAgentic Coding\nQwen3-Coder excels in tool calling capabilities.\nYou can simply define or use any tools as following example.\n# Your tool implementation\ndef square_the_number(num: float) -> dict:\nreturn num ** 2\n# Define Tools\ntools=[\n{\n\"type\":\"function\",\n\"function\":{\n\"name\": \"square_the_number\",\n\"description\": \"output the square of the number.\",\n\"parameters\": {\n\"type\": \"object\",\n\"required\": [\"input_num\"],\n\"properties\": {\n'input_num': {\n'type': 'number',\n'description': 'input_num is a number that will be squared'\n}\n},\n}\n}\n}\n]\nimport OpenAI\n# Define LLM\nclient = OpenAI(\n# Use a custom endpoint compatible with OpenAI API\nbase_url='http://localhost:8000/v1',  # api_base\napi_key=\"EMPTY\"\n)\nmessages = [{'role': 'user', 'content': 'square the number 1024'}]\ncompletion = client.chat.completions.create(\nmessages=messages,\nmodel=\"Qwen3-480B-A35B-Instruct\",\nmax_tokens=65536,\ntools=tools,\n)\nprint(completion.choice[0])\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using temperature=0.7, top_p=0.8, top_k=20, repetition_penalty=1.05.\nAdequate Output Length: We recommend using an output length of 65,536 tokens for most queries, which is adequate for instruct models.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "dongguanting/Qwen2.5-3B-ARPO": "ARPO: Agentic Reinforced Policy Optimization\nAbstract\nOverview\nUsage\nCitation\nLicense\nAcknowledge\nContact\nARPO: Agentic Reinforced Policy Optimization\nThis repository contains the official checkpoint for ARPO: Agentic Reinforced Policy Optimization, a novel agentic Reinforcement Learning (RL) algorithm designed for training multi-turn Large Language Model (LLM)-based agents.\nAbstract\nLarge-scale reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools to assist in task-solving processes. However, current RL algorithms inadequately balance the models' intrinsic long-horizon reasoning capabilities and their proficiency in multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training multi-turn LLM-based agents. Through preliminary experiments, we observe that LLMs tend to exhibit highly uncertain behavior, characterized by an increase in the entropy distribution of generated tokens, immediately following interactions with external tools. Motivated by this observation, ARPO incorporates an entropy-based adaptive rollout mechanism, dynamically balancing global trajectory sampling and step-level sampling, thereby promoting exploration at steps with high uncertainty after tool usage. By integrating an advantage attribution estimation, ARPO enables LLMs to internalize advantage differences in stepwise tool-use interactions. Our experiments across 13 challenging benchmarks in computational reasoning, knowledge reasoning, and deep search domains demonstrate ARPO's superiority over trajectory-level RL algorithms. Remarkably, ARPO achieves improved performance using only half of the tool-use budget required by existing methods, offering a scalable solution for aligning LLM-based agents with real-time dynamic environments. Our code and datasets are released at this https URL\nOverview\nARPO's core principle is to encourage the policy model to adaptively branch sampling during high-entropy tool-call rounds, thereby efficiently aligning step-level tool-use behaviors.\nIn figure (left), the initial tokens generated by the LLM after receiving each round of tool-call feedback consistently exhibit a high entropy. This indicates that external tool-call significantly introduces uncertainty into the LLM‚Äôs reasoning process.\nIn the figure (right), we validate ARPO's performance across 13 datasets. Notably, Qwen3-14B with ARPO excelled in Pass@5, achieving 61.2% on GAIA and 24.0% on HLE, while requiring only about half the tool calls compared to GRPO during training.\nUsage\nThis model can be loaded and used with the Hugging Face transformers library. Ensure you have the library installed (pip install transformers) and optionally accelerate for optimized loading (pip install accelerate).\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n# Load the model and tokenizer\n# You can choose other ARPO checkpoints from the Hugging Face collection:\n# https://huggingface.co/collections/dongguanting/arpo-688229ff8a6143fe5b4ad8ae\nmodel_id = \"dongguanting/Qwen2.5-7B-ARPO\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16, # Use torch.float16 if bfloat16 is not supported on your GPU\ndevice_map=\"auto\",          # Automatically distributes model across available devices (e.g., GPUs)\ntrust_remote_code=True      # Required for Qwen2 models due to custom code\n)\n# Example chat completion using the Qwen chat template\nmessages = [\n{\"role\": \"user\", \"content\": \"Hello, how are you today?\"},\n]\n# Apply chat template and tokenize\n# This formats the messages according to the model's specific chat format (e.g., <|im_start|>user\n...)\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ninput_ids = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# Generate response\ngenerated_ids = model.generate(\ninput_ids.input_ids,\nmax_new_tokens=512,\ndo_sample=True,\ntemperature=0.7,\ntop_k=20,\ntop_p=0.8,\nrepetition_penalty=1.05,\n# Use EOS and PAD token IDs from the model's configuration\neos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|im_end|>\")],\npad_token_id=tokenizer.pad_token_id,\n)\n# Decode and print the response, skipping special tokens\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\n# For detailed multi-turn conversations and tool-use examples,\n# refer to the official GitHub repository's Quick Start section.\nCitation\nIf you find this work helpful, please cite our paper:\n@misc{dong2025arpo,\ntitle={Agentic Reinforced Policy Optimization},\nauthor={Guanting Dong and Hangyu Mao and Kai Ma and Licheng Bao and Yifei Chen and Zhongyuan Wang and Zhongxia Chen and Jiazhen Du and Huiyang Wang and Fuzheng Zhang and Guorui Zhou and Yutao Zhu and Ji-Rong Wen and Zhicheng Dou},\nyear={2025},\neprint={2507.19849},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2507.19849},\n}\nLicense\nThis project is released under the MIT License.\nAcknowledge\nThis training implementation builds upon Tool-Star, Llama Factory, verl and ReCall. For evaluation, we rely on WebThinker, HIRA, WebSailor, Search-o1, and FlashRAG. The Python interpreter design references ToRA and ToRL, while our models are trained using Qwen2.5. We express our sincere gratitude to these projects for their invaluable contributions to the open-source community.\nContact\nFor any questions or feedback, please reach out to us at dongguanting@ruc.edu.cn.",
    "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8": "Qwen3-235B-A22B-Thinking-2507-FP8\nHighlights\nModel Overview\nPerformance\nQuickstart\nNote on FP8\nAgentic Use\nBest Practices\nCitation\nQwen3-235B-A22B-Thinking-2507-FP8\nHighlights\nOver the past three months, we have continued to scale the thinking capability of Qwen3-235B-A22B, improving both the quality and depth of reasoning. We are pleased to introduce Qwen3-235B-A22B-Thinking-2507-FP8, featuring the following key enhancements:\nSignificantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise ‚Äî achieving state-of-the-art results among open-source thinking models.\nMarkedly better general capabilities, such as instruction following, tool usage, text generation, and alignment with human preferences.\nEnhanced 256K long-context understanding capabilities.\nNOTE: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\nModel Overview\nThis repo contains the FP8 version of Qwen3-235B-A22B-Thinking-2507, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 235B in total and 22B activated\nNumber of Paramaters (Non-Embedding): 234B\nNumber of Layers: 94\nNumber of Attention Heads (GQA): 64 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 262,144 natively.\nNOTE: This model supports only thinking mode. Meanwhile, specifying enable_thinking=True is no longer required.\nAdditionally, to enforce model thinking, the default chat template automatically includes <think>. Therefore, it is normal for the model's output to contain only </think> without an explicit opening <think> tag.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nPerformance\nDeepseek-R1-0528\nOpenAI O4-mini\nOpenAI O3\nGemini-2.5 Pro\nClaude4 Opus Thinking\nQwen3-235B-A22B Thinking\nQwen3-235B-A22B-Thinking-2507\nKnowledge\nMMLU-Pro\n85.0\n81.9\n85.9\n85.6\n-\n82.8\n84.4\nMMLU-Redux\n93.4\n92.8\n94.9\n94.4\n94.6\n92.7\n93.8\nGPQA\n81.0\n81.4*\n83.3*\n86.4\n79.6\n71.1\n81.1\nSuperGPQA\n61.7\n56.4\n-\n62.3\n-\n60.7\n64.9\nReasoning\nAIME25\n87.5\n92.7*\n88.9*\n88.0\n75.5\n81.5\n92.3\nHMMT25\n79.4\n66.7\n77.5\n82.5\n58.3\n62.5\n83.9\nLiveBench 20241125\n74.7\n75.8\n78.3\n82.4\n78.2\n77.1\n78.4\nHLE\n17.7#\n18.1*\n20.3\n21.6\n10.7\n11.8#\n18.2#\nCoding\nLiveCodeBench v6 (25.02-25.05)\n68.7\n71.8\n58.6\n72.5\n48.9\n55.7\n74.1\nCFEval\n2099\n1929\n2043\n2001\n-\n2056\n2134\nOJBench\n33.6\n33.3\n25.4\n38.9\n-\n25.6\n32.5\nAlignment\nIFEval\n79.1\n92.4\n92.1\n90.8\n89.7\n83.4\n87.8\nArena-Hard v2$\n72.2\n59.3\n80.8\n72.5\n59.1\n61.5\n79.7\nCreative Writing v3\n86.3\n78.8\n87.7\n85.9\n83.8\n84.6\n86.1\nWritingBench\n83.2\n78.4\n85.3\n83.1\n79.1\n80.3\n88.3\nAgent\nBFCL-v3\n63.8\n67.2\n72.4\n67.2\n61.8\n70.8\n71.9\nTAU1-Retail\n63.9\n71.8\n73.9\n74.8\n-\n54.8\n67.8\nTAU1-Airline\n53.5\n49.2\n52.0\n52.0\n-\n26.0\n46.0\nTAU2-Retail\n64.9\n71.0\n76.3\n71.3\n-\n40.4\n71.9\nTAU2-Airline\n60.0\n59.0\n70.0\n60.0\n-\n30.0\n58.0\nTAU2-Telecom\n33.3\n42.0\n60.5\n37.4\n-\n21.9\n45.6\nMultilingualism\nMultiIF\n63.5\n78.0\n80.3\n77.8\n-\n71.9\n80.6\nMMLU-ProX\n80.6\n79.0\n83.3\n84.7\n-\n80.0\n81.0\nINCLUDE\n79.4\n80.8\n86.6\n85.1\n-\n78.7\n81.0\nPolyMATH\n46.9\n48.7\n49.7\n52.2\n-\n54.7\n60.1\n* For OpenAI O4-mini and O3, we use a medium reasoning effort, except for scores marked with *, which are generated using high reasoning effort.\n# According to the official evaluation criteria of HLE, scores marked with # refer to models that are not multi-modal and were evaluated only on the text-only subset.\n$ For reproducibility, we report the win rates evaluated by GPT-4.1.\n& For highly challenging tasks (including PolyMATH and all reasoning and coding tasks), we use an output length of 81,920 tokens. For all other tasks, we set the output length to 32,768.\nQuickstart\nThe code of Qwen3-MoE has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3_moe'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-235B-A22B-Thinking-2507-FP8\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content)  # no opening <think> tag\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-Thinking-2507-FP8 --tp 4 --context-length 262144  --reasoning-parser deepseek-r1\nvLLM:vllm serve Qwen/Qwen3-235B-A22B-Thinking-2507-FP8 --tensor-parallel-size 4 --max-model-len 262144 --enable-reasoning --reasoning-parser deepseek_r1\nNote: If you encounter out-of-memory (OOM) issues, you may consider reducing the context length to a smaller value. However, since the model may require longer token sequences for reasoning, we strongly recommend using a context length greater than 131,072 when possible.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nNote on FP8\nFor convenience and performance, we have provided fp8-quantized model checkpoint for Qwen3, whose name ends with -FP8. The quantization method is fine-grained fp8 quantization with block size of 128. You can find more details in the quantization_config field in config.json.\nYou can use the Qwen3-235B-A22B-Thinking-2507-FP8 model with serveral inference frameworks, including transformers, sglang, and vllm, as the original bfloat16 model.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\n# Using Alibaba Cloud Model Studio\nllm_cfg = {\n'model': 'qwen3-235b-a22b-thinking-2507',\n'model_type': 'qwen_dashscope',\n}\n# Using OpenAI-compatible API endpoint. It is recommended to disable the reasoning and the tool call parsing\n# functionality of the deployment frameworks and let Qwen-Agent automate the related operations. For example,\n# `VLLM_USE_MODELSCOPE=true vllm serve Qwen/Qwen3-235B-A22B-Thinking-2507-FP8 --served-model-name Qwen3-235B-A22B-Thinking-2507 --tensor-parallel-size 4 --max-model-len 262144`.\n#\n# llm_cfg = {\n#     'model': 'Qwen3-235B-A22B-Thinking-2507',\n#\n#     # Use a custom endpoint compatible with OpenAI API:\n#     'model_server': 'http://localhost:8000/v1',  # api_base without reasoning and tool call parsing\n#     'api_key': 'EMPTY',\n#     'generate_cfg': {\n#         'thought_in_content': True,\n#     },\n# }\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.6, TopP=0.95, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 81,920 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nNo Thinking Content in History: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "Wan-AI/Wan2.2-T2V-A14B-Diffusers": "Wan2.2\nVideo Demos\nüî• Latest News!!\nCommunity Works\nüìë Todo List\nRun Wan2.2\nComputational Efficiency on Different GPUs\nIntroduction of Wan2.2\nCitation\nLicense Agreement\nAcknowledgements\nContact Us\nWan2.2\nüíú Wan ¬†¬† ÔΩú ¬†¬† üñ•Ô∏è GitHub ¬†¬†  | ¬†¬†ü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope¬†¬† | ¬†¬† üìë Technical Report ¬†¬† | ¬†¬† üìë Blog ¬†¬† | ¬†¬†üí¨ WeChat Group¬†¬† | ¬†¬† üìñ Discord\nWan: Open and Advanced Large-Scale Video Generative Models\nWe are excited to introduce Wan2.2, a major upgrade to our foundational video models. With Wan2.2, we have focused on incorporating the following innovations:\nüëç Effective MoE Architecture: Wan2.2 introduces a Mixture-of-Experts (MoE) architecture into video diffusion models. By separating the denoising process cross timesteps with specialized powerful expert models, this enlarges the overall model capacity while maintaining the same computational cost.\nüëç Cinematic-level Aesthetics: Wan2.2 incorporates meticulously curated aesthetic data, complete with detailed labels for lighting, composition, contrast, color tone, and more. This allows for more precise and controllable cinematic style generation, facilitating the creation of videos with customizable aesthetic preferences.\nüëç Complex Motion Generation: Compared to Wan2.1, Wan2.2 is trained on a significantly larger data, with +65.6% more images and +83.2% more videos. This expansion notably enhances the model's generalization across multiple dimensions such as motions,  semantics, and aesthetics, achieving TOP performance among all open-sourced and closed-sourced models.\nüëç Efficient High-Definition Hybrid TI2V:  Wan2.2 open-sources a 5B model built with our advanced Wan2.2-VAE that achieves a compression ratio of 16√ó16√ó4. This model supports both text-to-video and image-to-video generation at 720P resolution with 24fps and can also run on consumer-grade graphics cards like 4090. It is one of the fastest 720P@24fps models currently available, capable of serving both the industrial and academic sectors simultaneously.\nThis repository contains our T2V-A14B model, which supports generating 5s videos at both 480P and 720P resolutions. Built with a Mixture-of-Experts (MoE) architecture, it delivers outstanding video generation quality. On our new benchmark Wan-Bench 2.0, the model surpasses leading commercial models across most key evaluation dimensions.\nVideo Demos\nYour browser does not support the video tag.\nüî• Latest News!!\nJul 28, 2025: üëã We've released the inference code and model weights of Wan2.2.\nCommunity Works\nIf your research or project builds upon Wan2.1 or Wan2.2, we welcome you to share it with us so we can highlight it for the broader community.\nüìë Todo List\nWan2.2 Text-to-Video\nMulti-GPU Inference code of the A14B and 14B models\nCheckpoints of the A14B and 14B models\nComfyUI integration\nDiffusers integration\nWan2.2 Image-to-Video\nMulti-GPU Inference code of the A14B model\nCheckpoints of the A14B model\nComfyUI integration\nDiffusers integration\nWan2.2 Text-Image-to-Video\nMulti-GPU Inference code of the 5B model\nCheckpoints of the 5B model\nComfyUI integration\nDiffusers integration\nRun Wan2.2\nInstallation\nClone the repo:\ngit clone https://github.com/Wan-Video/Wan2.2.git\ncd Wan2.2\nInstall dependencies:\n# Ensure torch >= 2.4.0\npip install -r requirements.txt\nModel Download\nModels\nDownload Links\nDescription\nT2V-A14B\nü§ó Huggingface    ü§ñ ModelScope\nText-to-Video MoE model, supports 480P & 720P\nI2V-A14B\nü§ó Huggingface    ü§ñ ModelScope\nImage-to-Video MoE model, supports 480P & 720P\nTI2V-5B\nü§ó Huggingface     ü§ñ ModelScope\nHigh-compression VAE, T2V+I2V, supports 720P\nüí°Note:\nThe TI2V-5B model supports 720P video generation at 24 FPS.\nDownload models using huggingface-cli:\npip install \"huggingface_hub[cli]\"\nhuggingface-cli download Wan-AI/Wan2.2-T2V-A14B --local-dir ./Wan2.2-T2V-A14B\nDownload models using modelscope-cli:\npip install modelscope\nmodelscope download Wan-AI/Wan2.2-T2V-A14B --local_dir ./Wan2.2-T2V-A14B\nRun Text-to-Video Generation\nThis repository supports the Wan2.2-T2V-A14B Text-to-Video model and can simultaneously support video generation at 480P and 720P resolutions.\n(1) Without Prompt Extension\nTo facilitate implementation, we will start with a basic version of the inference process that skips the prompt extension step.\nSingle-GPU inference\npython generate.py  --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --offload_model True --convert_model_dtype --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\nüí° This command can run on a GPU with at least 80GB VRAM.\nüí°If you encounter OOM (Out-of-Memory) issues, you can use the --offload_model True, --convert_model_dtype and --t5_cpu options to reduce GPU memory usage.\nMulti-GPU inference using FSDP + DeepSpeed Ulysses\nWe use PyTorch FSDP and DeepSpeed Ulysses to accelerate inference.\ntorchrun --nproc_per_node=8 generate.py --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\n(2) Using Prompt Extension\nExtending the prompts can effectively enrich the details in the generated videos, further enhancing the video quality. Therefore, we recommend enabling prompt extension. We provide the following two methods for prompt extension:\nUse the Dashscope API for extension.\nApply for a dashscope.api_key in advance (EN | CN).\nConfigure the environment variable DASH_API_KEY to specify the Dashscope API key. For users of Alibaba Cloud's international site, you also need to set the environment variable DASH_API_URL to 'https://dashscope-intl.aliyuncs.com/api/v1'. For more detailed instructions, please refer to the dashscope document.\nUse the qwen-plus model for text-to-video tasks and qwen-vl-max for image-to-video tasks.\nYou can modify the model used for extension with the parameter --prompt_extend_model. For example:\nDASH_API_KEY=your_key torchrun --nproc_per_node=8 generate.py  --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage\" --use_prompt_extend --prompt_extend_method 'dashscope' --prompt_extend_target_lang 'zh'\nUsing a local model for extension.\nBy default, the Qwen model on HuggingFace is used for this extension. Users can choose Qwen models or other models based on the available GPU memory size.\nFor text-to-video tasks, you can use models like Qwen/Qwen2.5-14B-Instruct, Qwen/Qwen2.5-7B-Instruct and Qwen/Qwen2.5-3B-Instruct.\nFor image-to-video tasks, you can use models like Qwen/Qwen2.5-VL-7B-Instruct and Qwen/Qwen2.5-VL-3B-Instruct.\nLarger models generally provide better extension results but require more GPU memory.\nYou can modify the model used for extension with the parameter --prompt_extend_model , allowing you to specify either a local model path or a Hugging Face model. For example:\ntorchrun --nproc_per_node=8 generate.py  --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage\" --use_prompt_extend --prompt_extend_method 'local_qwen' --prompt_extend_target_lang 'zh'\nRunning with Diffusers\nimport torch\nimport numpy as np\nfrom diffusers import WanPipeline, AutoencoderKLWan\nfrom diffusers.utils import export_to_video, load_image\ndtype = torch.bfloat16\ndevice = \"cuda:2\"\nvae = AutoencoderKLWan.from_pretrained(\"Wan-AI/Wan2.2-T2V-A14B-Diffusers\", subfolder=\"vae\", torch_dtype=torch.float32)\npipe = WanPipeline.from_pretrained(\"Wan-AI/Wan2.2-T2V-A14B-Diffusers\", vae=vae, torch_dtype=dtype)\npipe.to(device)\nheight = 720\nwidth = 1280\nprompt = \"Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.\"\nnegative_prompt = \"Ëâ≤Ë∞ÉËâ≥‰∏ΩÔºåËøáÊõùÔºåÈùôÊÄÅÔºåÁªÜËäÇÊ®°Á≥ä‰∏çÊ∏ÖÔºåÂ≠óÂπïÔºåÈ£éÊ†ºÔºå‰ΩúÂìÅÔºåÁîª‰ΩúÔºåÁîªÈù¢ÔºåÈùôÊ≠¢ÔºåÊï¥‰ΩìÂèëÁÅ∞ÔºåÊúÄÂ∑ÆË¥®ÈáèÔºå‰ΩéË¥®ÈáèÔºåJPEGÂéãÁº©ÊÆãÁïôÔºå‰∏ëÈôãÁöÑÔºåÊÆãÁº∫ÁöÑÔºåÂ§ö‰ΩôÁöÑÊâãÊåáÔºåÁîªÂæó‰∏çÂ•ΩÁöÑÊâãÈÉ®ÔºåÁîªÂæó‰∏çÂ•ΩÁöÑËÑ∏ÈÉ®ÔºåÁï∏ÂΩ¢ÁöÑÔºåÊØÅÂÆπÁöÑÔºåÂΩ¢ÊÄÅÁï∏ÂΩ¢ÁöÑËÇ¢‰ΩìÔºåÊâãÊåáËûçÂêàÔºåÈùôÊ≠¢‰∏çÂä®ÁöÑÁîªÈù¢ÔºåÊùÇ‰π±ÁöÑËÉåÊôØÔºå‰∏âÊù°ËÖøÔºåËÉåÊôØ‰∫∫ÂæàÂ§öÔºåÂÄíÁùÄËµ∞\"\noutput = pipe(\nprompt=prompt,\nnegative_prompt=negative_prompt,\nheight=height,\nwidth=width,\nnum_frames=81,\nguidance_scale=4.0,\nguidance_scale_2=3.0,\nnum_inference_steps=40,\n).frames[0]\nexport_to_video(output, \"t2v_out.mp4\", fps=16)\nüí°Note:This model requires features that are currently available only in the main branch of diffusers. The latest stable release on PyPI does not yet include these updates.\nTo use this model, please install the library from source:\npip install git+https://github.com/huggingface/diffusers\nComputational Efficiency on Different GPUs\nWe test the computational efficiency of different Wan2.2 models on different GPUs in the following table. The results are presented in the format: Total time (s) / peak GPU memory (GB).\nThe parameter settings for the tests presented in this table are as follows:\n(1) Multi-GPU: 14B: --ulysses_size 4/8 --dit_fsdp --t5_fsdp, 5B: --ulysses_size 4/8 --offload_model True --convert_model_dtype --t5_cpu; Single-GPU: 14B: --offload_model True --convert_model_dtype, 5B: --offload_model True --convert_model_dtype --t5_cpu\n(--convert_model_dtype converts model parameter types to config.param_dtype);\n(2) The distributed testing utilizes the built-in FSDP and Ulysses implementations, with FlashAttention3 deployed on Hopper architecture GPUs;\n(3) Tests were run without the --use_prompt_extend flag;\n(4) Reported results are the average of multiple samples taken after the warm-up phase.\nIntroduction of Wan2.2\nWan2.2 builds on the foundation of Wan2.1 with notable improvements in generation quality and model capability. This upgrade is driven by a series of key technical innovations, mainly including the Mixture-of-Experts (MoE) architecture, upgraded training data, and high-compression video generation.\n(1) Mixture-of-Experts (MoE) Architecture\nWan2.2 introduces Mixture-of-Experts (MoE) architecture into the video generation diffusion model. MoE has been widely validated in large language models as an efficient approach to increase total model parameters while keeping inference cost nearly unchanged. In Wan2.2, the A14B model series adopts a two-expert design tailored to the denoising process of diffusion models: a high-noise expert for the early stages, focusing on overall layout; and a low-noise expert for the later stages, refining video details. Each expert model has about 14B parameters, resulting in a total of 27B parameters but only 14B active parameters per step, keeping inference computation and GPU memory nearly unchanged.\nThe transition point between the two experts is determined by the signal-to-noise ratio (SNR), a metric that decreases monotonically as the denoising step $t$ increases. At the beginning of the denoising process, $t$ is large and the noise level is high, so the SNR is at its minimum, denoted as ${SNR}{min}$. In this stage, the high-noise expert is activated. We define a threshold step ${t}{moe}$ corresponding to half of the ${SNR}{min}$, and switch to the low-noise expert when $t<{t}{moe}$.\nTo validate the effectiveness of the MoE architecture, four settings are compared based on their validation loss curves. The baseline Wan2.1 model does not employ the MoE architecture. Among the MoE-based variants, the Wan2.1 & High-Noise Expert reuses the Wan2.1 model as the low-noise expert while uses the  Wan2.2's high-noise expert, while the Wan2.1 & Low-Noise Expert uses Wan2.1 as the high-noise expert and employ the Wan2.2's low-noise expert. The Wan2.2 (MoE) (our final version) achieves the lowest validation loss, indicating that its generated video distribution is closest to ground-truth and exhibits superior convergence.\n(2) Efficient High-Definition Hybrid TI2V\nTo enable more efficient deployment, Wan2.2 also explores a high-compression design. In addition to the 27B MoE models, a 5B dense model, i.e., TI2V-5B, is released. It is supported by a high-compression Wan2.2-VAE, which achieves a $T\\times H\\times W$ compression ratio of $4\\times16\\times16$, increasing the overall compression rate to 64 while maintaining high-quality video reconstruction. With an additional patchification layer, the total compression ratio of TI2V-5B reaches $4\\times32\\times32$. Without specific optimization, TI2V-5B can generate a 5-second 720P video in under 9 minutes on a single consumer-grade GPU, ranking among the fastest 720P@24fps video generation models. This model also natively supports both text-to-video and image-to-video tasks within a single unified framework, covering both academic research and practical applications.\nComparisons to SOTAs\nWe compared Wan2.2 with leading closed-source commercial models on our new Wan-Bench 2.0, evaluating performance across multiple crucial dimensions. The results demonstrate that Wan2.2 achieves superior performance compared to these leading models.\nCitation\nIf you find our work helpful, please cite us.\n@article{wan2025,\ntitle={Wan: Open and Advanced Large-Scale Video Generative Models},\nauthor={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu},\njournal = {arXiv preprint arXiv:2503.20314},\nyear={2025}\n}\nLicense Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the license.\nAcknowledgements\nWe would like to thank the contributors to the SD3, Qwen, umt5-xxl, diffusers and HuggingFace repositories, for their open research.\nContact Us\nIf you would like to leave a message to our research or product teams, feel free to join our Discord or WeChat groups!",
    "QuantStack/Wan2.2-TI2V-5B-GGUF": "This GGUF file is a direct conversion of Wan-AI/Wan2.2-TI2V-5B\nSince this is a quantized model, all original licensing terms and usage restrictions remain in effect.\nUsage\nThe model can be used with the ComfyUI custom node ComfyUI-GGUF by city96\nPlace model files in ComfyUI/models/unet see the GitHub readme for further installation instructions.",
    "calcuis/wan2-gguf": "gguf quantized version of wan2.2 models\nupdate\nreference\ngguf quantized version of wan2.2 models\ndrag wan to > ./ComfyUI/models/diffusion_models\ndrag umt5xxl to > ./ComfyUI/models/text_encoders\ndrag pig to > ./ComfyUI/models/vae\nPrompt\na cute anime girl picking up a little pinky pig and moving quickly\nNegative Prompt\nblurry ugly bad\nPrompt\ndrone shot of a volcano erupting with a pig walking on it\nNegative Prompt\nblurry ugly bad\nPrompt\ndrone shot of a volcano erupting with a pig walking on it\nNegative Prompt\nblurry ugly bad\ntip: the lite lora for s2v [1.23GB], can apply to animate model also\ntip: for 5b model, use pig-wan2-vae [1.41GB]; for 14b model, please use pig-wan-vae [254MB]\nfor s2v model setup, please refer to here\nupdate\nupgrade your node (see last item from reference) for new/full quant support\nget more umt5xxl gguf encoder either here or here\nreference\nbase model from wan-ai\n4/8-step lightning lora from lightx2v\ncomfyui from comfyanonymous\ngguf-node (pypi|repo|pack)",
    "fdtn-ai/Foundation-Sec-8B-Instruct": "Foundation-Sec-8B-Instruct - Model Card\nModel Information\nIntended Use\nIntended Use Cases\nDownstream Use\nOut-of-Scope Use\nHow to Get Started with the Model\nTraining and Evaluation\nTraining Data\nTraining Setup\nEvaluation\nSafety Alignment\nLimitations\nRecommendations\nFoundation-Sec-8B-Instruct - Model Card\nModel Information\nLlama-3.1-FoundationAI-SecurityLLM-8B-Instruct (Foundation-Sec-8B-Instruct) is an open-weight, 8-billion parameter instruction-tuned language model specialized for cybersecurity applications.\nIt extends the Foundation-Sec-8B base model with instruction-following capabilities.\nIt leverages prior training to understand security concepts, terminology, and practices across multiple security domains.\nFurther instruction-tuning allows the model to interact with human users in a chat-like interface.\nFoundation-Sec-8B-Instruct enables organizations to build AI-driven security tools that can be deployed locally, reducing dependency on cloud-based AI services while maintaining high performance on security-related tasks.\nModel Name: Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct (Foundation-Sec-8B-Instruct)\nModel Developer: Foundation AI at Cisco\nModel Card Contact: https://fdtn.ai/contact\nTechnical Report: Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report\nModel Release Date: August 1st, 2025\nSupported Language(s): English\nModel Architecture: Auto-regressive language model that uses an optimized transformer architecture (Meta Llama-3.1-8B backbone)\nTraining Objective: Instruction following and alignment with human preferences\nTraining Data Status: This is a static model trained on an offline dataset. Future versions of the tuned models will be released on updated data.\nLicense: See NOTICE.md\nIntended Use\nIntended Use Cases\nFoundation-Sec-8B-Instruct is designed for security practitioners, researchers, and developers building AI-powered security workflows and applications.\nFoundation-Sec-8B-Instruct is optimized for three core use case categories:\nSOC Acceleration: Automating triage, summarization, case note generation, and evidence collection.\nProactive Threat Defense: Simulating attacks, prioritizing vulnerabilities, mapping TTPs, and modeling attacker behavior.\nEngineering Enablement: Providing security assistance, validating configurations, assessing compliance evidence, and improving security posture.\nThe model is intended for local deployment in environments prioritizing data security, regulatory compliance, and operational control.\nDownstream Use\nFoundation-Sec-8B-Instruct can be used directly for security-related chat use cases. Example downstream applications include:\nSummarization\nSummarizing detection playbooks and incident reports\nConsolidating fragmented analyst notes into structured case summaries\nClassification\nMapping threats to MITRE ATT&CK techniques\nPrioritizing vulnerabilities based on contextual risk\nClassifying security-relevant emails and leaked file contents\nNamed Entity Recognition\nExtracting compliance evidence from documents\nBuilding network behavior profiles from technical manuals\nQuestion & Answer\nAssisting SOC analysts with alert triage and investigation\nResponding to cloud security and software compliance queries\nReasoning and Text Generation\nGenerating red-team attack plans and threat models\nPredicting attacker next steps in active investigations\nEnriching vulnerability scan results with contextual insights\nFor questions or assistance with fine-tuning Foundation-Sec-8B-Instruct, please reach out to the team.\nOut-of-Scope Use\nThe following uses are out-of-scope and are neither recommended nor intended use cases:\nGenerating harmful content - The model should not be used to:\nGenerate malware or other malicious code\nCreate phishing content or social engineering scripts\nDevelop attack plans targeting specific organizations\nDesign exploitation techniques for vulnerabilities without legitimate security research purposes\nCritical security decisions without human oversight - The model should not be used for:\nAutonomous security decision-making without human review\nCritical infrastructure protection without expert supervision\nFinal determination of security compliance without human verification\nAutonomous vulnerability remediation without testing\nLegal or medical advice - The model is not qualified to provide:\nLegal advice regarding security regulations, compliance requirements, or intellectual property disputes\nLegal advice regarding security issues that would reference legal statutes, precedents, or case law necessary to provide legal advice\nMedical advice regarding health impacts of security incidents\nNon-security use cases - The model is specifically optimized for cybersecurity and may not perform as well on general tasks as models trained for broader applications.\nViolation of Laws or Regulations - Any use that violates applicable laws or regulations.\nHow to Get Started with the Model\nUse the code below to get started with the model.\nThe cookbook provides example use cases, code samples for adoption, and references.\n# Import the required libraries\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n# Load the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"fdtn-ai/Foundation-Sec-8B-Instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\"fdtn-ai/Foundation-Sec-8B-Instruct\")\nprompt = \"CVE-2015-10011 is a vulnerability about OpenDNS OpenResolve improper log output neutralization. What is the corresponding CWE?\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\nmodel_inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer(model_inputs, return_tensors=\"pt\", add_special_tokens=False)\noutput = model.generate(**inputs, temperature=0.1, max_new_tokens=250)\nresp = tokenizer.batch_decode(output)[0]\nprint(resp.replace(model_inputs, \"\"))\nTraining and Evaluation\nTraining Data\nFoundation-Sec-8B-Instruct was trained on a wide variety of public and proprietary question answer/pairs for general and security-specific instruction-following.\nData cutoff:¬†April 10th, 2025.\nA more detailed description of the methodology is available in the¬†technical report.\nTraining Setup\nFoundation-Sec-8B-Instruct is based on the¬†Llama 3.1 8B¬†architecture. Training was performed on Cisco Foundation AI‚Äôs internal compute cluster.\nKey training details:\nInstruction fine-tuning to follow human instructions\nRLHF to align model answers to human preferences\n4096-token¬†sequence length\nOptimizer:¬†AdamW\nA more detailed description of the methodology is available in the¬†technical report.\nEvaluation\nFoundation-Sec-8B-Instruct was benchmarked on cybersecurity and general reasoning tasks, using a standardized 0-shot instruction prompting setup (temperature = 0.3).\nBenchmark\nFoundation-sec-8B\nLlama 3.1 8B\nGPT-4o-mini\nCTI-MCQA\n0.644\n0.617\n0.672\nCTI-RCM\n0.692\n0.558\n0.655\nCTI-VSP\n0.802\n0.815\n0.792\nIF-Eval\n0.811\n0.791\n0.834\nAlpaca Eval 2\n35.453\n24.477\n52.720\nBenchmark Overview:\nCTI-MCQA:¬†2,500 multiple-choice questions testing cybersecurity knowledge across frameworks like MITRE ATT&CK, NIST, GDPR, and threat intelligence best practices.\nCTI-RCM: 1,000 vulnerability root cause mapping examples linking CVEs to CWE categories, assessing deep understanding of security weaknesses.\nCTI-VSP:‚ÄØA set of 1,000 CVE descriptions where models predict the CVSS v3 Base metrics and compute the overall score, with performance measured by the average absolute difference from the true scores.\nIF-Eval: 541 instruction-following prompts designed for automated, reproducible assessment of LLM instruction-following capabilities.\nAlpaca Eval 2: 805 single-turn prompts auto-scored by GPT-4 Turbo against a GPT-4 Turbo reference, validated with 20,000 human preference votes, and closely matching ChatBot Arena results.\nKey highlights:\n+3 to +11 point gains over Llama-3.1-8B-Instruct across security-specific benchmarks.\nExceptional Instruction-Following capabilities exceeding that of Llama-3.1-8B-Instruct.\nCompetitive against small Frontier Models such as GPT-4o-mini on instruction-following capabilities and cybersecurity tasks.\nFor full benchmark details and evaluation methodology, please refer to the¬†technical report.\nSafety Alignment\nStandard best practices were followed to align the model with general safety values.\nDespite the alignment, however, safe out-of-the-box performance cannot be guaranteed.\nOur evaluations show that while the model can achieve reasonable safety performance out-of-the-box, LlamaGuard provides much better protection against malicious requests.\nIt is recommended to deploy this model with additional safeguards (such as LlamaGuard) and human oversight.\nModel\nHarmBench Performance\nLlama-3.1-8b-Instruct\n72.43%\nFoundation-Sec-8B-Instruct\n91.98%\nLlamaGuard + Foundation-Sec-8B-Instruct\n99.25%\nLimitations\nFoundation-Sec-8B-Instruct has several limitations that users should be aware of:\nDomain-specific knowledge limitations:\nFoundation-Sec-8B-Instruct may not be familiar with recent vulnerabilities, exploits, or novel attack vectors or security technologies released after its training cutoff date\nKnowledge of specialized or proprietary security systems or tools may be limited\nPotential biases:\nThe model may reflect biases present in security literature and documentation\nThe model may be trained on known attack patterns and have difficulty recognizing novel attack vectors\nSecurity practices and recommendations may be biased toward certain technological ecosystems\nGeographic and cultural biases in security approaches may be present\nSecurity risks:\nThe model cannot verify the identity or intentions of users\nAdversarial prompting techniques might potentially bypass safety mechanisms\nThe model may unintentionally provide information that could be misused if proper prompting guardrails are not implemented\nContextual blindness:\nThe model may struggle to understand the complex interrelationships between systems, users, and data in order to provide accurate context.\nTechnical limitations:\nPerformance varies based on how security concepts are described in prompts\nMay not fully understand complex, multi-step security scenarios without clear explanation\nCannot access external systems or actively scan environments\nCannot independently verify factual accuracy of its outputs\nEthical considerations:\nDual-use nature of security knowledge requires careful consideration of appropriate use cases\nRecommendations\nTo address the limitations of Foundation-Sec-8B-Instruct, we recommend:\nHuman oversight:\nAlways have qualified security professionals review model outputs before implementation\nUse the model as an assistive tool rather than a replacement for expert human judgment\nImplement a human-in-the-loop approach for security-critical applications\nSystem design safeguards:\nImplement additional validation layers for applications built with this model\nConsider architectural constraints that limit the model's ability to perform potentially harmful actions (excessive agency)\nDeploy the model in environments with appropriate access controls\nPrompt engineering:\nUse carefully designed prompts that encourage ethical security practices\nInclude explicit instructions regarding responsible disclosure and ethical hacking principles\nStructure interactions to minimize the risk of inadvertently harmful outputs\nKnowledge supplementation:\nSupplement the model with up-to-date security feeds and databases\nImplement retrieval-augmented generation for current threat intelligence sources\nUsage policies:\nDevelop and enforce clear acceptable use policies for applications using this model\nImplement monitoring and auditing for high-risk applications\nCreate documentation for end users about the model's limitations",
    "arcee-ai/AFM-4.5B": "AFM-4.5B\nModel Details\nBenchmarks\nHow to use with transformers\nHow to use with vllm\nHow to use with Together API\nPython (Official Together SDK)\ncURL\nQuantization support\nLicense\nAFM-4.5B\nAFM-4.5B is a 4.5 billion parameter instruction-tuned model developed by Arcee.ai, designed for enterprise-grade performance across diverse deployment environments from cloud to edge. The base model was trained on a dataset of 8 trillion tokens, comprising 6.5 trillion tokens of general pretraining data followed by 1.5 trillion tokens of midtraining data with enhanced focus on mathematical reasoning and code generation. Following pretraining, the model underwent supervised fine-tuning on high-quality instruction datasets. The instruction-tuned model was further refined through reinforcement learning on verifiable rewards as well as for human preference. We use a modified version of TorchTitan for pretraining, Axolotl for supervised fine-tuning, and a modified version of Verifiers for reinforcement learning.\nThe development of AFM-4.5B prioritized data quality as a fundamental requirement for achieving robust model performance. We collaborated with DatologyAI, a company specializing in large-scale data curation. DatologyAI's curation pipeline integrates a suite of proprietary algorithms‚Äîmodel-based quality filtering, embedding-based curation, target distribution-matching, source mixing, and synthetic data. Their expertise enabled the creation of a curated dataset tailored to support strong real-world performance.\nThe model architecture follows a standard transformer decoder-only design based on Vaswani et al., incorporating several key modifications for enhanced performance and efficiency. Notable architectural features include grouped query attention for improved inference efficiency and ReLU^2 activation functions instead of SwiGLU to enable sparsification while maintaining or exceeding performance benchmarks.\nThe model available in this repo is the instruct model following supervised fine-tuning and reinforcement learning.\nView our documentation here for more details: https://docs.arcee.ai/arcee-foundation-models/introduction-to-arcee-foundation-models\nModel Details\nModel Architecture: ArceeForCausalLM\nParameters: 4.5B\nTraining Tokens: 8T\nLicense: Apache 2.0\nRecommended settings:\ntemperature: 0.5\ntop_k: 50\ntop_p: 0.95\nrepeat_penalty: 1.1\nBenchmarks\n*Qwen3 and SmolLM's reasoning approach causes their scores to vary wildly from suite to suite - but these are all scores on our internal harness with the same hyperparameters. Be sure to reference their reported scores. SmolLM just released its bench.\nHow to use with transformers\nYou can use the model directly with the transformers library.\nWe recommend a lower temperature, around 0.5, for optimal performance.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nmodel_id = \"arcee-ai/AFM-4.5B\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\"\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\ninput_ids = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\"\n).to(model.device)\noutputs = model.generate(\ninput_ids,\nmax_new_tokens=256,\ndo_sample=True,\ntemperature=0.5,\ntop_k=50,\ntop_p=0.95\n)\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)\nHow to use with vllm\nEnsure you are on version 0.10.1 or newer\npip install vllm>=0.10.1\nYou can then serve the model natively\nvllm serve arcee-ai/AFM-4.5B\nHow to use with Together API\nYou can access this model directly via the Together Playground.\nPython (Official Together SDK)\nfrom together import Together\nclient = Together()\nresponse = client.chat.completions.create(\nmodel=\"arcee-ai/AFM-4.5B\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": \"What are some fun things to do in New York?\"\n}\n]\n)\nprint(response.choices[0].message.content)\ncURL\ncurl -X POST \"https://api.together.xyz/v1/chat/completions\" \\\n-H \"Authorization: Bearer $TOGETHER_API_KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"arcee-ai/AFM-4.5B\",\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": \"What are some fun things to do in New York?\"\n}\n]\n}'\nQuantization support\nSupport for llama.cpp and Intel OpenVINO is available:\nhttps://huggingface.co/arcee-ai/AFM-4.5B-GGUF\nhttps://huggingface.co/arcee-ai/AFM-4.5B-ov\nLicense\nAFM-4.5B is released under the Apache-2.0 license.",
    "Piero2411/YOLOV8s-Barcode-Detection": "Documentation & Usage\nCLI\nPython\nModel Details\nDataset\nTraining Parameters\nPerformance and Results\nKey Metrics from Training (Last Epoch)\nVisual Results\nExample Prediction Batches\nLicense\nYOLOV8s Barcode/QR Code Detection Model\nSpecial thanks to Armaggheddon97 and his 4090\nThis repository hosts a finetuned Ultralytics YOLOv8s model specifically designed for barcode and QR code classification and detection. Building upon the robust and efficient architecture of YOLOv8, this model has been fine-tuned on a comprehensive dataset of over 5,000 images, encompassing various barcode types (EAN13, Code128, etc.) and QR codes.\nYOLOv8 is renowned for its speed, accuracy, and ease of use, making it an excellent foundation for specialized object detection tasks like barcode and QR code recognition. This finetuned model aims to provide a reliable and performant solution for applications requiring automated barcode and QR code scanning and identification.\nWe hope this model proves useful for your projects! For general YOLOv8 documentation, please refer to the Ultralytics Docs. For specific questions or discussions related to this finetuned model, please raise an issue on its Hugging Face model page.\nDocumentation & Usage\nBelow is a quickstart guide for installing the necessary dependencies and using this finetuned model.\nInstall\nTo use this model, you'll need the ultralytics package, which can be installed via pip. Ensure you have a Python>=3.8 environment with PyTorch>=1.8.\npip install ultralytics\nFor alternative installation methods or more detailed instructions on setting up your environment, please refer to the Ultralytics Quickstart Guide.\nUsage\nCLI\nYou can use this model directly from the Command Line Interface (CLI) using the yolo command. Make sure the YOLOV8s_Barcode_Detection.pt model file is accessible in your current directory or specified with its full path.\nyolo predict model=YOLOV8s_Barcode_Detection.pt source='path/to/your/image.jpg'\nThe yolo command offers various modes and arguments (e.g., imgsz=640). For more details, consult the YOLOv8 CLI Docs.\nPython\nThis model can also be easily integrated into your Python applications. The usage mirrors the standard Ultralytics YOLO API.\nfrom ultralytics import YOLO\n# Load your finetuned model\nmodel = YOLO(\"YOLOV8s_Barcode_Detection.pt\")\n# Perform object detection on an image\nresults = model(\"path/to/image.jpg\")\n# Optionally, visualize the results\nresults[0].show()\n# You can also export the model to other formats (e.g., ONNX)\n# This is useful for deployment in various environments.\n# path = model.export(format=\"onnx\") # uncomment to export\nFor more advanced Python usage, including training, validation, and different prediction modes, refer to the YOLOv8 Python Docs.\nModel Details\nThis specific model, YOLOV8s_Barcode_Detection.pt, is a finetuned instance of the YOLOv8s (small) detection model. It has been specifically trained for barcode and QR code classification and detection.\nDataset\nThe model was finetuned on a custom dataset comprising over 5,000 images. This dataset includes a wide variety of barcode types such as EAN13, Code128, and many more, alongside various QR code instances, ensuring robustness across different real-world scenarios.\nTraining Parameters\nThe training process leveraged a pre-trained yolov8s.pt checkpoint and was conducted with the following key parameters:\n# Base model used for finetuning\nmodel = YOLO('yolov8s.pt')\n# Training parameters\nresults = model.train(\ndata='dataset/data_autosplit.yaml', # Path to the custom dataset\nepochs=30,           # Number of training epochs\npatience=10,         # Early stopping patience\nbatch=16,            # Batch size (adjust based on GPU memory)\nimgsz=640,           # Image size for training\nname='yolov8_large_dataset_v1', # Run name\n# Augmentation parameters\ndegrees=180,         # Random rotation (0-180 degrees)\ntranslate=0.1,       # Image translation (0-1)\nscale=0.5,           # Image scaling (0-1)\nshear=10,            # Image shearing (degrees)\nperspective=0.001,   # Image perspective (0-0.001)\nfliplr=0.5,          # Horizontal flip (probability)\nflipud=0.5,          # Vertical flip (probability)\ncutmix=0.2,          # CutMix augmentation (probability)\n)\nThe combination of a robust base model, a diverse custom dataset, and carefully selected training parameters aims to provide high accuracy and generalization for barcode and QR code detection across various real-world scenarios.\nWhile the table below provides a general overview of the YOLOv8s base model's performance on the COCO dataset, which serves as a reference for its architectural capabilities. Please note that the performance metrics for YOLOV8s_Barcode_Detection.pt will differ, reflecting its specialized training on barcode and QR code data.\nBase YOLOv8s Detection (COCO Reference)\nSee Detection Docs for usage examples with these models trained on COCO, which include 80 pre-trained classes.\nModel\nsize(pixels)\nmAPval50-95\nSpeedCPU ONNX(ms)\nSpeedA100 TensorRT(ms)\nparams(M)\nFLOPs(B)\nYOLOv8s\n640\n44.9\n128.4\n1.20\n11.2\n28.6\nmAPval values are for single-model single-scale on COCO val2017 dataset. Reproduce by yolo val detect data=coco.yaml device=0\nSpeed averaged over COCO val images using an Amazon EC2 P4d instance. Reproduce by yolo val detect data=coco.yaml batch=1 device=0|cpu\nPerformance and Results\nBelow are the key performance metrics and visualizations from the training and validation of the YOLOV8s_Barcode_Detection.pt model.\nKey Metrics from Training (Last Epoch)\nMetric\nValue\nmetrics/precision(B)\n0.97545\nmetrics/recall(B)\n0.99127\nmetrics/mAP50(B)\n0.98643\nmetrics/mAP50-95(B)\n0.77643\nval/box_loss\n0.988\nval/cls_loss\n0.43007\nval/dfl_loss\n1.11172\nThese metrics demonstrate the model's high precision and recall, especially at an IoU threshold of 0.5 (mAP50), indicating excellent performance in detecting both barcode and QR code instances.\nVisual Results\nHere are various plots generated during the training process, providing deeper insights into the model's performance and behavior:\nPlot Type\nDescription\nImage\nF1-Confidence Curve\nThis curve shows the F1 score across different confidence thresholds for both barcode and QR code classes, as well as the overall F1 score. The peak F1 of 0.98 is achieved at a confidence of 0.553.\nPrecision-Confidence Curve\nThis plot illustrates how precision changes with varying confidence thresholds. A high precision of 1.00 is achieved at a confidence of 0.928.\nPrecision-Recall Curve\nThe Precision-Recall curve highlights the trade-off between precision and recall. The model achieved an impressive overall mAP@0.5 of 0.986, with individual class scores of 0.979 for barcode and 0.992 for qrcode.\nRecall-Confidence Curve\nThis curve shows the recall performance at different confidence thresholds.\nConfusion Matrix (Normalized)\nThe normalized confusion matrix shows the proportion of true positives, false positives, and false negatives for each class, normalized by the true class count.\nConfusion Matrix (Absolute Counts)\nThe absolute confusion matrix shows the raw counts of true positives, false positives, and false negatives for each class.\nDataset Distribution\nThis plot provides insights into the distribution of instances within the dataset, including class counts, bounding box shapes, and centroid locations.\nTraining & Validation Curves\nThese plots track the training and validation losses (box, classification, DFL) and performance metrics (precision, recall, mAP) across all epochs, demonstrating the model's learning progression.\nExample Prediction Batches\nVisual examples of the model's predictions on validation images, showing detected bounding boxes and class labels (before and after confidence thresholding for clarity).\nLabeled Predictions (Ground Truth)\nPredicted Bounding Boxes (with Confidence)\nLicense\nUltralytics offers two licensing options to accommodate diverse use cases:\nAGPL-3.0 License: This OSI-approved open-source license is ideal for students and enthusiasts, promoting open collaboration and knowledge sharing. See the LICENSE file for more details.\n‚Äì‚Äì‚Äì",
    "sbintuitions/sarashina-embedding-v2-1b": "Sarashina-Embedding-v2-1B\nModel Details\nModel Description\nFull Model Architecture\nUsage\nHow to add instructions and prefixes\nTemplates for instructions and prefixes\nTraining\nStage 1: Weakly-supervised Learning\nStep2: Supervised Fine-tuning\nStage 3: Model Merging\nEvaluation Results (*) with JMTEB\nLicense\nSarashina-Embedding-v2-1B\nÊó•Êú¨Ë™û„ÅÆREADME/Japanese README\n\"Sarashina-Embedding-v2-1B\" is a Japanese text embedding model, based on the Japanese LLM \"Sarashina2.2-1B\".\nWe trained this model with multi-stage contrastive learning. We achieved the state-of-the-art average score across 28 datasets in  JMTEB (Japanese Massive Text Embedding Benchmark).(Benchmarked on July 28, 2025. )\nThis model maps sentences & paragraphs to a 1792-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and other applications.\nModel Details\nModel Description\nModel Type: Sentence Transformer\nBase model: Sarashina2.2-1B\nMaximum Sequence Length: 8,192 tokens\nOutput Dimensionality: 1,792 dimensions\nSimilarity Function: Cosine Similarity\nLanguage:  Japanese\nLicense: Sarashina Model NonCommercial License Agreement\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 8192, 'do_lower_case': False}) with Transformer model: LlamaModel\n(1): Pooling({'word_embedding_dimension': 1792, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': True, 'include_prompt': False})\n)\nUsage\nFirst install the Sentence Transformers library:\npip install sentence-transformers==4.0.2\nThen you can load this model and run inference.\nfrom sentence_transformers import SentenceTransformer\n# Download from the ü§ó Hub\nmodel = SentenceTransformer(\"sbintuitions/sarashina-embedding-v2-1b\")\n# Run inference\nquery = [\n'task: „ÇØ„Ç®„É™„Çí‰∏é„Åà„Çã„ÅÆ„Åß„ÄÅ‰∏é„Åà„Çâ„Çå„ÅüWebÊ§úÁ¥¢„ÇØ„Ç®„É™„Å´Á≠î„Åà„ÇãÈñ¢ÈÄ£ÊñáÁ´†„ÇíÊ§úÁ¥¢„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\\nquery: Sarashina„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàÂüã„ÇÅËæº„Åø„É¢„Éá„É´„ÅØ„ÅÇ„Çä„Åæ„Åô„Åã?'\n]\ntexts = [\n'text: Êõ¥Á¥öÊó•Ë®ò„ÅØ„ÄÅÂπ≥ÂÆâÊôÇ‰ª£‰∏≠Êúü„Å´ËèÖÂéüÂ≠ùÊ®ôÂ•≥„Å´„Çà„Å£„Å¶Êõ∏„Åã„Çå„ÅüÂõûÊÉ≥Èå≤„Åß„Åô„ÄÇ',\n'text: Sarashina„ÅØ„ÄÅSB Intuitions„ÅåÈñãÁô∫„Åó„ÅüÊó•Êú¨Ë™ûÂ§ßË¶èÊ®°Ë®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ„Åì„Çå„Åæ„Åß„Å´7B, 13B, 70B, 8x70B„ÅÆ„É¢„Éá„É´„ÅåÂÖ¨Èñã„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ',\n'text: „Çµ„É©„Ç∑„Éä„Ç®„É≥„Éô„Éá„Ç£„É≥„Ç∞„ÅØÊó•Êú¨Ë™ûË®ÄË™û„É¢„Éá„É´„Çí„Éô„Éº„Çπ„Å´„Åó„ÅüÊó•Êú¨Ë™ûÂüã„ÇÅËæº„Åø„É¢„Éá„É´„Åß„Åô„ÄÇ'\n]\nquery_embedding = model.encode(query)\ntext_embeddings = model.encode(texts)\n# Get the similarity scores between the embeddings\nsimilarities = model.similarity(query_embedding, text_embeddings)\nprint(similarities)\n# tensor([[0.7403, 0.8651, 0.8775]])\nHow to add instructions and prefixes\nFor both the query and document sides, use different prefix formats. On the query side, add the prefix task: followed by instructions. (Only for STS task, both sentences are considered as query, and should be prefixed with the same instruction.)\nQuery Side: task: {Instrcution}\\nquery: {Query}\nDocument Side: text: {Document}\nTemplates for instructions and prefixes\nThe table below provides instruction and prefix templates for five main tasks.\nTask\nQuery Side\nDocument Side\nRetrievalReranking\ntask: Ë≥™Âïè„Çí‰∏é„Åà„Çã„ÅÆ„Åß„ÄÅ„Åù„ÅÆË≥™Âïè„Å´Á≠î„Åà„Çã„ÅÆ„Å´ÂΩπÁ´ã„Å§Èñ¢ÈÄ£ÊñáÊõ∏„ÇíÊ§úÁ¥¢„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\\nquery:\ntext:\nClustering\ntask: ‰∏é„Åà„Çâ„Çå„Åü„Éâ„Ç≠„É•„É°„É≥„Éà„ÅÆ„Éà„Éî„ÉÉ„ÇØ„Åæ„Åü„ÅØ„ÉÜ„Éº„Éû„ÇíÁâπÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\\nquery:\n-\nClassification\ntask: ‰∏é„Åà„Çâ„Çå„Åü„É¨„Éì„É•„Éº„ÇíÈÅ©Âàá„Å™Ë©ï‰æ°„Ç´„ÉÜ„Ç¥„É™„Å´ÂàÜÈ°û„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\\nquery:\n-\nSTS\ntask: „ÇØ„Ç®„É™„Çí‰∏é„Åà„Çã„ÅÆ„ÅßÔºå„ÇÇ„Å£„Å®„ÇÇ„ÇØ„Ç®„É™„Å´ÊÑèÂë≥„Åå‰ºº„Å¶„ÅÑ„Çã‰∏ÄÁØÄ„ÇíÊé¢„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\\nquery:\ntask: „ÇØ„Ç®„É™„Çí‰∏é„Åà„Çã„ÅÆ„ÅßÔºå„ÇÇ„Å£„Å®„ÇÇ„ÇØ„Ç®„É™„Å´ÊÑèÂë≥„Åå‰ºº„Å¶„ÅÑ„Çã‰∏ÄÁØÄ„ÇíÊé¢„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\\nquery:\nTraining\nSarashina-Embedding-v2-1B is created through the following three-stage learning process:\nStage 1: Weakly-supervised Learning\nTo build a general-purpose and high-performance embedding model for a wide range of domains, we employed contrastive learning using weak supervision data, which consists of our own web-crawled data and open datasets.\nStep2: Supervised Fine-tuning\nTo further train the model to better understand the similarity between queries and documents, we performed fine-tuning using higher-quality data than that used in Stage 1. Additionally, we trained multiple models by modifying parts of the data.\nStage 3: Model Merging\nTo enhance performance, we merged the weights of the two models that yielded the highest JMTEB scores in Stage 2 through linear merging.\nEvaluation Results (*) with JMTEB\nModel\nAvg.\nRetrieval\nSTS\nClassification\nReranking\nClustering\nSarashina-Embedding-v2-1B (This model)\n76.38\n76.48\n84.22\n77.14\n86.28\n52.56\ncl-nagoya/ruri-v3-310m\n75.85\n76.03\n81.59\n77.65\n85.84\n50.52\nsbintuitions/sarashina-embedding-v1-1b\n74.87\n74.53\n81.71\n77.20\n84.36\n50.30\nOpenAI/text-embedding-3-large\n73.86\n71.95\n82.52\n77.27\n83.06\n51.82\n(*) Evaluated on July 28, 2025.\nLicense\nThis model is licensed under Sarashina Model NonCommercial License Agreement.\nIf you are interested in using this model for commercial purposes, please feel free to contact us through our contact page.",
    "QuantTrio/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8": "Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8\n„ÄêvLLM 4-GPU Single Node Launch Command„Äë\n„ÄêDependencies„Äë\n„ÄêModel Update Date„Äë\n„ÄêModel Files„Äë\n„ÄêModel Download„Äë\n„ÄêOverview„Äë\nQwen3-30B-A3B-Instruct-2507\nHighlights\nModel Overview\nPerformance\nQuickstart\nAgentic Use\nBest Practices\nCitation\nQwen3-30B-A3B-Instruct-2507-GPTQ-Int8\nBase model: Qwen/Qwen3-30B-A3B-Instruct-2507\n„ÄêvLLM 4-GPU Single Node Launch Command„Äë\nNote: When using 4 GPUs, you must include --enable-expert-parallel because expert tensor TP must be evenly divisible; for 2 GPUs this is not necessary.\nCONTEXT_LENGTH=32768  # 262144\nvllm serve \\\nQuantTrio/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8 \\\n--served-model-name Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8 \\\n--enable-expert-parallel \\\n--swap-space 16 \\\n--max-num-seqs 512 \\\n--max-model-len $CONTEXT_LENGTH \\\n--max-seq-len-to-capture $CONTEXT_LENGTH \\\n--gpu-memory-utilization 0.9 \\\n--tensor-parallel-size 4 \\\n--trust-remote-code \\\n--disable-log-requests \\\n--host 0.0.0.0 \\\n--port 8000\n„ÄêDependencies„Äë\nvllm>=0.9.2\n„ÄêModel Update Date„Äë\n2025-08-19\n1.[BugFix] Fix compatibility issues with vLLM 0.10.1\n2025-07-30\n1. Initial commit\n„ÄêModel Files„Äë\nFile Size\nLast Updated\n30GB\n2025-07-30\n„ÄêModel Download„Äë\nfrom huggingface_hub import snapshot_download\nsnapshot_download('QuantTrio/Qwen3-30B-A3B-Instruct-2507-GPTQ-Int8', cache_dir=\"your_local_path\")\n„ÄêOverview„Äë\nQwen3-30B-A3B-Instruct-2507\nHighlights\nWe introduce the updated version of the Qwen3-30B-A3B non-thinking mode, named Qwen3-30B-A3B-Instruct-2507, featuring the following key enhancements:\nSignificant improvements in general capabilities, including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage.\nSubstantial gains in long-tail knowledge coverage across multiple languages.\nMarkedly better alignment with user preferences in subjective and open-ended tasks, enabling more helpful responses and higher-quality text generation.\nEnhanced capabilities in 256K long-context understanding.\nModel Overview\nQwen3-30B-A3B-Instruct-2507 has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 30.5B in total and 3.3B activated\nNumber of Paramaters (Non-Embedding): 29.9B\nNumber of Layers: 48\nNumber of Attention Heads (GQA): 32 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 262,144 natively.\nNOTE: This model supports only non-thinking mode and does not generate <think></think> blocks in its output. Meanwhile, specifying enable_thinking=False is no longer required.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nPerformance\nDeepseek-V3-0324\nGPT-4o-0327\nGemini-2.5-Flash Non-Thinking\nQwen3-235B-A22B Non-Thinking\nQwen3-30B-A3B Non-Thinking\nQwen3-30B-A3B-Instruct-2507\nKnowledge\nMMLU-Pro\n81.2\n79.8\n81.1\n75.2\n69.1\n78.4\nMMLU-Redux\n90.4\n91.3\n90.6\n89.2\n84.1\n89.3\nGPQA\n68.4\n66.9\n78.3\n62.9\n54.8\n70.4\nSuperGPQA\n57.3\n51.0\n54.6\n48.2\n42.2\n53.4\nReasoning\nAIME25\n46.6\n26.7\n61.6\n24.7\n21.6\n61.3\nHMMT25\n27.5\n7.9\n45.8\n10.0\n12.0\n43.0\nZebraLogic\n83.4\n52.6\n57.9\n37.7\n33.2\n90.0\nLiveBench 20241125\n66.9\n63.7\n69.1\n62.5\n59.4\n69.0\nCoding\nLiveCodeBench v6 (25.02-25.05)\n45.2\n35.8\n40.1\n32.9\n29.0\n43.2\nMultiPL-E\n82.2\n82.7\n77.7\n79.3\n74.6\n83.8\nAider-Polyglot\n55.1\n45.3\n44.0\n59.6\n24.4\n35.6\nAlignment\nIFEval\n82.3\n83.9\n84.3\n83.2\n83.7\n84.7\nArena-Hard v2*\n45.6\n61.9\n58.3\n52.0\n24.8\n69.0\nCreative Writing v3\n81.6\n84.9\n84.6\n80.4\n68.1\n86.0\nWritingBench\n74.5\n75.5\n80.5\n77.0\n72.2\n85.5\nAgent\nBFCL-v3\n64.7\n66.5\n66.1\n68.0\n58.6\n65.1\nTAU1-Retail\n49.6\n60.3#\n65.2\n65.2\n38.3\n59.1\nTAU1-Airline\n32.0\n42.8#\n48.0\n32.0\n18.0\n40.0\nTAU2-Retail\n71.1\n66.7#\n64.3\n64.9\n31.6\n57.0\nTAU2-Airline\n36.0\n42.0#\n42.5\n36.0\n18.0\n38.0\nTAU2-Telecom\n34.0\n29.8#\n16.9\n24.6\n18.4\n12.3\nMultilingualism\nMultiIF\n66.5\n70.4\n69.4\n70.2\n70.8\n67.9\nMMLU-ProX\n75.8\n76.2\n78.3\n73.2\n65.1\n72.0\nINCLUDE\n80.1\n82.1\n83.8\n75.6\n67.8\n71.9\nPolyMATH\n32.2\n25.5\n41.9\n27.0\n23.3\n43.1\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\n#: Results were generated using GPT-4o-20241120, as access to the native function calling API of GPT-4o-0327 was unavailable.\nQuickstart\nThe code of Qwen3-MoE has been in the latest Hugging Face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3_moe'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-30B-A3B-Instruct-2507\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=16384\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"content:\", content)\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Instruct-2507 --context-length 262144\nvLLM:vllm serve Qwen/Qwen3-30B-A3B-Instruct-2507 --max-model-len 262144\nNote: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as 32,768.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-30B-A3B-Instruct-2507',\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}",
    "nvidia/Audio2Emotion-v2.2": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nBy clicking ‚ÄúAgree and Access,‚Äù you agree that you will use the NVIDIA Audio2Emotion Model consistent with the License Agreement for NVIDIA Audio2Emotion Model, which allows you to use the Model only with the NVIDIA Audio2Face Project and prohibits use of the Model or any of its components for emotion recognition.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nBias\nExplainability\nPrivacy\nSafety & Security\nCitation\nModel Overview\nDescription\nThis model is a speech emotion recognition (SER) classifier that can predict six emotions from speech: anger, disgust, fear, joy, neutral, and sadness. It is based on the Wav2Vec2 architecture and is trained to classify emotions in a sequence of audio frames. This model is ready for commercial/non-commercial use.\nFor source code, documentation, helper scripts, packaged builds, and links to all components in the Audio2Face-3D technology stack, visit the Audio2Face-3D GitHub repository\nLicense/Terms of Use\nUse of this model is governed by the License Agreement for NVIDIA Audio2Emotion Model for Use with Audio2Face Project\nDeployment Geography:\nGlobal\nUse Case:\nIMPORTANT: This Model and any technology included with this Model may only be used in connection with the NVIDIA Audio2Face project (https://docs.omniverse.nvidia.com/audio2face/latest/overview.html) consistent with all applicable documentation. You may not use this Model and any technology included with it outside of the Audio2Emotion model outside the Audio2Face project. You may not use this Model or any of its components for the purpose of emotion recognition.\nThis speech emotion recognition model is specifically designed and optimized for the NVIDIA Audio2Face project to generate realistic facial expressions for 3D characters. The model's primary and intended use case is converting speech audio into emotional states that drive realistic 3D facial animations. The model is not intended for standalone emotion recognition applications or general-purpose audio analysis. It has been specifically trained and optimized to work as a component within the Audio2Face pipeline to produce high-quality, emotionally accurate 3D facial expressions that enhance the realism of virtual characters and digital humans.\nRelease Date:\nRelease Date: 09/24/2025 HuggingFace\nModel Architecture\nArchitecture Type: Transformer\nNetwork Architecture: Wav2Vec2\nThis model was developed based on: Wav2Vec2-Large-LV60\nNumber of model parameters: 3.1 x 10^8\nInput\nInput Type(s): Audio\nInput Format(s): Raw audio input - an array of float32\nInput Parameters: 2D\nOther Properties Related to Input: A batch of input waveforms for classification\nOutput\nOutput Type(s): Probabilities of emotional classes\nOutput Format: An array of float32\nOutput Parameters: 2D\nOther Properties Related to Output: The model can predict six emotions from speech: Anger, disgust, fear, joy, neutral, and sadness.\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems [or name equivalent hardware preference]. By leveraging NVIDIA‚Äôs hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nSoftware Integration\nRuntime Engine(s)\nNeMo - 1.0.0\nSupported Hardware Microarchitecture Compatibility\nNVIDIA Ampere\nNVIDIA Blackwell\nNVIDIA Hopper\nNVIDIA Lovelace\nNVIDIA Pascal\nNVIDIA Turing\n[Preferred/Supported] Operating System(s)\nLinux\nThe integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment.This AI model can be embedded as an Application Programming Interface (API) call into the software environment described above.\nModel Version(s)\nAudio2Emotion-v2.2\nTraining, Testing, and Evaluation Datasets\nTraining Dataset\nData Modality\nAudio\nAudio Training Data Size\nLess than 10,000 Hours\nLink\nInternal datasets\nRAVDESS\nCREMA-D\nJL Corpus\nEMO-DB\nEmozionalmente\nData Collection Method by dataset\nAutomated\nLabeling Method by dataset\nHuman\nProperties (Quantity, Dataset Descriptions, Sensor(s))\nMultiple datasets, including RAVDESS, CREMA-D, JL, EMO-DB, Emozionalmente, TTS GPT 4o (internal), Lindy & Rodney (internal)\nQuantity: 30029 samples\nTesting Dataset\nLink\nInternal dataset\nData Collection Method by dataset\nAutomated\nLabeling Method by dataset\nHuman\nProperties (Quantity, Dataset Descriptions, Sensor(s))\nInternal crowdsourced dataset\nQuantity: 1350 samples\nEvaluation Dataset\nLink\nInternal dataset\nData Collection Method by dataset\nAutomated\nLabeling Method by dataset\nHuman\nProperties (Quantity, Dataset Descriptions, Sensor(s))\nInternal crowdsourced dataset\nQuantity: 1350 samples\nInference\nEngine\nTensor(RT)\nTest Hardware\nT4, T10, A10, A40, L4, L40S, A100\nRTX 6000ADA, A6000, Pro 6000 Blackwell\nRTX 3080, 3090, 4080, 4090, 5090\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nFor more detailed information on ethical considerations for this model, please see the Model Card++ Bias, Explainability, Safety & Security, and Privacy Subcards.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nThis Model and any technology included with this Model may only be used in connection with the NVIDIA Audio2Face project (https://docs.omniverse.nvidia.com/audio2face/latest/overview.html) consistent with all applicable documentation. You may not use this Model and any technology included with it outside of the Audio2Emotion model outside the Audio2Face project. You may not use this Model or any of its components for the purpose of emotion recognition.\nBias\nField\nResponse\nParticipation considerations from adversely impacted groups protected classes in model design and testing:\nAge, Gender, Linguistic Background, Accent, Speech Patterns, and Cultural Context\nMeasures taken to mitigate against unwanted bias:\nTraining data includes diverse speakers across multiple datasets (RAVDESS, CREMA-D, JL, Lindy & Rodney, EMO-DB, Emozionalmente, TTS GPT 4o) to reduce demographic bias\nExplainability\nField\nResponse\nIntended Task/Domain:\nSpeech Emotion Recognition, Audio Analysis, Human-Computer Interaction, and Audio2Face Integration\nModel Type:\nSpeech emotion recognition classifier\nIntended Users:\nAudio2Face developers, Speech analysis researchers, Human-computer interaction developers, Affective computing researchers\nOutput:\nEmotion probabilities (six classes: anger, disgust, fear, joy, neutral, and sadness)\nDescribe how the model works:\nAudio input is processed through Wav2Vec2 architecture to classify emotions from speech, outputting probability scores for six emotional states\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:\nPeople with speech disorders or non-native accents, Non-English speakers or those with strong regional accents, Elderly individuals with age-related speech changes\nTechnical Limitations & Mitigation:\nModel requires clear audio input at 16kHz sampling rate, may struggle with overlapping speech or very noisy environments\nVerified to have met prescribed NVIDIA quality standards:\nYes - Model achieves high accuracy on clean audio inputs, validated on internal crowdsourced dataset\nPerformance Metrics:\nAccuracy (Top-1) - 80%+ on clean audio, Throughput & Latency, Emotion classification confidence scores\nPotential Known Risks:\nModel may misclassify emotions in edge cases, should not be used for standalone emotion analysis without Audio2Face integration\nLicensing:\nUse of this model is governed by the License Agreement for NVIDIA Audio2Emotion Model for Use with Audio2Face Project\nPrivacy\nField\nResponse\nGeneratable or reverse engineerable personal data?\nEmotion classification probabilities from audio input\nPersonal data used to create this model?\nYes - Audio recordings containing human speech and emotional expressions\nWas consent obtained for any personal data used?\nYes\nHow often is dataset reviewed?\nBefore Every Release\nIs a mechanism in place to honor data subject right of access or deletion of personal data?\nYes\nIf personal data was collected for the development of the model, was it collected directly by NVIDIA?\nYes\nIf personal data was collected for the development of the model by NVIDIA, do you maintain or have access to disclosures made to data subjects?\nYes\nIf personal data was collected for the development of this AI model, was it minimized to only what was required?\nYes - Only audio features necessary for emotion recognition are processed\nIs there provenance for all datasets used in training?\nYes\nDoes data labeling (annotation, metadata) comply with privacy laws?\nYes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?\nYes\nApplicable Privacy Policy\nhttps://www.nvidia.com/en-us/about-nvidia/privacy-policy/\nSafety & Security\nField\nResponse\nModel Application Field(s):\nSpeech emotion recognition for driving Audio2Face 3D facial animations\nDescribe the life critical impact (if present).\nNot Applicable - Model is designed for entertainment and communication applications, not life-critical systems\nUse Case Restrictions:\nThis Model and any technology included with this Model may only be used in connection with the NVIDIA Audio2Face project (https://docs.omniverse.nvidia.com/audio2face/latest/overview.html) consistent with all applicable documentation. You may not use this Model and any technology included with it outside of the Audio2Emotion model outside the Audio2Face project. You may not use this Model or any of its components for the purpose of emotion recognition. Abide by License Agreement for NVIDIA Audio2Emotion Model for Use with Audio2Face Project.\nModel and dataset restrictions:\nThe Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training, and dataset license constraints adhered to.\nCitation\n@misc{nvidia2025audio2face3d,\ntitle={Audio2Face-3D: Audio-driven Realistic Facial Animation For Digital Avatars},\nauthor={Chaeyeon Chung and Ilya Fedorov and Michael Huang and Aleksey Karmanov and Dmitry Korobchenko and Roger Ribera and Yeongho Seol},\nyear={2025},\neprint={2508.16401},\narchivePrefix={arXiv},\nprimaryClass={cs.GR},\nurl={https://arxiv.org/abs/2508.16401},\nnote={Authors listed in alphabetical order}\n}",
    "facebook/dinov3-vit7b16-pretrain-lvd1689m": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Card for DINOv3\nModel Details\nModel Description\nModel Sources\nUses\nDirect Use\nDownstream Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nEnvironmental Impact\nTechnical Specifications\nModel Architecture and Objective\nCompute Infrastructure\nMore Information\nCitation\nModel Card for DINOv3\nDINOv3 is a family of versatile vision foundation models that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models.\nModel Details\nThese are Vision Transformer and ConvNeXt models trained following the method described in the DINOv3 paper. 12 models are provided:\n10 models pretrained on web data (LVD-1689M dataset)\n1 ViT-7B trained from scratch,\n5 ViT-S/S+/B/L/H+ models distilled from the ViT-7B,\n4 ConvNeXt-{T/S/B/L} models distilled from the ViT-7B,\n2 models pretrained on satellite data (SAT-493M dataset)\n1 ViT-7B trained from scratch\n1 ViT-L distilled from the ViT-7B\nEach Transformer-based model takes an image as input and returns a class token, patch tokens (and register tokens). These models follow a ViT architecture, with a patch size of 16. For a 224x224 image, this results in 1 class token + 4 register tokens + 196 patch tokens = 201 tokens (for DINOv2 with registers this resulted in 1 + 4 + 256 = 261 tokens).\nThe models can accept larger images provided the image shapes are multiples of the patch size (16). If this condition is not verified, the model will crop to the closest smaller multiple of the patch size.\nModel Description\nDeveloped by: Meta AI\nModel type: Vision Transformer, ConvNeXt\nLicense: DINOv3 License\nModel Sources\nRepository: https://github.com/facebookresearch/dinov3\nPaper: https://arxiv.org/abs/2508.10104\nUses\nThe models are vision backbones providing multi-purpose features for downstream tasks.\nDirect Use\nThe models can be used without fine-tuning, with downstream classifiers as simple as linear layers, to obtain competitive results:\non image classification, using k-NN classifiers on the class token\non image classification, with logistic regression classifiers applied on the class token\non image classification, with a linear layer applied on the class token and the average of the patch tokens\non image retrieval using nearest neighbors\non geometric and semantic 3D keypoint correspondances\non depth estimation, semantic segmentation, using linear layers\non unsupervised object discovery\non video segmentation tracking\non video classification, using a small 4-layer attentive probe\nDownstream Use\nWhile fine-tuning the models can yield some gains, it is recommended to keep this option as a last resort: the frozen features are expected to provide good performance out-of-the-box.\nBias, Risks, and Limitations\nCompared to DINOv2 and SEERv2, DINOv3 delivers somewhat consistent performance across income categories on geographical fairness and diversity, although with a notable performance drop in the low-income bucket compared to the highest-income bucket.\nDINOv3 also achieves relatively good scores across different regions, improving over its predecessor DINOv2. However, a relative difference is still observed between Europe and Africa.\nRecommendations\nFine-tuning is expected to increase the biases in the features produced by the model as they will be tuned to the fine-tuning labels.\nHow to Get Started with the Model\nThe example below demonstrates how to obtain an image embedding with [Pipeline] or the [AutoModel] class.\nfrom transformers import pipeline\nfrom transformers.image_utils import load_image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\nimage = load_image(url)\nfeature_extractor = pipeline(\nmodel=\"facebook/dinov3-vit7b16-pretrain-lvd1689m\",\ntask=\"image-feature-extraction\",\n)\nfeatures = feature_extractor(image)\nimport torch\nfrom transformers import AutoImageProcessor, AutoModel\nfrom transformers.image_utils import load_image\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = load_image(url)\npretrained_model_name = \"facebook/dinov3-vit7b16-pretrain-lvd1689m\"\nprocessor = AutoImageProcessor.from_pretrained(pretrained_model_name)\nmodel = AutoModel.from_pretrained(\npretrained_model_name,\ndevice_map=\"auto\",\n)\ninputs = processor(images=image, return_tensors=\"pt\").to(model.device)\nwith torch.inference_mode():\noutputs = model(**inputs)\npooled_output = outputs.pooler_output\nprint(\"Pooled output shape:\", pooled_output.shape)\nTraining Details\nTraining Data\nWeb dataset (LVD-1689M): a curated dataset of 1,689 millions of images extracted from a large data\npool of 17 billions web images collected from public posts on Instagram\nSatellite dataset (SAT-493M): a dataset of 493 millions of 512x512 images sampled randomly from Maxar RGB ortho-rectified imagery at 0.6 meter resolution\nTraining Procedure\nTraining objective:\nDINO self-distillation loss with multi-crop\niBOT masked-image modeling loss\nKoLeo regularization on [CLS] tokens\nGram anchoring\nTraining regime: PyTorch FSDP2 (with bf16 and fp8 matrix multiplications)\nDistillation:\nDistillation follows the standard DINOv3 pretraining procedure, except the teacher is a frozen pretrained ViT-7B.\nEvaluation\nResults\nThe reader is referred to the associated paper for details on the evaluation protocols\nResults for ViT backbones pretrained (or distilled) on web (LVD-1689M)\nGlobal Tasks\nDense Tasks\nModel\nIN-ReaL\nIN-R\nObj.Net\nOx.-H\nADE20k\nNYU‚Üì\nDAVIS\nNAVI\nSPair\nDINOv3 ViT-S/16\n87.0\n60.4\n50.9\n49.5\n47.0\n0.403\n72.7\n56.3\n50.4\nDINOv3 ViT-S+/16\n88.0\n68.8\n54.6\n50.0\n48.8\n0.399\n75.5\n57.1\n55.2\nDINOv3 ViT-B/16\n89.3\n76.7\n64.1\n58.5\n51.8\n0.373\n77.2\n58.8\n57.2\nDINOv3 ViT-L/16\n90.2\n88.1\n74.8\n63.1\n54.9\n0.352\n79.9\n62.3\n61.3\nDINOv3 ViT-H+/16\n90.3\n90.0\n78.6\n64.5\n54.8\n0.352\n79.3\n63.3\n56.3\nDINOv3 ViT-7B/16\n90.4\n91.1\n91.1\n72.8\n55.9\n0.309\n79.7\n64.4\n58.7\nResults for ConvNeXt backbones distilled on web (LVD-1689M)\nGlobal Tasks\nDense Tasks\nModel\nIN-ReaL\nIN-R\nObj.Net\nADE20k\nNYU‚Üì\n@256px\n@512px\n@256px\n@512px\n@256px\n@512px\nDINOv3 ConvNeXt Tiny\n86.6\n87.7\n73.7\n74.1\n52.6\n58.7\n42.7\n0.448\nDINOv3 ConvNeXt Small\n87.9\n88.7\n73.7\n74.1\n52.6\n58.7\n44.8\n0.432\nDINOv3 ConvNeXt Base\n88.5\n89.2\n77.2\n78.2\n56.2\n61.3\n46.3\n0.420\nDINOv3 ConvNeXt Large\n88.9\n89.4\n81.3\n82.4\n59.3\n65.2\n47.8\n0.403\nResults for ViT backbones pretrained (or distilled) on satellite (SAT-493M)\n(GEO-Bench) Classification\nModel\nm-BEnet\nm-brick-kiln\nm-eurosat\nm-forestnet\nm-pv4ger\nm-so2sat\nmean\nDINOv3 ViT-L/16\n73.0\n96.5\n94.1\n60.6\n96.0\n57.4\n79.6\nDINOv3 ViT-7B/16\n74.0\n97.2\n94.8\n62.3\n96.1\n62.1\n81.1\n(GEO-Bench) Segmentation\nModel\nm-cashew\nm-chesapeake\nm-NeonTree\nm-nz-cattle\nm-pv4ger-seg\nm-SA-crop\nmean\nDINOv3 ViT-L/16\n94.2\n75.6\n61.8\n83.7\n95.2\n36.8\n74.5\nDINOv3 ViT-7B/16\n94.1\n76.6\n62.6\n83.4\n95.5\n37.6\n75.0\nEnvironmental Impact\nHardware Type: Nvidia H100\nHours used: 61,440 hours for ViT-7B model training\nCloud Provider: Private infrastructure\nCompute Region: USA\nCarbon Emitted: 18t CO2eq\nTechnical Specifications\nModel Architecture and Objective\nVision Transformer models:\nViT-S (21M parameters): patch size 16, embedding dimension 384, 4 register tokens, 6 heads, MLP FFN, RoPE\nViT-S+ (29M parameters): patch size 16, embedding dimension 384, 4 register tokens, 6 heads, SwiGLU FFN, RoPE\nViT-B (86M parameters): patch size 16, embedding dimension 768, 4 register tokens, 12 heads, MLP FFN, RoPE\nViT-L (300M parameters): patch size 16, embedding dimension 1024, 4 register tokens, 16 heads, MLP FFN, RoPE\nViT-H+ (840M parameters): patch size 16, embedding dimension 1280, 4 register tokens, 20 heads, SwiGLU FFN, RoPE\nViT-7B (6716M parameters): patch size 16, embedding dimension 4096, 4 register tokens, 32 heads, SwiGLU FFN, RoPE\nConvNeXt models:\nConvNeXt Tiny (29M parameters)\nConvNeXt Small (50M parameters)\nConvNeXt Base (89M parameters)\nConvNeXt Large (198M parameters)\nCompute Infrastructure\nHardware\nNvidia H100 GPUs\nSoftware\nPyTorch 2.7\nMore Information\nSee the blog post and the associated website.\nCitation\nBibTeX\n@misc{simeoni2025dinov3,\ntitle={{DINOv3}},\nauthor={Sim{\\'e}oni, Oriane and Vo, Huy V. and Seitzer, Maximilian and Baldassarre, Federico and Oquab, Maxime and Jose, Cijo and Khalidov, Vasil and Szafraniec, Marc and Yi, Seungeun and Ramamonjisoa, Micha{\\\"e}l and Massa, Francisco and Haziza, Daniel and Wehrstedt, Luca and Wang, Jianyuan and Darcet, Timoth{\\'e}e and Moutakanni, Th{\\'e}o and Sentana, Leonel and Roberts, Claire and Vedaldi, Andrea and Tolan, Jamie and Brandt, John and Couprie, Camille and Mairal, Julien and J{\\'e}gou, Herv{\\'e} and Labatut, Patrick and Bojanowski, Piotr},\nyear={2025},\neprint={2508.10104},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2508.10104},\n}",
    "ibm-granite/granite-embedding-reranker-english-r2": "granite-embedding-reranker-english-r2\nModel Details\nUsage\nEvaluation Results\nModel Architecture and Key Features\nTraining and Optimization\nData Collection\nInfrastructure\nEthical Considerations and Limitations\nResources\nCitation\ngranite-embedding-reranker-english-r2\nModel Summary: granite-embedding-reranker-english-r2 is a 149M parameter dense cross-encoder model from the Granite Embeddings collection that can be used to generate high quality text embeddings. This model produces embedding vectors of size 768 based on context length of upto 8192 tokens. Compared to most other open-source models, this model was only trained using open-source relevance-pair datasets with permissive, enterprise-friendly license, plus IBM collected and generated datasets.\nThe granite-embedding-reranker-english-r2 model uses a cross-encoder architecture to compute high-quality relevance scores between queries and documents by jointly encoding their text, enabling precise reranking based on contextual alignment.\nThe model is trained with ranking-specific loss functions such as pListMLE, along with model merging techniques to enhance performance. The reranker model shows strong performance on standard information retrieval benchmark (BEIR, MIRACL), long-document search benchmarks (MLDR), and on many enterprise use cases.\nThe latest granite embedding r2 release introduces two English embedding models, and one English reranking all based on the ModernBERT architecture:\ngranite-embedding-english-r2 (149M parameters): with an output embedding size of 768, replacing granite-embedding-125m-english.\ngranite-embedding-small-english-r2 (47M parameters): A first-of-its-kind reduced-size model, with 8192 context length support, fewer layers and a smaller output embedding size (384), replacing granite-embedding-30m-english.\ngranite-embedding-reranker-english-r2 (149M parameters): reranker model based on granite-embedding-english-r2, with an output embedding size of 768.\nModel Details\nDeveloped by: Granite Embedding Team, IBM\nRepository: ibm-granite/granite-embedding-models\nPaper: Granite Embedding R2 Models\nLanguage(s) (NLP): English\nRelease Date: Sep 8, 2025\nLicense: Apache 2.0\nUsage\nThe model is designed to compute relevance scores for query-document pairs, making it well-suited for reranking tasks in information retrieval and search applications.\nUsage with Sentence Transformers:\nThe model is compatible with SentenceTransformer library and is very easy to use:\nFirst, install the sentence transformers library\npip install sentence_transformers\nThe model can then be used to jointly encode pairs of text to compute a relevance score.\nfrom sentence_transformers import CrossEncoder, util\nmodel_path = \"ibm-granite/granite-embedding-reranker-english-r2\"\n# Load the Sentence Transformer model\nmodel = CrossEncoder(model_path)\npassages = [\n\"Romeo and Juliet is a play by William Shakespeare.\",\n\"Climate change refers to long-term shifts in temperatures.\",\n\"Shakespeare also wrote Hamlet and Macbeth.\",\n\"Water is an inorganic compound with the chemical formula H2O.\",\n\"In liquid form, H2O is also called 'water' at standard temperature and pressure.\"\n]\nquery = \"what is the chemical formula of water?\"\n# encodes query and passages jointly and computes relevance score.\nranks = model.rank(query, passages, return_documents=True)\n# Print document rank and relevance score\nfor rank in ranks:\nprint(f\"- #{rank['corpus_id']} ({rank['score']:.2f}): {rank['text']}\")\nUsage with Huggingface Transformers:\nThis is a simple example of how to use the reranking model with the Transformers library and PyTorch.\nFirst, install the required libraries\npip install transformers torch\nThe model can then be used to encode pairs of text\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nmodel_path = \"ibm-granite/granite-embedding-reranker-english-r2\"\n# Load the model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path).eval()\ntokenizer = AutoTokenizer.from_pretrained(model_path)\npairs = [\n[\"what is the chemical formula of water?\", \"Water is an inorganic compound with the chemical formula H2O.\"],\n[\"what is the chemical formula of water?\", \"In liquid form, H2O is also called 'water' at standard temperature and pressure.\"],\n[\"how to implement quick sort in python?\", \"The weather is nice today\"],\n]\n# tokenize inputs\ntokenized_pairs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt')\n# encode and compute scores\nwith torch.no_grad():\nscores = model(**tokenized_pairs, return_dict=True).logits.view(-1, ).float()\nprint(scores)\nUsage with Huggingface Transformers (Retriever + Reranker E2E):\nThis is a simple example of how to use the Granite retriever and reranker together end-to-end with the Transformers library and PyTorch. The retriever first finds the most relevant candidate documents for a query, and then the reranker re-orders those candidates to produce the final ranked list.\nimport torch\nfrom transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n# --------------------------\n# 1. Load retriever (149M)\n# --------------------------\nretriever_model_path = \"ibm-granite/granite-embedding-english-r2\"\nretriever = AutoModel.from_pretrained(retriever_model_path).eval()\nretriever_tokenizer = AutoTokenizer.from_pretrained(retriever_model_path)\n# Example query + candidate documents\nquery = \"what is the chemical formula of water?\"\ndocuments = [\n\"Water is an inorganic compound with the chemical formula H2O.\",\n\"In liquid form, H2O is also called 'water' at standard temperature and pressure.\",\n\"The weather is nice today\",\n\"Quick sort is a divide and conquer algorithm that sorts by partitioning.\"\n]\n# Encode query and documents\nwith torch.no_grad():\nquery_emb = retriever(\n**retriever_tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True)\n).last_hidden_state[:, 0, :]   # CLS embedding\ndoc_embs = retriever(\n**retriever_tokenizer(documents, return_tensors=\"pt\", truncation=True, padding=True)\n).last_hidden_state[:, 0, :]\n# Compute cosine similarity\nquery_emb = torch.nn.functional.normalize(query_emb, dim=-1)\ndoc_embs = torch.nn.functional.normalize(doc_embs, dim=-1)\nsimilarities = torch.matmul(query_emb, doc_embs.T).squeeze(0)\n# Rank docs by retriever\nretriever_ranked = sorted(\nzip(documents, similarities.tolist()),\nkey=lambda x: x[1],\nreverse=True\n)\nprint(\"Retriever ranking:\")\nfor doc, score in retriever_ranked:\nprint(f\"{score:.4f} | {doc}\")\n# --------------------------\n# 2. Load reranker (149M)\n# --------------------------\nreranker_model_path = \"ibm-granite/granite-embedding-reranker-english-r2\"\nreranker = AutoModelForSequenceClassification.from_pretrained(reranker_model_path).eval()\nreranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_path)\n# Prepare top-k candidates (say top 3 from retriever)\ntop_k = 3\ncandidate_pairs = [[query, doc] for doc, _ in retriever_ranked[:top_k]]\n# Tokenize and rerank\nwith torch.no_grad():\ntokenized_pairs = reranker_tokenizer(\ncandidate_pairs, padding=True, truncation=True, return_tensors=\"pt\"\n)\nrerank_scores = reranker(**tokenized_pairs).logits.view(-1, ).float()\n# Rank docs by reranker\nreranker_ranked = sorted(\nzip([doc for doc, _ in retriever_ranked[:top_k]], rerank_scores.tolist()),\nkey=lambda x: x[1],\nreverse=True\n)\nprint(\"\\nReranker final ranking:\")\nfor doc, score in reranker_ranked:\nprint(f\"{score:.4f} | {doc}\")\nEvaluation Results\nThe performance of the Granite Embedding English reranking model on BEIR, MLDR, and Miracl benchmarks is reported below. All models are evaluated on the top-20 documents retrieved from the granite-embedding-english-small-r2 or granite-embedding-english-r2 retrievers respectively.\nEach reranking model is evaluated with its maximum supported sequence length, while queries are truncated to 64 tokens.\nModel\nParameters (M)\nSeq. Length\nBEIR Avg.\nMLDR (en)\nMiracl (en)\nRetriever: granite-embedding-small-english-r2\n47\n8192\n50.9\n40.1\n42.4\nms-marco-MiniLM-L12-v2\n33\n512\n52.0\n34.8\n54.5\nbge-reranker-base\n278\n512\n51.6\n36.7\n40.7\nbge-reranker-large\n560\n512\n53.0\n37.9\n42.2\ngte-reranker-modernbert-base\n149\n8192\n54.8\n50.4\n54.3\ngranite-embedding-reranker-english-r2\n149\n8192\n55.0\n44.9\n54.2\nRetriever: granite-embedding-english-r2\n149\n8192\n53.1\n41.6\n43.6\nms-marco-MiniLM-L12-v2\n33\n512\n53.2\n34.5\n55.4\nbge-reranker-base\n278\n512\n53.0\n36.6\n40.9\nbge-reranker-large\n560\n512\n54.3\n38.0\n42.3\ngte-reranker-modernbert-base\n149\n8192\n56.1\n51.2\n54.8\ngranite-embedding-reranker-english-r2\n149\n8192\n55.8\n45.8\n55.2\nModel Architecture and Key Features\nThe latest Granite Reranking r2 release introduces an English ranking model, based on the ModernBERT architecture:\ngranite-embedding-reranker-english-r2 (149M parameters): with an output embedding size of 768.\nThe following table shows the structure of the two R2 models:\nModel\ngranite-embedding-reranker-english-r2\nEmbedding size\n768\nNumber of layers\n22\nNumber of attention heads\n12\nIntermediate size\n1152\nActivation Function\nGeGLU\nVocabulary Size\n50368\nMax. Sequence Length\n8192\n# Parameters\n149M\nTraining and Optimization\nThe r2 models incorporate key enhancements from the ModernBERT architecture, including:\nAlternating attention lengths to accelerate processing\nRotary position embeddings for extended sequence length\nA newly trained tokenizer optimized with code and text data\nFlash Attention 2.0 for improved efficiency\nStreamlined parameters, eliminating unnecessary bias terms\nData Collection\nGranite reranking models is trained using data from four key sources:\nUnsupervised title-body paired data scraped from the web\nPublicly available paired with permissive, enterprise-friendly license\nIBM-internal paired data targetting specific technical domains\nIBM-generated synthetic data\nNotably, we do not use the popular MS-MARCO retrieval dataset in our training corpus due to its non-commercial license (many open-source models use this dataset due to its high quality).\nThe underlying encoder models using GneissWeb, an IBM-curated dataset composed exclusively of open, commercial-friendly sources.\nFor governance, all our data undergoes a data clearance process subject to technical, business, and governance review.\nThis comprehensive process captures critical information about the data, including but not limited to their content description ownership, intended use, data classification, licensing information, usage restrictions, how the data will be acquired, as well as an assessment of sensitive information (i.e, personal information).\nInfrastructure\nWe train Granite Reranking Model using IBM's computing cluster, BlueVela Cluster, which is outfitted with NVIDIA H100 80gb GPUs. This cluster provides a scalable and efficient infrastructure for training our models over multiple GPUs.\nEthical Considerations and Limitations\nThe data used to train the base language model was filtered to remove text containing hate, abuse, and profanity. granite-embedding-reranker-english-r2 is finetuned on English, and has a context length of 8192 tokens (longer texts will be truncated to this size).\nResources\n‚≠êÔ∏è Learn about the latest updates with Granite: https://www.ibm.com/granite\nüìÑ Get started with tutorials, best practices, and prompt engineering advice: https://www.ibm.com/granite/docs/\nüí° Learn about the latest Granite learning resources: https://ibm.biz/granite-learning-resources\nCitation\n@misc{awasthy2025graniteembeddingr2models,\ntitle={Granite Embedding R2 Models},\nauthor={Parul Awasthy and Aashka Trivedi and Yulong Li and Meet Doshi and Riyaz Bhat and Vignesh P and Vishwajeet Kumar and Yushu Yang and Bhavani Iyer and Abraham Daniels and Rudra Murthy and Ken Barker and Martin Franz and Madison Lee and Todd Ward and Salim Roukos and David Cox and Luis Lastras and Jaydeep Sen and Radu Florian},\nyear={2025},\neprint={2508.21085},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2508.21085},\n}",
    "ddh0/GLM-4.5-Air-GGUF": "GLM-4.5-Air-GGUF\nOriginal quantizations\nv2 quantizations\nGLM-4.5-Air-GGUF\nThis repository contains several custom GGUF quantizations of GLM-4.5-Air, to be used with llama.cpp.\nThe naming scheme for these custom quantizations is as follows:\nModelName-DefaultType-FFN-UpType-GateType-DownType.gguf\nWhere DefaultType refers to the default tensor type, and UpType, GateType, and DownType refer to the tensor types used for the ffn_up_exps,  ffn_gate_exps, and ffn_down_exps tensors respectively.\nOriginal quantizations\nThese quantizations use Q8_0 for all tensors by default - only the dense FFN block and conditional experts are downgraded. The shared expert is always kept in Q8_0. They were quantized using bartowski's imatrix.\nFilename\nSize (GB)\nSize (GiB)\nAverage BPW\nDirect link\nGLM-4.5-Air-Q8_0-FFN-IQ3_S-IQ3_S-Q5_0.gguf\n61.66\n57.43\n4.47\nDownload\nGLM-4.5-Air-Q8_0-FFN-IQ4_XS-IQ4_XS-Q5_0.gguf\n68.56\n63.86\n4.97\nDownload\nGLM-4.5-Air-Q8_0-FFN-Q4_K-Q4_K-Q5_1.gguf\n72.82\n67.82\n5.27\nDownload\nGLM-4.5-Air-Q8_0-FFN-Q4_K-Q4_K-Q8_0.gguf\n83.44\n77.71\n6.04\nDownload\nGLM-4.5-Air-Q8_0-FFN-Q5_K-Q5_K-Q8_0.gguf\n91.94\n85.63\n6.66\nDownload\nGLM-4.5-Air-Q8_0-FFN-Q6_K-Q6_K-Q8_0.gguf\n100.97\n94.04\n7.31\nDownload\nGLM-4.5-Air-Q8_0.gguf\n117.45\n109.39\n8.50\nDownload\nGLM-4.5-Air-bf16.gguf\n220.98\n205.81\n16.00\nDownload\nv2 quantizations\nThese quantizations use Q8_0 for all tensors by default, including the dense FFN block. Only the conditional experts are downgraded. The shared expert is always kept in Q8_0. They were quantized using my own imatrix (the calibration text corpus can be found here).\nFilename\nSize (GB)\nSize (GiB)\nAverage BPW\nDirect link\nGLM-4.5-Air-Q8_0-FFN-IQ4_XS-IQ3_S-IQ4_NL-v2.gguf\n60.94\n56.76\n4.41\nDownload\nGLM-4.5-Air-Q8_0-FFN-IQ4_XS-IQ4_XS-IQ4_NL-v2.gguf\n64.39\n59.97\n4.66\nDownload\nGLM-4.5-Air-Q8_0-FFN-IQ4_XS-IQ4_XS-Q5_0-v2.gguf\n68.63\n63.92\n4.97\nDownload\nGLM-4.5-Air-Q8_0-FFN-IQ4_XS-IQ4_XS-Q8_0-v2.gguf\n81.36\n75.78\n5.89\nDownload\nGLM-4.5-Air-Q8_0-FFN-Q5_K-Q5_K-Q8_0-v2.gguf\n91.97\n85.66\n6.66\nDownload\nGLM-4.5-Air-Q8_0-FFN-Q6_K-Q6_K-Q8_0-v2.gguf\n100.99\n94.06\n7.31\nDownload",
    "facebook/dinov3-vits16-pretrain-lvd1689m": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Card for DINOv3\nModel Details\nModel Description\nModel Sources\nUses\nDirect Use\nDownstream Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nEnvironmental Impact\nTechnical Specifications\nModel Architecture and Objective\nCompute Infrastructure\nMore Information\nCitation\nModel Card for DINOv3\nDINOv3 is a family of versatile vision foundation models that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models.\nModel Details\nThese are Vision Transformer and ConvNeXt models trained following the method described in the DINOv3 paper. 12 models are provided:\n10 models pretrained on web data (LVD-1689M dataset)\n1 ViT-7B trained from scratch,\n5 ViT-S/S+/B/L/H+ models distilled from the ViT-7B,\n4 ConvNeXt-{T/S/B/L} models distilled from the ViT-7B,\n2 models pretrained on satellite data (SAT-493M dataset)\n1 ViT-7B trained from scratch\n1 ViT-L distilled from the ViT-7B\nEach Transformer-based model takes an image as input and returns a class token, patch tokens (and register tokens). These models follow a ViT architecture, with a patch size of 16. For a 224x224 image, this results in 1 class token + 4 register tokens + 196 patch tokens = 201 tokens (for DINOv2 with registers this resulted in 1 + 4 + 256 = 261 tokens).\nThe models can accept larger images provided the image shapes are multiples of the patch size (16). If this condition is not verified, the model will crop to the closest smaller multiple of the patch size.\nModel Description\nDeveloped by: Meta AI\nModel type: Vision Transformer, ConvNeXt\nLicense: DINOv3 License\nModel Sources\nRepository: https://github.com/facebookresearch/dinov3\nPaper: https://arxiv.org/abs/2508.10104\nUses\nThe models are vision backbones providing multi-purpose features for downstream tasks.\nDirect Use\nThe models can be used without fine-tuning, with downstream classifiers as simple as linear layers, to obtain competitive results:\non image classification, using k-NN classifiers on the class token\non image classification, with logistic regression classifiers applied on the class token\non image classification, with a linear layer applied on the class token and the average of the patch tokens\non image retrieval using nearest neighbors\non geometric and semantic 3D keypoint correspondances\non depth estimation, semantic segmentation, using linear layers\non unsupervised object discovery\non video segmentation tracking\non video classification, using a small 4-layer attentive probe\nDownstream Use\nWhile fine-tuning the models can yield some gains, it is recommended to keep this option as a last resort: the frozen features are expected to provide good performance out-of-the-box.\nBias, Risks, and Limitations\nCompared to DINOv2 and SEERv2, DINOv3 delivers somewhat consistent performance across income categories on geographical fairness and diversity, although with a notable performance drop in the low-income bucket compared to the highest-income bucket.\nDINOv3 also achieves relatively good scores across different regions, improving over its predecessor DINOv2. However, a relative difference is still observed between Europe and Africa.\nRecommendations\nFine-tuning is expected to increase the biases in the features produced by the model as they will be tuned to the fine-tuning labels.\nHow to Get Started with the Model\nThe example below demonstrates how to obtain an image embedding with [Pipeline] or the [AutoModel] class.\nfrom transformers import pipeline\nfrom transformers.image_utils import load_image\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\nimage = load_image(url)\nfeature_extractor = pipeline(\nmodel=\"facebook/dinov3-vits16-pretrain-lvd1689m\",\ntask=\"image-feature-extraction\",\n)\nfeatures = feature_extractor(image)\nimport torch\nfrom transformers import AutoImageProcessor, AutoModel\nfrom transformers.image_utils import load_image\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = load_image(url)\npretrained_model_name = \"facebook/dinov3-vits16-pretrain-lvd1689m\"\nprocessor = AutoImageProcessor.from_pretrained(pretrained_model_name)\nmodel = AutoModel.from_pretrained(\npretrained_model_name,\ndevice_map=\"auto\",\n)\ninputs = processor(images=image, return_tensors=\"pt\").to(model.device)\nwith torch.inference_mode():\noutputs = model(**inputs)\npooled_output = outputs.pooler_output\nprint(\"Pooled output shape:\", pooled_output.shape)\nTraining Details\nTraining Data\nWeb dataset (LVD-1689M): a curated dataset of 1,689 millions of images extracted from a large data\npool of 17 billions web images collected from public posts on Instagram\nSatellite dataset (SAT-493M): a dataset of 493 millions of 512x512 images sampled randomly from Maxar RGB ortho-rectified imagery at 0.6 meter resolution\nTraining Procedure\nTraining objective:\nDINO self-distillation loss with multi-crop\niBOT masked-image modeling loss\nKoLeo regularization on [CLS] tokens\nGram anchoring\nTraining regime: PyTorch FSDP2 (with bf16 and fp8 matrix multiplications)\nDistillation:\nDistillation follows the standard DINOv3 pretraining procedure, except the teacher is a frozen pretrained ViT-7B.\nEvaluation\nResults\nThe reader is referred to the associated paper for details on the evaluation protocols\nResults for ViT backbones pretrained (or distilled) on web (LVD-1689M)\nGlobal Tasks\nDense Tasks\nModel\nIN-ReaL\nIN-R\nObj.Net\nOx.-H\nADE20k\nNYU‚Üì\nDAVIS\nNAVI\nSPair\nDINOv3 ViT-S/16\n87.0\n60.4\n50.9\n49.5\n47.0\n0.403\n72.7\n56.3\n50.4\nDINOv3 ViT-S+/16\n88.0\n68.8\n54.6\n50.0\n48.8\n0.399\n75.5\n57.1\n55.2\nDINOv3 ViT-B/16\n89.3\n76.7\n64.1\n58.5\n51.8\n0.373\n77.2\n58.8\n57.2\nDINOv3 ViT-L/16\n90.2\n88.1\n74.8\n63.1\n54.9\n0.352\n79.9\n62.3\n61.3\nDINOv3 ViT-H+/16\n90.3\n90.0\n78.6\n64.5\n54.8\n0.352\n79.3\n63.3\n56.3\nDINOv3 ViT-7B/16\n90.4\n91.1\n91.1\n72.8\n55.9\n0.309\n79.7\n64.4\n58.7\nResults for ConvNeXt backbones distilled on web (LVD-1689M)\nGlobal Tasks\nDense Tasks\nModel\nIN-ReaL\nIN-R\nObj.Net\nADE20k\nNYU‚Üì\n@256px\n@512px\n@256px\n@512px\n@256px\n@512px\nDINOv3 ConvNeXt Tiny\n86.6\n87.7\n73.7\n74.1\n52.6\n58.7\n42.7\n0.448\nDINOv3 ConvNeXt Small\n87.9\n88.7\n73.7\n74.1\n52.6\n58.7\n44.8\n0.432\nDINOv3 ConvNeXt Base\n88.5\n89.2\n77.2\n78.2\n56.2\n61.3\n46.3\n0.420\nDINOv3 ConvNeXt Large\n88.9\n89.4\n81.3\n82.4\n59.3\n65.2\n47.8\n0.403\nResults for ViT backbones pretrained (or distilled) on satellite (SAT-493M)\n(GEO-Bench) Classification\nModel\nm-BEnet\nm-brick-kiln\nm-eurosat\nm-forestnet\nm-pv4ger\nm-so2sat\nmean\nDINOv3 ViT-L/16\n73.0\n96.5\n94.1\n60.6\n96.0\n57.4\n79.6\nDINOv3 ViT-7B/16\n74.0\n97.2\n94.8\n62.3\n96.1\n62.1\n81.1\n(GEO-Bench) Segmentation\nModel\nm-cashew\nm-chesapeake\nm-NeonTree\nm-nz-cattle\nm-pv4ger-seg\nm-SA-crop\nmean\nDINOv3 ViT-L/16\n94.2\n75.6\n61.8\n83.7\n95.2\n36.8\n74.5\nDINOv3 ViT-7B/16\n94.1\n76.6\n62.6\n83.4\n95.5\n37.6\n75.0\nEnvironmental Impact\nHardware Type: Nvidia H100\nHours used: 61,440 hours for ViT-7B model training\nCloud Provider: Private infrastructure\nCompute Region: USA\nCarbon Emitted: 18t CO2eq\nTechnical Specifications\nModel Architecture and Objective\nVision Transformer models:\nViT-S (21M parameters): patch size 16, embedding dimension 384, 4 register tokens, 6 heads, MLP FFN, RoPE\nViT-S+ (29M parameters): patch size 16, embedding dimension 384, 4 register tokens, 6 heads, SwiGLU FFN, RoPE\nViT-B (86M parameters): patch size 16, embedding dimension 768, 4 register tokens, 12 heads, MLP FFN, RoPE\nViT-L (300M parameters): patch size 16, embedding dimension 1024, 4 register tokens, 16 heads, MLP FFN, RoPE\nViT-H+ (840M parameters): patch size 16, embedding dimension 1280, 4 register tokens, 20 heads, SwiGLU FFN, RoPE\nViT-7B (6716M parameters): patch size 16, embedding dimension 4096, 4 register tokens, 32 heads, SwiGLU FFN, RoPE\nConvNeXt models:\nConvNeXt Tiny (29M parameters)\nConvNeXt Small (50M parameters)\nConvNeXt Base (89M parameters)\nConvNeXt Large (198M parameters)\nCompute Infrastructure\nHardware\nNvidia H100 GPUs\nSoftware\nPyTorch 2.7\nMore Information\nSee the blog post and the associated website.\nCitation\nBibTeX\n@misc{simeoni2025dinov3,\ntitle={{DINOv3}},\nauthor={Sim{\\'e}oni, Oriane and Vo, Huy V. and Seitzer, Maximilian and Baldassarre, Federico and Oquab, Maxime and Jose, Cijo and Khalidov, Vasil and Szafraniec, Marc and Yi, Seungeun and Ramamonjisoa, Micha{\\\"e}l and Massa, Francisco and Haziza, Daniel and Wehrstedt, Luca and Wang, Jianyuan and Darcet, Timoth{\\'e}e and Moutakanni, Th{\\'e}o and Sentana, Leonel and Roberts, Claire and Vedaldi, Andrea and Tolan, Jamie and Brandt, John and Couprie, Camille and Mairal, Julien and J{\\'e}gou, Herv{\\'e} and Labatut, Patrick and Bojanowski, Piotr},\nyear={2025},\neprint={2508.10104},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2508.10104},\n}",
    "unsloth/gpt-oss-120b-GGUF": "gpt-oss-120b Details\nHighlights\nInference examples\nTransformers\nvLLM\nPyTorch / Triton\nOllama\nDownload the model\nReasoning levels\nTool use\nFine-tuning\nThe F16 quant is gpt-oss in its original precision. All GGUFs have our fixes. Read our guide here.\nSee our collection for all versions of gpt-oss including GGUF, 4-bit & 16-bit formats.\nLearn to run gpt-oss correctly - Read our Guide.\nSee Unsloth Dynamic 2.0 GGUFs for our quantization benchmarks.\n‚ú® Read our gpt-oss Guide here!\nRead our Blog about gpt-oss support: unsloth.ai/blog/gpt-oss\nView the rest of our notebooks in our docs here.\nThank you to the llama.cpp team for their work on supporting this model. We wouldn't be able to release quants without them!\ngpt-oss-120b Details\nTry gpt-oss ¬∑\nGuides ¬∑\nSystem card ¬∑\nOpenAI blog\nWelcome to the gpt-oss series, OpenAI‚Äôs open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.\nWe‚Äôre releasing two flavors of the open models:\ngpt-oss-120b ‚Äî for production, general purpose, high reasoning use cases that fits into a single H100 GPU (117B parameters with 5.1B active parameters)\ngpt-oss-20b ‚Äî for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)\nBoth models were trained on our harmony response format and should only be used with the harmony format as it will not work correctly otherwise.\nThis model card is dedicated to the larger gpt-oss-120b model. Check out gpt-oss-20b for the smaller model.\nHighlights\nPermissive Apache 2.0 license: Build freely without copyleft restrictions or patent risk‚Äîideal for experimentation, customization, and commercial deployment.\nConfigurable reasoning effort: Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.\nFull chain-of-thought: Gain complete access to the model‚Äôs reasoning process, facilitating easier debugging and increased trust in outputs. It‚Äôs not intended to be shown to end users.\nFine-tunable: Fully customize models to your specific use case through parameter fine-tuning.\nAgentic capabilities: Use the models‚Äô native capabilities for function calling, web browsing, Python code execution, and Structured Outputs.\nNative MXFP4 quantization: The models are trained with native MXFP4 precision for the MoE layer, making gpt-oss-120b run on a single H100 GPU and the gpt-oss-20b model run within 16GB of memory.\nInference examples\nTransformers\nYou can use gpt-oss-120b and gpt-oss-20b with Transformers. If you use the Transformers chat template, it will automatically apply the harmony response format. If you use model.generate directly, you need to apply the harmony format manually using the chat template or use our openai-harmony package.\nTo get started, install the necessary dependencies to setup your environment:\npip install -U transformers kernels torch\nOnce, setup you can proceed to run the model by running the snippet below:\nfrom transformers import pipeline\nimport torch\nmodel_id = \"openai/gpt-oss-120b\"\npipe = pipeline(\n\"text-generation\",\nmodel=model_id,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n]\noutputs = pipe(\nmessages,\nmax_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\nAlternatively, you can run the model via Transformers Serve to spin up a OpenAI-compatible webserver:\ntransformers serve\ntransformers chat localhost:8000 --model-name-or-path openai/gpt-oss-120b\nLearn more about how to use gpt-oss with Transformers.\nvLLM\nvLLM recommends using uv for Python dependency management. You can use vLLM to spin up an OpenAI-compatible webserver. The following command will automatically download the model and start the server.\nuv pip install --pre vllm==0.10.1+gptoss \\\n--extra-index-url https://wheels.vllm.ai/gpt-oss/ \\\n--extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\\n--index-strategy unsafe-best-match\nvllm serve openai/gpt-oss-120b\nLearn more about how to use gpt-oss with vLLM.\nPyTorch / Triton\nTo learn about how to use this model with PyTorch and Triton, check out our reference implementations in the gpt-oss repository.\nOllama\nIf you are trying to run gpt-oss on consumer hardware, you can use Ollama by running the following commands after installing Ollama.\n# gpt-oss-120b\nollama pull gpt-oss:120b\nollama run gpt-oss:120b\nLearn more about how to use gpt-oss with Ollama.\nLM Studio\nIf you are using LM Studio you can use the following commands to download.\n# gpt-oss-120b\nlms get openai/gpt-oss-120b\nCheck out our awesome list for a broader collection of gpt-oss resources and inference partners.\nDownload the model\nYou can download the model weights from the Hugging Face Hub directly from Hugging Face CLI:\n# gpt-oss-120b\nhuggingface-cli download openai/gpt-oss-120b --include \"original/*\" --local-dir gpt-oss-120b/\npip install gpt-oss\npython -m gpt_oss.chat model/\nReasoning levels\nYou can adjust the reasoning level that suits your task across three levels:\nLow: Fast responses for general dialogue.\nMedium: Balanced speed and detail.\nHigh: Deep and detailed analysis.\nThe reasoning level can be set in the system prompts, e.g., \"Reasoning: high\".\nTool use\nThe gpt-oss models are excellent for:\nWeb browsing (using built-in browsing tools)\nFunction calling with defined schemas\nAgentic operations like browser tasks\nFine-tuning\nBoth gpt-oss models can be fine-tuned for a variety of specialized use cases.\nThis larger model gpt-oss-120b can be fine-tuned on a single H100 node, whereas the smaller gpt-oss-20b can even be fine-tuned on consumer hardware.",
    "facebook/dinov3-vitl16-pretrain-lvd1689m": "",
    "facebook/dinov3-vith16plus-pretrain-lvd1689m": "",
    "quanhaol/Wan2.2-TI2V-5B-Turbo": ""
}