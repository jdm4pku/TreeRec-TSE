{
    "xey/sldr_flux_nsfw_v2-studio": "Source",
    "keras/bert_tiny_en_uncased": "Links\nInstallation\nPresets\nExample Usage\nExample Usage with Hugging Face URI\nModel Overview\nBERT (Bidirectional Encoder Representations from Transformers) is a set of language models published by Google. They are intended for classification and embedding of text, not for text-generation. See the model card below for benchmarks, data sources, and intended use cases.\nWeights and Keras model code are released under the Apache 2 License.\nLinks\nBert Quickstart Notebook\nBert API Documentation\nBert Model Card\nKerasHub Beginner Guide\nKerasHub Model Publishing Guide\nInstallation\nKeras and KerasHub can be installed with:\npip install -U -q keras-hub\npip install -U -q keras>=3\nJax, TensorFlow, and Torch come preinstalled in Kaggle Notebooks. For instruction on installing them in another environment see the Keras Getting Started page.\nPresets\nThe following model checkpoints are provided by the Keras team. Full code examples for each are available below.\nPreset name\nParameters\nDescription\nbert_tiny_en_uncased\n4.39M\n2-layer BERT model where all input is lowercased.\nbert_small_en_uncased\n28.76M\n4-layer BERT model where all input is lowercased.\nbert_medium_en_uncased\n41.37M\n8-layer BERT model where all input is lowercased.\nbert_base_en_uncased\n109.48M\n12-layer BERT model where all input is lowercased.\nbert_base_en\n108.31M\n12-layer BERT model where case is maintained.\nbert_base_zh\n102.27M\n12-layer BERT model. Trained on Chinese Wikipedia.\nbert_base_multi\n177.85M\n12-layer BERT model where case is maintained.\nbert_large_en_uncased\n335.14M\n24-layer BERT model where all input is lowercased.\nbert_large_en\n333.58M\n24-layer BERT model where case is maintained.\nbert_tiny_en_uncased_sst2\n4.39M\nhe bert_tiny_en_uncased backbone model fine-tuned on the SST-2 sentiment analysis dataset.\nExample Usage\nimport keras\nimport keras_hub\nimport numpy as np\nRaw string data.\nfeatures = [\"The quick brown fox jumped.\", \"I forgot my homework.\"]\nlabels = [0, 3]\n# Pretrained classifier.\nclassifier = keras_hub.models.BertClassifier.from_preset(\n\"bert_tiny_en_uncased\",\nnum_classes=4,\n)\nclassifier.fit(x=features, y=labels, batch_size=2)\nclassifier.predict(x=features, batch_size=2)\n# Re-compile (e.g., with a new learning rate).\nclassifier.compile(\nloss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\noptimizer=keras.optimizers.Adam(5e-5),\njit_compile=True,\n)\n# Access backbone programmatically (e.g., to change `trainable`).\nclassifier.backbone.trainable = False\n# Fit again.\nclassifier.fit(x=features, y=labels, batch_size=2)\nPreprocessed integer data.\nfeatures = {\n\"token_ids\": np.ones(shape=(2, 12), dtype=\"int32\"),\n\"segment_ids\": np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]] * 2),\n\"padding_mask\": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2),\n}\nlabels = [0, 3]\n# Pretrained classifier without preprocessing.\nclassifier = keras_hub.models.BertClassifier.from_preset(\n\"bert_tiny_en_uncased\",\nnum_classes=4,\npreprocessor=None,\n)\nclassifier.fit(x=features, y=labels, batch_size=2)\nExample Usage with Hugging Face URI\nimport keras\nimport keras_hub\nimport numpy as np\nRaw string data.\nfeatures = [\"The quick brown fox jumped.\", \"I forgot my homework.\"]\nlabels = [0, 3]\n# Pretrained classifier.\nclassifier = keras_hub.models.BertClassifier.from_preset(\n\"hf://keras/bert_tiny_en_uncased\",\nnum_classes=4,\n)\nclassifier.fit(x=features, y=labels, batch_size=2)\nclassifier.predict(x=features, batch_size=2)\n# Re-compile (e.g., with a new learning rate).\nclassifier.compile(\nloss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\noptimizer=keras.optimizers.Adam(5e-5),\njit_compile=True,\n)\n# Access backbone programmatically (e.g., to change `trainable`).\nclassifier.backbone.trainable = False\n# Fit again.\nclassifier.fit(x=features, y=labels, batch_size=2)\nPreprocessed integer data.\nfeatures = {\n\"token_ids\": np.ones(shape=(2, 12), dtype=\"int32\"),\n\"segment_ids\": np.array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0]] * 2),\n\"padding_mask\": np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]] * 2),\n}\nlabels = [0, 3]\n# Pretrained classifier without preprocessing.\nclassifier = keras_hub.models.BertClassifier.from_preset(\n\"hf://keras/bert_tiny_en_uncased\",\nnum_classes=4,\npreprocessor=None,\n)\nclassifier.fit(x=features, y=labels, batch_size=2)",
    "arcee-ai/Arcee-VyLinh": "Arcee-VyLinh\nModel Details\nIntended Use\nPerformance and Limitations\nStrengths\nBenchmarks\nLimitations\nTraining Process\nUsage Examples\nQuantized Version: arcee-ai/Arcee-VyLinh-GGUF\nArcee-VyLinh\nArcee-VyLinh is a 3B parameter instruction-following model specifically optimized for Vietnamese language understanding and generation. Built through an innovative training process combining evolved hard questions and iterative Direct Preference Optimization (DPO), it achieves remarkable performance despite its compact size.\nModel Details\nArchitecture: Based on Qwen2.5-3B\nParameters: 3 billion\nContext Length: 32K tokens\nTraining Data: Custom evolved dataset + ORPO-Mix-40K (Vietnamese)\nTraining Method: Multi-stage process including EvolKit, proprietary merging, and iterative DPO\nInput Format: Supports both English and Vietnamese, optimized for Vietnamese\nIntended Use\nVietnamese language chat and instruction following\nText generation and completion\nQuestion answering\nGeneral language understanding tasks\nContent creation and summarization\nPerformance and Limitations\nStrengths\nExceptional performance on complex Vietnamese language tasks\nEfficient 3B parameter architecture\nStrong instruction-following capabilities\nCompetitive with larger models (4B-8B parameters)\nBenchmarks\nTested on Vietnamese subset of m-ArenaHard (CohereForAI), with Claude 3.5 Sonnet as judge:\nLimitations\nMight still hallucinate on cultural-specific content.\nPrimary focus on Vietnamese language understanding\nMay not perform optimally for specialized technical domains\nTraining Process\nOur training pipeline consisted of several innovative stages:\nBase Model Selection: Started with Qwen2.5-3B\nHard Question Evolution: Generated 20K challenging questions using EvolKit\nInitial Training: Created VyLinh-SFT through supervised fine-tuning\nModel Merging: Proprietary merging technique with Qwen2.5-3B-Instruct\nDPO Training: 6 epochs of iterative DPO using ORPO-Mix-40K\nFinal Merge: Combined with Qwen2.5-3B-Instruct for optimal performance\nUsage Examples\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n# Load the model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"arcee-ai/Arcee-VyLinh\")\ntokenizer = AutoTokenizer.from_pretrained(\"arcee-ai/Arcee-VyLinh\")\nprompt = \"Một cộng một bằng mấy?\"\nmessages = [\n{\"role\": \"system\", \"content\": \"Bạn là trợ lí hữu ích.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\ngenerated_ids = model.generate(\nmodel_inputs.input_ids,\nmax_new_tokens=1024,\neos_token_id=tokenizer.eos_token_id,\ntemperature=0.25,\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids)[0]\nprint(response)",
    "allenai/OLMo-2-1124-7B": "Model Card for OLMo 2 7B\nInstallation\nInference\nFine-tuning\nModel Description\nModel Sources\nEvaluation\nModel Details\nPretraining\nBias, Risks, and Limitations\nCitation\nModel Card Contact\nModel Details\nModel Card for OLMo 2 7B\nWe introduce OLMo 2, a new family of 7B and 13B models featuring a 9-point increase in MMLU, among other evaluation improvements, compared to the original OLMo 7B model. These gains come from training on OLMo-mix-1124 and Dolmino-mix-1124 datasets and staged training approach.\nOLMo is a series of Open Language Models designed to enable the science of language models.\nThese models are trained on the Dolma dataset. We are releasing all code, checkpoints, logs (coming soon), and associated training details.\nSize\nTraining Tokens\nLayers\nHidden Size\nAttention Heads\nContext Length\nOLMo 2-7B\n4 Trillion\n32\n4096\n32\n4096\nOLMo 2-13B\n5 Trillion\n40\n5120\n40\n4096\nThe core models released in this batch include the following:\nStage\nOLMo 2 7B\nOLMo 2 13B\nBase Model\nallenai/OLMo-2-1124-7B\nallenai/OLMo-2-1124-13B\nSFT\nallenai/OLMo-2-1124-7B-SFT\nallenai/OLMo-2-1124-13B-SFT\nDPO\nallenai/OLMo-2-1124-7B-DPO\nallenai/OLMo-2-1124-13B-DPO\nFinal Models (RLVR)\nallenai/OLMo-2-1124-7B-Instruct\nallenai/OLMo-2-1124-13B-Instruct\nReward Model (RM)\nallenai/OLMo-2-1124-7B-RM\n(Same as 7B)\nInstallation\nOLMo 2 will be supported in the next version of Transformers, and you need to install it from the main branch using:\npip install --upgrade git+https://github.com/huggingface/transformers.git\nInference\nYou can use OLMo with the standard HuggingFace transformers library:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nolmo = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-2-1124-7B\")\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-2-1124-7B\")\nmessage = [\"Language modeling is \"]\ninputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\n# optional verifying cuda\n# inputs = {k: v.to('cuda') for k,v in inputs.items()}\n# olmo = olmo.to('cuda')\nresponse = olmo.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\nprint(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n>> 'Language modeling is  a key component of any text-based application, but its effectiveness...'\nFor faster performance, you can quantize the model using the following method:\nAutoModelForCausalLM.from_pretrained(\"allenai/OLMo-2-1124-7B\",\ntorch_dtype=torch.float16,\nload_in_8bit=True)  # Requires bitsandbytes\nThe quantized model is more sensitive to data types and CUDA operations. To avoid potential issues, it's recommended to pass the inputs directly to CUDA using:\ninputs.input_ids.to('cuda')\nWe have released checkpoints for these models. For pretraining, the naming convention is stepXXX-tokensYYYB. For checkpoints with ingredients of the soup, the naming convention is stage2-ingredientN-stepXXX-tokensYYYB\nTo load a specific model revision with HuggingFace, simply add the argument revision:\nolmo = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-2-1124-7B\", revision=\"step1000-tokens5B\")\nOr, you can access all the revisions for the models via the following code snippet:\nfrom huggingface_hub import list_repo_refs\nout = list_repo_refs(\"allenai/OLMo-2-1124-7B\")\nbranches = [b.name for b in out.branches]\nFine-tuning\nModel fine-tuning can be done from the final checkpoint (the main revision of this model) or many intermediate checkpoints. Two recipes for tuning are available.\nFine-tune with the OLMo repository:\ntorchrun --nproc_per_node=8 scripts/train.py {path_to_train_config} \\\n--data.paths=[{path_to_data}/input_ids.npy] \\\n--data.label_mask_paths=[{path_to_data}/label_mask.npy] \\\n--load_path={path_to_checkpoint} \\\n--reset_trainer_state\nFor more documentation, see the GitHub readme.\nFurther fine-tuning support is being developing in AI2's Open Instruct repository. Details are here.\nModel Description\nDeveloped by: Allen Institute for AI (Ai2)\nModel type: a Transformer style autoregressive language model.\nLanguage(s) (NLP): English\nLicense: The code and model are released under Apache 2.0.\nContact: Technical inquiries: olmo@allenai.org. Press: press@allenai.org\nDate cutoff: Dec. 2023.\nModel Sources\nProject Page: https://allenai.org/olmo\nRepositories:\nCore repo (training, inference, fine-tuning etc.): https://github.com/allenai/OLMo\nEvaluation code: https://github.com/allenai/OLMo-Eval\nFurther fine-tuning code: https://github.com/allenai/open-instruct\nPaper: https://arxiv.org/abs/2501.00656\nEvaluation\nCore model results for OLMo 2 7B and 13B models are found below.\nModel\nTrain FLOPs\nAverage\nARC/C\nHSwag\nWinoG\nMMLU\nDROP\nNQ\nAGIEval\nGSM8k\nMMLUPro\nTriviaQA\nOpen weights models:\nLlama-2-13B\n1.6·10²³\n54.1\n67.3\n83.9\n74.9\n55.7\n45.6\n38.4\n41.5\n28.1\n23.9\n81.3\nMistral-7B-v0.3\nn/a\n58.8\n78.3\n83.1\n77.7\n63.5\n51.8\n37.2\n47.3\n40.1\n30\n79.3\nLlama-3.1-8B\n7.2·10²³\n61.8\n79.5\n81.6\n76.6\n66.9\n56.4\n33.9\n51.3\n56.5\n34.7\n80.3\nMistral-Nemo-12B\nn/a\n66.9\n85.2\n85.6\n81.5\n69.5\n69.2\n39.7\n54.7\n62.1\n36.7\n84.6\nQwen-2.5-7B\n8.2·10²³\n67.4\n89.5\n89.7\n74.2\n74.4\n55.8\n29.9\n63.7\n81.5\n45.8\n69.4\nGemma-2-9B\n4.4·10²³\n67.8\n89.5\n87.3\n78.8\n70.6\n63\n38\n57.3\n70.1\n42\n81.8\nQwen-2.5-14B\n16.0·10²³\n72.2\n94\n94\n80\n79.3\n51.5\n37.3\n71\n83.4\n52.8\n79.1\nPartially open models:\nStableLM-2-12B\n2.9·10²³\n62.2\n81.9\n84.5\n77.7\n62.4\n55.5\n37.6\n50.9\n62\n29.3\n79.9\nZamba-2-7B\nn/c\n65.2\n92.2\n89.4\n79.6\n68.5\n51.7\n36.5\n55.5\n67.2\n32.8\n78.8\nFully open models:\nAmber-7B\n0.5·10²³\n35.2\n44.9\n74.5\n65.5\n24.7\n26.1\n18.7\n21.8\n4.8\n11.7\n59.3\nOLMo-7B\n1.0·10²³\n38.3\n46.4\n78.1\n68.5\n28.3\n27.3\n24.8\n23.7\n9.2\n12.1\n64.1\nMAP-Neo-7B\n2.1·10²³\n49.6\n78.4\n72.8\n69.2\n58\n39.4\n28.9\n45.8\n12.5\n25.9\n65.1\nOLMo-0424-7B\n0.9·10²³\n50.7\n66.9\n80.1\n73.6\n54.3\n50\n29.6\n43.9\n27.7\n22.1\n58.8\nDCLM-7B\n1.0·10²³\n56.9\n79.8\n82.3\n77.3\n64.4\n39.3\n28.8\n47.5\n46.1\n31.3\n72.1\nOLMo-2-1124-7B\n1.8·10²³\n62.9\n79.8\n83.8\n77.2\n63.7\n60.8\n36.9\n50.4\n67.5\n31\n78\nOLMo-2-1124-13B\n4.6·10²³\n68.3\n83.5\n86.4\n81.5\n67.5\n70.7\n46.7\n54.2\n75.1\n35.1\n81.9\nModel Details\nPretraining\nOLMo 2 7B\nOLMo 2 13B\nPretraining Stage 1(OLMo-Mix-1124)\n4 trillion tokens(1 epoch)\n5 trillion tokens(1.2 epochs)\nPretraining Stage 2(Dolmino-Mix-1124)\n50B tokens (3 runs)merged\n100B tokens (3 runs)300B tokens (1 run)merged\nPost-training(Tulu 3 SFT OLMo mix)\nSFT + DPO + PPO(preference mix)\nSFT + DPO + PPO(preference mix)\nStage 1: Initial Pretraining\nDataset: OLMo-Mix-1124 (3.9T tokens)\nCoverage: 90%+ of total pretraining budget\n7B Model: ~1 epoch\n13B Model: 1.2 epochs (5T tokens)\nStage 2: Fine-tuning\nDataset: Dolmino-Mix-1124 (843B tokens)\nThree training mixes:\n50B tokens\n100B tokens\n300B tokens\nMix composition: 50% high-quality data + academic/Q&A/instruction/math content\nModel Merging\n7B Model: 3 versions trained on 50B mix, merged via model souping\n13B Model: 3 versions on 100B mix + 1 version on 300B mix, merged for final checkpoint\nBias, Risks, and Limitations\nLike any base language model or fine-tuned model without safety filtering, these models can easily be prompted by users to generate harmful and sensitive content. Such content may also be produced unintentionally, especially in cases involving bias, so we recommend that users consider the risks when applying this technology. Additionally, many statements from OLMo or any LLM are often inaccurate, so facts should be verified.\nCitation\n@misc{olmo20242olmo2furious,\ntitle={2 OLMo 2 Furious},\nauthor={Team OLMo and Pete Walsh and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Shane Arora and Akshita Bhagia and Yuling Gu and Shengyi Huang and Matt Jordan and Nathan Lambert and Dustin Schwenk and Oyvind Tafjord and Taira Anderson and David Atkinson and Faeze Brahman and Christopher Clark and Pradeep Dasigi and Nouha Dziri and Michal Guerquin and Hamish Ivison and Pang Wei Koh and Jiacheng Liu and Saumya Malik and William Merrill and Lester James V. Miranda and Jacob Morrison and Tyler Murray and Crystal Nam and Valentina Pyatkin and Aman Rangapur and Michael Schmitz and Sam Skjonsberg and David Wadden and Christopher Wilhelm and Michael Wilson and Luke Zettlemoyer and Ali Farhadi and Noah A. Smith and Hannaneh Hajishirzi},\nyear={2024},\neprint={2501.00656},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.00656},\n}\nModel Card Contact\nFor errors in this model card, contact olmo@allenai.org.",
    "mradermacher/Arcee-VyLinh-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/arcee-ai/Arcee-VyLinh\nFor a convenient overview and download list, visit our model page for this model.\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Arcee-VyLinh-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n1.5\nGGUF\nQ3_K_S\n1.7\nGGUF\nQ3_K_M\n1.8\nlower quality\nGGUF\nQ3_K_L\n1.9\nGGUF\nIQ4_XS\n2.0\nGGUF\nQ4_K_S\n2.1\nfast, recommended\nGGUF\nQ4_K_M\n2.2\nfast, recommended\nGGUF\nQ5_K_S\n2.5\nGGUF\nQ5_K_M\n2.5\nGGUF\nQ6_K\n2.9\nvery good quality\nGGUF\nQ8_0\n3.7\nfast, best quality\nGGUF\nf16\n6.9\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "mradermacher/Arcee-VyLinh-i1-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nweighted/imatrix quants of https://huggingface.co/arcee-ai/Arcee-VyLinh\nFor a convenient overview and download list, visit our model page for this model.\nstatic quants are available at https://huggingface.co/mradermacher/Arcee-VyLinh-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\ni1-IQ1_S\n1.0\nfor the desperate\nGGUF\ni1-IQ1_M\n1.1\nmostly desperate\nGGUF\ni1-IQ2_XXS\n1.2\nGGUF\ni1-IQ2_XS\n1.2\nGGUF\ni1-IQ2_S\n1.3\nGGUF\ni1-IQ2_M\n1.4\nGGUF\ni1-Q2_K\n1.5\nIQ3_XXS probably better\nGGUF\ni1-IQ3_XXS\n1.5\nlower quality\nGGUF\ni1-IQ3_XS\n1.6\nGGUF\ni1-Q3_K_S\n1.7\nIQ3_XS probably better\nGGUF\ni1-IQ3_S\n1.7\nbeats Q3_K*\nGGUF\ni1-IQ3_M\n1.7\nGGUF\ni1-Q3_K_M\n1.8\nIQ3_S probably better\nGGUF\ni1-Q3_K_L\n1.9\nIQ3_M probably better\nGGUF\ni1-IQ4_XS\n2.0\nGGUF\ni1-Q4_0_4_4\n2.1\nfast on arm, low quality\nGGUF\ni1-Q4_0_4_8\n2.1\nfast on arm+i8mm, low quality\nGGUF\ni1-Q4_0_8_8\n2.1\nfast on arm+sve, low quality\nGGUF\ni1-Q4_0\n2.1\nfast, low quality\nGGUF\ni1-Q4_K_S\n2.1\noptimal size/speed/quality\nGGUF\ni1-Q4_K_M\n2.2\nfast, recommended\nGGUF\ni1-Q5_K_S\n2.5\nGGUF\ni1-Q5_K_M\n2.5\nGGUF\ni1-Q6_K\n2.9\npractically like static Q6_K\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time. Additional thanks to @nicoboss for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.",
    "TroyDoesAI/BlackSheep-Llama3.2-3B-Context_Obedient": "Uploaded  model\nContextual DPO\nOverview\nReferences in response\nUploaded  model\nDeveloped by: TroyDoesAI\nLicense: apache-2.0\nFinetuned from model : TroyDoesAI/BlackSheep-Llama3.2-3B\nThis llama model was trained 2x faster with Unsloth and Huggingface's TRL library.\nOverview\nThis model is meant to enhance adherence to provided context (e.g., for RAG applications) and reduce hallucinations, inspired by airoboros context-obedient question answer format.\nContextual DPO\nOverview\nThe format for a contextual prompt is as follows:\nBEGININPUT\nBEGINCONTEXT\n[key0: value0]\n[key1: value1]\n... other metdata ...\nENDCONTEXT\n[insert your text blocks here]\nENDINPUT\n[add as many other blocks, in the exact same format]\nBEGININSTRUCTION\n[insert your instruction(s).  The model was tuned with single questions, paragraph format, lists, etc.]\nENDINSTRUCTION\nI know it's a bit verbose and annoying, but after much trial and error, using these explicit delimiters helps the model understand where to find the responses and how to associate specific sources with it.\nBEGININPUT - denotes a new input block\nBEGINCONTEXT - denotes the block of context (metadata key/value pairs) to associate with the current input block\nENDCONTEXT - denotes the end of the metadata block for the current input\n[text] - Insert whatever text you want for the input block, as many paragraphs as can fit in the context.\nENDINPUT - denotes the end of the current input block\n[repeat as many input blocks in this format as you want]\nBEGININSTRUCTION - denotes the start of the list (or one) instruction(s) to respond to for all of the input blocks above.\n[instruction(s)]\nENDINSTRUCTION - denotes the end of instruction set\nHere's a trivial, but important example to prove the point:\nBEGININPUT\nBEGINCONTEXT\ndate: 2021-01-01\nurl: https://web.site/123\nENDCONTEXT\nIn a shocking turn of events, blueberries are now green, but will be sticking with the same name.\nENDINPUT\nBEGININSTRUCTION\nWhat color are bluberries?  Source?\nENDINSTRUCTION\nAnd the expected response:\nBlueberries are now green.\nSource:\ndate: 2021-01-01\nurl: https://web.site/123\nReferences in response\nAs shown in the example, the dataset includes many examples of including source details in the response, when the question asks for source/citation/references.\nWhy do this?  Well, the R in RAG seems to be the weakest link in the chain.\nRetrieval accuracy, depending on many factors including the overall dataset size, can be quite low.\nThis accuracy increases when retrieving more documents, but then you have the issue of actually using\nthe retrieved documents in prompts. If you use one prompt per document (or document chunk), you know\nexactly which document the answer came from, so there's no issue.  If, however, you include multiple\nchunks in a single prompt, it's useful to include the specific reference chunk(s) used to generate the\nresponse, rather than naively including references to all of the chunks included in the prompt.\nFor example, suppose I have two documents:\nurl: http://foo.bar/1\nStrawberries are tasty.\nurl: http://bar.foo/2\nThe cat is blue.\nIf the question being asked is What color is the cat?, I would only expect the 2nd document to be referenced in the response, as the other link is irrelevant.",
    "HuggingFaceTB/SmolLM2-1.7B": "SmolLM2\nTable of Contents\nModel Summary\nHow to use\nEvaluation\nBase Pre-Trained Model\nInstruction Model\nLimitations\nTraining\nModel\nHardware\nSoftware\nLicense\nCitation\nSmolLM2\nTable of Contents\nModel Summary\nEvaluation\nLimitations\nTraining\nLicense\nCitation\nModel Summary\nSmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device. More details in our paper: https://arxiv.org/abs/2502.02737v1\nThe 1.7B variant demonstrates significant advances over its predecessor SmolLM1-1.7B, particularly in instruction following, knowledge, reasoning, and mathematics. It was trained on 11 trillion tokens using a diverse dataset combination: FineWeb-Edu, DCLM, The Stack, along with new mathematics and coding datasets that we curated and will release soon. We developed the instruct version through supervised fine-tuning (SFT) using a combination of public datasets and our own curated datasets. We then applied Direct Preference Optimization (DPO) using UltraFeedback.\nThe instruct model additionally supports tasks such as text rewriting, summarization and function calling thanks to datasets developed by Argilla such as Synth-APIGen-v0.1.\nYou can find the SFT dataset here: https://huggingface.co/datasets/HuggingFaceTB/smoltalk and finetuning code in the alignement handbook.\nFor more details refer to: https://github.com/huggingface/smollm. You will find pre-training, post-training, evaluation and local inference code.\nHow to use\npip install transformers\nRunning the model on CPU/GPU/multi GPU\nUsing full precision\n# pip install transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"HuggingFaceTB/SmolLM2-1.7B\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\ninputs = tokenizer.encode(\"Gravity is\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\nUsing torch.bfloat16\n# pip install accelerate\n# for fp16 use `torch_dtype=torch.float16` instead\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16)\ninputs = tokenizer.encode(\"Gravity is\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\nMemory footprint: 3422.76 MB\nEvaluation\nIn this section, we report the evaluation results of SmolLM2. All evaluations are zero-shot unless stated otherwise, and we use lighteval to run them.\nBase Pre-Trained Model\nMetric\nSmolLM2-1.7B\nLlama-1B\nQwen2.5-1.5B\nSmolLM1-1.7B\nHellaSwag\n68.7\n61.2\n66.4\n62.9\nARC (Average)\n60.5\n49.2\n58.5\n59.9\nPIQA\n77.6\n74.8\n76.1\n76.0\nMMLU-Pro (MCF)\n19.4\n11.7\n13.7\n10.8\nCommonsenseQA\n43.6\n41.2\n34.1\n38.0\nTriviaQA\n36.7\n28.1\n20.9\n22.5\nWinogrande\n59.4\n57.8\n59.3\n54.7\nOpenBookQA\n42.2\n38.4\n40.0\n42.4\nGSM8K (5-shot)\n31.0\n7.2\n61.3\n5.5\nInstruction Model\nMetric\nSmolLM2-1.7B-Instruct\nLlama-1B-Instruct\nQwen2.5-1.5B-Instruct\nSmolLM1-1.7B-Instruct\nIFEval (Average prompt/inst)\n56.7\n53.5\n47.4\n23.1\nMT-Bench\n6.13\n5.48\n6.52\n4.33\nOpenRewrite-Eval (micro_avg RougeL)\n44.9\n39.2\n46.9\nNaN\nHellaSwag\n66.1\n56.1\n60.9\n55.5\nARC (Average)\n51.7\n41.6\n46.2\n43.7\nPIQA\n74.4\n72.3\n73.2\n71.6\nMMLU-Pro (MCF)\n19.3\n12.7\n24.2\n11.7\nBBH (3-shot)\n32.2\n27.6\n35.3\n25.7\nGSM8K (5-shot)\n48.2\n26.8\n42.8\n4.62\nLimitations\nSmolLM2 models primarily understand and generate content in English. They can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.\nTraining\nModel\nArchitecture: Transformer decoder\nPretraining tokens: 11T\nPrecision: bfloat16\nHardware\nGPUs: 256 H100\nSoftware\nTraining Framework: nanotron\nLicense\nApache 2.0\nCitation\n@misc{allal2025smollm2smolgoesbig,\ntitle={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model},\nauthor={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Martín Blázquez and Guilherme Penedo and Lewis Tunstall and Andrés Marafioti and Hynek Kydlíček and Agustín Piqueres Lajarín and Vaibhav Srivastav and Joshua Lochner and Caleb Fahlgren and Xuan-Son Nguyen and Clémentine Fourrier and Ben Burtenshaw and Hugo Larcher and Haojun Zhao and Cyril Zakka and Mathieu Morlon and Colin Raffel and Leandro von Werra and Thomas Wolf},\nyear={2025},\neprint={2502.02737},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2502.02737},\n}",
    "mradermacher/dolphin-2.7-mixtral-8x7b-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/dphn/dolphin-2.7-mixtral-8x7b\nFor a convenient overview and download list, visit our model page for this model.\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/dolphin-2.7-mixtral-8x7b-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n17.4\nGGUF\nQ3_K_S\n20.5\nGGUF\nQ3_K_M\n22.6\nlower quality\nGGUF\nQ3_K_L\n24.3\nGGUF\nIQ4_XS\n25.5\nGGUF\nQ4_K_S\n26.8\nfast, recommended\nGGUF\nQ4_K_M\n28.5\nfast, recommended\nGGUF\nQ5_K_S\n32.3\nGGUF\nQ5_K_M\n33.3\nGGUF\nQ6_K\n38.5\nvery good quality\nGGUF\nQ8_0\n49.7\nfast, best quality\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "Freddthink/llama3.2-3B-instruct-summarization-lora": "Model Card for Model ID\nModel Details\nModel Description\nModel Sources [optional]\nUses\nDirect Use\nDownstream Use [optional]\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nModel Examination [optional]\nEnvironmental Impact\nTechnical Specifications [optional]\nModel Architecture and Objective\nCompute Infrastructure\nCitation [optional]\nGlossary [optional]\nMore Information [optional]\nModel Card Authors [optional]\nModel Card Contact\nFramework versions\nModel Card for Model ID\nModel Details\nModel Description\nDeveloped by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nModel type: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\nFinetuned from model [optional]: [More Information Needed]\nModel Sources [optional]\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemo [optional]: [More Information Needed]\nUses\nDirect Use\n[More Information Needed]\nDownstream Use [optional]\n[More Information Needed]\nOut-of-Scope Use\n[More Information Needed]\nBias, Risks, and Limitations\n[More Information Needed]\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\nHow to Get Started with the Model\nUse the code below to get started with the model.\n[More Information Needed]\nTraining Details\nTraining Data\n[More Information Needed]\nTraining Procedure\nPreprocessing [optional]\n[More Information Needed]\nTraining Hyperparameters\nTraining regime: [More Information Needed]\nSpeeds, Sizes, Times [optional]\n[More Information Needed]\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\n[More Information Needed]\nFactors\n[More Information Needed]\nMetrics\n[More Information Needed]\nResults\n[More Information Needed]\nSummary\nModel Examination [optional]\n[More Information Needed]\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: [More Information Needed]\nHours used: [More Information Needed]\nCloud Provider: [More Information Needed]\nCompute Region: [More Information Needed]\nCarbon Emitted: [More Information Needed]\nTechnical Specifications [optional]\nModel Architecture and Objective\n[More Information Needed]\nCompute Infrastructure\n[More Information Needed]\nHardware\n[More Information Needed]\nSoftware\n[More Information Needed]\nCitation [optional]\nBibTeX:\n[More Information Needed]\nAPA:\n[More Information Needed]\nGlossary [optional]\n[More Information Needed]\nMore Information [optional]\n[More Information Needed]\nModel Card Authors [optional]\n[More Information Needed]\nModel Card Contact\n[More Information Needed]\nFramework versions\nPEFT 0.13.2",
    "HuggingFaceTB/SmolLM2-1.7B-Instruct": "SmolLM2\nTable of Contents\nModel Summary\nHow to use\nEvaluation\nBase Pre-Trained Model\nInstruction Model\nExamples\nText rewriting\nSummarization\nFunction calling\nLimitations\nTraining\nModel\nHardware\nSoftware\nLicense\nCitation\nSmolLM2\nTable of Contents\nModel Summary\nEvaluation\nExamples\nLimitations\nTraining\nLicense\nCitation\nModel Summary\nSmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device. More details in our paper: https://arxiv.org/abs/2502.02737v1\nThe 1.7B variant demonstrates significant advances over its predecessor SmolLM1-1.7B, particularly in instruction following, knowledge, reasoning, and mathematics. It was trained on 11 trillion tokens using a diverse dataset combination: FineWeb-Edu, DCLM, The Stack, along with new mathematics and coding datasets that we curated and will release soon. We developed the instruct version through supervised fine-tuning (SFT) using a combination of public datasets and our own curated datasets. We then applied Direct Preference Optimization (DPO) using UltraFeedback.\nThe instruct model additionally supports tasks such as text rewriting, summarization and function calling thanks to datasets developed by Argilla such as Synth-APIGen-v0.1.\nYou can find the SFT dataset here: https://huggingface.co/datasets/HuggingFaceTB/smoltalk.\nFor more details refer to: https://github.com/huggingface/smollm. You will find pre-training, post-training, evaluation and local inference code.\nHow to use\nTransformers\npip install transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\nmessages = [{\"role\": \"user\", \"content\": \"What is the capital of France.\"}]\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))\nChat in TRL\nYou can also use the TRL CLI to chat with the model from the terminal:\npip install trl\ntrl chat --model_name_or_path HuggingFaceTB/SmolLM2-1.7B-Instruct --device cpu\nTransformers.js\nnpm i @huggingface/transformers\nimport { pipeline } from \"@huggingface/transformers\";\n// Create a text generation pipeline\nconst generator = await pipeline(\n\"text-generation\",\n\"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n);\n// Define the list of messages\nconst messages = [\n{ role: \"system\", content: \"You are a helpful assistant.\" },\n{ role: \"user\", content: \"Tell me a joke.\" },\n];\n// Generate a response\nconst output = await generator(messages, { max_new_tokens: 128 });\nconsole.log(output[0].generated_text.at(-1).content);\n// \"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\"\nEvaluation\nIn this section, we report the evaluation results of SmolLM2. All evaluations are zero-shot unless stated otherwise, and we use lighteval to run them.\nBase Pre-Trained Model\nMetric\nSmolLM2-1.7B\nLlama-1B\nQwen2.5-1.5B\nSmolLM1-1.7B\nHellaSwag\n68.7\n61.2\n66.4\n62.9\nARC (Average)\n60.5\n49.2\n58.5\n59.9\nPIQA\n77.6\n74.8\n76.1\n76.0\nMMLU-Pro (MCF)\n19.4\n11.7\n13.7\n10.8\nCommonsenseQA\n43.6\n41.2\n34.1\n38.0\nTriviaQA\n36.7\n28.1\n20.9\n22.5\nWinogrande\n59.4\n57.8\n59.3\n54.7\nOpenBookQA\n42.2\n38.4\n40.0\n42.4\nGSM8K (5-shot)\n31.0\n7.2\n61.3\n5.5\nInstruction Model\nMetric\nSmolLM2-1.7B-Instruct\nLlama-1B-Instruct\nQwen2.5-1.5B-Instruct\nSmolLM1-1.7B-Instruct\nIFEval (Average prompt/inst)\n56.7\n53.5\n47.4\n23.1\nMT-Bench\n6.13\n5.48\n6.52\n4.33\nOpenRewrite-Eval (micro_avg RougeL)\n44.9\n39.2\n46.9\nNaN\nHellaSwag\n66.1\n56.1\n60.9\n55.5\nARC (Average)\n51.7\n41.6\n46.2\n43.7\nPIQA\n74.4\n72.3\n73.2\n71.6\nMMLU-Pro (MCF)\n19.3\n12.7\n24.2\n11.7\nBBH (3-shot)\n32.2\n27.6\n35.3\n25.7\nGSM8K (5-shot)\n48.2\n26.8\n42.8\n4.62\nExamples\nBelow are some system and instruct prompts that work well for special tasks\nText rewriting\nsystem_prompt_rewrite = \"You are an AI writing assistant. Your task is to rewrite the user's email to make it more professional and approachable while maintaining its main points and key message. Do not return any text other than the rewritten message.\"\nuser_prompt_rewrite = \"Rewrite the message below to make it more friendly and approachable while maintaining its main points and key message. Do not add any new information or return any text other than the rewritten message\\nThe message:\"\nmessages = [{\"role\": \"system\", \"content\": system_prompt_rewrite}, {\"role\": \"user\", \"content\":f\"{user_prompt_rewrite} The CI is failing after your last commit!\"}]\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))\nHey there! I noticed that the CI isn't passing after your latest commit. Could you take a look and let me know what's going on? Thanks so much for your help!\nSummarization\nsystem_prompt_summarize = \"Provide a concise, objective summary of the input text in up to three sentences, focusing on key actions and intentions without using second or third person pronouns.\"\nmessages = [{\"role\": \"system\", \"content\": system_prompt_summarize}, {\"role\": \"user\", \"content\": INSERT_LONG_EMAIL}]\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))\nFunction calling\nSmolLM2-1.7B-Instruct can handle function calling, it scores 27% on the BFCL Leaderboard. Here's how you can leverage it:\nimport json\nimport re\nfrom typing import Optional\nfrom jinja2 import Template\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.utils import get_json_schema\nsystem_prompt = Template(\"\"\"You are an expert in composing functions. You are given a question and a set of possible functions.\nBased on the question, you will need to make one or more function/tool calls to achieve the purpose.\nIf none of the functions can be used, point it out and refuse to answer.\nIf the given question lacks the parameters required by the function, also point it out.\nYou have access to the following tools:\n<tools>{{ tools }}</tools>\nThe output MUST strictly adhere to the following format, and NO other text MUST be included.\nThe example format is as follows. Please make sure the parameter type is correct. If no function call is needed, please make the tool calls an empty list '[]'.\n<tool_call>[\n{\"name\": \"func_name1\", \"arguments\": {\"argument1\": \"value1\", \"argument2\": \"value2\"}},\n... (more tool calls as required)\n]</tool_call>\"\"\")\ndef prepare_messages(\nquery: str,\ntools: Optional[dict[str, any]] = None,\nhistory: Optional[list[dict[str, str]]] = None\n) -> list[dict[str, str]]:\n\"\"\"Prepare the system and user messages for the given query and tools.\nArgs:\nquery: The query to be answered.\ntools: The tools available to the user. Defaults to None, in which case if a\nlist without content will be passed to the model.\nhistory: Exchange of messages, including the system_prompt from\nthe first query. Defaults to None, the first message in a conversation.\n\"\"\"\nif tools is None:\ntools = []\nif history:\nmessages = history.copy()\nmessages.append({\"role\": \"user\", \"content\": query})\nelse:\nmessages = [\n{\"role\": \"system\", \"content\": system_prompt.render(tools=json.dumps(tools))},\n{\"role\": \"user\", \"content\": query}\n]\nreturn messages\ndef parse_response(text: str) -> str | dict[str, any]:\n\"\"\"Parses a response from the model, returning either the\nparsed list with the tool calls parsed, or the\nmodel thought or response if couldn't generate one.\nArgs:\ntext: Response from the model.\n\"\"\"\npattern = r\"<tool_call>(.*?)</tool_call>\"\nmatches = re.findall(pattern, text, re.DOTALL)\nif matches:\nreturn json.loads(matches[0])\nreturn text\nmodel_name_smollm = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_smollm, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name_smollm)\nfrom datetime import datetime\nimport random\ndef get_current_time() -> str:\n\"\"\"Returns the current time in 24-hour format.\nReturns:\nstr: Current time in HH:MM:SS format.\n\"\"\"\nreturn datetime.now().strftime(\"%H:%M:%S\")\ndef get_random_number_between(min: int, max: int) -> int:\n\"\"\"\nGets a random number between min and max.\nArgs:\nmin: The minimum number.\nmax: The maximum number.\nReturns:\nA random number between min and max.\n\"\"\"\nreturn random.randint(min, max)\ntools = [get_json_schema(get_random_number_between), get_json_schema(get_current_time)]\ntoolbox = {\"get_random_number_between\": get_random_number_between, \"get_current_time\": get_current_time}\nquery = \"Give me a number between 1 and 300\"\nmessages = prepare_messages(query, tools=tools)\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nresult = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\ntool_calls = parse_response(result)\n# [{'name': 'get_random_number_between', 'arguments': {'min': 1, 'max': 300}}\n# Get tool responses\ntool_responses = [toolbox.get(tc[\"name\"])(*tc[\"arguments\"].values()) for tc in tool_calls]\n# [63]\n# For the second turn, rebuild the history of messages:\nhistory = messages.copy()\n# Add the \"parsed response\"\nhistory.append({\"role\": \"assistant\", \"content\": result})\nquery = \"Can you give me the hour?\"\nhistory.append({\"role\": \"user\", \"content\": query})\ninputs = tokenizer.apply_chat_template(history, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nresult = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\ntool_calls = parse_response(result)\ntool_responses = [toolbox.get(tc[\"name\"])(*tc[\"arguments\"].values()) for tc in tool_calls]\n# ['07:57:25']\nMore details such as parallel function calls and tools not available can be found here\nLimitations\nSmolLM2 models primarily understand and generate content in English. They can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.\nTraining\nModel\nArchitecture: Transformer decoder\nPretraining tokens: 11T\nPrecision: bfloat16\nHardware\nGPUs: 256 H100\nSoftware\nTraining Framework: nanotron\nAlignment Handbook alignment-handbook\nLicense\nApache 2.0\nCitation\n@misc{allal2025smollm2smolgoesbig,\ntitle={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model},\nauthor={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Martín Blázquez and Guilherme Penedo and Lewis Tunstall and Andrés Marafioti and Hynek Kydlíček and Agustín Piqueres Lajarín and Vaibhav Srivastav and Joshua Lochner and Caleb Fahlgren and Xuan-Son Nguyen and Clémentine Fourrier and Ben Burtenshaw and Hugo Larcher and Haojun Zhao and Cyril Zakka and Mathieu Morlon and Colin Raffel and Leandro von Werra and Thomas Wolf},\nyear={2025},\neprint={2502.02737},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2502.02737},\n}",
    "mradermacher/BlackSheep-Llama3.2-3B-Context_Obedient-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/TroyDoesAI/BlackSheep-Llama3.2-3B-Context_Obedient\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/BlackSheep-Llama3.2-3B-Context_Obedient-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n1.5\nGGUF\nQ3_K_S\n1.6\nGGUF\nQ3_K_M\n1.8\nlower quality\nGGUF\nQ3_K_L\n1.9\nGGUF\nIQ4_XS\n1.9\nGGUF\nQ4_K_S\n2.0\nfast, recommended\nGGUF\nQ4_K_M\n2.1\nfast, recommended\nGGUF\nQ5_K_S\n2.4\nGGUF\nQ5_K_M\n2.4\nGGUF\nQ6_K\n2.7\nvery good quality\nGGUF\nQ8_0\n3.5\nfast, best quality\nGGUF\nf16\n6.5\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "bartowski/SmolLM2-360M-Instruct-GGUF": "Llamacpp imatrix Quantizations of SmolLM2-360M-Instruct\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nQ4_0_X_X\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of SmolLM2-360M-Instruct\nUsing llama.cpp release b3991 for quantization.\nOriginal model: https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct\nAll quants made using imatrix option with dataset from here\nRun them in LM Studio\nPrompt format\n<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nSmolLM2-360M-Instruct-f16.gguf\nf16\n0.73GB\nfalse\nFull F16 weights.\nSmolLM2-360M-Instruct-Q8_0.gguf\nQ8_0\n0.39GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nSmolLM2-360M-Instruct-Q6_K_L.gguf\nQ6_K_L\n0.37GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nSmolLM2-360M-Instruct-Q6_K.gguf\nQ6_K\n0.37GB\nfalse\nVery high quality, near perfect, recommended.\nSmolLM2-360M-Instruct-Q5_K_L.gguf\nQ5_K_L\n0.29GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nSmolLM2-360M-Instruct-Q5_K_M.gguf\nQ5_K_M\n0.29GB\nfalse\nHigh quality, recommended.\nSmolLM2-360M-Instruct-Q5_K_S.gguf\nQ5_K_S\n0.28GB\nfalse\nHigh quality, recommended.\nSmolLM2-360M-Instruct-Q4_K_L.gguf\nQ4_K_L\n0.27GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nSmolLM2-360M-Instruct-Q4_K_M.gguf\nQ4_K_M\n0.27GB\nfalse\nGood quality, default size for must use cases, recommended.\nSmolLM2-360M-Instruct-Q4_K_S.gguf\nQ4_K_S\n0.26GB\nfalse\nSlightly lower quality with more space savings, recommended.\nSmolLM2-360M-Instruct-Q3_K_XL.gguf\nQ3_K_XL\n0.25GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nSmolLM2-360M-Instruct-Q3_K_L.gguf\nQ3_K_L\n0.25GB\nfalse\nLower quality but usable, good for low RAM availability.\nSmolLM2-360M-Instruct-Q4_0_8_8.gguf\nQ4_0_8_8\n0.23GB\nfalse\nOptimized for ARM inference. Requires 'sve' support (see link below). Don't use on Mac or Windows.\nSmolLM2-360M-Instruct-Q4_0_4_8.gguf\nQ4_0_4_8\n0.23GB\nfalse\nOptimized for ARM inference. Requires 'i8mm' support (see link below). Don't use on Mac or Windows.\nSmolLM2-360M-Instruct-Q4_0_4_4.gguf\nQ4_0_4_4\n0.23GB\nfalse\nOptimized for ARM inference. Should work well on all ARM chips, pick this if you're unsure. Don't use on Mac or Windows.\nSmolLM2-360M-Instruct-Q4_0.gguf\nQ4_0\n0.23GB\nfalse\nLegacy format, generally not worth using over similarly sized formats\nSmolLM2-360M-Instruct-IQ4_XS.gguf\nIQ4_XS\n0.23GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nSmolLM2-360M-Instruct-Q3_K_M.gguf\nQ3_K_M\n0.23GB\nfalse\nLow quality.\nSmolLM2-360M-Instruct-IQ3_M.gguf\nIQ3_M\n0.22GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nSmolLM2-360M-Instruct-Q3_K_S.gguf\nQ3_K_S\n0.22GB\nfalse\nLow quality, not recommended.\nSmolLM2-360M-Instruct-IQ3_XS.gguf\nIQ3_XS\n0.22GB\nfalse\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nSmolLM2-360M-Instruct-Q2_K_L.gguf\nQ2_K_L\n0.22GB\nfalse\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\nSmolLM2-360M-Instruct-Q2_K.gguf\nQ2_K\n0.22GB\nfalse\nVery low quality but surprisingly usable.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nSome say that this improves the quality, others don't notice any difference. If you use these models PLEASE COMMENT with your findings. I would like feedback that these are actually used and useful so I don't keep uploading quants no one is using.\nThanks!\nDownloading using huggingface-cli\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/SmolLM2-360M-Instruct-GGUF --include \"SmolLM2-360M-Instruct-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/SmolLM2-360M-Instruct-GGUF --include \"SmolLM2-360M-Instruct-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (SmolLM2-360M-Instruct-Q8_0) or download them all in place (./)\nQ4_0_X_X\nThese are NOT for Metal (Apple) offloading, only ARM chips.\nIf you're using an ARM chip, the Q4_0_X_X quants will have a substantial speedup. Check out Q4_0_4_4 speed comparisons on the original pull request\nTo check which one would work best for your ARM chip, you can check AArch64 SoC features (thanks EloyOn!).\nWhich file should I choose?\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset\nThank you ZeroWw for the inspiration to experiment with embed/output\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF": "ngxson/SmolLM2-1.7B-Instruct-Q4_K_M-GGUF\nUse with llama.cpp\nCLI:\nServer:\nngxson/SmolLM2-1.7B-Instruct-Q4_K_M-GGUF\nThis model was converted to GGUF format from HuggingFaceTB/SmolLM2-1.7B-Instruct using llama.cpp via the ggml.ai's GGUF-my-repo space.\nRefer to the original model card for more details on the model.\nUse with llama.cpp\nInstall llama.cpp through brew (works on Mac and Linux)\nbrew install llama.cpp\nInvoke the llama.cpp server or the CLI.\nCLI:\nllama-cli --hf-repo ngxson/SmolLM2-1.7B-Instruct-Q4_K_M-GGUF --hf-file smollm2-1.7b-instruct-q4_k_m.gguf -p \"The meaning to life and the universe is\"\nServer:\nllama-server --hf-repo ngxson/SmolLM2-1.7B-Instruct-Q4_K_M-GGUF --hf-file smollm2-1.7b-instruct-q4_k_m.gguf -c 2048\nNote: You can also use this checkpoint directly through the usage steps listed in the Llama.cpp repo as well.\nStep 1: Clone llama.cpp from GitHub.\ngit clone https://github.com/ggerganov/llama.cpp\nStep 2: Move into the llama.cpp folder and build it with LLAMA_CURL=1 flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).\ncd llama.cpp && LLAMA_CURL=1 make\nStep 3: Run inference through the main binary.\n./llama-cli --hf-repo ngxson/SmolLM2-1.7B-Instruct-Q4_K_M-GGUF --hf-file smollm2-1.7b-instruct-q4_k_m.gguf -p \"The meaning to life and the universe is\"\nor\n./llama-server --hf-repo ngxson/SmolLM2-1.7B-Instruct-Q4_K_M-GGUF --hf-file smollm2-1.7b-instruct-q4_k_m.gguf -c 2048",
    "unsloth/SmolLM2-1.7B-Instruct": "Finetune SmolLM2, Llama 3.2, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nunsloth/SmolLM2-1.7B-Instruct\n✨ Finetune for Free\nSpecial Thanks\nModel Summary\nSmolLM2\nFinetune SmolLM2, Llama 3.2, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nWe have a free Google Colab Tesla T4 notebook for Llama 3.2 (3B) here: https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing\nunsloth/SmolLM2-1.7B-Instruct\nFor more details on the model, please go to Hugging Face's original model card\n✨ Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.2 (3B)\n▶️ Start on Colab\n2.4x faster\n58% less\nLlama-3.2 (11B vision)\n▶️ Start on Colab\n2.4x faster\n58% less\nLlama-3.1 (8B)\n▶️ Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n▶️ Start on Colab\n2x faster\n50% less\nGemma 2 (9B)\n▶️ Start on Colab\n2.4x faster\n58% less\nMistral (7B)\n▶️ Start on Colab\n2.2x faster\n62% less\nDPO - Zephyr\n▶️ Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nSpecial Thanks\nA huge thank you to the Hugging Face team for creating and releasing these models.\nModel Summary\nSmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device.\nThe 1.7B variant demonstrates significant advances over its predecessor SmolLM1-1.7B, particularly in instruction following, knowledge, reasoning, and mathematics. It was trained on 11 trillion tokens using a diverse dataset combination: FineWeb-Edu, DCLM, The Stack, along with new mathematics and coding datasets that we curated and will release soon. We developed the instruct version through supervised fine-tuning (SFT) using a combination of public datasets and our own curated datasets. We then applied Direct Preference Optimization (DPO) using UltraFeedback.\nThe instruct model additionally supports tasks such as text rewriting, summarization and function calling thanks to datasets developed by Argilla such as Synth-APIGen-v0.1.\nSmolLM2",
    "tablegpt/TableGPT2-7B": "",
    "tencent/Hunyuan3D-1": "",
    "nvidia/stt_ar_fastconformer_hybrid_large_pc_v1.0": "",
    "zai-org/CogVideoX1.5-5B": "CogVideoX1.5-5B\nModel Introduction\nGetting Started Quickly 🤗\nQuantized Inference\nFurther Exploration\nModel License\nCitation\nCogVideoX1.5-5B\n📄 中文阅读 |\n🤗 Huggingface Space |\n🌐 Github  |\n📜 arxiv\n📍 Visit QingYing and API Platform to experience larger-scale commercial video generation models.\nModel Introduction\nCogVideoX is an open-source video generation model similar to QingYing. The table below displays the list of video generation models we currently offer, along with their foundational information.\nModel Name\nCogVideoX1.5-5B (Latest)\nCogVideoX1.5-5B-I2V (Latest)\nCogVideoX-2B\nCogVideoX-5B\nCogVideoX-5B-I2V\nRelease Date\nNovember 8, 2024\nNovember 8, 2024\nAugust 6, 2024\nAugust 27, 2024\nSeptember 19, 2024\nVideo Resolution\n1360 * 768\nMin(W, H) = 768  768 ≤ Max(W, H) ≤ 1360  Max(W, H) % 16 = 0\n720 * 480\nNumber of Frames\nShould be 16N + 1 where N <= 10 (default 81)\nShould be 8N + 1 where N <= 6 (default 49)\nInference Precision\nBF16 (Recommended), FP16, FP32, FP8*, INT8, Not supported: INT4\nFP16*(Recommended), BF16, FP32, FP8*, INT8, Not supported: INT4\nBF16 (Recommended), FP16, FP32, FP8*, INT8, Not supported: INT4\nSingle GPU Memory Usage\nSAT BF16: 76GB diffusers BF16: from 10GB*diffusers INT8(torchao): from 7GB*\nSAT FP16: 18GB diffusers FP16: 4GB minimum* diffusers INT8 (torchao): 3.6GB minimum*\nSAT BF16: 26GB diffusers BF16 : 5GB minimum* diffusers INT8 (torchao): 4.4GB minimum*\nMulti-GPU Memory Usage\nBF16: 24GB* using diffusers\nFP16: 10GB* using diffusers\nBF16: 15GB* using diffusers\nInference Speed(Step = 50, FP/BF16)\nSingle A100: ~1000 seconds (5-second video)Single H100: ~550 seconds (5-second video)\nSingle A100: ~90 secondsSingle H100: ~45 seconds\nSingle A100: ~180 secondsSingle H100: ~90 seconds\nPrompt Language\nEnglish*\nPrompt Token Limit\n224 Tokens\n226 Tokens\nVideo Length\n5 seconds or 10 seconds\n6 seconds\nFrame Rate\n16 frames / second\n8 frames / second\nPosition Encoding\n3d_rope_pos_embed\n3d_sincos_pos_embed\n3d_rope_pos_embed\n3d_rope_pos_embed + learnable_pos_embed\nDownload Link (Diffusers)\n🤗 HuggingFace🤖 ModelScope🟣 WiseModel\n🤗 HuggingFace🤖 ModelScope🟣 WiseModel\n🤗 HuggingFace🤖 ModelScope🟣 WiseModel\n🤗 HuggingFace🤖 ModelScope🟣 WiseModel\n🤗 HuggingFace🤖 ModelScope🟣 WiseModel\nDownload Link (SAT)\n🤗 HuggingFace🤖 ModelScope🟣 WiseModel\nSAT\nData Explanation\nTesting with the diffusers library enabled all optimizations included in the library. This scheme has not been\ntested on non-NVIDIA A100/H100 devices. It should generally work with all NVIDIA Ampere architecture or higher\ndevices. Disabling optimizations can triple VRAM usage but increase speed by 3-4 times. You can selectively disable\ncertain optimizations, including:\npipe.enable_sequential_cpu_offload()\npipe.vae.enable_slicing()\npipe.vae.enable_tiling()\nIn multi-GPU inference, enable_sequential_cpu_offload() optimization needs to be disabled.\nUsing an INT8 model reduces inference speed, meeting the requirements of lower VRAM GPUs while retaining minimal video\nquality degradation, at the cost of significant speed reduction.\nPytorchAO and Optimum-quanto can be\nused to quantize the text encoder, Transformer, and VAE modules, reducing CogVideoX’s memory requirements, making it\nfeasible to run the model on smaller VRAM GPUs. TorchAO quantization is fully compatible with torch.compile,\nsignificantly improving inference speed. FP8 precision is required for NVIDIA H100 and above, which requires source\ninstallation of torch, torchao, diffusers, and accelerate. Using CUDA 12.4 is recommended.\nInference speed testing also used the above VRAM optimizations, and without optimizations, speed increases by about\n10%. Only diffusers versions of models support quantization.\nModels support English input only; other languages should be translated into English during prompt crafting with a\nlarger model.\nNote\nUse SAT for inference and fine-tuning SAT version models. Check our\nGitHub for more details.\nGetting Started Quickly 🤗\nThis model supports deployment using the Hugging Face diffusers library. You can follow the steps below to get started.\nWe recommend that you visit our GitHub to check out prompt optimization and\nconversion to get a better experience.\nInstall the required dependencies\n# diffusers (from source)\n# transformers>=4.46.2\n# accelerate>=1.1.1\n# imageio-ffmpeg>=0.5.1\npip install git+https://github.com/huggingface/diffusers\npip install --upgrade transformers accelerate diffusers imageio-ffmpeg\nRun the code\nimport torch\nfrom diffusers import CogVideoXPipeline\nfrom diffusers.utils import export_to_video\nprompt = \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance.\"\npipe = CogVideoXPipeline.from_pretrained(\n\"THUDM/CogVideoX1.5-5B\",\ntorch_dtype=torch.bfloat16\n)\npipe.enable_sequential_cpu_offload()\npipe.vae.enable_tiling()\npipe.vae.enable_slicing()\nvideo = pipe(\nprompt=prompt,\nnum_videos_per_prompt=1,\nnum_inference_steps=50,\nnum_frames=81,\nguidance_scale=6,\ngenerator=torch.Generator(device=\"cuda\").manual_seed(42),\n).frames[0]\nexport_to_video(video, \"output.mp4\", fps=8)\nQuantized Inference\nPytorchAO and Optimum-quanto can be\nused to quantize the text encoder, transformer, and VAE modules to reduce CogVideoX's memory requirements. This allows\nthe model to run on free T4 Colab or GPUs with lower VRAM! Also, note that TorchAO quantization is fully compatible\nwith torch.compile, which can significantly accelerate inference.\n# To get started, PytorchAO needs to be installed from the GitHub source and PyTorch Nightly.\n# Source and nightly installation is only required until the next release.\nimport torch\nfrom diffusers import AutoencoderKLCogVideoX, CogVideoXTransformer3DModel, CogVideoXImageToVideoPipeline\nfrom diffusers.utils import export_to_video\nfrom transformers import T5EncoderModel\nfrom torchao.quantization import quantize_, int8_weight_only\nquantization = int8_weight_only\ntext_encoder = T5EncoderModel.from_pretrained(\"THUDM/CogVideoX1.5-5B\", subfolder=\"text_encoder\",\ntorch_dtype=torch.bfloat16)\nquantize_(text_encoder, quantization())\ntransformer = CogVideoXTransformer3DModel.from_pretrained(\"THUDM/CogVideoX1.5-5B\", subfolder=\"transformer\",\ntorch_dtype=torch.bfloat16)\nquantize_(transformer, quantization())\nvae = AutoencoderKLCogVideoX.from_pretrained(\"THUDM/CogVideoX1.5-5B\", subfolder=\"vae\", torch_dtype=torch.bfloat16)\nquantize_(vae, quantization())\n# Create pipeline and run inference\npipe = CogVideoXImageToVideoPipeline.from_pretrained(\n\"THUDM/CogVideoX1.5-5B\",\ntext_encoder=text_encoder,\ntransformer=transformer,\nvae=vae,\ntorch_dtype=torch.bfloat16,\n)\npipe.enable_model_cpu_offload()\npipe.vae.enable_tiling()\npipe.vae.enable_slicing()\nprompt = \"A little girl is riding a bicycle at high speed. Focused, detailed, realistic.\"\nvideo = pipe(\nprompt=prompt,\nnum_videos_per_prompt=1,\nnum_inference_steps=50,\nnum_frames=81,\nguidance_scale=6,\ngenerator=torch.Generator(device=\"cuda\").manual_seed(42),\n).frames[0]\nexport_to_video(video, \"output.mp4\", fps=8)\nAdditionally, these models can be serialized and stored using PytorchAO in quantized data types to save disk space. You\ncan find examples and benchmarks at the following links:\ntorchao\nquanto\nFurther Exploration\nFeel free to enter our GitHub, where you'll find:\nMore detailed technical explanations and code.\nOptimized prompt examples and conversions.\nDetailed code for model inference and fine-tuning.\nProject update logs and more interactive opportunities.\nCogVideoX toolchain to help you better use the model.\nINT8 model inference code.\nModel License\nThis model is released under the CogVideoX LICENSE.\nCitation\n@article{yang2024cogvideox,\ntitle={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer},\nauthor={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},\njournal={arXiv preprint arXiv:2408.06072},\nyear={2024}\n}",
    "allura-org/G2-9B-Aletheia-v1": "Gemma-2-9B Aletheia v1\nFormat\nMergekit config\nImage by CalamitousFelicitousness\nGemma-2-9B Aletheia v1\nA merge of Sugarquill and Sunfall. I wanted to combine Sugarquill's more novel-like writing style with something that would improve it's RP perfomance and make it more steerable, w/o adding superfluous synthetic writing patterns.\nI quite like Crestfall's Sunfall models and I felt like Gemma version of Sunfall will steer the model in this direction when merged in. To keep more of Gemma-2-9B-it-SPPO-iter3's smarts, I've decided to apply Sunfall LoRA on top of it, instead of using the published Sunfall model.\nI'm generally pleased with the result, this model has nice, fresh writing style, good charcard adherence and good system prompt following.\nIt still should work well for raw completion storywriting, as it's a trained feature in both merged models.\nMade by Auri.\nThanks to Prodeus, Inflatebot and ShotMisser for testing and giving feedback.\nFormat\nModel responds to Gemma instruct formatting, exactly like it's base model.\n<bos><start_of_turn>user\n{user message}<end_of_turn>\n<start_of_turn>model\n{response}<end_of_turn><eos>\nMergekit config\nThe following YAML configuration was used to produce this model:\nmodels:\n- model: allura-org/G2-9B-Sugarquill-v0\nparameters:\nweight: 0.55\ndensity: 0.4\n- model: UCLA-AGI/Gemma-2-9B-It-SPPO-Iter3+AuriAetherwiing/sunfall-g2-lora\nparameters:\nweight: 0.45\ndensity: 0.3\nmerge_method: ties\nbase_model: UCLA-AGI/Gemma-2-9B-It-SPPO-Iter3\nparameters:\nnormalize: true\ndtype: bfloat16",
    "amd/Nitro-1-SD": "AMD Nitro-1\nIntroduction\nDetails\nQuickstart\nResults\nLicense\nAMD Nitro-1\nIntroduction\nNitro-1 is a series of efficient text-to-image generation models that are distilled from popular diffusion models on AMD Instinct™ GPUs. The release consists of:\nNitro-1-SD: a UNet-based one-step model distilled from Stable Diffusion 2.1.\nNitro-1-PixArt: a high resolution transformer-based one-step model distilled from PixArt-Sigma.\n⚡️ Open-source code! The models are based on our re-implementation of Latent Adversarial Diffusion Distillation, the method used to build the popular Stable Diffusion 3 Turbo model. Since the original authors didn't provide training code, we release our re-implementation to help advance further research in the field.\nDetails\nModel architecture: Nitro-1-SD has the same architecture as Stable Diffusion 2.1 and is compatible with the diffusers pipeline.\nInference steps: This model is distilled to perform inference in just a single step. However, the training code also supports distilling a model for 2, 4 or 8 steps.\nHardware: We use a single node consisting of 4 AMD Instinct™ MI250 GPUs for distilling Nitro-1-SD.\nDataset: We use 1M prompts from DiffusionDB and generate the corresponding images from the base Stable Diffusion 2.1 model.\nTraining cost: The distillation process achieves reasonable results in less than 2 days on a single node.\nQuickstart\nfrom diffusers import DDPMScheduler, DiffusionPipeline\nimport torch\nscheduler = DDPMScheduler.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", subfolder=\"scheduler\")\npipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", scheduler=scheduler)\nckpt_path = '<path to distilled checkpoint>'\nunet_state_dict = torch.load(ckpt_path)\npipe.unet.load_state_dict(unet_state_dict)\npipe = pipe.to(\"cuda\")\nimage = pipe(prompt='a photo of a cat',\nnum_inference_steps=1,\nguidance_scale=0,\ntimesteps=[999]).images[0]\nFor more details on training and evaluation please visit the GitHub repo.\nResults\nCompared to the Stable Diffusion 2.1 base model, we achieve 95.9% reduction in FLOPs at the cost of just 2.5% lower CLIP score and 2.2% higher FID.\nModel\nFID ↓\nCLIP ↑\nFLOPs\nLatency on AMD Instinct MI250 (sec)\nStable Diffusion 2.1 base, 50 steps (cfg=7.5)\n25.47\n0.3286\n83.04\n4.94\nNitro-1-SD, 1 step\n26.04\n0.3204\n3.36\n0.18\nLicense\nCopyright (c) 2018-2024 Advanced Micro Devices, Inc. All Rights Reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.",
    "Efficient-Large-Model/Sana_1600M_1024px": "🐱 Sana Model Card\nModel\nModel Description\nModel Sources\nLicense/Terms of Use\n🧨 Diffusers\nUses\nDirect Use\nOut-of-Scope Use\nLimitations and Bias\nLimitations\nBias\n🐱 Sana Model Card\nModel\nWe introduce Sana, a text-to-image framework that can efficiently generate images up to 4096 × 4096 resolution.\nSana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU.\nSource code is available at https://github.com/NVlabs/Sana.\nModel Description\nDeveloped by: NVIDIA, Sana\nModel type: Linear-Diffusion-Transformer-based text-to-image generative model\nModel size: 1648M parameters\nModel resolution: This model is developed to generate 1024px based images with multi-scale heigh and width.\nLicense: NSCL v2-custom. Governing Terms:  NVIDIA License.  Additional Information:  Gemma Terms of Use  |  Google AI for Developers for Gemma-2-2B-IT, Gemma Prohibited Use Policy  |  Google AI for Developers.\nModel Description: This is a model that can be used to generate and modify images based on text prompts.\nIt is a Linear Diffusion Transformer that uses one fixed, pretrained text encoders (Gemma2-2B-IT)\nand one 32x spatial-compressed latent feature encoder (DC-AE).\nResources for more information: Check out our GitHub Repository and the Sana report on arXiv.\nModel Sources\nFor research purposes, we recommend our generative-models Github repository (https://github.com/NVlabs/Sana),\nwhich is more suitable for both training and inference and for which most advanced diffusion sampler like Flow-DPM-Solver is integrated.\nMIT Han-Lab provides free Sana inference.\nRepository: ttps://github.com/NVlabs/Sana\nDemo: https://nv-sana.mit.edu/\nLicense/Terms of Use\nGOVERNING TERMS: This trial service is governed by the NVIDIA API Trial Terms of Service. Use of this model is governed by the NVIDIA Open Model License Agreement.\n🧨 Diffusers\nPR developing: Sana and DC-AE\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and tasks include\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nExcluded uses are described below.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render complex legible text\nfingers, .etc in general may not be generated properly.\nThe autoencoding part of the model is lossy.\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.",
    "bartowski/Qwen2.5-Coder-14B-Instruct-GGUF": "Llamacpp imatrix Quantizations of Qwen2.5-Coder-14B-Instruct\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nQ4_0_X_X\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of Qwen2.5-Coder-14B-Instruct\nUsing llama.cpp release b4014 for quantization.\nOriginal model: https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct\nAll quants made using imatrix option with dataset from here\nRun them in LM Studio\nPrompt format\n<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nQwen2.5-Coder-14B-Instruct-f16.gguf\nf16\n29.55GB\nfalse\nFull F16 weights.\nQwen2.5-Coder-14B-Instruct-Q8_0.gguf\nQ8_0\n15.70GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nQwen2.5-Coder-14B-Instruct-Q6_K_L.gguf\nQ6_K_L\n12.50GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nQwen2.5-Coder-14B-Instruct-Q6_K.gguf\nQ6_K\n12.12GB\nfalse\nVery high quality, near perfect, recommended.\nQwen2.5-Coder-14B-Instruct-Q5_K_L.gguf\nQ5_K_L\n10.99GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nQwen2.5-Coder-14B-Instruct-Q5_K_M.gguf\nQ5_K_M\n10.51GB\nfalse\nHigh quality, recommended.\nQwen2.5-Coder-14B-Instruct-Q5_K_S.gguf\nQ5_K_S\n10.27GB\nfalse\nHigh quality, recommended.\nQwen2.5-Coder-14B-Instruct-Q4_K_L.gguf\nQ4_K_L\n9.57GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nQwen2.5-Coder-14B-Instruct-Q4_K_M.gguf\nQ4_K_M\n8.99GB\nfalse\nGood quality, default size for most use cases, recommended.\nQwen2.5-Coder-14B-Instruct-Q3_K_XL.gguf\nQ3_K_XL\n8.61GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nQwen2.5-Coder-14B-Instruct-Q4_K_S.gguf\nQ4_K_S\n8.57GB\nfalse\nSlightly lower quality with more space savings, recommended.\nQwen2.5-Coder-14B-Instruct-IQ4_NL.gguf\nIQ4_NL\n8.55GB\nfalse\nSimilar to IQ4_XS, but slightly larger.\nQwen2.5-Coder-14B-Instruct-Q4_0.gguf\nQ4_0\n8.54GB\nfalse\nLegacy format, generally not worth using over similarly sized formats\nQwen2.5-Coder-14B-Instruct-Q4_0_8_8.gguf\nQ4_0_8_8\n8.52GB\nfalse\nOptimized for ARM inference. Requires 'sve' support (see link below). Don't use on Mac or Windows.\nQwen2.5-Coder-14B-Instruct-Q4_0_4_8.gguf\nQ4_0_4_8\n8.52GB\nfalse\nOptimized for ARM inference. Requires 'i8mm' support (see link below). Don't use on Mac or Windows.\nQwen2.5-Coder-14B-Instruct-Q4_0_4_4.gguf\nQ4_0_4_4\n8.52GB\nfalse\nOptimized for ARM inference. Should work well on all ARM chips, pick this if you're unsure. Don't use on Mac or Windows.\nQwen2.5-Coder-14B-Instruct-IQ4_XS.gguf\nIQ4_XS\n8.12GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nQwen2.5-Coder-14B-Instruct-Q3_K_L.gguf\nQ3_K_L\n7.92GB\nfalse\nLower quality but usable, good for low RAM availability.\nQwen2.5-Coder-14B-Instruct-Q3_K_M.gguf\nQ3_K_M\n7.34GB\nfalse\nLow quality.\nQwen2.5-Coder-14B-Instruct-IQ3_M.gguf\nIQ3_M\n6.92GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nQwen2.5-Coder-14B-Instruct-Q3_K_S.gguf\nQ3_K_S\n6.66GB\nfalse\nLow quality, not recommended.\nQwen2.5-Coder-14B-Instruct-Q2_K_L.gguf\nQ2_K_L\n6.53GB\nfalse\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\nQwen2.5-Coder-14B-Instruct-IQ3_XS.gguf\nIQ3_XS\n6.38GB\nfalse\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nQwen2.5-Coder-14B-Instruct-Q2_K.gguf\nQ2_K\n5.77GB\nfalse\nVery low quality but surprisingly usable.\nQwen2.5-Coder-14B-Instruct-IQ2_M.gguf\nIQ2_M\n5.36GB\nfalse\nRelatively low quality, uses SOTA techniques to be surprisingly usable.\nQwen2.5-Coder-14B-Instruct-IQ2_S.gguf\nIQ2_S\n5.00GB\nfalse\nLow quality, uses SOTA techniques to be usable.\nQwen2.5-Coder-14B-Instruct-IQ2_XS.gguf\nIQ2_XS\n4.70GB\nfalse\nLow quality, uses SOTA techniques to be usable.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nSome say that this improves the quality, others don't notice any difference. If you use these models PLEASE COMMENT with your findings. I would like feedback that these are actually used and useful so I don't keep uploading quants no one is using.\nThanks!\nDownloading using huggingface-cli\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/Qwen2.5-Coder-14B-Instruct-GGUF --include \"Qwen2.5-Coder-14B-Instruct-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/Qwen2.5-Coder-14B-Instruct-GGUF --include \"Qwen2.5-Coder-14B-Instruct-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (Qwen2.5-Coder-14B-Instruct-Q8_0) or download them all in place (./)\nQ4_0_X_X\nThese are NOT for Metal (Apple) offloading, only ARM chips.\nIf you're using an ARM chip, the Q4_0_X_X quants will have a substantial speedup. Check out Q4_0_4_4 speed comparisons on the original pull request\nTo check which one would work best for your ARM chip, you can check AArch64 SoC features (thanks EloyOn!).\nWhich file should I choose?\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset.\nThank you ZeroWw for the inspiration to experiment with embed/output.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "opendiffusionai/XLSD-V0.0": "This contains the merge of the SDXL VAE, with the SD1.5 unet, in assorted formats.\nIt's not particularly usable by itself. The colors are all wrong, etc.\nIt is mostly here as an open-source documented starting point for the XLsd model, which is currently in development.\nFeel free to try to train it yourself! :)\nNote that there are THREE versions of this model\na fp32 model, and a bf16 model, in single checkpoint format, and then the full huggingface diffusers format across the multiple subdirs.\nCurrent advice, until proven otherwise, is that it is best to train on the fp32 model, in fp32, until it starts giving actual good output.\nIf you are in a hurry, the traditional \"mixed precision\" training method is to load fp32, tell your training program to use mixed\nprecision (bf16), but then STILL SAVE IN FP32. This is the critical bit.\nTbe bf16 model is here basically just for the curious, who want to poke at it on a small VRAM machine. Do not train on it.\nSample output:\nPS: technically, I manually and seperately recreated the diffusers-format stuff after I had created the lone single-file versions.\nI would not think anyone would really care, but I believe in full disclosure of resources.",
    "RLHFlow/Llama3.1-8B-PRM-Deepseek-Data": "BoN evaluation result for Mistral generator:\nScaling the inference sampling to N=1024 for Deepseek generator:\nVisualization\nUsage\nCitation\nThis is a process-supervised reward (PRM) from the project RLHFlow/RLHF-Reward-Modeling\nThe model is trained from meta-llama/Llama-3.1-8B-Instruct on RLHFlow/Deepseek-PRM-Data for 1 epochs. We use a global batch size of 32 and a learning rate of 2e-6, where we pack the samples and split them into chunks of 8192 token. See more training details at https://github.com/RLHFlow/RLHF-Reward-Modeling/tree/main/math-rm.\nBoN evaluation result for Mistral generator:\nModel\nMethod\nGSM8K\nMATH\nMistral-7B\nPass@1\n77.9\n28.4\nMistral-7B\nMajority Voting@1024\n84.2\n36.8\nMistral-7B\nMistral-ORM@1024\n90.1\n43.6\nMistral-7B\nMistral-PRM@1024\n92.4\n46.3\nScaling the inference sampling to N=1024 for Deepseek generator:\nModel\nMethod\nGSM8K\nMATH\nDeepseek-7B\nPass@1\n83.9\n38.4\nDeepseek-7B\nMajority Voting@1024\n89.7\n57.4\nDeepseek-7B\nDeepseek-ORM@1024\n93.4\n52.4\nDeepseek-7B\nDeepseek-PRM@1024\n93.0\n58.1\nDeepseek-7B\nMistral-ORM@1024 (OOD)\n90.3\n54.9\nDeepseek-7B\nMistral-PRM@1024 (OOD)\n91.9\n56.9\nVisualization\nUsage\nSee https://github.com/RLHFlow/RLHF-Reward-Modeling/tree/main/math-rm for detailed examples.\nCitation\nThe automatic annotation was proposed in the Math-shepherd paper:\n@inproceedings{wang2024math,\ntitle={Math-shepherd: Verify and reinforce llms step-by-step without human annotations},\nauthor={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, Runxin and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang},\nbooktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\npages={9426--9439},\nyear={2024}\n}\nIf you find the training recipe useful, please consider cite it as follows.\n@misc{xiong2024implementation,\ntitle={An implementation of generative prm},\nauthor={Xiong, Wei and Zhang, Hanning and Jiang, Nan and Zhang, Tong},\nyear={2024}\n}",
    "gpustack/stable-diffusion-v2-1-turbo-GGUF": "stable-diffusion-v2-1-turbo-GGUF\nSD-Turbo Model Card\nModel Details\nModel Description\nModel Sources\nEvaluation\nUses\nDirect Use\nDiffusers\nOut-of-Scope Use\nLimitations and Bias\nLimitations\nRecommendations\nHow to Get Started with the Model\nstable-diffusion-v2-1-turbo-GGUF\nModel creator: Stability AI\nOriginal model: sd-turbo\nGGUF quantization: based on stable-diffusion.cpp ac54e that patched by llama-box.\nQuantization\nOpenCLIP ViT-H/14 Quantization\nVAE Quantization\nFP16\nFP16\nFP16\nQ8_0\nFP16\nFP16\nQ4_1\nFP16\nFP16\nQ4_0\nFP16\nFP16\nSD-Turbo Model Card\nSD-Turbo is a fast generative text-to-image model that can synthesize photorealistic images from a text prompt in a single network evaluation.\nWe release SD-Turbo as a research artifact, and to study small, distilled text-to-image models. For increased quality and prompt understanding,\nwe recommend SDXL-Turbo.\nPlease note: For commercial use, please refer to https://stability.ai/license.\nModel Details\nModel Description\nSD-Turbo is a distilled version of Stable Diffusion 2.1, trained for real-time synthesis.\nSD-Turbo is based on a novel training method called Adversarial Diffusion Distillation (ADD) (see the technical report), which allows sampling large-scale foundational\nimage diffusion models in 1 to 4 steps at high image quality.\nThis approach uses score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal and combines this with an\nadversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps.\nDeveloped by: Stability AI\nFunded by: Stability AI\nModel type: Generative text-to-image model\nFinetuned from model: Stable Diffusion 2.1\nModel Sources\nFor research purposes, we recommend our generative-models Github repository (https://github.com/Stability-AI/generative-models),\nwhich implements the most popular diffusion frameworks (both training and inference).\nRepository: https://github.com/Stability-AI/generative-models\nPaper: https://stability.ai/research/adversarial-diffusion-distillation\nDemo [for the bigger SDXL-Turbo]: http://clipdrop.co/stable-diffusion-turbo\nEvaluation\nThe charts above evaluate user preference for SD-Turbo over other single- and multi-step models.\nSD-Turbo evaluated at a single step is preferred by human voters in terms of image quality and prompt following over LCM-Lora XL and LCM-Lora 1.5.\nNote: For increased quality, we recommend the bigger version SDXL-Turbo.\nFor details on the user study, we refer to the research paper.\nUses\nDirect Use\nThe model is intended for both non-commercial and commercial usage. Possible research areas and tasks include\nResearch on generative models.\nResearch on real-time applications of generative models.\nResearch on the impact of real-time generative models.\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nFor commercial use, please refer to https://stability.ai/membership.\nExcluded uses are described below.\nDiffusers\npip install diffusers transformers accelerate --upgrade\nText-to-image:\nSD-Turbo does not make use of guidance_scale or negative_prompt, we disable it with guidance_scale=0.0.\nPreferably, the model generates images of size 512x512 but higher image sizes work as well.\nA single step is enough to generate high quality images.\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sd-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.to(\"cuda\")\nprompt = \"A cinematic shot of a baby racoon wearing an intricate italian priest robe.\"\nimage = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\nImage-to-image:\nWhen using SD-Turbo for image-to-image generation, make sure that num_inference_steps * strength is larger or equal\nto 1. The image-to-image pipeline will run for int(num_inference_steps * strength) steps, e.g. 0.5 * 2.0 = 1 step in our example\nbelow.\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import load_image\nimport torch\npipe = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/sd-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.to(\"cuda\")\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\").resize((512, 512))\nprompt = \"cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k\"\nimage = pipe(prompt, image=init_image, num_inference_steps=2, strength=0.5, guidance_scale=0.0).images[0]\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events,\nand therefore using the model to generate such content is out-of-scope for the abilities of this model.\nThe model should not be used in any way that violates Stability AI's Acceptable Use Policy.\nLimitations and Bias\nLimitations\nThe quality and prompt alignment is lower than that of SDXL-Turbo.\nThe generated images are of a fixed resolution (512x512 pix), and the model does not achieve perfect photorealism.\nThe model cannot render legible text.\nFaces and people in general may not be generated properly.\nThe autoencoding part of the model is lossy.\nRecommendations\nThe model is intended for both non-commercial and commercial usage.\nHow to Get Started with the Model\nCheck out https://github.com/Stability-AI/generative-models",
    "Qwen/Qwen2.5-Coder-0.5B": "Qwen2.5-Coder-0.5B\nIntroduction\nRequirements\nEvaluation & Performance\nCitation\nQwen2.5-Coder-0.5B\nIntroduction\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\nSignificantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\nA more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\nThis repo contains the 0.5B Qwen2.5-Coder model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\nNumber of Parameters: 0.49B\nNumber of Paramaters (Non-Embedding): 0.36B\nNumber of Layers: 24\nNumber of Attention Heads (GQA): 14 for Q and 2 for KV\nContext Length: Full 32,768 tokens\nWe do not recommend using base language models for conversations. Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., or fill in the middle tasks on this model.\nFor more details, please refer to our blog, GitHub, Documentation, Arxiv.\nRequirements\nThe code of Qwen2.5-Coder has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nEvaluation & Performance\nDetailed evaluation results are reported in this 📑 blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{hui2024qwen2,\ntitle={Qwen2. 5-Coder Technical Report},\nauthor={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\njournal={arXiv preprint arXiv:2409.12186},\nyear={2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "Snowflake/snowflake-arctic-embed-l-v2.0": "News\nModels\nQuality Benchmarks\nUsage\nUsing Sentence Transformers\nUsing Huggingface Transformers\nUsing Huggingface Transformers.js\nContact\nLicense\nSnowflake's Arctic-embed-l-v2.0\nNews |\nModels |\nUsage  |\nEvaluation |\nContact |\nFAQ\nLicense |\nAcknowledgement\nNews\n12/11/2024: Release of Technical Report\n12/04/2024: Release of snowflake-arctic-embed-l-v2.0 and snowflake-arctic-embed-m-v2.0 our newest models with multilingual workloads in mind.\nModels\nSnowflake arctic-embed-l-v2.0 is the newest addition to the suite of embedding models Snowflake has released optimizing for retrieval performance and inference efficiency.\nArctic Embed 2.0 introduces a new standard for multilingual embedding models, combining high-quality multilingual text retrieval without sacrificing performance in English.\nReleased under the permissive Apache 2.0 license, Arctic Embed 2.0 is ideal for applications that demand reliable, enterprise-grade multilingual search and retrieval at scale.\nKey Features:\nMultilingual without compromise: Excels in English and non-English retrieval, outperforming leading open-source and proprietary models on benchmarks like MTEB Retrieval, CLEF, and MIRACL.\nInference efficiency: Its 303m non-embedding parameters inference is fast and efficient for any scale.\nCompression-friendly: Achieves high-quality retrieval with embeddings as small as 128 bytes/vector using Matryoshka Representation Learning (MRL) and quantization-aware embedding training. Please note that like our v1.5 model, the MRL for this model is 256 dimensions, and high-quality 128-byte compression is achieved via 4-bit quantization (e.g. using a pq256x4fs fast-scan FAISS index or using the example code published alongside our 1.5 model).\nDrop-In Replacement: arctic-embed-l-v2.0 builds on BAAI/bge-m3-retromae which allows direct drop-in inference replacement with any form of new libraries, kernels, inference engines etc.\nLong Context Support: arctic-embed-l-v2.0 builds on BAAI/bge-m3-retromae which can support a context window of up to 8192 via the use of RoPE.\nQuality Benchmarks\nUnlike most other open-source models, Arctic-embed-l-v2.0 excels across English (via MTEB Retrieval) and multilingual (via MIRACL and CLEF).\nYou no longer need to support models to empower high-quality English and multilingual retrieval. All numbers mentioned below are the average NDCG@10 across the dataset being discussed.\nModel Name\n# params\n# non-emb params\n# dimensions\nBEIR (15)\nMIRACL (4)\nCLEF (Focused)\nCLEF (Full)\nsnowflake-arctic-l-v2.0\n568M\n303M\n1024\n55.6\n55.8\n52.9\n54.3\nsnowflake-arctic-m\n109M\n86M\n768\n54.9\n24.9\n34.4\n29.1\nsnowflake-arctic-l\n335M\n303M\n1024\n56.0\n34.8\n38.2\n33.7\nme5 base\n560M\n303M\n1024\n51.4\n54.0\n43.0\n34.6\nbge-m3 (BAAI)\n568M\n303M\n1024\n48.8\n56.8\n40.8\n41.3\ngte (Alibaba)\n305M\n113M\n768\n51.1\n52.3\n47.7\n53.1\nAside from high-quality retrieval arctic delivers embeddings that are easily compressible. Leverage vector truncation via MRL to decrease vector size by 4x with less than 3% degredation in quality.\nCombine MRLed vectors with vector compression (Int4) to power retrieval in 128 bytes per doc.\nModel\nBEIR (15)\nRelative Performance\nMIRACL (4)\nRelative Performance\nCLEF (5)\nRelative Performance\nCLEF (Full)\nRelative Performance\nsnowflake-arctic-l-v2.0\n1024\n55.6\nN/A\n55.8\nN/A\n52.9\nN/A\n54.3\nN/A\nsnowflake-arctic-l-v2.0\n256\n54.3\n-0.18%\n54.3\n-2.70%\n51.9\n-1.81%\n53.4\n-1.53%\nUsage\nUsing Sentence Transformers\nfrom sentence_transformers import SentenceTransformer\n# Load the model\nmodel_name = 'Snowflake/snowflake-arctic-embed-l-v2.0'\nmodel = SentenceTransformer(model_name)\n# Define the queries and documents\nqueries = ['what is snowflake?', 'Where can I get the best tacos?']\ndocuments = ['The Data Cloud!', 'Mexico City of Course!']\n# Compute embeddings: use `prompt_name=\"query\"` to encode queries!\nquery_embeddings = model.encode(queries, prompt_name=\"query\")\ndocument_embeddings = model.encode(documents)\n# Compute cosine similarity scores\nscores = model.similarity(query_embeddings, document_embeddings)\n# Output the results\nfor query, query_scores in zip(queries, scores):\ndoc_score_pairs = list(zip(documents, query_scores))\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\nprint(\"Query:\", query)\nfor document, score in doc_score_pairs:\nprint(score, document)\nUsing Huggingface Transformers\nYou can use the transformers package to use Snowflake's arctic-embed model, as shown below. For optimal retrieval quality, use the CLS token to embed each text portion and use the query prefix below (just on the query).\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\nmodel_name = 'Snowflake/snowflake-arctic-embed-l-v2.0'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name, add_pooling_layer=False)\nmodel.eval()\nquery_prefix = 'query: '\nqueries  = ['what is snowflake?', 'Where can I get the best tacos?']\nqueries_with_prefix = [\"{}{}\".format(query_prefix, i) for i in queries]\nquery_tokens = tokenizer(queries_with_prefix, padding=True, truncation=True, return_tensors='pt', max_length=8192)\ndocuments = ['The Data Cloud!', 'Mexico City of Course!']\ndocument_tokens =  tokenizer(documents, padding=True, truncation=True, return_tensors='pt', max_length=8192)\n# Compute token embeddings\nwith torch.no_grad():\nquery_embeddings = model(**query_tokens)[0][:, 0]\ndocument_embeddings = model(**document_tokens)[0][:, 0]\n# normalize embeddings\nquery_embeddings = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)\ndocument_embeddings = torch.nn.functional.normalize(document_embeddings, p=2, dim=1)\nscores = torch.mm(query_embeddings, document_embeddings.transpose(0, 1))\nfor query, query_scores in zip(queries, scores):\ndoc_score_pairs = list(zip(documents, query_scores))\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n#Output passages & scores\nprint(\"Query:\", query)\nfor document, score in doc_score_pairs:\nprint(score, document)\nThis should produce the following scores\nQuery: what is snowflake?\ntensor(0.2715) The Data Cloud!\ntensor(0.0661) Mexico City of Course!\nQuery: Where can I get the best tacos?\ntensor(0.2797) Mexico City of Course!\ntensor(0.1250) The Data Cloud!\nUsing Huggingface Transformers.js\nIf you haven't already, you can install the Transformers.js JavaScript library from NPM using:\nnpm i @huggingface/transformers\nYou can then use the model for retrieval, as follows:\nimport { pipeline, dot } from '@huggingface/transformers';\n// Create feature extraction pipeline\nconst extractor = await pipeline('feature-extraction', 'Snowflake/snowflake-arctic-embed-m-v2.0', {\ndtype: 'q8',\n});\n// Generate sentence embeddings\nconst sentences = [\n'query: what is snowflake?',\n'The Data Cloud!',\n'Mexico City of Course!',\n]\nconst output = await extractor(sentences, { normalize: true, pooling: 'cls' });\n// Compute similarity scores\nconst [source_embeddings, ...document_embeddings ] = output.tolist();\nconst similarities = document_embeddings.map(x => dot(source_embeddings, x));\nconsole.log(similarities); // [0.24783534471401417, 0.05313122704326892]\nContact\nFeel free to open an issue or pull request if you have any questions or suggestions about this project.\nYou also can email Daniel Campos(daniel.campos@snowflake.com).\nLicense\nArctic is licensed under the Apache-2. The released models can be used for commercial purposes free of charge.",
    "brgx53/3Blarenegv3-ECE-PRYMMAL-Martial": "my-output\nMerge Details\nMerge Method\nModels Merged\nConfiguration\nOpen LLM Leaderboard Evaluation Results\nmy-output\nThis is a merge of pre-trained language models created using mergekit.\nMerge Details\nMerge Method\nThis model was merged using the SLERP merge method.\nModels Merged\nThe following models were included in the merge:\nfblgit/cybertron-v4-qw7B-MGS\nTsunami-th/Tsunami-0.5x-7B-Instruct\nConfiguration\nThe following YAML configuration was used to produce this model:\nslices:\n- sources:\n- model: fblgit/cybertron-v4-qw7B-MGS\nlayer_range: [0, 28]\n- model: Tsunami-th/Tsunami-0.5x-7B-Instruct\nlayer_range: [0, 28]\nmerge_method: slerp\nbase_model: Tsunami-th/Tsunami-0.5x-7B-Instruct\nparameters:\nt:\n- filter: self_attn\nvalue: [1, 0.75, 0.5, 0.25, 0]\n- filter: mlp\nvalue: [0, 0.25, 0.5, 0.75, 1]\n- value: 0.5\ndtype: bfloat16\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n30.78\nIFEval (0-Shot)\n56.77\nBBH (3-Shot)\n37.25\nMATH Lvl 5 (4-Shot)\n30.74\nGPQA (0-shot)\n8.17\nMuSR (0-shot)\n12.79\nMMLU-PRO (5-shot)\n38.95",
    "Qwen/Qwen2.5-Coder-14B-Instruct-GGUF": "Qwen2.5-Coder-14B-Instruct-GGUF\nIntroduction\nQuickstart\nEvaluation & Performance\nCitation\nQwen2.5-Coder-14B-Instruct-GGUF\nIntroduction\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\nSignificantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\nA more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\nLong-context Support up to 128K tokens.\nThis repo contains the instruction-tuned 14B Qwen2.5-Coder model in the GGUF Format, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 14.7B\nNumber of Paramaters (Non-Embedding): 13.1B\nNumber of Layers: 48\nNumber of Attention Heads (GQA): 40 for Q and 8 for KV\nContext Length: Full 32,768 tokens\nNote: Currently, only vLLM supports YARN for length extrapolating. If you want to process sequences up to 131,072 tokens, please refer to non-GGUF models.\nQuantization: q2_K, q3_K_M, q4_0, q4_K_M, q5_0, q5_K_M, q6_K, q8_0\nFor more details, please refer to our blog, GitHub, Documentation, Arxiv.\nQuickstart\nCheck out our llama.cpp documentation for more usage guide.\nWe advise you to clone llama.cpp and install it following the official guide. We follow the latest version of llama.cpp.\nIn the following demonstration, we assume that you are running commands under the repository llama.cpp.\nSince cloning the entire repo may be inefficient, you can manually download the GGUF file that you need or use huggingface-cli:\nInstallpip install -U huggingface_hub\nDownload:huggingface-cli download Qwen/Qwen2.5-Coder-14B-Instruct-GGUF --include \"qwen2.5-coder-14b-instruct-q5_k_m*.gguf\" --local-dir . --local-dir-use-symlinks False\nFor large files, we split them into multiple segments due to the limitation of file upload. They share a prefix, with a suffix indicating its index. For examples, qwen2.5-coder-14b-instruct-q5_k_m-00001-of-00002.gguf and qwen2.5-coder-14b-instruct-q5_k_m-00002-of-00002.gguf. The above command will download all of them.\n(Optional) Merge:\nFor split files, you need to merge them first with the command llama-gguf-split as shown below:# ./llama-gguf-split --merge <first-split-file-path> <merged-file-path>\n./llama-gguf-split --merge qwen2.5-coder-14b-instruct-q5_k_m-00001-of-00002.gguf qwen2.5-coder-14b-instruct-q5_k_m.gguf\nFor users, to achieve chatbot-like experience, it is recommended to commence in the conversation mode:\n./llama-cli -m <gguf-file-path> \\\n-co -cnv -p \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\" \\\n-fa -ngl 80 -n 512\nEvaluation & Performance\nDetailed evaluation results are reported in this 📑 blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{hui2024qwen2,\ntitle={Qwen2. 5-Coder Technical Report},\nauthor={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\njournal={arXiv preprint arXiv:2409.12186},\nyear={2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}"
}