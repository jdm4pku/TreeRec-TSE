{
    "purplesmartai/pony-v7-base": "Pony V7\nFictional\nGet in touch with us\nImportant model information\nImportant HuggingFace links\nModel prompting\nSpecial Tags\nFactual description of image\nStylistic description of image\nTags\nCaptioning Colab\nSupported inference settings\nHighlights compared to V6\nSpecial thanks\nTechnical details\nLimitations\nLoRA training\nCommercial API\nLicense\nPony V7\nPony V7 is a versatile character generation model based on AuraFlow architecture. It supports a wide range of styles and species types (humanoid, anthro, feral, and more) and handles character interactions through natural language prompts.\nFictional\nFirst, let me introduce Fictional - our multimodal platform where AI Characters come alive through text, images, voice, and (soon) video. Powered by PonyV7, V6, Chroma, Seedream 4, and other advanced models, Fictional lets you discover, create, and interact with characters who live their own lives and share their own stories.\nFictional is also what enables the development of models like V7, so if you're excited about the future of multimodal AI characters, please download Fictional on iOS or Android and help shape our future!\niOS: https://apps.apple.com/us/app/fictional/id6739802573\nAndroid: https://play.google.com/store/apps/details?id=ai.fictional.app\nGet in touch with us\nPlease join our Discord Server if you have questions about Fictional and Pony models.\nImportant model information\nPlease check this article to learn more about why it took so long for us to ship V7 and upcoming model releases.\nImportant HuggingFace links\nGGUF Models - Quantized models for lower VRAM usage (Q8_0 recommended for best quality/size balance)\nSafetensor Model - Single-file safetensors format for easier loading\nLoRA Training - Information and tools for training LoRAs with SimpleTuner\nWorkflows - ComfyUI workflow examples for standard and GGUF inference\nComfyUI Nodes - Custom PonyNoise node for GPU/CPU noise selection\nModel prompting\nThis model supports a wide array of styles and aesthetics but provides an opinionated default prompt template:\nspecial tags, factual description of image, stylistic description of image, additional content tags\nSpecial Tags\nscore_X, style_cluster_x, source_X - warning: V7 prompting may be inconsistent, please see the article as we are working on V7.1 to address this.\nFactual description of image\nDescription of what is portrayed in the image without any stylistic indicators. Two recommendations:\nStart with a single phrase describing what you want in the image before going into details\nWhen referring to characters use pattern: <species> <gender> <name> from <source>\nFor example \"Anthro bunny female Lola Bunny from Space Jam\".\nThis model is capable of recognizing many popular and obscure characters and series.\nStylistic description of image\nAny information about image medium, shot type, lighting, etc. (More info TBD with captioning Colab)\nTags\nV7 is trained on a combination of natural language prompts and tags and is capable of understanding both, so describing the intended result using normal language works in most cases, although you can add some tags after the main prompt to boost them.\nCaptioning Colab\nTo get a better understanding of V7 prompting, we are releasing a captioning Colab with all the models used for V7 captioning.\nSupported inference settings\nV7 supports resolutions in the range of 768px to 1536px. It is recommended to go for higher resolutions and at least 30 steps during inference.\nHighlights compared to V6\nMuch stronger understanding of prompts, especially when it comes to spatial information and multiple characters\nMuch stronger background support - both generation of backgrounds and using background with character\nMuch stronger realism support out of the box\nAbility to generate very dark and very light images\nResolution up to 1536x1536 pixels\nExpanded character recognition (some V6 characters may get less recognized, but generally we extended the knowledge by a lot)\nSpecial thanks\nIceman for helping to procure necessary training resources\nSimo Ryu and the rest of FAL.ai team for creating AuraFlow and emotional support\nRunpod for providing captioning compute\nPiclumen for being our partners\nCity96 for help with GGUF support\ndiffusers team for supporting AuraFlow integration work\nPSAI Server Subscribers for supporting the project costs\nPSAI Server Moderators for being vigilant and managing the community\nMany supporters that decided to remain anonymous but their help has been critical for getting V7 done\nTechnical details\nThe model has been trained on ~10M images aesthetically ranked and selected from a superset of over 30M images with roughly 1:1 ratio between anime/cartoon/furry/pony datasets and 1:1 ratio between safe/questionable/explicit ratings. 100% of all images have been tagged and captioned with high quality detailed captions.\nAll images have been used in training with both captions and tags. Artists' names have been removed and source data has been filtered based on our Opt-in/Opt-out program. Any inappropriate explicit content has been filtered out.\nLimitations\nThis model does not support text generation and has degraded text generation capabilities compared to base AuraFlow\nSpecial tags (including quality tags) have much weaker performance compared to V6, meaning score_9 would not necessarily yield better results on some prompts. We are working on a V7.1 follow-up to improve this\nSmall details and especially faces may degrade significantly depending on art style, this is a combination of outdated VAE and insufficient training which we are trying to improve in V7.1\nLoRA training\nWe recommend using SimpleTuner for LoRA training following this guide.\nFor information on converting SimpleTuner LoRAs to diffusers/ComfyUI compatible format, see the LoRA folder. A LoRA workflow example is also available.\nCommercial API\nWe provide commercial API via our exclusive partner FAL.ai\nLicense\nThis model is licensed under a Pony License\nIn short, you can use this model and its outputs commercially unless you provide an inference service or application, have a company with over 1M revenue or use in professional video production. This limitations do not apply if you use first party commercial APIs.\nIf you want to use this model commercially, please reach us at contact@purplesmart.ai.\nExplicit permission for commercial inference has been granted to CivitAi and Hugging Face.",
    "black-forest-labs/FLUX.1-dev": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nBy clicking \"Agree\", you agree to the FluxDev Non-Commercial License Agreement and acknowledge the Acceptable Use Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nKey Features\nUsage\nAPI Endpoints\nComfyUI\nDiffusers\nLimitations\nOut-of-Scope Use\nLicense\nFLUX.1 [dev] is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions.\nFor more information, please read our blog post.\nKey Features\nCutting-edge output quality, second only to our state-of-the-art model FLUX.1 [pro].\nCompetitive prompt following, matching the performance of closed source alternatives .\nTrained using guidance distillation, making FLUX.1 [dev] more efficient.\nOpen weights to drive new scientific research, and empower artists to develop innovative workflows.\nGenerated outputs can be used for personal, scientific, and commercial purposes as described in the FLUX.1 [dev] Non-Commercial License.\nUsage\nWe provide a reference implementation of FLUX.1 [dev], as well as sampling code, in a dedicated github repository.\nDevelopers and creatives looking to build on top of FLUX.1 [dev] are encouraged to use this as a starting point.\nAPI Endpoints\nThe FLUX.1 models are also available via API from the following sources\nbfl.ml (currently FLUX.1 [pro])\nreplicate.com\nfal.ai\nmystic.ai\nComfyUI\nFLUX.1 [dev] is also available in Comfy UI for local inference with a node-based workflow.\nDiffusers\nTo use FLUX.1 [dev] with the üß® diffusers python library, first install or upgrade diffusers\npip install -U diffusers\nThen you can use FluxPipeline to run the model\nimport torch\nfrom diffusers import FluxPipeline\npipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", torch_dtype=torch.bfloat16)\npipe.enable_model_cpu_offload() #save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\nprompt = \"A cat holding a sign that says hello world\"\nimage = pipe(\nprompt,\nheight=1024,\nwidth=1024,\nguidance_scale=3.5,\nnum_inference_steps=50,\nmax_sequence_length=512,\ngenerator=torch.Generator(\"cpu\").manual_seed(0)\n).images[0]\nimage.save(\"flux-dev.png\")\nTo learn more check out the diffusers documentation\nLimitations\nThis model is not intended or able to provide factual information.\nAs a statistical model this checkpoint might amplify existing societal biases.\nThe model may fail to generate output that matches the prompts.\nPrompt following is heavily influenced by the prompting-style.\nOut-of-Scope Use\nThe model and its derivatives may not be used\nIn any way that violates any applicable national, federal, state, local or international law or regulation.\nFor the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.\nTo generate or disseminate verifiably false information and/or content with the purpose of harming others.\nTo generate or disseminate personal identifiable information that can be used to harm an individual.\nTo harass, abuse, threaten, stalk, or bully individuals or groups of individuals.\nTo create non-consensual nudity or illegal pornographic content.\nFor fully automated decision making that adversely impacts an individual's legal rights or otherwise creates or modifies a binding, enforceable obligation.\nGenerating or facilitating large-scale disinformation campaigns.\nLicense\nThis model falls under the FLUX.1 [dev] Non-Commercial License.",
    "Qwen/Qwen3-VL-32B-Thinking": "Qwen3-VL-32B-Thinking\nModel Performance\nQuickstart\nUsing ü§ó Transformers to Chat\nGeneration Hyperparameters\nCitation\nQwen3-VL-32B-Thinking\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-32B-Thinking.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.\nThe code of Qwen3-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ü§ó Transformers to Chat\nHere we show a code snippet to show you how to use the chat model with transformers:\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-32B-Thinking\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-32B-Thinking\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-32B-Thinking\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\ninputs = inputs.to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.95\nexport top_k=20\nexport repetition_penalty=1.0\nexport presence_penalty=0.0\nexport temperature=1.0\nexport out_seq_length=40960\nText\nexport greedy='false'\nexport top_p=0.95\nexport top_k=20\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport temperature=1.0\nexport out_seq_length=32768 (for aime, lcb, and gpqa, it is recommended to set to 81920)\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "deepseek-ai/DeepSeek-V3.2-Exp": "DeepSeek-V3.2-Exp\nIntroduction\nHow to Run Locally\nHuggingFace\nSGLang\nvLLM\nOpen-Source Kernels\nLicense\nCitation\nContact\nDeepSeek-V3.2-Exp\nIntroduction\nWe are excited to announce the official release of DeepSeek-V3.2-Exp, an experimental version of our model. As an intermediate step toward our next-generation architecture, V3.2-Exp builds upon V3.1-Terminus by introducing DeepSeek Sparse Attention‚Äîa sparse attention mechanism designed to explore and validate optimizations for training and inference efficiency in long-context scenarios.\nThis experimental release represents our ongoing research into more efficient transformer architectures, particularly focusing on improving computational efficiency when processing extended text sequences.\nDeepSeek Sparse Attention (DSA) achieves fine-grained sparse attention for the first time, delivering substantial improvements in long-context training and inference efficiency while maintaining virtually identical model output quality.\nTo rigorously evaluate the impact of introducing sparse attention, we deliberately aligned the training configurations of DeepSeek-V3.2-Exp with V3.1-Terminus. Across public benchmarks in various domains, DeepSeek-V3.2-Exp demonstrates performance on par with V3.1-Terminus.\nBenchmark\nDeepSeek-V3.1-Terminus\nDeepSeek-V3.2-Exp\nReasoning Mode w/o Tool Use\nMMLU-Pro\n85.0\n85.0\nGPQA-Diamond\n80.7\n79.9\nHumanity's Last Exam\n21.7\n19.8\nLiveCodeBench\n74.9\n74.1\nAIME 2025\n88.4\n89.3\nHMMT 2025\n86.1\n83.6\nCodeforces\n2046\n2121\nAider-Polyglot\n76.1\n74.5\nAgentic Tool Use\nBrowseComp\n38.5\n40.1\nBrowseComp-zh\n45.0\n47.9\nSimpleQA\n96.8\n97.1\nSWE Verified\n68.4\n67.8\nSWE-bench Multilingual\n57.8\n57.9\nTerminal-bench\n36.7\n37.7\nHow to Run Locally\nHuggingFace\nWe provide an updated inference demo code in the inference folder to help the community quickly get started with our model and understand its architectural details.\nFirst convert huggingface model weights to the the format required by our inference demo. Set MP to match your available GPU count:\ncd inference\nexport EXPERTS=256\npython convert.py --hf-ckpt-path ${HF_CKPT_PATH} --save-path ${SAVE_PATH} --n-experts ${EXPERTS} --model-parallel ${MP}\nLaunch the interactive chat interface and start exploring DeepSeek's capabilities:\nexport CONFIG=config_671B_v3.2.json\ntorchrun --nproc-per-node ${MP} generate.py --ckpt-path ${SAVE_PATH} --config ${CONFIG} --interactive\nSGLang\nInstallation with Docker\n# H200\ndocker pull lmsysorg/sglang:dsv32\n# MI350\ndocker pull lmsysorg/sglang:dsv32-rocm\n# NPUs\ndocker pull lmsysorg/sglang:dsv32-a2\ndocker pull lmsysorg/sglang:dsv32-a3\nLaunch Command\npython -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 --dp 8 --enable-dp-attention\nvLLM\nvLLM provides day-0 support of DeepSeek-V3.2-Exp. See the recipes for up-to-date details.\nOpen-Source Kernels\nFor TileLang kernels with better readability and research-purpose design, please refer to TileLang.\nFor high-performance CUDA kernels, indexer logit kernels (including paged versions) are available in DeepGEMM. Sparse attention kernels are released in FlashMLA.\nLicense\nThis repository and the model weights are licensed under the MIT License.\nCitation\n@misc{deepseekai2024deepseekv32,\ntitle={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention},\nauthor={DeepSeek-AI},\nyear={2025},\n}\nContact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "tahoebio/Tahoe-x1": "Tahoe-x1\nü§ñ Model Sizes\nüöÄ Quickstart\nüì¶ Installation\nüìö Tutorials\nüß¨ Generating Cell and Gene Embeddings\nUsing Configuration Files\nAdvanced Usage\nüèãÔ∏è Training and Fine-tuning\nQuick Training Example\nüìä Model Details\nArchitecture\nBenchmarks\nüìÑ Citation\nüìú License\nüìß Contact\nTahoe-x1\nTahoe-x1 is a family of perturbation-trained single-cell foundation models with up to 3 billion parameters, developed by Tahoe Therapeutics. Pretrained on 266 million single-cell transcriptomic profiles including the Tahoe-100M perturbation compendium, Tahoe-x1 achieves state-of-the-art performance on cancer-relevant tasks with 3‚Äì30√ó higher compute efficiency than other cell-state models.\nQuick Links:\n‚ú® Blog Post - Read our announcement\nüìÑ Preprint - Read our preprint on bioRxiv\nüíª GitHub Repository - Access the code\nüéÆ Interactive Demo - Try the model with no code required!\nü§ñ Model Sizes\nWe provide pretrained weights for three model sizes:\nTx1-70M: ~70M parameters\nTx1-1B: ~1.3B parameters\nTx1-3B: ~3B parameters\nüöÄ Quickstart\nLoad a model directly from Hugging Face and generate cell embeddings:\nfrom tahoe_x1.model import ComposerTX\nimport scanpy as sc\n# Load model from Hugging Face in a single line\n# Options: \"70m\", \"1b\", or \"3b\"\nmodel, vocab, model_cfg, collator_cfg = ComposerTX.from_hf(\nrepo_id=\"tahoebio/Tahoe-x1\",\nmodel_size=\"3b\"\n)\n# Load your single-cell data\nadata = sc.read_h5ad(\"your_data.h5ad\")\n# Generate embeddings (see tutorials for full example)\n# Cell embeddings are stored in adata.obsm\nüì¶ Installation\nTo use the models, install the tahoex package from GitHub:\n# Clone the repository\ngit clone https://github.com/tahoebio/tahoe-x1\ncd tahoe-x1\n# Install using Docker (recommended) or uv\n# See installation guide: https://github.com/tahoebio/tahoe-x1#installation\nDocker installation provides better reproducibility and is recommended for the best experience. For native installation, use uv or pip for dependency management.\nModel checkpoints and configs are automatically downloaded from this Hugging Face repository when using ComposerTX.from_hf(). Training data is hosted publicly on S3 (s3://tahoe-hackathon-data) and will be downloaded as needed.\nüìö Tutorials\nPlease refer to the tutorials in the GitHub repository for detailed examples:\nTutorial\nDescription\nLink\nClustering Tutorial\nCell clustering and UMAP visualization with Tahoe-x1 embeddings\nclustering_tutorial.ipynb\nTraining Tutorial\nStep-by-step guide to training and fine-tuning Tahoe-x1 models\ntraining_tutorial.ipynb\nüß¨ Generating Cell and Gene Embeddings\nUsing Configuration Files\nCreate a configuration file (see scripts/inference/configs/predict.yaml in the GitHub repo):\n# Key configuration options:\n#   - paths.hf_repo_id: Hugging Face repository (tahoebio/Tahoe-x1)\n#   - paths.hf_model_size: model size (70m, 1b, or 3b)\n#   - paths.adata_output: where to save AnnData output including embeddings\n#   - predict.return_gene_embeddings: True (for extracting gene embeddings)\nSee the Clustering Tutorial for a full example of preparing input data and configuration files.\n2. Run the embedding script:\npython scripts/inference/predict_embeddings.py path/to/config.yaml\n# Optional: override config values via command line\npython scripts/inference/predict_embeddings.py path/to/config.yaml \\\n--paths.model_name=tx --batch_size=128\nAdvanced Usage\nFor memory-efficient gene embedding extraction over large datasets, use the lower-level API:\nfrom tahoex.tasks import get_batch_embeddings\ncell_embs, gene_embs = get_batch_embeddings(\nadata=adata,\nmodel=model,\nvocab=vocab,\nmodel_cfg=model_cfg,\ncollator_cfg=collator_cfg,\nreturn_gene_embeddings=True\n)\nCell embeddings are saved to adata.obsm and gene embeddings to adata.varm (if return_gene_embeddings=True).\nüèãÔ∏è Training and Fine-tuning\nTahoe-x1 models can be trained from scratch or fine-tuned on your own data. See the GitHub repository for detailed instructions.\nQuick Training Example\n# Use the test configuration to train a small model on Tahoe-100M\ncomposer scripts/train.py -f configs/test_config.yaml\n# Fine-tune from a pretrained checkpoint\ncomposer scripts/train.py -f configs/finetune_config.yaml \\\n--load_path s3://tahoe-hackathon-data/MFM/ckpts/3b/\nFor more details on training infrastructure, datasets, and benchmarks, please visit the GitHub repository.\nüìä Model Details\nArchitecture\nBase Architecture: Transformer-based encoder pretrained with masked language modeling (MLM)\nTraining Data: 266M single-cell profiles from CellxGene, scBaseCamp, and Tahoe-100M\nTraining Infrastructure: Built on MosaicML Composer with FlashAttention, FSDP, and mixed precision\nBenchmarks\nTahoe-x1 achieves state-of-the-art performance on cancer-relevant tasks:\nDepMap Essentiality: Predicting gene dependencies in cancer cell lines\nMSigDB Hallmarks: Recovering pathway memberships from gene embeddings\nCell-Type Classification: Classifying cell types across multiple tissues\nPerturbation Prediction: Predicting transcriptional responses to perturbations\nSee the paper for detailed benchmark results.\nüìÑ Citation\nIf you use Tahoe-x1 in your research, please cite:\n@article{gandhi2025tahoe,\ntitle        = {Tahoe-x1: Scaling Perturbation-Trained Single-Cell Foundation Models to 3 Billion Parameters},\nauthor       = {Gandhi, Shreshth and Javadi, Farnoosh and Svensson, Valentine and Khan, Umair and Jones, Matthew G. and Yu, Johnny and Merico, Daniele and Goodarzi, Hani and Alidoust, Nima},\njournal      = {bioRxiv},\nyear         = {2025},\ndoi          = {10.1101/2025.10.23.683759},\nurl          = {https://www.biorxiv.org/content/10.1101/2025.10.23.683759v1},\npublisher    = {Cold Spring Harbor Laboratory}\n}\nüìú License\nModel weights and code are released under the Apache 2.0 license.\nüìß Contact\nFor questions or collaboration inquiries, please:\nOpen an issue on GitHub or HuggingFace\nEmail us at admin@tahoebio.ai",
    "cerebras/GLM-4.5-Air-REAP-82B-A12B": "GLM-4.5-Air-REAP-82B-A12B\n‚ú® Highlights\nüìã Model Overview\nüìä Evaluations\nüöÄ Deployment\nüß© Model Creation\nHow REAP Works\nKey Advantages\nCalibration\n‚öñÔ∏è License\nüßæ Citation\nìå≥ REAPìå≥  the Experts: Why Pruning Prevails for One-Shot MoE Compression\nGLM-4.5-Air-REAP-82B-A12B\n‚ú® Highlights\nIntroducing GLM-4.5-Air-REAP-82B-A12B, a memory-efficient compressed variant of GLM-4.5-Air that maintains near-identical performance while being 25% lighter.\nThis model was created using REAP (Router-weighted Expert Activation Pruning), a novel expert pruning method that selectively removes redundant experts while preserving the router's independent control over remaining experts. Key features include:\nNear-Lossless Performance: Maintains almost identical accuracy on code generation, agentic coding, and function calling tasks compared to the full 106B model\n25% Memory Reduction: Compressed from 106B to 82B parameters, significantly lowering deployment costs and memory requirements\nPreserved Capabilities: Retains all core functionalities including code generation, agentic workflows, repository-scale understanding, and function calling\nDrop-in Compatibility: Works with vanilla vLLM - no source modifications or custom patches required\nOptimized for Real-World Use: Particularly effective for resource-constrained environments, local deployments, and academic research\nüìã Model Overview\nGLM-4.5-Air-REAP-82B-A12B has the following specifications:\nBase Model: GLM-4.5-Air\nCompression Method: REAP (Router-weighted Expert Activation Pruning)\nCompression Ratio: 25% expert pruning\nType: Sparse Mixture-of-Experts (SMoE) Causal Language Model\nNumber of Parameters: 82B total, 12B activated per token\nNumber of Layers: 46\nNumber of Attention Heads (GQA): 96 for Q and 8 for KV\nNumber of Experts: 96 (uniformly pruned from 128)\nNumber of Activated Experts: 8 per token\nContext Length: 131,072 tokens\nLicense: MIT\nüìä Evaluations\nBenchmark\nGLM-4.5-Air\nGLM-4.5-Air-REAP-82B-A12B\nCompression\n‚Äî\n25%\nCoding\nHumanEval\n92.7\n89.6\nHumanEval+\n86.0\n84.8\nMBPP\n86.2\n84.4\nMBPP+\n69.8\n69.6\nReasoning\nLiveCodeBench (25.01 - 25.05, thinking)\n39.6\n42.9\nGPQA diamond (thinking)\n65.2\n65.2\nAIME24 (thinking)\n83.3\n80.0\nMATH-500 (thinking)\n94.8\n94.8\nTool Calling\nBFCL-v3\n73.4\n71.8\nBFCL-v3 (thinking)\n76.8\n76.3\nùúè¬≤-bench (airline)\n63.3\n64.0\nùúè¬≤-bench (retail)\n72.8\n75.1\nùúè¬≤-bench (telecom)\n28.4\n30.7\nùúè¬≤-bench (telecom, thinking)\n27.2\n26.9\nüü© This checkpoint maintains almost identical performance while being 25% lighter.\nFor more details on the evaluation setup, refer to the REAP arXiv preprint.\nüöÄ Deployment\nYou can deploy the model directly using the latest vLLM (v0.11.0), no source modifications or custom patches required.\nvllm serve cerebras/GLM-4.5-Air-REAP-82B-A12B \\\n--tensor-parallel-size 4 \\\n--tool-call-parser glm45 \\\n--enable-auto-tool-choice \\\n--enable-expert-parallel\nIf you encounter insufficient memory when running this model, you might need to set a lower value for --max-num-seqs flag (e.g. set to 64).\nüß© Model Creation\nThis checkpoint was created by applying the REAP (Router-weighted Expert Activation Pruning) method uniformly across all Mixture-of-Experts (MoE) blocks of GLM-4.5-Air, with a 25% pruning rate.\nHow REAP Works\nREAP selects experts to prune based on a novel saliency criterion that considers both:\nRouter gate values: How frequently and strongly the router activates each expert\nExpert activation norms: The magnitude of each expert's output contributions\nThis dual consideration ensures that experts contributing minimally to the layer's output are pruned, while preserving those that play critical roles in the model's computations.\nKey Advantages\nOne-Shot Compression: No fine-tuning required after pruning - the model is immediately ready for deployment\nPreserved Router Control: Unlike expert merging methods, REAP maintains the router's independent, input-dependent control over remaining experts, avoiding \"functional subspace collapse\"\nGenerative Task Superiority: REAP significantly outperforms expert merging approaches on generative benchmarks (code generation, creative writing, mathematical reasoning) while maintaining competitive performance on discriminative tasks\nCalibration\nThe model was calibrated using a diverse mixture of domain-specific datasets including:\nCode generation samples (evol-codealpaca)\nFunction calling examples (xlam-function-calling)\nAgentic multi-turn trajectories (SWE-smith-trajectories)\nüìö For more details, refer to the following resources:\nüßæ arXiv Preprint\nüßæ REAP Blog\nüíª REAP Codebase (GitHub)\n‚öñÔ∏è License\nThis model is derived from\nzai-org/GLM-4.5-Air\nand distributed under the MIT license.\nüßæ Citation\nIf you use this checkpoint, please cite the REAP paper:\n@article{lasby-reap,\ntitle={REAP the Experts: Why Pruning Prevails for One-Shot MoE compression},\nauthor={Lasby, Mike and Lazarevich, Ivan and Sinnadurai, Nish and Lie, Sean and Ioannou, Yani and Thangarasa, Vithursan},\njournal={arXiv preprint arXiv:2510.13999},\nyear={2025}\n}",
    "inclusionAI/LLaDA2.0-flash-preview": "LLaDA2.0-flash-preview\nüöÄ Performance Highlights\nüó∫Ô∏è What's Next\nüì¶ Model Variants\nüîç Model Overview\nü§ó Hugging Face Transformers\nBest Practices\nüåê License\nü§ù Contact & Collaboration\nLLaDA2.0-flash-preview\nLLaDA2.0-flash-preview is a diffusion language model featuring a 100BA6B Mixture-of-Experts (MoE) architecture. As an enhanced, instruction-tuned iteration of the LLaDA2.0 series, it is optimized for practical applications.\nBenchmark\nLing-flash-2.0\nLLaDA2.0-mini-preview\nLLaDA2.0-flash-preview\nAverage\n79.93\n66.89\n77.03\nKnowledge\nMMLU\n87.98\n72.49\n83.15\nMMLU-PRO\n76.84\n49.22\n66.16\nCMMLU\n86.59\n67.53\n79.64\nC-EVAL\n88.03\n66.54\n79.28\nReasoning\nsquad2.0\n81.32\n85.61\n90.61\ndrop\n88.32\n79.49\n88.17\nkorbench\n68.96\n37.26\n53.28\nCoding\nCruxEval-O\n82.75\n61.88\n74.50\nmbpp\n85.01\n77.75\n86.65\nMultiPL-E\n65.76\n62.43\n72.38\nhumaneval\n85.98\n80.49\n88.41\nBigcodebench-Full\n40.70\n30.44\n40.44\nMath\nGSM8K\n95.45\n89.01\n95.75\nmath\n96.1\n73.50\n83.52\nAgent & Alignment\nBFCL_Live\n67.57\n74.11\n74.86\nIFEval-strict -prompt\n81.52\n62.50\n75.60\nüöÄ Performance Highlights\nLeading MoE Architecture:\nThe open-source Mixture-of-Experts (MoE) diffusion large language model, pre-trained from scratch on approximately 20 trillion tokens.\nEfficient Inference:\nWith 100 billion total parameters, only 6.1 billion are activated during inference. LLaDA2.0-flash-preview significantly reduces computational costs while outperforming open-source dense models of similar scale.\nImpressive Performance on Code & Complex Reasoning:\nExcels in tasks such as code generation and advanced mathematical reasoning, demonstrating strong reasoning capabilities.\nTool Use:\nSupports tool calling and achieves excellent performance in complex agent-based tasks.\nOpen & Extensible:\nFully open-source with commitment to transparency. We plan to release a leading inference framework in the future and continue investing in cutting-edge areas like diffusion LLMs (dLLM) to drive disruptive innovation.\nüó∫Ô∏è What's Next\nSupercharged Reasoning with LLaDA 2.0: LLaDA 2.0 series will be fine-tuned with Reinforcement Learning, unlocking a new level of sophisticated reasoning and problem-solving abilities.\nTools for Innovators: The model was finetuned on the VeOmni framework using Fully Sharded Data Parallel (FSDP2). We will release a detailed tutorial and our complete post-training framework. Whether you want to master the current model or build your own customized versions, you'll have the tools you need. Stay tuned\nüì¶ Model Variants\nModel ID\nDescription\nHugging Face Link\ninclusionAI/LLaDA2.0-mini-preview\nInstruction-tuned model, ready for downstream applications.\nü§ó Model Card\ninclusionAI/LLaDA2.0-flash-preview\nInstruction-tuned model, ready for downstream applications.\nü§ó Model Card\nüîç Model Overview\nLLaDA2.0-flash-preview has the following specifications:\nType: Mixture-of-Experts (MoE) Diffusion Language Model\nTotal Parameters (Non-Embedding): 100B\nNumber of Layers: 32\nAttention Heads: 32\nContext Length: 4,096 tokens\nPosition Embedding: Rotary (RoPE)\nVocabulary Size: 157,184\nü§ó Hugging Face Transformers\nMake sure you have transformers and its dependencies installed:\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer\nmodel_path = \"/path/to/LLaDA2.0-mini-preview\"\ndevice = \"auto\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_path, trust_remote_code=True, device_map=device\n)\nmodel = model.to(torch.bfloat16)\nmodel.eval()\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\nprompt = \"Why does Camus think that Sisyphus is happy?\"\ninput_ids = tokenizer.apply_chat_template(\n[{\"role\": \"user\", \"content\": prompt}],\nadd_generation_prompt=True,\ntokenize=True,\nreturn_tensors=\"pt\",\n)\ngenerated_tokens = model.generate(\ninputs=input_ids,\neos_early_stop=True,\ngen_length=512,\nblock_length=32,\nsteps=32,\ntemperature=0.0,\n)\ngenerated_answer = tokenizer.decode(\ngenerated_tokens[0],\nskip_special_tokens=True,\n)\nprint(generated_answer)\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.0, block_length=32, and steps=32. Using a higher temperature value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length:\nWe recommend using an output length of 2048 tokens for most queries. For benchmarking on problems require more output length, such as those found in math and programming competitions, we suggest setting the max output length to 4096 tokens.\nüåê License\nThis project is licensed under the terms of the Apache License 2.0.\nü§ù Contact & Collaboration\nFor questions, collaborations, or feedback, please reach out via Hugging Face or open an issue in the repository.\nüëâ Join us in advancing open, efficient, and intelligent language models!",
    "openai/gpt-oss-120b": "Highlights\nInference examples\nTransformers\nvLLM\nPyTorch / Triton\nOllama\nDownload the model\nReasoning levels\nTool use\nFine-tuning\nCitation\nTry gpt-oss ¬∑\nGuides ¬∑\nModel card ¬∑\nOpenAI blog\nWelcome to the gpt-oss series, OpenAI‚Äôs open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.\nWe‚Äôre releasing two flavors of these open models:\ngpt-oss-120b ‚Äî for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)\ngpt-oss-20b ‚Äî for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)\nBoth models were trained on our harmony response format and should only be used with the harmony format as it will not work correctly otherwise.\nThis model card is dedicated to the larger gpt-oss-120b model. Check out gpt-oss-20b for the smaller model.\nHighlights\nPermissive Apache 2.0 license: Build freely without copyleft restrictions or patent risk‚Äîideal for experimentation, customization, and commercial deployment.\nConfigurable reasoning effort: Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.\nFull chain-of-thought: Gain complete access to the model‚Äôs reasoning process, facilitating easier debugging and increased trust in outputs. It‚Äôs not intended to be shown to end users.\nFine-tunable: Fully customize models to your specific use case through parameter fine-tuning.\nAgentic capabilities: Use the models‚Äô native capabilities for function calling, web browsing, Python code execution, and Structured Outputs.\nMXFP4 quantization: The models were post-trained with MXFP4 quantization of the MoE weights, making gpt-oss-120b run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the gpt-oss-20b model run within 16GB of memory. All evals were performed with the same MXFP4 quantization.\nInference examples\nTransformers\nYou can use gpt-oss-120b and gpt-oss-20b with Transformers. If you use the Transformers chat template, it will automatically apply the harmony response format. If you use model.generate directly, you need to apply the harmony format manually using the chat template or use our openai-harmony package.\nTo get started, install the necessary dependencies to setup your environment:\npip install -U transformers kernels torch\nOnce, setup you can proceed to run the model by running the snippet below:\nfrom transformers import pipeline\nimport torch\nmodel_id = \"openai/gpt-oss-120b\"\npipe = pipeline(\n\"text-generation\",\nmodel=model_id,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n]\noutputs = pipe(\nmessages,\nmax_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\nAlternatively, you can run the model via Transformers Serve to spin up a OpenAI-compatible webserver:\ntransformers serve\ntransformers chat localhost:8000 --model-name-or-path openai/gpt-oss-120b\nLearn more about how to use gpt-oss with Transformers.\nvLLM\nvLLM recommends using uv for Python dependency management. You can use vLLM to spin up an OpenAI-compatible webserver. The following command will automatically download the model and start the server.\nuv pip install --pre vllm==0.10.1+gptoss \\\n--extra-index-url https://wheels.vllm.ai/gpt-oss/ \\\n--extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\\n--index-strategy unsafe-best-match\nvllm serve openai/gpt-oss-120b\nLearn more about how to use gpt-oss with vLLM.\nPyTorch / Triton\nTo learn about how to use this model with PyTorch and Triton, check out our reference implementations in the gpt-oss repository.\nOllama\nIf you are trying to run gpt-oss on consumer hardware, you can use Ollama by running the following commands after installing Ollama.\n# gpt-oss-120b\nollama pull gpt-oss:120b\nollama run gpt-oss:120b\nLearn more about how to use gpt-oss with Ollama.\nLM Studio\nIf you are using LM Studio you can use the following commands to download.\n# gpt-oss-120b\nlms get openai/gpt-oss-120b\nCheck out our awesome list for a broader collection of gpt-oss resources and inference partners.\nDownload the model\nYou can download the model weights from the Hugging Face Hub directly from Hugging Face CLI:\n# gpt-oss-120b\nhuggingface-cli download openai/gpt-oss-120b --include \"original/*\" --local-dir gpt-oss-120b/\npip install gpt-oss\npython -m gpt_oss.chat model/\nReasoning levels\nYou can adjust the reasoning level that suits your task across three levels:\nLow: Fast responses for general dialogue.\nMedium: Balanced speed and detail.\nHigh: Deep and detailed analysis.\nThe reasoning level can be set in the system prompts, e.g., \"Reasoning: high\".\nTool use\nThe gpt-oss models are excellent for:\nWeb browsing (using built-in browsing tools)\nFunction calling with defined schemas\nAgentic operations like browser tasks\nFine-tuning\nBoth gpt-oss models can be fine-tuned for a variety of specialized use cases.\nThis larger model gpt-oss-120b can be fine-tuned on a single H100 node, whereas the smaller gpt-oss-20b can even be fine-tuned on consumer hardware.\nCitation\n@misc{openai2025gptoss120bgptoss20bmodel,\ntitle={gpt-oss-120b & gpt-oss-20b Model Card},\nauthor={OpenAI},\nyear={2025},\neprint={2508.10925},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2508.10925},\n}",
    "lrzjason/QwenImage-Rebalance": "Rebalance Model Overview\nModel Overview\nTraining Strategy\nCaptioning & Metadata\nFor cosplay images, the JSON includes:\nFor high-quality photographs, the JSON structure emphasizes scene composition:\nInference Guidance\nTechnical Details\nPrevious Work\nContact\nSponsors me for more open source projects:\nRebalance Model Overview\nModel Overview\nRebalance is a high-fidelity image generation model trained on a curated dataset comprising thousands of cosplay photographs and handpicked, high-quality real-world images. All training data was sourced exclusively from publicly accessible internet content, and the dataset explicitly excludes any NSFW material.\nThe primary goal of Rebalance is to produce photorealistic outputs that overcome common AI artifacts‚Äîsuch as an oily, plastic, or overly flat appearance‚Äîdelivering images with natural texture, depth, and visual authenticity.\nTraining Strategy\nTraining was conducted in multiple stages, broadly divided into two phases:\nCosplay Photo TrainingFocused on refining facial expressions, pose dynamics, and overall human figure realism‚Äîparticularly for female subjects.\nHigh-Quality Photograph EnhancementAimed at elevating atmospheric depth, compositional balance, and aesthetic sophistication by leveraging professionally curated photographic references.\nCaptioning & Metadata\nThe model was trained using two complementary caption formats: plain text and structured JSON. Each data subset employed a tailored JSON schema to guide fine-grained control during generation.\nFor cosplay images, the JSON includes:\n{\n\"caption\": \"...\",\n\"image_type\": \"...\",\n\"image_style\": \"...\",\n\"lighting_environment\": \"...\",\n\"tags_list\": [...],\n\"brightness\": number,\n\"brightness_name\": \"...\",\n\"hpsv3_score\": score,\n\"aesthetics\": \"...\",\n\"cosplayer\": \"anonymous_id\"\n}\nNote: Cosplayer names are anonymized (using placeholder IDs) solely to help the model associate multiple images of the same subject during training‚Äîno real identities are preserved.\nFor high-quality photographs, the JSON structure emphasizes scene composition:\n{\n\"subject\": \"...\",\n\"foreground\": \"...\",\n\"midground\": \"...\",\n\"background\": \"...\",\n\"composition\": \"...\",\n\"visual_guidance\": \"...\",\n\"color_tone\": \"...\",\n\"lighting_mood\": \"...\",\n\"caption\": \"...\"\n}\nIn addition to structured JSON, all images were also trained with plain-text captions and with randomized caption dropout (i.e., some training steps used no caption or partial metadata). This dual approach enhances both controllability and generalization.\nInference Guidance\nFor maximum aesthetic precision and stylistic control, use the full JSON format during inference.\nFor broader generalization or simpler prompting, plain-text captions are recommended.\nTechnical Details\nAll training was performed using lrzjason/T2ITrainer, a customized extension of the Hugging Face Diffusers DreamBooth training script. The framework supports advanced text-to-image architectures, including Qwen and Qwen-Edit (2509).\nPrevious Work\nThis project builds upon several prior tools developed to enhance controllability and efficiency in diffusion-based image generation and editing:\nComfyUI-QwenEditUtilsA collection of utility nodes for Qwen-based image editing in ComfyUI, enabling multi-reference image conditioning, flexible resizing, and precise prompt encoding for advanced editing workflows.\nComfyUI-LoraUtilsA suite of nodes for advanced LoRA manipulation in ComfyUI, supporting fine-grained control over LoRA loading, layer-wise modification (via regex and index ranges), and selective application to diffusion or CLIP models.\nT2ITrainerA lightweight, Diffusers-based training framework designed for efficient LoRA (and LoKr) training across multiple architectures‚Äîincluding Qwen Image, Qwen Edit, Flux, SD3.5, and Kolors‚Äîwith support for single-image, paired, and multi-reference training paradigms.\nThese tools collectively establish a robust ecosystem for training, editing, and deploying personalized diffusion models with high precision and flexibility.\nContact\nFeel free to reach out via any of the following channels:\nTwitter: @Lrzjason\nEmail: lrzjason@gmail.com\nQQ Group: 866612947\nWeChat ID: fkdeai\nCivitAI: xiaozhijason\nSponsors me for more open source projects:\nBuy me a coffee:\nWeChat:",
    "Kijai/WanVideo_comfy": "CausVid LoRAs are experimental extractions from the CausVid finetunes, the aim with them is to benefit from the distillation in CausVid, rather than any actual causal inference.\nCombined and quantized models for WanVideo, originating from here:\nhttps://huggingface.co/Wan-AI/\nCan be used with: https://github.com/kijai/ComfyUI-WanVideoWrapper and ComfyUI native WanVideo nodes.\nI've also started to do fp8_scaled versions over here: https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled\nOther model sources:\nTinyVAE from https://github.com/madebyollin/taehv\nSkyReels: https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9\nWanVideoFun: https://huggingface.co/collections/alibaba-pai/wan21-fun-v11-680f514c89fe7b4df9d44f17\nLightx2v:\nCausVid 14B: https://huggingface.co/lightx2v/Wan2.1-T2V-14B-CausVid\nCFG and Step distill 14B: https://huggingface.co/lightx2v/Wan2.1-T2V-14B-StepDistill-CfgDistill\nCausVid 1.3B: https://huggingface.co/tianweiy/CausVid\nAccVideo: https://huggingface.co/aejion/AccVideo-WanX-T2V-14B\nPhantom: https://huggingface.co/bytedance-research/Phantom\nATI: https://huggingface.co/bytedance-research/ATI\nMiniMaxRemover: https://huggingface.co/zibojia/minimax-remover\nMAGREF: https://huggingface.co/MAGREF-Video/MAGREF\nFantasyTalking: https://github.com/Fantasy-AMAP/fantasy-talking\nMultiTalk: https://github.com/MeiGen-AI/MultiTalk\nAnisora: https://huggingface.co/IndexTeam/Index-anisora/tree/main/14B\nPusa: https://huggingface.co/RaphaelLiu/PusaV1/tree/main\nFastVideo: https://huggingface.co/FastVideo\nEchoShot: https://github.com/D2I-ai/EchoShot\nWan22 5B Turbo: https://huggingface.co/quanhaol/Wan2.2-TI2V-5B-Turbo\nOvi: https://github.com/character-ai/Ovi\nFlashVSR: https://huggingface.co/JunhaoZhuang/FlashVSR\nrCM: https://huggingface.co/worstcoder/rcm-Wan/tree/main\nCausVid LoRAs are experimental extractions from the CausVid finetunes, the aim with them is to benefit from the distillation in CausVid, rather than any actual causal inference.\nv1 = direct extraction, has adverse effects on motion and introduces flashing artifact at full strength.\nv1.5 = same as above, but without the first block which fixes the flashing at full strength.\nv2 = further pruned version with only attention layers and no first block, fixes flashing and retains motion better, needs more steps and can also benefit from cfg.",
    "hlwang06/HoloCine": "README.md exists but content is empty.",
    "Qwen/Qwen3-VL-4B-Instruct": "Qwen3-VL-4B-Instruct\nModel Performance\nQuickstart\nUsing ü§ó Transformers to Chat\nGeneration Hyperparameters\nCitation\nQwen3-VL-4B-Instruct\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-4B-Instruct.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.\nThe code of Qwen3-VL has been in the latest Hugging Face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ü§ó Transformers to Chat\nHere we show a code snippet to show how to use the chat model with transformers:\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-4B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-4B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-4B-Instruct\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\ninputs = inputs.to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.8\nexport top_k=20\nexport temperature=0.7\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport out_seq_length=16384\nText\nexport greedy='false'\nexport top_p=1.0\nexport top_k=40\nexport repetition_penalty=1.0\nexport presence_penalty=2.0\nexport temperature=1.0\nexport out_seq_length=32768\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "QingyanBai/Ditto_models": "Ditto: Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset\nAbstract\nModel Usage\n1. Using with DiffSynth\n2. Using with ComfyUI\nCitation\nAcknowledgments\nLicense\nDitto: Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset\nThis repository contains the Ditto framework and the Editto model, which are introduced in the paper Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset. Ditto provides a holistic approach to address the scarcity of high-quality training data for instruction-based video editing, enabling the creation of the Ditto-1M dataset and the training of the state-of-the-art Editto model.\nüìÑ Paper\nüåê Project Page\nüíª GitHub Repository\nüì¶ Model Weights (on HF)\nüìä Dataset (on HF)\nAbstract\nInstruction-based video editing promises to democratize content creation, yet its progress is severely hampered by the scarcity of large-scale, high-quality training data. We introduce Ditto, a holistic framework designed to tackle this fundamental challenge. At its heart, Ditto features a novel data generation pipeline that fuses the creative diversity of a leading image editor with an in-context video generator, overcoming the limited scope of existing models. To make this process viable, our framework resolves the prohibitive cost-quality trade-off by employing an efficient, distilled model architecture augmented by a temporal enhancer, which simultaneously reduces computational overhead and improves temporal coherence. Finally, to achieve full scalability, this entire pipeline is driven by an intelligent agent that crafts diverse instructions and rigorously filters the output, ensuring quality control at scale. Using this framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of one million high-fidelity video editing examples. We trained our model, Editto, on Ditto-1M with a curriculum learning strategy. The results demonstrate superior instruction-following ability and establish a new state-of-the-art in instruction-based video editing.\nModel Usage\n1. Using with DiffSynth\nEnvironment Setup\n# Create conda environment (if you already have a DiffSynth conda environment, you can reuse it)\nconda create -n ditto python=3.10\nconda activate ditto\npip install -e .\nDownload Models\nDownload the base model and our models from Google Drive or Hugging Face:\n# Download Wan-AI/Wan2.1-VACE-14B from Hugging Face to models/Wan-AI/\nhuggingface-cli download Wan-AI/Wan2.1-VACE-14B --local-dir models/Wan-AI/\n# Download Ditto models\nhuggingface-cli download QingyanBai/Ditto_models --include=\"models/*\" --local-dir ./\nUsage\nYou can either use the provided script or run Python directly:\n# Option 1: Use the provided script\nbash infer.sh\n# Option 2: Run Python directly\npython inference/infer_ditto.py \\\n--input_video /path/to/input_video.mp4 \\\n--output_video /path/to/output_video.mp4 \\\n--prompt \"Editing instruction.\" \\\n--lora_path /path/to/model.safetensors \\\n--num_frames 73 \\\n--device_id 0\nSome test cases could be found at HF Dataset. You can also find some reference editing prompts in inference/example_prompts.txt.\n2. Using with ComfyUI\nNote: While ComfyUI runs faster with lower computational requirements (832√ó480x73 videos need 11G GPU memory and ~4min on A6000), please note that due to the use of quantized and distilled models, there may be some quality degradation.\nEnvironment Setup\nFirst, follow the ComfyUI installation guide to set up the base ComfyUI environment.\nWe strongly recommend installing ComfyUI-Manager for easy custom node management:\n# Install ComfyUI-Manager\ncd ComfyUI/custom_nodes\ngit clone https://github.com/Comfy-Org/ComfyUI-Manager.git\nAfter installing ComfyUI, you can either:\nOption 1 (Recommended): Use ComfyUI-Manager to automatically install all required custom nodes with the function Install Missing Custom Nodes.\nOption 2: Manually install the required custom nodes (you can refer to this page):\nComfyUI-WanVideoWrapper\nKJNodes for ComfyUI\ncomfyui-mixlab-nodes\nComfyUI-VideoHelperSuite\nDownload Models\nDownload the required model weights from: Kijai/WanVideo_comfy to subfolders of models/. Required files include:\nWan2_1-T2V-14B_fp8_e4m3fn.safetensors to diffusion_models/\nWan21_CausVid_14B_T2V_lora_rank32_v2.safetensors to loras/ for inference acceleration\nWan2_1_VAE_bf16.safetensors to vae/wan/\numt5-xxl-enc-bf16.safetensors to text_encoders/\nDownload our models from Google Drive or Hugging Face to diffusion_models/ (use VACE Module Select node for loading).\nUsage\nUse the workflow ditto_comfyui_workflow.json in this repo to get started.\nWe provided some reference prompts in the note.\nSome test cases could be found at HF Dataset.\nNote: If you want to test sim2real cases, you can try prompts like 'Turn it into the real domain'.\nCitation\nIf you find this work useful, please consider citing our paper:\n@article{bai2025ditto,\ntitle={Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset},\nauthor={Bai, Qingyan and Wang, Qiuyu and Ouyang, Hao and Yu, Yue and Wang, Hanlin and Wang, Wen and Cheng, Ka Leong and Ma, Shuailei and Zeng, Yanhong and Liu, Zichen and Xu, Yinghao and Shen, Yujun and Chen, Qifeng},\njournal={arXiv preprint arXiv:2510.15742},\nyear={2025}\n}\nAcknowledgments\nWe thank Wan & VACE & Qwen-Image for providing the powerful foundation model, and QwenVL for the advanced visual understanding capabilities. We also thank DiffSynth-Studio serving as the codebase for this repository.\nLicense\nThis project is licensed under the CC BY-NC-SA 4.0(Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License).\nThe code is provided for academic research purposes only.\nFor any questions, please contact qingyanbai@hotmail.com.",
    "Qwen/Qwen-Image-Edit-2509": "Introduction\nQuick Start\nShowcase\nLicense Agreement\nCitation\nüíú Qwen Chat¬†¬† | ¬†¬†ü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope¬†¬† | ¬†¬† üìë Tech Report ¬†¬† | ¬†¬† üìë Blog\nüñ•Ô∏è Demo¬†¬† | ¬†¬†üí¨ WeChat (ÂæÆ‰ø°)¬†¬† | ¬†¬†ü´® Discord¬†¬†| ¬†¬† Github\nIntroduction\nThis September, we are pleased to introduce Qwen-Image-Edit-2509, the monthly iteration of Qwen-Image-Edit. To experience the latest model, please visit Qwen Chat  and select the \"Image Editing\" feature.\nCompared with Qwen-Image-Edit released in August, the main improvements of Qwen-Image-Edit-2509 include:\nMulti-image Editing Support: For multi-image inputs, Qwen-Image-Edit-2509 builds upon the Qwen-Image-Edit architecture and is further trained via image concatenation to enable multi-image editing. It supports various combinations such as \"person + person,\" \"person + product,\" and \"person + scene.\" Optimal performance is currently achieved with 1 to 3 input images.\nEnhanced Single-image Consistency: For single-image inputs, Qwen-Image-Edit-2509 significantly improves editing consistency, specifically in the following areas:\nImproved Person Editing Consistency: Better preservation of facial identity, supporting various portrait styles and pose transformations;\nImproved Product Editing Consistency: Better preservation of product identity, supporting product poster editingÔºõ\nImproved Text Editing Consistency: In addition to modifying text content, it also supports editing text fonts, colors, and materialsÔºõ\nNative Support for ControlNet: Including depth maps, edge maps, keypoint maps, and more.\nQuick Start\nInstall the latest version of diffusers\npip install git+https://github.com/huggingface/diffusers\nThe following contains a code snippet illustrating how to use Qwen-Image-Edit-2509:\nimport os\nimport torch\nfrom PIL import Image\nfrom diffusers import QwenImageEditPlusPipeline\npipeline = QwenImageEditPlusPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit-2509\", torch_dtype=torch.bfloat16)\nprint(\"pipeline loaded\")\npipeline.to('cuda')\npipeline.set_progress_bar_config(disable=None)\nimage1 = Image.open(\"input1.png\")\nimage2 = Image.open(\"input2.png\")\nprompt = \"The magician bear is on the left, the alchemist bear is on the right, facing each other in the central park square.\"\ninputs = {\n\"image\": [image1, image2],\n\"prompt\": prompt,\n\"generator\": torch.manual_seed(0),\n\"true_cfg_scale\": 4.0,\n\"negative_prompt\": \" \",\n\"num_inference_steps\": 40,\n\"guidance_scale\": 1.0,\n\"num_images_per_prompt\": 1,\n}\nwith torch.inference_mode():\noutput = pipeline(**inputs)\noutput_image = output.images[0]\noutput_image.save(\"output_image_edit_plus.png\")\nprint(\"image saved at\", os.path.abspath(\"output_image_edit_plus.png\"))\nShowcase\nThe primary update in Qwen-Image-Edit-2509 is support for multi-image inputs.\nLet‚Äôs first look at a \"person + person\" example:\nHere is a \"person + scene\" example:\nBelow is a \"person + object\" example:\nIn fact, multi-image input also supports commonly used ControlNet keypoint maps‚Äîfor example, changing a person‚Äôs pose:\nSimilarly, the following examples demonstrate results using three input images:\nAnother major update in Qwen-Image-Edit-2509 is enhanced consistency.\nFirst, regarding person consistency, Qwen-Image-Edit-2509 shows significant improvement over Qwen-Image-Edit. Below are examples generating various portrait styles:\nFor instance, changing a person‚Äôs pose while maintaining excellent identity consistency:\nLeveraging this improvement along with Qwen-Image‚Äôs unique text rendering capability, we find that Qwen-Image-Edit-2509 excels at creating meme images:\nOf course, even with longer text, Qwen-Image-Edit-2509 can still render it while preserving the person‚Äôs identity:\nPerson consistency is also evident in old photo restoration. Below are two examples:\nNaturally, besides real people, generating cartoon characters and cultural creations is also possible:\nSecond, Qwen-Image-Edit-2509 specifically enhances product consistency. We find that the model can naturally generate product posters from plain-background product images:\nOr even simple logos:\nThird, Qwen-Image-Edit-2509 specifically enhances text consistency and supports editing font type, font color, and font material:\nMoreover, the ability for precise text editing has been significantly enhanced:\nIt is worth noting that text editing can often be seamlessly integrated with image editing‚Äîfor example, in this poster editing case:\nThe final update in Qwen-Image-Edit-2509 is native support for commonly used ControlNet image conditions, such as keypoint control and sketches:\nLicense Agreement\nQwen-Image is licensed under Apache 2.0.\nCitation\nWe kindly encourage citation of our work if you find it useful.\n@misc{wu2025qwenimagetechnicalreport,\ntitle={Qwen-Image Technical Report},\nauthor={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and Zenan Liu},\nyear={2025},\neprint={2508.02324},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2508.02324},\n}",
    "hexgrad/Kokoro-82M": "Releases\nUsage\nModel Facts\nTraining Details\nCreative Commons Attribution\nAcknowledgements\nKokoro is an open-weight TTS model with 82 million parameters. Despite its lightweight architecture, it delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, Kokoro can be deployed anywhere from production environments to personal projects.\nüêà GitHub: https://github.com/hexgrad/kokoro\nüöÄ Demo: https://hf.co/spaces/hexgrad/Kokoro-TTS\nAs of April 2025, the market rate of Kokoro served over API is under $1 per million characters of text input, or under $0.06 per hour of audio output. (On average, 1000 characters of input is about 1 minute of output.) Sources: ArtificialAnalysis/Replicate at 65 cents per M chars and DeepInfra at 80 cents per M chars.\nThis is an Apache-licensed model, and Kokoro has been deployed in numerous projects and commercial APIs. We welcome the deployment of the model in real use cases.\nFake websites like kokorottsai_com (snapshot: https://archive.ph/nRRnk) and kokorotts_net (snapshot: https://archive.ph/60opa) are likely scams masquerading under the banner of a popular model.\nAny website containing \"kokoro\" in its root domain (e.g. kokorottsai_com, kokorotts_net) is NOT owned by and NOT affiliated with this model page or its author, and attempts to imply otherwise are red flags.\nReleases\nUsage\nEVAL.md ‚ÜóÔ∏è\nSAMPLES.md ‚ÜóÔ∏è\nVOICES.md ‚ÜóÔ∏è\nModel Facts\nTraining Details\nCreative Commons Attribution\nAcknowledgements\nReleases\nModel\nPublished\nTraining Data\nLangs & Voices\nSHA256\nv1.0\n2025 Jan 27\nFew hundred hrs\n8 & 54\n496dba11\nv0.19\n2024 Dec 25\n<100 hrs\n1 & 10\n3b0c392f\nTraining Costs\nv0.19\nv1.0\nTotal\nin A100 80GB GPU hours\n500\n500\n1000\naverage hourly rate\n$0.80/h\n$1.20/h\n$1/h\nin USD\n$400\n$600\n$1000\nUsage\nYou can run this basic cell on Google Colab. Listen to samples. For more languages and details, see Advanced Usage.\n!pip install -q kokoro>=0.9.2 soundfile\n!apt-get -qq -y install espeak-ng > /dev/null 2>&1\nfrom kokoro import KPipeline\nfrom IPython.display import display, Audio\nimport soundfile as sf\nimport torch\npipeline = KPipeline(lang_code='a')\ntext = '''\n[Kokoro](/kÀàOk…ô…πO/) is an open-weight TTS model with 82 million parameters. Despite its lightweight architecture, it delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, [Kokoro](/kÀàOk…ô…πO/) can be deployed anywhere from production environments to personal projects.\n'''\ngenerator = pipeline(text, voice='af_heart')\nfor i, (gs, ps, audio) in enumerate(generator):\nprint(i, gs, ps)\ndisplay(Audio(data=audio, rate=24000, autoplay=i==0))\nsf.write(f'{i}.wav', audio, 24000)\nUnder the hood, kokoro uses misaki, a G2P library at https://github.com/hexgrad/misaki\nModel Facts\nArchitecture:\nStyleTTS 2: https://arxiv.org/abs/2306.07691\nISTFTNet: https://arxiv.org/abs/2203.02395\nDecoder only: no diffusion, no encoder release\nArchitected by: Li et al @ https://github.com/yl4579/StyleTTS2\nTrained by: @rzvzn on Discord\nLanguages: Multiple\nModel SHA256 Hash: 496dba118d1a58f5f3db2efc88dbdc216e0483fc89fe6e47ee1f2c53f18ad1e4\nTraining Details\nData: Kokoro was trained exclusively on permissive/non-copyrighted audio data and IPA phoneme labels. Examples of permissive/non-copyrighted audio include:\nPublic domain audio\nAudio licensed under Apache, MIT, etc\nSynthetic audio[1] generated by closed[2] TTS models from large providers\n[1] https://copyright.gov/ai/ai_policy_guidance.pdf\n[2] No synthetic audio from open TTS models or \"custom voice clones\"\nTotal Dataset Size: A few hundred hours of audio\nTotal Training Cost: About $1000 for 1000 hours of A100 80GB vRAM\nCreative Commons Attribution\nThe following CC BY audio was part of the dataset used to train Kokoro v1.0.\nAudio Data\nDuration Used\nLicense\nAdded to Training Set After\nKoniwa tnc\n<1h\nCC BY 3.0\nv0.19 / 22 Nov 2024\nSIWIS\n<11h\nCC BY 4.0\nv0.19 / 22 Nov 2024\nAcknowledgements\nüõ†Ô∏è @yl4579 for architecting StyleTTS 2.\nüèÜ @Pendrokar for adding Kokoro as a contender in the TTS Spaces Arena.\nüìä Thank you to everyone who contributed synthetic training data.\n‚ù§Ô∏è Special thanks to all compute sponsors.\nüëæ Discord server: https://discord.gg/QuGxSWBfQy\nü™Ω Kokoro is a Japanese word that translates to \"heart\" or \"spirit\". It is also the name of an AI in the Terminator franchise.",
    "zai-org/Glyph": "Known Limitations\nCitation\nGlyph: Scaling Context Windows via Visual-Text Compression\nRepository: https://github.com/thu-coai/Glyph\nPaper: https://arxiv.org/abs/2510.17800\nGlyph is a framework for scaling the context length through visual-text compression.\nInstead of extending token-based context windows, Glyph renders long textual sequences into images and processes them using vision‚Äìlanguage models (VLMs).\nThis design transforms the challenge of long-context modeling into a multimodal problem, substantially reducing computational and memory costs while preserving semantic information.\nBackbone Model\nOur model is built on GLM-4.1V-9B-Base.\nQuick Inference\nThis is a simple example of running single-image inference using the transformers library.First, install the transformers library:\npip install transformers>=4.57.1\nThen, run the following code:\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nimport torch\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"url\": \"https://raw.githubusercontent.com/thu-coai/Glyph/main/assets/Little_Red_Riding_Hood.png\"\n},\n{\n\"type\": \"text\",\n\"text\": \"Who pretended to be Little Red Riding Hood's grandmother\"\n}\n],\n}\n]\nprocessor = AutoProcessor.from_pretrained(\"zai-org/Glyph\")\nmodel = AutoModelForImageTextToText.from_pretrained(\npretrained_model_name_or_path=\"zai-org/Glyph\",\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\noutput_text = processor.decode(generated_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=False)\nprint(output_text)\nSee our Github Repo for more detailed usage.\nKnown Limitations\nSensitivity to rendering parameters: Glyph‚Äôs performance can vary with rendering settings such as resolution, font, and spacing. Since our search procedure adopts a fixed rendering configuration during post-training, the model may not generalize well to unseen or substantially different rendering styles.\nOCR-related challenges: Recognizing fine-grained or rare alphanumeric strings (e.g., UUIDs) remains difficult for visual-language models, especially with ultra-long inputs, sometimes leading to minor character misclassification.\nLimited generalization: The training of Glyph mainly targets long-context understanding, and its capability on broader tasks is yet to be studied.\nCitation\nIf you find our model useful in your work, please cite it with:\n@article{cheng2025glyphscalingcontextwindows,\ntitle={Glyph: Scaling Context Windows via Visual-Text Compression},\nauthor={Jiale Cheng and Yusen Liu and Xinyu Zhang and Yulin Fei and Wenyi Hong and Ruiliang Lyu and Weihan Wang and Zhe Su and Xiaotao Gu and Xiao Liu and Yushi Bai and Jie Tang and Hongning Wang and Minlie Huang},\njournal={arXiv preprint arXiv:2510.17800},\nyear={2025}\n}",
    "katanemo/Arch-Router-1.5B": "katanemo/Arch-Router-1.5B\nOverview\nHow It Works\nKey Features\nQuickstart\nRequirements\nQuickstart\nHow to use\nQuickstart\nLicense\nkatanemo/Arch-Router-1.5B\nOverview\nWith the rapid proliferation of large language models (LLMs) -- each optimized for different strengths, style, or latency/cost profile -- routing has become an essential technique to operationalize the use of different models. However, existing LLM routing approaches are limited in two key ways: they evaluate performance using benchmarks that often fail to capture human preferences driven by subjective evaluation criteria, and they typically select from a limited pool of models.\nWe introduce a preference-aligned routing framework that guides model selection by matching queries to user-defined domains (e.g., travel) or action types (e.g., image editing) -- offering a practical mechanism to encode preferences in routing decisions. Specifically, we introduce Arch-Router, a compact 1.5B model that learns to map queries to domain-action preferences for model routing decisions. Experiments on conversational datasets demonstrate that our approach achieves state-of-the-art (SOTA) results in matching queries with human preferences, outperforming top proprietary models.\nThis model is described in the paper: https://arxiv.org/abs/2506.16655, and powers Arch the models-native proxy server for agents.\nHow It Works\nTo support effective routing, Arch-Router introduces two key concepts:\nDomain ‚Äì the high-level thematic category or subject matter of a request (e.g., legal, healthcare, programming).\nAction ‚Äì the specific type of operation the user wants performed (e.g., summarization, code generation, booking appointment, translation).\nBoth domain and action configs are associated with preferred models or model variants. At inference time, Arch-Router analyzes the incoming prompt to infer its domain and action using semantic similarity, task indicators, and contextual cues. It then applies the user-defined routing preferences to select the model best suited to handle the request.\nKey Features\nStructured Preference Routing: Aligns prompt request with model strengths using explicit domain‚Äìaction mappings.\nTransparent and Controllable: Makes routing decisions transparent and configurable, empowering users to customize system behavior.\nFlexible and Adaptive: Supports evolving user needs, model updates, and new domains/actions without retraining the router.\nProduction-Ready Performance: Optimized for low-latency, high-throughput applications in multi-model environments.\nRequirements\nThe code of Arch-Router-1.5B has been in the Hugging Face transformers library and we advise you to install latest version:\npip install transformers>=4.37.0\nHow to use\nWe use the following example to illustrate how to use our model to perform routing tasks. Please note that, our model works best with our provided prompt format.\nQuickstart\nimport json\nfrom typing import Any, Dict, List\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"katanemo/Arch-Router-1.5B\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# Please use our provided prompt for best performance\nTASK_INSTRUCTION = \"\"\"\nYou are a helpful assistant designed to find the best suited route.\nYou are provided with route description within <routes></routes> XML tags:\n<routes>\n{routes}\n</routes>\n<conversation>\n{conversation}\n</conversation>\n\"\"\"\nFORMAT_PROMPT = \"\"\"\nYour task is to decide which route is best suit with user intent on the conversation in <conversation></conversation> XML tags.  Follow the instruction:\n1. If the latest intent from user is irrelevant or user intent is full filled, response with other route {\"route\": \"other\"}.\n2. You must analyze the route descriptions and find the best match route for user latest intent.\n3. You only response the name of the route that best matches the user's request, use the exact name in the <routes></routes>.\nBased on your analysis, provide your response in the following JSON formats if you decide to match any route:\n{\"route\": \"route_name\"}\n\"\"\"\n# Define route config\nroute_config = [\n{\n\"name\": \"code_generation\",\n\"description\": \"Generating new code snippets, functions, or boilerplate based on user prompts or requirements\",\n},\n{\n\"name\": \"bug_fixing\",\n\"description\": \"Identifying and fixing errors or bugs in the provided code across different programming languages\",\n},\n{\n\"name\": \"performance_optimization\",\n\"description\": \"Suggesting improvements to make code more efficient, readable, or scalable\",\n},\n{\n\"name\": \"api_help\",\n\"description\": \"Assisting with understanding or integrating external APIs and libraries\",\n},\n{\n\"name\": \"programming\",\n\"description\": \"Answering general programming questions, theory, or best practices\",\n},\n]\n# Helper function to create the system prompt for our model\ndef format_prompt(\nroute_config: List[Dict[str, Any]], conversation: List[Dict[str, Any]]\n):\nreturn (\nTASK_INSTRUCTION.format(\nroutes=json.dumps(route_config), conversation=json.dumps(conversation)\n)\n+ FORMAT_PROMPT\n)\n# Define conversations\nconversation = [\n{\n\"role\": \"user\",\n\"content\": \"fix this module 'torch.utils._pytree' has no attribute 'register_pytree_node'. did you mean: '_register_pytree_node'?\",\n}\n]\nroute_prompt = format_prompt(route_config, conversation)\nmessages = [\n{\"role\": \"user\", \"content\": route_prompt},\n]\ninput_ids = tokenizer.apply_chat_template(\nmessages, add_generation_prompt=True, return_tensors=\"pt\"\n).to(model.device)\n# 2. Generate\ngenerated_ids = model.generate(\ninput_ids=input_ids,  # or just positional: model.generate(input_ids, ‚Ä¶)\nmax_new_tokens=32768,\n)\n# 3. Strip the prompt from each sequence\nprompt_lengths = input_ids.shape[1]  # same length for every row here\ngenerated_only = [\noutput_ids[prompt_lengths:]  # slice off the prompt tokens\nfor output_ids in generated_ids\n]\n# 4. Decode if you want text\nresponse = tokenizer.batch_decode(generated_only, skip_special_tokens=True)[0]\nprint(response)\nThen you should be able to see the following output string in JSON format:\n{\"route\": \"bug_fixing\"}\nTo better understand how to create the route descriptions, please take a look at our Katanemo API.\nLicense\nKatanemo Arch-Router model is distributed under the Katanemo license.\nGitHub: https://github.com/katanemo/arch",
    "Qwen/Qwen3-VL-30B-A3B-Instruct": "Qwen3-VL-30B-A3B-Instruct\nModel Performance\nQuickstart\nUsing ü§ó Transformers to Chat\nCitation\nQwen3-VL-30B-A3B-Instruct\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-30B-A3B-Instruct.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.\nThe code of Qwen3-VL has been in the latest Hugging Face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ü§ó Transformers to Chat\nHere we show a code snippet to show how to use the chat model with transformers:\nfrom transformers import Qwen3VLMoeForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-30B-A3B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-30B-A3B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-30B-A3B-Instruct\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "RUC-DataLab/DeepAnalyze-8B": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science\nDeepAnalyze: Agentic Large Language Models for Autonomous Data Science\nAuthors: Shaolei Zhang, Ju Fan*, Meihao Fan, Guoliang Li, Xiaoyong Du\nDeepAnalyze is the first agentic LLM for autonomous data science. It can autonomously complete a wide range of data-centric tasks without human intervention, supporting:\nüõ† Entire data science pipeline: Automatically perform any data science tasks such as data preparation, analysis, modeling, visualization, and report generation.\nüîç Open-ended data research: Conduct deep research on diverse data sources, including structured data (Databases, CSV, Excel), semi-structured data (JSON, XML, YAML), and unstructured data (TXT, Markdown), and finally produce analyst-grade research reports.\nüìä Fully open-source: The model, code, training data, and demo of DeepAnalyze are all open-sourced, allowing you to deploy or extend your own data analysis assistant.\nMore information refer to DeepAnalyze's Repo",
    "dx8152/White_to_Scene": "Welcome everyone to use Lora of Qwen-Edit-2509 of Image Fusion, his performance is very amazingÔºÅ\nVideo tutorial: https://youtu.be/xhPDto-XgFs\nTrigger words:ÁôΩÂ∫ïÂõæËΩ¨Âú∫ÊôØ\nYou can add subject and scene description after the trigger word, for example: ÁôΩÂ∫ïÂõæËΩ¨Âú∫ÊôØ,Â∞ÜÂõæÁâá‰∏≠ÁöÑÊ±ΩËΩ¶ÊîæÂú®Êà∑Â§ñÂú∫ÊôØ‰∏≠ÔºàMeaningÔºöConvert the white background image to a scene and place the car in the picture in an outdoor sceneÔºâ\nYou also need this lora to use together: https://huggingface.co/lightx2v/Qwen-Image-Lightning/tree/main",
    "facebook/MobileLLM-Pro": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nMobileLLM-Pro (P1) Model Card\nKey Features\nModel Information\nResults\nBase Pretrained Model\nInstruction Tuned Model\nTraining Data\nTraining Process\nPretraining\nInstruction Fine-Tuning\nQuantization\nHow to use\nLatency benchmarking\nCitation\nContact\nAcknowledgements\nLicense\nMobileLLM-Pro (P1) Model Card\nWe are introducing MobileLLM-P1 or Pro, a 1B foundational language model in the MobileLLM series, designed to deliver high-quality, efficient on-device inference across a wide range of general language modeling tasks.\nWe open-source two variants of the model: A pre-trained base model along with quantized checkpoints for CPU and accelerator inference, as well as an instruction tuned version, showing competitive performance against models in the this size range on tasks like tool calling, question answering, rewriting and summarization.\nYou are currently in the MobileLLM-Pro (instruct) repository. For more versions, check out:\nMobileLLM-Pro Collection\nMobileLLM-Pro-base\nMobileLLM-Pro-base Int4 CPU Quantized\nMobileLLM-Pro-base Int4 Accelerator Quantized\nFor an interactive demo: ü§ó Click here\nKey Features\nStrong Pre-training Performance: MobileLLM-Pro base achieves impressive pre-training results, outperforming Gemma 3 1B and Llama 3.2 1B by on average 5.7% and 7.9% respectively on reasoning, knowledge, and long-context retrieval benchmarks. This performance is achieved by pre-training on less than 2T fully open-source tokens.\n128k Context Window: The model supports up to 128k tokens, enabling long-context understanding for applications such as document summarization and information retrieval, implicitly learned from a large teacher model.\nEfficient Long-Context Inference: Interleaving local and global attention layers at a 3:1 ratio with 512 local attention, MobileLLM-Pro reduces prefill latency by 1.8x* and lowers KV cache size from 117MB to 40MB* compared to fully global attention, enabling faster and more memory-efficient inference. (*Assuming 8k context length)\nNear Lossless int4 Quantization: We provide int4 quantization-ready checkpoints for our pre-trained model with less than 1.3% quality degradation compared to floating point baselines:\nCPU: int4 weights (group size 32), int8 dynamic activations, int8 KV cache, with only 0.4% regression.\nAccelerators: int4 per-channel weights, with only 1.3% quality regression.\nInstruction Fine-Tuned Model: We provide a competitive instruction fine-tuned (IFT) model specializing in use-cases such as tool calling, question answering, rewriting and summarization.\nMobileLLM-Pro sets a new standard for efficient, high-quality on-device language modeling. We invite the community to explore, evaluate, and build upon this model.\nModel Information\nLayers: 30\nAttention Heads: 20\nKV Heads: 4\nDimension: 1280\nHidden Dimension: 6144\nVocabulary Size: 202,048\nTotal Parameters: 1,084M (1.08B)\nInput Modality: Text\nOutput Modality: Text\nLanguages: English\nTraining Method: Knowledge Distillation\nContext Length: 128k tokens\nTeacher Model: Llama 4-Scout\nLoss Function: KL Divergence\nQuantization: 16-bit, 4-bit\nOther Features: Shared Embeddings, Local-Global Attention\nModel Developer: Meta Reality Labs\nModel Release Date:  October 2025\nLicense: MobileLLM-Pro is FAIR NC licensed\nResults\nBase Pretrained Model\nBenchmark\nP1 (FP)\nP1  (Q-CPU)\nP1 (Q-Acc)\nGemma 3 1B\nLlama 3.2 1B\nHellaSwag\n67.11%\n64.89%\n65.10%\n62.30%\n65.69%\nBoolQ\n76.24%\n77.49%\n76.36%\n63.20%\n62.51%\nPIQA\n76.55%\n76.66%\n75.52%\n73.80%\n75.14%\nSocialIQA\n50.87%\n51.18%\n50.05%\n48.90%\n45.60%\nTriviaQA\n39.85%\n37.26%\n36.42%\n39.80%\n23.81%\nNatQ\n15.76%\n15.43%\n13.19%\n9.48%\n5.48%\nARC-c\n52.62%\n52.45%\n51.24%\n38.40%\n38.28%\nARC-e\n76.28%\n76.58%\n75.73%\n73.00%\n63.47%\nWinoGrande\n62.83%\n62.43%\n61.96%\n58.20%\n61.09%\nOBQA\n43.60%\n44.20%\n40.40%\n37.20%\nNIH\n100.00%\n96.44%\n98.67%\nFP = Full precision, bf16\nQ-CPU = int4, group-wise quantized (for CPU)\nQ-Acc = int4, channel-wise quantized (for Accelerators (ANE&HTP))\nInstruction Tuned Model\nBenchmark\nP1 (IFT)\nGemma 3 1B (IFT)\nLlama 3.2 1B (IFT)\nMMLU\n44.8%\n29.9%\n49.3%\nIFEval\n62.0%\n80.2%\n59.5%\nMBPP\n46.8%\n35.2%\n39.6%\nHumanEval\n59.8%\n41.5%\n37.8%\nARC-C\n62.7%\n59.4%\nHellaSwag\n58.4%\n41.2%\nBFCL v2\n29.4%\n25.7%\nOpen Rewrite\n51.0%\n41.6%\nTLDR9+\n16.8%\n16.8%\nTraining Data\nWe constructed our datamix by selecting publicly available datasets that cover a range of domains. Using data-specific simulation runs, each dataset's contribution to the training process was carefully balanced by assigning it a specific sampling weight. These weights remained consistent throughout the base model pretraining and were informed by the extended work of Automixer and additional ablation studies.\nThe pre-training datamix primarily consists of a large educational web dataset, which makes up the vast majority of the training data. Smaller but significant portions come from coding data, mathematics, Wikipedia, scientific papers, Q&A forums, and algebraic content. In total, the datamix includes approximately 1,500 million rows and 1,640 billion tokens.\nFor our instruction fine-tuned data-mix, we focus on data diversity from existing open-source fine-tuning corpora. Specifically, we combine datasets for general instruction tuning with chat, science, safety, coding and math domains. For our final DPO phase, we rely on completely synthetic datasets.\nTraining Process\nPretraining\nOur general pre-training process contains three distinct phases using logit-based knowledge distillation from the Llama 4-Scout model and a novel model merging paradigm:\nPhase 1 (KD): Language Learning ‚Äì Learn general language skills from high-quality, well balanced pre-training data\nPhase 2 (KD): Long-context awareness ‚Äì Extend the model context-length to 128k tokens using implicit positional distillation from the teacher model\nPhase 3 (KD): Domain abilities ‚Äì Acquire domain understanding through annealing of multiple models in parallel and merging the specialist models, resulting in improvements across a diverse range of domains\nOn top of the three pre-training phases, we add a fourth phase of Quantization-Aware Training (QAT) for our 4-bit quantized model checkpoint.\nInstruction Fine-Tuning\nWe split the instruction fine-tuning stage into three distinct phases combining SFT and DPO methods:\nPhase 1 (SFT): Learn general instruction-following with a focus on data diversity\nPhase 2 (SFT): Domain-weight the Phase 1 data given its shortcomings (e.g. upsample code data to improve logical reasoning)\nPhase 3 (SFT + DPO): Train and align the model for safety and self-identification\nQuantization\nWe apply Quantization Aware Training (QAT) to our baseline and instruction fine-tuned models, yielding quantization-ready checkpoints that can either be directly converted to integer datatype (with minimal quality loss) or used for QAT on additional data. We release two quantization-ready checkpoints:\n4-bit groupwise weight quantization with block size 32, 8-bit dynamic activations, and 8-bit kv-cache quantizations ‚Äî optimized for CPU/GPU backends (xnnpack).\n4-bit channelwise quantization without activation quantization and 8-bit kv-cache quantizations ‚Äî designed for edge hardware accelerators such as Apple Neural Engine (ANE) and Qualcomm‚Äôs Hexagon Tensor Processor (HTP).\nOur QAT approach incorporates long-context awareness (up to 128k tokens) and self-knowledge distillation using the full-precision teacher model. We compared the QAT-trained model to a standard round-to-nearest Post-Training Quantization (PTQ) baseline. In the groupwise pre-training setting, we observe a 34% (absolute) regression in average benchmark score when using PTQ and only a 1.5% (absolute) regression for QAT. For instruction fine-tuning, we observe less than 1% average regression using QAT.\nHow to use\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom huggingface_hub import login\nlogin(token=\"<HF_TOKEN>\")\nMODEL_ID = \"facebook/MobileLLM-Pro\"\ndef generate(user_input: str, model, tokenizer) -> str:\nuser_input = [{\"role\": \"user\", \"content\": user_input}]\ninputs = tokenizer.apply_chat_template(\nuser_input, return_tensors=\"pt\", add_generation_prompt=True\n).to(model.device)\noutputs = model.generate(inputs, max_new_tokens=128)\nreturn tokenizer.decode(outputs[0], skip_special_tokens=True)\ndef main():\ntokenizer = AutoTokenizer.from_pretrained(\nMODEL_ID, trust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\nMODEL_ID, trust_remote_code=True\n)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\nprompt = \"Why are open-source on-device language models great?\"\nresult = generate(prompt, model, tokenizer)\nprint(result)\nif __name__ == \"__main__\":\nmain()\nLatency benchmarking\nLatency benchmarking was done on a Samsung Galaxy S25 CPU and Samsung Galaxy S24 Hexagon Tensor Processor (HTP). Models were exported to ExecuTorch with XNNPACK backend (for CPU) and HTP backend (for accelerator). The model size of the CPU model with 4-bit groupwise quantization is 590MB. The CPU and HTP prefill latency for different input prompt lengths of 2k, 4k and 8k along with decode speed for generating 1k tokens is shown in the following table.\nModel / Prompt length\n2k\n4k\n8k\nCPU Prefill Latency (s)\n8.9\n24.8\n63.5\nCPU Decode Speed (tok/s)\n33.6\n24.8\n19.7\nHTP Prefill Latency (s)\n1.96\n3.38\n9.82\nHTP Decode Speed (tok/s)\n31.60\n28.95\n22.77\nKV Cache Size (MB)\n14\n23\n40\nTo validate the benefit of interleaved local-global attention (LGA), we benchmark models across different prompt lengths and measure the speed-up in prefill & decode relative to using global attention at every layer:\nCitation\n@misc{mobilellm_pro,\ntitle={MobileLLM-Pro Model Card},\nauthor={Patrick Huber*, Ernie Chang*, Wei Wen*, Igor Fedorov*, Tarek Elgamal, Hanxian Huang, Naveen Suda, Chinnadhurai Sankar, Vish Vogeti, Yanghan Wang, Alex Gladkov, Kai Sheng Tai, Abdelrahman Elogeel, Tarek Hefny, Vikas Chandra, Ahmed Aly, Anuj Kumar, Raghuraman Krishnamoorthi**, Adithya Sagar**},\nyear={2025},\nmonth={October},\nurl = {https://huggingface.co/facebook/MobileLLM-Pro}}\nContact\nPatrick Huber, Meta Inc, Reality Labs (patrickhuber@meta.com)\nErnie Chang, Meta Inc, Reality Labs (erniecyc@meta.com)\nWei Wen,  Meta Inc, Reality Labs (wewen@meta.com)\nIgor Fedorov, Meta Inc, Reality Labs (ifedorov@meta.com)\nRaghuraman Krishnamoorthi,  Meta Inc Reality Labs (raghuraman@meta.com)\nAdithya Sagar, Meta Inc, Reality Labs (adithyasagar@meta.com)\nAcknowledgements\nWe want to thank the team involved in this project, especially: Kimish Patel, Andrew Or, Min Guo, Shen Xu, Brian Moran, Maho Takahashi, Claire Lesage, Rylan Conway, Karan Chadha, Matthew Grange, Tomasz Wo≈Çcyrz, Shiv Desai, Amarlin Anand, Joele Sires, Robert Carrillo, Francisc Bungiu, Jayden Yu, AJ Brush, Yang Li, Samuel Selvan, Anand Sharma, Peng Shan, Anand Dass, Abhishek Sharma\nLicense\nMobileLLM-Pro is distributed under the FAIR NC license",
    "ByteDance/Video-As-Prompt-Wan2.1-14B": "Video-As-Prompt: Unified Semantic Control for Video Generation\nüî• News\nüñåÔ∏è Video-As-Prompt\nüéÅ Models Zoo\nü§ó Get Started with Video-As-Prompt\nInstall Requirements\nData\nCode Usage\nüîó BibTeX\nAcknowledgements\nVideo-As-Prompt: Unified Semantic Control for Video Generation\nüî• News\nOct 24, 2025: üìñ We release the first unified semantic video generation model, Video-As-Prompt (VAP)!\nOct 24, 2025: ü§ó We release the VAP-Data, the largest semantic-controlled video generation datasets with more than $100K$ samples!\nOct 24, 2025: üëã We present the technical report of Video-As-Prompt, please check out the details and spark some discussion!\nüñåÔ∏è Video-As-Prompt\nCore idea: Given a reference video with wanted semantics as a video prompt, Video-As-Prompt animate a reference image with the same semantics as the reference video.\nYour browser does not support HTML5 video. Here is a link to the video instead.\nE.g., Different Reference Videos + Same Reference Image ‚Üí New Videos with Different Semantics\nWelcome to see our project page for more interesting results!\nüéÅ Models Zoo\nTo demonstrate cross-architecture generality, Video-As-Prompt provides two variants, each with distinct trade-offs:\nCogVideoX-I2V-5B\nStrengths: Fewer backbone parameters let us train more steps under limited resources, yielding strong stability on most semantic conditions.\nLimitations: Due to backbone ability limitation, it is weaker on human-centric generation and on concepts underrepresented in pretraining (e.g., ladudu, Squid Game, Minecraft).\nWan2.1-I2V-14B\nStrengths: Strong performance on human actions and novel concepts, thanks to a more capable base model.\nLimitations: Larger model size reduced feasible training steps given our resources, lowering stability on some semantic conditions.\nüëèüëèüëè Contributions and further optimization from the community are welcome.\nModel\nDate\nSize\nHuggingface\nVideo-As-Prompt (CogVideoX-I2V-5B)\n2025-10-15\n5B (Pretrained DiT) + 5B (VAP)\nDownload\nVideo-As-Prompt (Wan2.1-I2V-14B)\n2025-10-15\n14B (Pretrained DiT) + 5B (VAP)\nDownload\nPlease download the pre-trained video DiTs and our corresponding Video-As-Prompt models, and structure them as follows\nckpts/\n‚îú‚îÄ‚îÄ Video-As-Prompt-CogVideoX-5B/\n‚îú‚îÄ‚îÄ scheduler\n‚îú‚îÄ‚îÄ vae\n‚îú‚îÄ‚îÄ transformer\n‚îú‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ Video-As-Prompt-Wan2.1-14B/\n‚îú‚îÄ‚îÄ scheduler\n‚îú‚îÄ‚îÄ vae\n‚îú‚îÄ‚îÄ transformer\n‚îú‚îÄ‚îÄ ...\nü§ó Get Started with Video-As-Prompt\nVideo-As-Prompt supports Macos, Windows, Linux. You may follow the next steps to use Video-As-Prompt via:\nInstall Requirements\nWe test our model with Python 3.10 and PyTorch 2.7.1+cu124.\nconda create -n video_as_prompt python=3.10 -y\nconda activate video_as_prompt\npip install -r requirements.txt\npip install -e ./diffusers\nconda install -c conda-forge ffmpeg -y\nData\nWe have published the VAP-Data dataset used in our paper on VAP-Data. Please download it and put it in the data folder. The structure should look like:\ndata/\n‚îú‚îÄ‚îÄ VAP-Data/\n‚îÇ   ‚îú‚îÄ‚îÄ vfx_videos/\n‚îÇ   ‚îú‚îÄ‚îÄ vfx_videos_hq/\n‚îÇ   ‚îú‚îÄ‚îÄ vfx_videos_hq_camera/\n‚îÇ   ‚îú‚îÄ‚îÄ benchmark/benchmark.csv\n‚îÇ   ‚îú‚îÄ‚îÄ vap_data.csv\nCode Usage\nWe mainly implement our code based on diffusers and finetrainers for their modular design.\nMinimal Demo\nBelow is a minimal demo of our CogVideoX-I2V-5B variant. The full code can be found in infer/cog_vap.py. The WAN2.1-I2V-14B variant is similar and can be found in infer/wan_vap.py.\nimport torch\nfrom diffusers import (\nAutoencoderKLCogVideoX,\nCogVideoXImageToVideoMOTPipeline,\nCogVideoXTransformer3DMOTModel,\n)\nfrom diffusers.utils import export_to_video, load_video\nfrom PIL import Image\nvae = AutoencoderKLCogVideoX.from_pretrained(\"ByteDance/Video-As-Prompt-CogVideoX-5B\", subfolder=\"vae\", torch_dtype=torch.bfloat16)\ntransformer = CogVideoXTransformer3DMOTModel.from_pretrained(\"ByteDance/Video-As-Prompt-CogVideoX-5B\", torch_dtype=torch.bfloat16)\npipe = CogVideoXImageToVideoMOTPipeline.from_pretrained(\n\"ByteDance/Video-As-Prompt-CogVideoX-5B\", vae=vae, transformer=transformer, torch_dtype=torch.bfloat16\n).to(\"cuda\")\nref_video = load_video(\"assets/videos/demo/object-725.mp4\")\nimage = Image.open(\"assets/images/demo/animal-2.jpg\").convert(\"RGB\")\nidx = torch.linspace(0, len(ref_video) - 1, 49).long().tolist()\nref_frames = [ref_video[i] for i in idx]\noutput_frames = pipe(\nimage=image,\nref_videos=[ref_frames],\nprompt=\"A chestnut-colored horse stands on a grassy hill against a backdrop of distant, snow-dusted mountains. The horse begins to inflate, its defined, muscular body swelling and rounding into a smooth, balloon-like form while retaining its rich, brown hide color. Without changing its orientation, the now-buoyant horse lifts silently from the ground. It begins a steady vertical ascent, rising straight up and eventually floating out of the top of the frame. The camera remains completely static throughout the entire sequence, holding a fixed shot on the landscape as the horse transforms and departs, ensuring the verdant hill and mountain range in the background stay perfectly still.\",\nprompt_mot_ref=[\n\"A hand holds up a single beige sneaker decorated with gold calligraphy and floral illustrations, with small green plants tucked inside. The sneaker immediately begins to inflate like a balloon, its shape distorting as the decorative details stretch and warp across the expanding surface. It rapidly transforms into a perfectly smooth, matte beige sphere, inheriting the primary color from the original shoe. Once the transformation is complete, the new balloon-like object quickly ascends, moving straight up and exiting the top of the frame. The camera remains completely static and the plain white background is unchanged throughout the entire sequence.\"\n],\nheight=480,\nwidth=720,\nnum_frames=49,\nframes_selection=\"evenly\",\nuse_dynamic_cfg=True,\n).frames[0]\nBenchmark Inference\nYou can alse refer the following code for benchmark inference. Then you can use Vbench to evaluate the results.\npython infer/cog_vap_bench.py\npython infer/wan_vap_bench.py\nWelcome to modify the scripts to see more results in our dataset VAP-Data and even in-the-wild reference videos or images.\nTraining\nPick a recipe, then run the corresponding script. Each script sets sensible defaults; override as needed.\nRecipes ‚Äî CogVideoX-I2V-5B\nGoal\nNodes\nObjective\nReferences / sample\nScript\nStandard SFT\n1\nSFT\n1\nexamples/training/sft/cogvideox/vap_mot/train_single_node.sh\nStandard SFT\n‚â•2\nSFT\n1\nexamples/training/sft/cogvideox/vap_mot/train_multi_node.sh\nPreference optimization\n1\nDPO\n1\nexamples/training/sft/cogvideox/vap_mot/train_single_node_dpo.sh\nPreference optimization\n‚â•2\nDPO\n1\nexamples/training/sft/cogvideox/vap_mot/train_multi_node_dpo.sh\nMulti-reference SFT\n1\nSFT\n‚â§3\nexamples/training/sft/cogvideox/vap_mot/train_single_node_3ref.sh\nDPO and multi-reference SFT are just our exploration. We provide the code for boost of the community research.\nRecipes ‚Äî Wan2.1-I2V-14B (SFT only)\nGoal\nNodes\nObjective\nReferences / sample\nScript\nStandard SFT\n1\nSFT\n1\nexamples/training/sft/wan/vap_mot/train_single_node.sh\nStandard SFT\n‚â•2\nSFT\n1\nexamples/training/sft/wan/vap_mot/train_multi_node.sh\nQuick start (CogVideoX-5B, single-node SFT)\nbash examples/training/sft/cogvideox/vap_mot/train_single_node.sh\nQuick start (Wan2.1-14B, single-node SFT)\nbash examples/training/sft/wan/vap_mot/train_single_node.sh\nMulti-node launch (example)\n# 6 nodes\nbash examples/training/sft/cogvideox/vap_mot/train_multi_node.sh xxx:xxx:xxx:xxx:xxx(MASTER_ADDR) 0\nbash examples/training/sft/cogvideox/vap_mot/train_multi_node.sh xxx:xxx:xxx:xxx:xxx(MASTER_ADDR) 1\n...\nbash examples/training/sft/cogvideox/vap_mot/train_multi_node.sh xxx:xxx:xxx:xxx:xxx(MASTER_ADDR) 5\n# or for Wan:\n# examples/training/sft/wan/vap_mot/train_multi_node.sh xxx:xxx:xxx:xxx:xxx(MASTER_ADDR) 0\n# examples/training/sft/wan/vap_mot/train_multi_node.sh xxx:xxx:xxx:xxx:xxx(MASTER_ADDR) 1\n...\n# examples/training/sft/wan/vap_mot/train_multi_node.sh xxx:xxx:xxx:xxx:xxx(MASTER_ADDR) 5\nNotes\nCogVideoX supports SFT, DPO, and a ‚â§3-reference SFT variant; Wan currently supports standard SFT only.\nAll scripts read shared config (datasets, output dir, batch size, etc.); edit the script to override.\nPlease edit train_multi_node*.sh base on your environment if you want to change the distributed settings (e.g., gpu num, node num, master addr/port, etc.).\nüîó BibTeX\n‚ù§Ô∏è If you found this repository helpful, please give us a star and cite our report:\n@article{bian2025videoasprompt,\ntitle   = {Video-As-Prompt: Unified Semantic Control for Video Generation},\nauthor  = {Yuxuan Bian and Xin Chen and Zenan Li and Tiancheng Zhi and Shen Sang and Linjie Luo and Qiang Xu},\njournal = {arXiv preprint arXiv:2510.20888},\nyear    = {2025},\nurl     = {https://arxiv.org/abs/2510.20888}\n}\nAcknowledgements\nWe would like to thank the contributors to the Finetrainers, Diffusers, CogVideoX, and Wan repositories, for their open research and exploration.",
    "dx8152/Fusion_lora": "Welcome everyone to use Lora of Qwen-Edit-2509 of Image Fusion, his performance is very amazingÔºÅ\nThe trigger words are: Ê∫∂Âõæ,Á∫†Ê≠£‰∫ßÂìÅÈÄèËßÜËßíÂ∫¶ÂíåÂÖâÂΩ±Âπ∂‰Ωø‰∫ßÂìÅËûçÂÖ•ËÉåÊôØ\nInstructions: Download the lora file to the models/loras folder.\nYou also need this lora to use together: https://huggingface.co/lightx2v/Qwen-Image-Lightning/tree/main\nThis is the effect used by Lora: https://youtu.be/0ObRTPbOPi0\nFor communication/cooperation, you can join the discord group to communicateÔºö https://discord.gg/Qbq3VdjK",
    "inclusionAI/Ming-flash-omni-Preview": "Ming-flash-omni Preview\nIntroduction\nüìå Updates\nKey Features\nUse Cases\nSteaming Video Conversation\nAudio Context ASR & Dialect ASR\nAudio Voice Clone\nImage Generation & Editing\nModel Downloads\nEvaluation\nExample Usage\nCitation\nMing-flash-omni Preview\nüìë Technical ReportÔΩúü§ó Hugging FaceÔΩú ü§ñ ModelScope\nIntroduction\nMing-flash-omni Preview, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100B total parameters, of which only 6B\nare active per token. Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in both contextual ASR and dialect-aware ASR. In image generation, Ming-flash-omni Preview introduces high-fidelity text rendering and demonstrates marked gains in scene consistency and identity preservation during image editing. Furthermore, Ming-flash-omni Preview introduces generative segmentation, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. It demonstrates highly competitive results in various modal benchmarks compared to industry-leading models.\nüìå Updates\n[2025.10.27] üî• We release the preview version of Ming-flash-omniÔºöMing-flash-omni Preview.\n[2025.07.15] üî• We release Ming-lite-omni v1.5 with significant improvements across all modalities.\n[2025.06.12] üî• Our Technical Report is in public on arxiv.\n[2025.05.28] üî• The official version of Ming-lite-omni v1 is released, with better performance and image generation support.\n[2025.05.04] üî• We release the test version of Ming-lite-omniÔºöMing-lite-omni-Preview.\nKey Features\nCompared to Ming-lite-omni v1.5, Ming-flash-omni Preview features key optimizations in the following 3 areas:\nSparse MoE Architecture for Omni-Modality: The Sparse MoE Architecture for Omni-Modality features a 100B-A6B MoE backbone (an extension of Ling-Flash-2.0). To ensure uniform expert activation and stable training across all modalities, Ming-flash-omni Preview  employs a Dual-Balanced Routing Mechanism that combines an Auxiliary Load Balancing Loss with a Modality-Level Router Bias Update.\nGenerative Segmentation-as-Editing Paradigm: It unifies segmentation and editing into a semantics-preserving generation task, and achieves $0.90$ on GenEval, surpassing non-RL methods in fine-grained spatial control.\nContext-Aware and Dialectal Speech Recognition: Ming-flash-omni Preview sets a new State-of-the-Art performance across all 12 ContextASR benchmarks, and it significantly improves recognition performance for 15 Chinese dialects.\nUse Cases\nSteaming Video Conversation\nSteaming Video Conversation\nAudio Context ASR & Dialect ASR\nAudio Context ASR & Dialect ASR\nAudio Voice Clone\nAudio Voice Clone\nImage Generation & Editing\nGenerative Segmentation-as-Editing\nModel Downloads\nYou can download our latest model from both Huggingface and ModelScope. For previous version model like Ming-Lite-Omni v1.5, Please refer to this link.\nModel\nInput modality\nOput modality\nDownload\nMing-flash-omni Preview\nImage,text,video,audio\nImage,text,audio\nü§ó HuggingFace ü§ñ ModelScope\nIf you're in mainland China, we strongly recommend you to download our model from ü§ñ ModelScope.\npip install modelscope\nmodelscope download --model inclusionAI/Ming-flash-omni-Preview --local_dir inclusionAI/Ming-flash-omni-Preview  --revision master\nNote: This download process will take several minutes to several hours, depending on your network conditions.\nEvaluation\nMing-flash-omni shows competitive performance in vision-text understanding, image generation, audio understanding and text-to-speech capabilities. For detailed evaluation resultsÔºåplease refer to our techinical report.\nExample Usage\nWe provide a simple example on the usage of this repo. For detailed usage, please refer to cookbook.ipynb.\nimport os\nimport torch\nimport warnings\nfrom bisect import bisect_left\nwarnings.filterwarnings(\"ignore\")\nfrom transformers import AutoProcessor\nfrom modeling_bailingmm2 import BailingMM2NativeForConditionalGeneration\ndef split_model():\ndevice_map = {}\nworld_size = torch.cuda.device_count()\nnum_layers = 32\nlayer_per_gpu = num_layers // world_size\nlayer_per_gpu = [i * layer_per_gpu for i in range(1, world_size + 1)]\nfor i in range(num_layers):\ndevice_map[f'model.model.layers.{i}'] = bisect_left(layer_per_gpu, i)\ndevice_map['vision'] = 0\ndevice_map['audio'] = 0\ndevice_map['linear_proj'] = 0\ndevice_map['linear_proj_audio'] = 0\ndevice_map['model.model.word_embeddings.weight'] = 0\ndevice_map['model.model.norm.weight'] = 0\ndevice_map['model.lm_head.weight'] = 0\ndevice_map['model.model.norm'] = 0\ndevice_map[f'model.model.layers.{num_layers - 1}'] = 0\nreturn device_map\n# Load pre-trained model with optimized settings, this will take ~10 minutes\nmodel_path = \"inclusionAI/Ming-flash-omni-Preview\"\nmodel = BailingMM2NativeForConditionalGeneration.from_pretrained(\nmodel_path,\ntorch_dtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\",\ndevice_map=split_model(),\nload_image_gen=True,\nload_talker=True,\n).to(dtype=torch.bfloat16)\n# Initialize processor for handling multimodal inputs\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n# Inference Pipeline\ndef generate(messages, processor, model, sys_prompt_exp=None, use_cot_system_prompt=False, max_new_tokens=512):\ntext = processor.apply_chat_template(\nmessages,\nsys_prompt_exp=sys_prompt_exp,\nuse_cot_system_prompt=use_cot_system_prompt\n)\nimage_inputs, video_inputs, audio_inputs = processor.process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\naudios=audio_inputs,\nreturn_tensors=\"pt\",\naudio_kwargs={\"use_whisper_encoder\": True},\n).to(model.device)\nfor k in inputs.keys():\nif k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\":\ninputs[k] = inputs[k].to(dtype=torch.bfloat16)\nwith torch.no_grad():\ngenerated_ids = model.generate(\n**inputs,\nmax_new_tokens=max_new_tokens,\nuse_cache=True,\neos_token_id=processor.gen_terminator,\nnum_logits_to_keep=1,\n)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)[0]\nreturn output_text\n# qa\nmessages = [\n{\n\"role\": \"HUMAN\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"ËØ∑ËØ¶ÁªÜ‰ªãÁªçÈπ¶ÈπâÁöÑÁîüÊ¥ª‰π†ÊÄß„ÄÇ\"}\n],\n},\n]\noutput_text = generate(messages, processor=processor, model=model)\nprint(output_text)\n# Output:\n# Èπ¶ÈπâÊòØ‰∏ÄÁßçÈùûÂ∏∏ËÅ™ÊòéÂíåÁ§æ‰∫§ÊÄßÂº∫ÁöÑÈ∏üÁ±ªÔºåÂÆÉ‰ª¨ÁöÑÁîüÊ¥ª‰π†ÊÄßÈùûÂ∏∏‰∏∞ÂØåÂíåÊúâË∂£„ÄÇ‰ª•‰∏ãÊòØ‰∏Ä‰∫õÂÖ≥‰∫éÈπ¶ÈπâÁîüÊ¥ª‰π†ÊÄßÁöÑËØ¶ÁªÜ‰ªãÁªçÔºö\n# ### 1. **Ê†ñÊÅØÂú∞**\n# Èπ¶Èπâ‰∏ªË¶ÅÂàÜÂ∏ÉÂú®ÁÉ≠Â∏¶Âíå‰∫öÁÉ≠Â∏¶Âú∞Âå∫ÔºåÂåÖÊã¨ÈùûÊ¥≤„ÄÅ‰∫öÊ¥≤„ÄÅÊæ≥Â§ßÂà©‰∫öÂíåÂçóÁæéÊ¥≤„ÄÇÂÆÉ‰ª¨ÈÄöÂ∏∏ÁîüÊ¥ªÂú®Ê£ÆÊûó„ÄÅËçâÂéü„ÄÅÊ≤ôÊº†ÂíåÂüéÂ∏ÇÁéØÂ¢É‰∏≠„ÄÇ‰∏çÂêåÁßçÁ±ªÁöÑÈπ¶ÈπâÂØπÊ†ñÊÅØÂú∞ÁöÑË¶ÅÊ±ÇÊúâÊâÄ‰∏çÂêåÔºå‰ΩÜÂ§ßÂ§öÊï∞Èπ¶ÈπâÂñúÊ¨¢Êúâ‰∏∞ÂØåÊ§çË¢´ÂíåÊ∞¥Ê∫êÁöÑÂú∞Êñπ„ÄÇ\n# ### 2. **È•ÆÈ£ü**\n# Èπ¶ÈπâÊòØÊùÇÈ£üÊÄßÂä®Áâ©ÔºåÂÆÉ‰ª¨ÁöÑÈ•ÆÈ£üÈùûÂ∏∏Â§öÊ†∑Âåñ„ÄÇÂÆÉ‰ª¨ÁöÑÈ£üÁâ©ÂåÖÊã¨ÁßçÂ≠ê„ÄÅÂùöÊûú„ÄÅÊ∞¥Êûú„ÄÅËî¨Ëèú„ÄÅËä±ËúúÂíåÊòÜËô´„ÄÇÈπ¶ÈπâÁöÑÂñôÈùûÂ∏∏Âº∫Â£ÆÔºåËÉΩÂ§üËΩªÊùæÂú∞ÊâìÂºÄÂùöÁ°¨ÁöÑÊûúÂ£≥ÂíåÂùöÊûú„ÄÇ‰∏Ä‰∫õÈπ¶ÈπâËøò‰ºöÂêÉÊ≥•ÂúüÊàñÊ≤ôÂ≠êÔºå‰ª•Â∏ÆÂä©Ê∂àÂåñÂíåË°•ÂÖÖÁüøÁâ©Ë¥®„ÄÇ\n# ......\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{Mingomni2025,\ntitle  = {Ming-Omni: A Unified Multimodal Model for Perception and Generation},\nauthor = {Inclusion AI},\nyear = {2025},\neprint = {2506.09344},\narchivePrefix = {arXiv},\nurl = {https://arxiv.org/abs/2506.09344}\n}",
    "cerebras/GLM-4.6-REAP-218B-A32B-FP8": "GLM-4.6-REAP-218B-A32B-FP8\n‚ú® Highlights\nüìã Model Overview\nüìä Evaluations\nüöÄ Deployment\nüß© Model Creation\nHow REAP Works\nKey Advantages\nCalibration\n‚öñÔ∏è License\nüßæ Citation\nìå≥ REAPìå≥  the Experts: Why Pruning Prevails for One-Shot MoE Compression\nGLM-4.6-REAP-218B-A32B-FP8\n‚ú® Highlights\nIntroducing GLM-4.6-REAP-218B-A32B-FP8, a memory-efficient compressed variant of GLM-4.6-FP8 that maintains near-identical performance while being 40% lighter.\nThis model was created using REAP (Router-weighted Expert Activation Pruning), a novel expert pruning method that selectively removes redundant experts while preserving the router's independent control over remaining experts. Key features include:\nNear-Lossless Performance: Maintains almost identical accuracy on code generation, agentic coding, and function calling tasks compared to the full 355B model\n40% Memory Reduction: Compressed from 355B to 218B parameters, significantly lowering deployment costs and memory requirements\nPreserved Capabilities: Retains all core functionalities including code generation, agentic workflows, repository-scale understanding, and function calling\nDrop-in Compatibility: Works with vanilla vLLM - no source modifications or custom patches required\nOptimized for Real-World Use: Particularly effective for resource-constrained environments, local deployments, and academic research\nFor downstream low-bit quantization, we suggest using the BF16 variant.\nüìã Model Overview\nGLM-4.6-REAP-218B-A32B-FP8 has the following specifications:\nBase Model: GLM-4.6-FP8\nCompression Method: REAP (Router-weighted Expert Activation Pruning)\nCompression Ratio: 40% expert pruning\nType: Sparse Mixture-of-Experts (SMoE) Causal Language Model\nNumber of Parameters: 218B total, 32B activated per token\nNumber of Layers: 92\nNumber of Attention Heads (GQA): 96 for Q and 8 for KV\nNumber of Experts: 96 (uniformly pruned from 160)\nNumber of Activated Experts: 8 per token\nContext Length: 202,752 tokens\nLicense: MIT\nüìä Evaluations\nBenchmark\nGLM-4.6-FP8\nGLM-4.6-REAP-268B-A32B-FP8\nGLM-4.6-REAP-252B-A32B-FP8\nGLM-4.6-REAP-218B-A32B-FP8\nCompression\n‚Äî\n25%\n30%\n40%\nCoding\nHumanEval\n96.3\n96.3\n95.7\n95.1\nHumanEval+\n93.3\n91.5\n90.9\n90.2\nMBPP\n87.6\n89.9\n89.9\n89.4\nMBPP+\n73.5\n74.9\n73.5\n73.8\nReasoning\nGPQA diamond (thinking)\n78.8\n76.8\n75.8\n69.7\nAIME25 (thinking)\n90.0\n93.3\n90.0\n90.0\nMATH-500 (thinking)\n95.5\n97.0\n94.8\n93.3\nTool Calling\nBFCL-v3 (thinking)\n78.4\n77.3\n76.8\n74.2\nüü© This checkpoint maintains almost identical performance while being 40% lighter.\nFor more details on the evaluation setup, refer to the REAP arXiv preprint.\nüöÄ Deployment\nYou can deploy the model directly using the latest vLLM (v0.11.0), no source modifications or custom patches required.\nvllm serve cerebras/GLM-4.6-REAP-218B-A32B-FP8 \\\n--tensor-parallel-size 8 \\\n--tool-call-parser glm45 \\\n--enable-auto-tool-choice \\\n--enable-expert-parallel\nIf you encounter insufficient memory when running this model, you might need to set a lower value for --max-num-seqs flag (e.g. set to 64).\nüß© Model Creation\nThis checkpoint was created by applying the REAP (Router-weighted Expert Activation Pruning) method uniformly across all Mixture-of-Experts (MoE) blocks of GLM-4.6-FP8, with a 40% pruning rate.\nHow REAP Works\nREAP selects experts to prune based on a novel saliency criterion that considers both:\nRouter gate values: How frequently and strongly the router activates each expert\nExpert activation norms: The magnitude of each expert's output contributions\nThis dual consideration ensures that experts contributing minimally to the layer's output are pruned, while preserving those that play critical roles in the model's computations.\nKey Advantages\nOne-Shot Compression: No fine-tuning required after pruning - the model is immediately ready for deployment\nPreserved Router Control: Unlike expert merging methods, REAP maintains the router's independent, input-dependent control over remaining experts, avoiding \"functional subspace collapse\"\nGenerative Task Superiority: REAP significantly outperforms expert merging approaches on generative benchmarks (code generation, creative writing, mathematical reasoning) while maintaining competitive performance on discriminative tasks\nCalibration\nThe model was calibrated using a diverse mixture of domain-specific datasets including:\nCode generation samples (evol-codealpaca)\nFunction calling examples (xlam-function-calling)\nAgentic multi-turn trajectories (SWE-smith-trajectories)\nüìö For more details, refer to the following resources:\nüßæ arXiv Preprint\nüßæ REAP Blog\nüíª REAP Codebase (GitHub)\n‚öñÔ∏è License\nThis model is derived from\nzai-org/GLM-4.6-FP8\nand distributed under the MIT license.\nüßæ Citation\nIf you use this checkpoint, please cite the REAP paper:\n@article{lasby-reap,\ntitle={REAP the Experts: Why Pruning Prevails for One-Shot MoE compression},\nauthor={Lasby, Mike and Lazarevich, Ivan and Sinnadurai, Nish and Lie, Sean and Ioannou, Yani and Thangarasa, Vithursan},\njournal={arXiv preprint arXiv:2510.13999},\nyear={2025}\n}",
    "nvidia/llama-embed-nemotron-8b": "llama-embed-nemotron-8b\nModel Overview\nDescription:\nLicense/Terms of Use\nTeam\nCitation\nNVIDIA‚Äôs Retrieval Models\nDeployment Geography:\nUse Case:\nRelease Date:\nModel Architecture:\nInput:\nOutput:\nUsage\nSoftware Integration:\nModel Version(s):\nTraining and Testing Datasets\nTraining Dataset:\nTesting Dataset:\nInference:\nEthical Considerations:\nllama-embed-nemotron-8b\nModel Overview\nDescription:\nllama-embed-nemotron-8b is a versatile text embedding model trained by NVIDIA and optimized for retrieval, reranking, semantic similarity, and classification use cases. This model has robust capabilities for multilingual and cross-lingual text retrieval. It is designed to serve as a foundational component in text-based Retrieval-Augmented Generation (RAG) systems.\nThis model achieves state-of-the-art performance on the multilingual MTEB leaderboard (as of October 21, 2025).\nThis model is for non-commercial/research use only.\nLicense/Terms of Use\nGoverning Terms for llama-embed-nemotron-8b model: NVIDIA License\nAdditional Information: Llama-3.1 Community License Agreement for meta-llama/Llama-3.1-8B. Acceptable Use Policy. Built with Llama.\nTeam\nYauhen Babakhin\nRadek Osmulski\nRonay Ak\nGabriel Moreira\nMengyao Xu\nBenedikt Schifferer\nBo Liu\nEven Oldridge\nCorrespondence to Yauhen Babakhin (ybabakhin@nvidia.com) and Bo Liu (boli@nvidia.com).\nCitation\nThe technical report for the llama-embed-nemotron-8b model will be published soon.\n@misc{lee2024nv,\ntitle={NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models},\nauthor={Lee, Chankyu and Roy, Rajarshi and Xu, Mengyao and Raiman, Jonathan and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},\njournal={arXiv preprint arXiv:2405.17428},\nyear={2024}\n}\n@misc{moreira2025nvretrieverimprovingtextembedding,\ntitle={NV-Retriever: Improving text embedding models with effective hard-negative mining},\nauthor={Gabriel de Souza P. Moreira and Radek Osmulski and Mengyao Xu and Ronay Ak and Benedikt Schifferer and Even Oldridge},\nyear={2025},\neprint={2407.15831},\narchivePrefix={arXiv},\nprimaryClass={cs.IR},\nurl={https://arxiv.org/abs/2407.15831},\n}\nNVIDIA‚Äôs Retrieval Models\nModel Name\nUse Case\nComment\nnvidia/omni-embed-nemotron-3b\nResearch-Only\nOmni-Modal Embedding Model for Retrieving Text, Images, Audio, or Video\nnvidia/llama-NemoRetriever-ColEmbed-3B-v1\nResearch-Only\n#1 ViDoRe V1, V2 and MTEB VisualDocumentRetrieval as of June 27, 2025\nnvidia/llama-3.2-nv-embedqa-1b-v2\nCommercial Application\nText Embedding Model for Production Use Case of Text Document Retrieval\nnvidia/llama-3.2-nv-rerankqa-1b-v2\nCommercial Application\nText Reranker Model for Production Use Case of Text Document Retrieval\nllama-3_2-nemoretriever-1b-vlm-embed-v1\nCommercial Application\nMultiModal Embedding Model for Production Use Case of Visual Document Retrieval\nnvidia/llama-NemoRetriever-ColEmbed-1B-v1\nResearch-Only\nSmaller Version of nvidia/llama-NemoRetriever-ColEmbed-3B-v1\nnvidia/NV-Embed-v2\nResearch-Only\n#1 MTEB as of Aug 30, 2024\nnvidia/MM-Embed\nResearch-Only\nImproved nvidia/NV-Embed-v1 and multimodal embeddings\nnvidia/NV-Retriever-v1\nResearch-Only\n#1 MTEB BEIR as of July 12, 2024\nDeployment Geography:\nGlobal\nUse Case:\nThe llama-embed-nemotron-8b model is intended for researchers developing applications that need to understand or retrieve information from text. It is well-suited for multilingual RAG systems in which queries and documents are textual and may be in different languages.\nRelease Date:\nHugging Face on 10/21/2025 via https://huggingface.co/nvidia/llama-embed-nemotron-8b\nModel Architecture:\nArchitecture Type: Transformer Decoder\nNetwork Architecture: Llama-3.1-8B with bi-directional attention\nThis model was developed based on meta-llama/Llama-3.1-8B model.\nNumber of model parameters: 7,504,924,672\nThis llama-embed-nemotron-8b embedding model is a fine-tuned version of Llama-3.1-8B transformer decoder architecture, with a bidirectional attention mechanism. The model consists of 32 hidden layers and an embedding size of 4096, and trained on public datasets and synthetically generated datasets. Embedding models for text retrieval are typically trained using a bi-encoder architecture. This involves encoding a pair of sentences (for example, query and chunked passages) independently using the embedding model. Contrastive learning is used to maximize the similarity between the query and the passage that contains the answer, while minimizing the similarity between the query and sampled negative passages not useful to answer the question.\nInput:\nProperty\nQuery\nDocument\nInput Type\nText\nText\nInput Format\nList of strings\nList of strings\nInput Parameter\nOne-Dimensional (1D)\n1D\nOther Properties\nMaximum input sequence length is 32768 tokens.\nMaximum input sequence length is 32768 tokens.\nOutput:\nOutput Type(s): Floats\nOutput Format: List of floats\nOutput Parameters: One-Dimensional (1D)\nOther Properties Related to Output: Model outputs embedding vectors of a dimension 4096 for each text input.\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA‚Äôs hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nUsage\nThe llama-embed-nemotron-8b model is instruction-aware, meaning that it supports custom instructions to improve performance for specific use cases or scenarios. In particular, for Retrieval use case, model expects:\nQueries accompanied with the task instruction in the following template: f\"Instruct: {task_instruction}\\nQuery: {query}\"\nDocuments (passages) without any special handling\nThe model requires transformers version 4.51.0 and flash-attention (for GPU processing)\npip install transformers==4.51.0\npip install flash-attn==2.6.3\nYou can use either Sentence Transformers like here:\npip install sentence-transformers\nfrom sentence_transformers import SentenceTransformer\nattn_implementation = \"eager\"  # Or \"flash_attention_2\"\nmodel = SentenceTransformer(\n\"nvidia/llama-embed-nemotron-8b\",\ntrust_remote_code=True,\nmodel_kwargs={\"attn_implementation\": attn_implementation, \"torch_dtype\": \"float16\"},\ntokenizer_kwargs={\"padding_side\": \"left\"},\n)\nqueries = [\n\"How do neural networks learn patterns from examples?\"\n]\ndocuments = [\n\"Deep learning models adjust their weights through backpropagation, using gradient descent to minimize error on training data and improve predictions over time.\",\n\"Market prices are determined by the relationship between how much people want to buy a product and how much is available for sale, with scarcity driving prices up and abundance driving them down.\",\n]\n# NOTE: encode_query uses the \"query\" prompt automatically\nquery_embeddings = model.encode_query(queries)\ndocument_embeddings = model.encode_document(documents)\nscores = (query_embeddings @ document_embeddings.T)\nprint(scores.tolist())\n# [[0.37646484375, 0.057891845703125]]\nOr using Hugging Face Transformers like here:\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModel, AutoTokenizer\ndef average_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n\"\"\"Average pooling with attention mask.\"\"\"\nlast_hidden_states_masked = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\nembedding = last_hidden_states_masked.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\nembedding = F.normalize(embedding, dim=-1)\nreturn embedding\n# Define task and queries\ndef get_instruction(task_instruction: str, query: str) -> str:\nreturn f\"Instruct: {task_instruction}\\nQuery: {query}\"\nmodel_name_or_path = \"nvidia/llama-embed-nemotron-8b\"\nattn_implementation = \"flash_attention_2\" if torch.cuda.is_available() else \"eager\"\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\nmodel_name_or_path,\ntrust_remote_code=True,\npadding_side=\"left\",\n)\n# Load model\nmodel = AutoModel.from_pretrained(\nmodel_name_or_path,\ntrust_remote_code=True,\ntorch_dtype=torch.float16,\nattn_implementation=attn_implementation,\n).eval()\nmodel = model.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# Model is instruction-aware, which requires each query to have a short instruction with the task instruction\ntask = \"Given a question, retrieve passages that answer the question\"\nqueries = [\nget_instruction(task, \"How do neural networks learn patterns from examples?\"),\n]\n# No instruction is required for documents corpus\ndocuments = [\n\"Deep learning models adjust their weights through backpropagation, using gradient descent to minimize error on training data and improve predictions over time.\",\n\"Market prices are determined by the relationship between how much people want to buy a product and how much is available for sale, with scarcity driving prices up and abundance driving them down.\",\n]\ninput_texts = queries + documents\n# Tokenize the input texts\nbatch_dict = tokenizer(\ntext=input_texts,\nmax_length=4096,\npadding=True,\ntruncation=True,\nreturn_tensors=\"pt\",\n).to(model.device)\nattention_mask = batch_dict[\"attention_mask\"]\n# Forward pass\nmodel_outputs = model(**batch_dict)\n# Average pooling\nembeddings = average_pool(model_outputs.last_hidden_state, attention_mask)\nscores = (embeddings[:1] @ embeddings[1:].T)\nprint(scores.tolist())\n# [[0.37646484375, 0.0579833984375]]\nSoftware Integration:\nRuntime Engine(s):\nTensorRT, Triton\nSupported Hardware Microarchitecture Compatibility:\nNVIDIA Ampere\nNVIDIA Hopper\nNVIDIA Lovelace\nNVIDIA Pascal\nNVIDIA Turing\nNVIDIA Volta\nPreferred/Supported Operating System(s):\nLinux\nThe integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment. This AI model can be embedded as an Application Programming Interface (API) call into the software environment described above.\nModel Version(s):\nllama-embed-nemotron-8b-v1\nTraining and Testing Datasets\nTraining Dataset:\nData Modality\nText\nText Training Data Size\n1 Billion to 10 Trillion Tokens\nData Collection Method by dataset\nHybrid: Human, Automated, Synthetic\nLabeling Method by dataset\nHybrid: Human, Automated, Synthetic\nProperties: 16.4M query-passage pairs from public and synthetically generated datasets.\nTesting Dataset:\nWe test the model on 131 tasks from MMTEB: Massive Multilingual Text Embedding Benchmark (MTEB(Multilingual, v2) split).\nBenchmark specs:\nNumber of languages: 1038\nNumber of task types: 9\nNumber of domains: 20\nMMTEB Leaderboard Benchmark Ranking\nBelow we present results for MTEB(Multilingual, v2) split of MMTEB benchmark (as of October 21, 2025). Ranking on MMTEB Leaderboards is performed based on the Borda rank. Each task is treated as a preference voter, which gives votes on the models per their relative performance on the task. The best model obtains the highest number of votes. The model with the highest number of votes across tasks obtains the highest rank. The Borda rank tends to prefer models that perform well broadly across tasks.\nBorda Rank\nModel\nBorda Votes\nMean (Task)\n1.\nllama-embed-nemotron-8b\n39,573\n69.46\n2.\ngemini-embedding-001\n39,368\n68.37\n3.\nQwen3-Embedding-8B\n39,364\n70.58\n4.\nQwen3-Embedding-4B\n39,099\n69.45\n5.\nQwen3-Embedding-0.6B\n37,419\n64.34\n6.\ngte-Qwen2-7B-instruct\n37,167\n62.51\n7.\nLinq-Embed-Mistral\n37,149\n61.47\nData Collection Method by dataset:\nHybrid: Automated, Human, Synthetic\nLabeling Method by dataset:\nHybrid: Automated, Human, Synthetic\nProperties:  More details about MMTEB benchmark can be found on their leaderboard or in their published paper.\nInference:\nAcceleration Engine: GPU\nTest Hardware: A100 80GB, H100 80GB\nEthical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nPlease report model quality, risk, security vulnerabilities or NVIDIA AI Concerns here.",
    "merve/smol-vision": "Smol Vision üê£\nSmol Vision üê£\nRecipes for shrinking, optimizing, customizing cutting edge vision and multimodal AI models. Original GH repository is here migrated to Hugging Face since notebooks there aren't rendered ü•≤\nLatest examples üëáüèª\nFine-tune ColPali for Multimodal RAG\nFine-tune Gemma-3n for all modalities (audio-text-image)\nAny-to-Any (Video) RAG with OmniEmbed and Qwen\nNote: The script and notebook are updated to fix few issues related to QLoRA!\nNotebook\nDescription\nQuantization/ONNX\nFaster and Smaller Zero-shot Object Detection with Optimum\nQuantize the state-of-the-art zero-shot object detection model OWLv2 using Optimum ONNXRuntime tools.\nVLM Fine-tuning\nFine-tune PaliGemma\nFine-tune state-of-the-art vision language backbone PaliGemma using transformers.\nIntro to Optimum/ORT\nOptimizing DETR with ü§ó Optimum\nA soft introduction to exporting vision models to ONNX and quantizing them.\nModel Shrinking\nKnowledge Distillation for Computer Vision\nKnowledge distillation for image classification.\nQuantization\nFit in vision models using Quanto\nFit in vision models to smaller hardware using quanto\nSpeed-up\nFaster foundation models with torch.compile\nImproving latency for foundation models using torch.compile\nVLM Fine-tuning\nFine-tune Florence-2\nFine-tune Florence-2 on DocVQA dataset\nVLM Fine-tuning\nQLoRA/Fine-tune IDEFICS3 or SmolVLM on VQAv2\nQLoRA/Full Fine-tune IDEFICS3 or SmolVLM on VQAv2 dataset\nVLM Fine-tuning (Script)\nQLoRA Fine-tune IDEFICS3 on VQAv2\nQLoRA/Full Fine-tune IDEFICS3 or SmolVLM on VQAv2 dataset\nMultimodal RAG\nMultimodal RAG using ColPali and Qwen2-VL\nLearn to retrieve documents and pipeline to RAG without hefty document processing using ColPali through Byaldi and do the generation with Qwen2-VL\nMultimodal Retriever Fine-tuning\nFine-tune ColPali for Multimodal RAG\nLearn to apply contrastive fine-tuning on ColPali to customize it for your own multimodal document RAG use case\nVLM Fine-tuning\nFine-tune Gemma-3n for all modalities (audio-text-image)\nFine-tune Gemma-3n model to handle any modality: audio, text, and image.\nMultimodal RAG\nAny-to-Any (Video) RAG with OmniEmbed and Qwen\nDo retrieval and generation across modalities (including video) using OmniEmbed and Qwen.\nSpeed-up/Memory Optimization\nVision language model serving using TGI (SOON)\nExplore speed-ups and memory improvements for vision-language model serving with text-generation inference\nQuantization/Optimum/ORT\nAll levels of quantization and graph optimizations for Image Segmentation using Optimum (SOON)\nEnd-to-end model optimization using Optimum",
    "vita-video-gen/svi-model": "üéØ About This Repository\nüåü Key Highlights\nüì¶ Resources\nüìù Citation\nStable Video Infinity: Infinite-Length Video Generation with Error Recycling\nüéØ About This Repository\nStable-Video-Infinity(SVI) is able to generate ANY-length videos with high temporal consistency, plausible scene transitions, and controllable streaming storylines in ANY domains.\nThis repository contains the model weights of SVI Family.\nüåü Key Highlights\nOpenSVI: Everything is open-sourced: training & evaluation scripts, datasets, and more.\nInfinite Length: No inherent limit on video duration; generate arbitrarily long stories (see the 10‚Äëminute ‚ÄúTom and Jerry‚Äù demo).\nVersatile: Supports diverse in-the-wild generation tasks: multi-scene short films, single‚Äëscene animations, skeleton-/audio-conditioned generation, cartoons, and more.\nEfficient: Only LoRA adapters are tuned, requiring very little training data: anyone can make their own SVI easily.\nüì¶ Resources\nModel\nTask\nInput\nOutput\nHugging Face Link\nComments\nALL\nInfinite possibility\nImage + X\nX video\nü§ó Folder\nFamily bucket! I want to play with all!\nSVI-Shot\nSingle-scene generation\nImage + Text prompt\nLong video\nü§ó Model\nGenerate consistent long video with 1 text prompt. (This will never drift)\nSVI-Film\nMulti-scene generation\nImage + Text prompt stream\nFilm-style video\nü§ó Model\nGenerate creative long video with 1 text prompt stream (5 second per text).\nSVI-Film (Transition)\nMulti-scene generation\nImage + Text prompt stream\nFilm-style video\nü§ó Model\nGenerate creative long video with 1 text prompt stream. (More scene transitions due to the training data)\nSVI-Tom&Jerry\nCartoon animation\nImage\nCartoon video\nü§ó Model\nGenerate creative long cartoon videos with 1 text prompt stream (This will never drift in our 20 min test)\nSVI-Talk\nTalking head\nImage + Audio\nTalking video\nü§ó Model\nGenerate long videos with audio-conditioned human speaking\nSVI-Dance\nDancing animation\nImage + Skeleton\nDance video\nü§ó Model\nGenerate long videos with skeleton-conditioned human dancing\nNote: If you want to play with T2V, you can directly use SVI with an image generated by any T2I model!\nüìù Citation\nIf you find our work helpful for your research, please consider citing our paper. Thank you so much!\n@article{li2025stable,\ntitle={Stable Video Infinity: Infinite-Length Video Generation with Error Recycling},\nauthor={Wuyang Li and Wentao Pan and Po-Chien Luan and Yang Gao and Alexandre Alahi},\njournal={arXiv preprint arXiv: arXiv:2510.09212},\nyear={2025},\nurl={https://huggingface.co/papers/2510.09212},\n}",
    "xiabs/DreamOmni2": "README.md exists but content is empty.",
    "chestnutlzj/Edit-R1-Qwen-Image-Edit-2509": "Edit-R1-Qwen-Image-Edit-2509: A model from UniWorld-V2\nPerformance\nUsage\nEdit-R1-Qwen-Image-Edit-2509: A model from UniWorld-V2\nThis model is a checkpoint (Edit-R1-Qwen-Image-Edit-2509) developed using the Edit-R1 framework, as presented in the paper Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback. The Edit-R1 framework focuses on reinforcing instruction-based image editing through Diffusion Negative-aware Finetuning (DiffusionNFT) and MLLM Implicit Feedback.\nPaper | Code | Dataset\nPerformance\nBenchmark\nQwen-Image-Edit-2509\nEdit-R1-Qwen-Image-Edit-2509\nGEdit-Bench\n7.54\n7.76\nImgEdit\n4.35\n4.48\nUsage\nimport os\nimport torch\nfrom PIL import Image\nfrom diffusers import QwenImageEditPlusPipeline\npipeline = QwenImageEditPlusPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit-2509\", torch_dtype=torch.bfloat16)\nprint(\"pipeline loaded\")\npipeline.load_lora_weights(\n\"chestnutlzj/Edit-R1-Qwen-Image-Edit-2509\",\nadapter_name=\"lora\",\n)\npipeline.set_adapters([\"lora\"], adapter_weights=[1])\npipeline.to('cuda')\npipeline.set_progress_bar_config(disable=None)\nimage1 = Image.open(\"input1.png\")\nimage2 = Image.open(\"input2.png\")\nprompt = \"The magician bear is on the left, the alchemist bear is on the right, facing each other in the central park square.\"\ninputs = {\n\"image\": [image1, image2],\n\"prompt\": prompt,\n\"generator\": torch.manual_seed(0),\n\"true_cfg_scale\": 4.0,\n\"negative_prompt\": \" \",\n\"num_inference_steps\": 40,\n\"guidance_scale\": 1.0,\n\"num_images_per_prompt\": 1,\n}\nwith torch.inference_mode():\noutput = pipeline(**inputs)\noutput_image = output.images[0]\noutput_image.save(\"output_image_edit_plus.png\")\nprint(\"image saved at\", os.path.abspath(\"output_image_edit_plus.png\"))"
}