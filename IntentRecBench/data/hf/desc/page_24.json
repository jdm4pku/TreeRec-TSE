{
    "inclusionAI/LLaDA-MoE-7B-A1B-Instruct": "LLaDA-MoE\nüöÄ Performance Highlights\nüì¶ Model Variants\nüîç Model Overview\n‚ö° Infra\n1. We highly recommend you generate with dInferÔºà1000+ Tokens/SÔºâ\n2. No Speedup: transformers\nüìö Citation LLaDA-MoE\nüåê License\nü§ù Contact & Collaboration\nLLaDA-MoE\nLLaDA-MoE is a new and upgraded series of the LLaDA diffusion language model. This pre-release includes two cutting-edge models:\nLLaDA-MoE-7B-A1B-Base: A base pre-trained model designed for research and secondary development.\nLLaDA-MoE-7B-A1B-Instruct: An instruction-tuned model optimized for practical applications.\nLLaDA-MoE-7B-A1B-Instruct-TD: A specialized instruction-tuned model, further optimized for accelerated inference using Trajectory Distillation.\nüöÄ Performance Highlights\nLeading MoE Architecture:The first open-source Mixture-of-Experts (MoE) diffusion large language model, pre-trained from scratch on approximately 20 trillion tokens.\nEfficient Inference:With 7 billion total parameters, only 1.4 billion are activated during inference. LLaDA-MoE significantly reduces computational costs while outperforming open-source dense models of similar scale.\nImpressive Performance on Code & Complex Reasoning:Excels in tasks such as code generation and advanced mathematical reasoning, demonstrating strong reasoning capabilities.\nTool Use:Supports tool calling and achieves excellent performance in complex agent-based tasks.\nOpen & Extensible:Fully open-source with commitment to transparency. We plan to release a leading inference framework in the future and continue investing in cutting-edge areas like diffusion LLMs (dLLM) to drive disruptive innovation.\nüì¶ Model Variants\nModel ID\nDescription\nHugging Face Link\ninclusionAI/LLaDA-MoE-7B-A1B-Base\nBase pre-trained model for research and fine-tuning.\nü§ó Model Card\ninclusionAI/LLaDA-MoE-7B-A1B-Instruct\nInstruction-tuned model, ready for downstream applications.\nü§ó Model Card\ninclusionAI/LLaDA-MoE-7B-A1B-Instruct-TD\nAn instruction-tuned model further optimized with Trajectory Distillation (TD) for accelerated inference. Decodes multiple tokens per forward pass.\nü§ó Model Card\nüîç Model Overview\nLLaDA-MoE-7B-A1B has the following specifications:\nType: Mixture-of-Experts (MoE) Diffusion Language Model\nTotal Parameters (Non-Embedding): 7.03B\nNumber of Layers: 16\nAttention Heads: 16\nContext Length: 4,096 tokens\nPosition Embedding: Rotary (RoPE)\nVocabulary Size: 157,184\n‚ö° Infra\n1. We highly recommend you generate with dInferÔºà1000+ Tokens/SÔºâ\nFigure: Display of generation speed\nOn HumanEval, dInfer achieves over 1,100 TPS at batch size 1, and averages more than 800 TPS across six benchmarks on\na single node with 8 H800 GPUs.\nInstall dInfer\ngit clone https://github.com/inclusionAI/dInfer.git\ncd dInfer\npip install .\nConvert to FusedMoE\nUse the conversion tool to fuse the experts.\n# From repo root\npython tools/transfer.py \\\n--input  /path/to/LLaDA-MoE-7B-A1B-Instruct \\\n--output /path/to/LLaDA-MoE-7B-A1B-Instruct-fused\nUse the model in dInfer\nimport torch\nfrom transformers import AutoTokenizer\nfrom dinfer.model import AutoModelForCausalLM\nfrom dinfer.model import FusedOlmoeForCausalLM\nfrom dinfer import BlockIteratorFactory, KVCacheFactory\nfrom dinfer import ThresholdParallelDecoder, BlockWiseDiffusionLLM\nm = \"/path/to/LLaDA-MoE-7B-A1B-Instruct-fused\"\ntok = AutoTokenizer.from_pretrained(m, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(m, trust_remote_code=True, torch_dtype=\"bfloat16\")\ndecoder = ThresholdParallelDecoder(0, threshold=0.9)\ndllm = BlockWiseDiffusionLLM(model, decoder, BlockIteratorFactory(True), cache_factory=KVCacheFactory('dual'))\nprompt = \"Lily can run 12 kilometers per hour for 4 hours. After that, she can run 6 kilometers per hour. How many kilometers can she run in 8 hours?\"\ninput_ids = tokenizer(prompt)['input_ids']\ninput_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\nres = dllm.generate(input_ids, gen_length=gen_len, block_length=block_len)\n2. No Speedup: transformers\nMake sure you have transformers and its dependencies installed:\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\ndef add_gumbel_noise(logits, temperature):\nif temperature == 0:\nreturn logits\nlogits = logits.to(torch.float64)\nnoise = torch.rand_like(logits, dtype=torch.float64)\ngumbel_noise = (- torch.log(noise)) ** temperature\nreturn logits.exp() / gumbel_noise\ndef get_num_transfer_tokens(mask_index, steps):\nmask_num = mask_index.sum(dim=1, keepdim=True)\nbase = mask_num // steps\nremainder = mask_num % steps\nnum_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\nfor i in range(mask_num.size(0)):\nnum_transfer_tokens[i, :remainder[i]] += 1\nreturn num_transfer_tokens\n@ torch.no_grad()\ndef generate(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,\ncfg_scale=0., remasking='low_confidence', mask_id=156895):\nx = torch.full((1, prompt.shape[1] + gen_length), mask_id, dtype=torch.long).to(model.device)\nx[:, :prompt.shape[1]] = prompt.clone()\nprompt_index = (x != mask_id)\nassert gen_length % block_length == 0\nnum_blocks = gen_length // block_length\nassert steps % num_blocks == 0\nsteps = steps // num_blocks\nfor num_block in range(num_blocks):\nblock_mask_index = (x[:, prompt.shape[1] + num_block * block_length: prompt.shape[1] + (num_block + 1) * block_length:] == mask_id)\nnum_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)\nfor i in range(steps):\nmask_index = (x == mask_id)\nif cfg_scale > 0.:\nun_x = x.clone()\nun_x[prompt_index] = mask_id\nx_ = torch.cat([x, un_x], dim=0)\nlogits = model(x_).logits\nlogits, un_logits = torch.chunk(logits, 2, dim=0)\nlogits = un_logits + (cfg_scale + 1) * (logits - un_logits)\nelse:\nlogits = model(x).logits\nlogits_with_noise = add_gumbel_noise(logits, temperature=temperature)\nx0 = torch.argmax(logits_with_noise, dim=-1) # b, l\nif remasking == 'low_confidence':\np = F.softmax(logits, dim=-1)\nx0_p = torch.squeeze(\ntorch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1) # b, l\nelif remasking == 'random':\nx0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)\nelse:\nraise NotImplementedError(remasking)\nx0_p[:, prompt.shape[1] + (num_block + 1) * block_length:] = -np.inf\nx0 = torch.where(mask_index, x0, x)\nconfidence = torch.where(mask_index, x0_p, -np.inf)\ntransfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)\nfor j in range(confidence.shape[0]):\n_, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j, i])\ntransfer_index[j, select_index] = True\nx[transfer_index] = x0[transfer_index]\nreturn x\ndevice = 'cuda'\nmodel = AutoModel.from_pretrained('inclusionAI/LLaDA-MoE-7B-A1B-Instruct', trust_remote_code=True, torch_dtype=torch.bfloat16).to(device).eval()\ntokenizer = AutoTokenizer.from_pretrained('inclusionAI/LLaDA-MoE-7B-A1B-Instruct', trust_remote_code=True)\nprompt = \"Lily can run 12 kilometers per hour for 4 hours. After that, she runs 6 kilometers per hour. How many kilometers can she run in 8 hours?\"\nm = [\n{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\nprompt = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\ninput_ids = tokenizer(prompt)['input_ids']\ninput_ids = torch.tensor(input_ids).to(device).unsqueeze(0)\ntext = generate(model, input_ids, steps=128, gen_length=128, block_length=32, temperature=0., cfg_scale=0., remasking='low_confidence')\nprint(tokenizer.batch_decode(text[:, input_ids.shape[1]:], skip_special_tokens=False)[0])\nüìö Citation LLaDA-MoE\nIf you find LLaDA-MoE useful in your research or applications, please cite our paper:\n@article{zhu2025llada,\ntitle={LLaDA-MoE: A Sparse MoE Diffusion Language Model},\nauthor={Fengqi Zhu and Zebin You and Yipeng Xing and Zenan Huang and Lin Liu and Yihong Zhuang and Guoshan Lu and Kangyu Wang and Xudong Wang and Lanning Wei and Hongrui Guo and Jiaqi Hu and Wentao Ye and Tieyuan Chen and Chenchen Li and Chengfu Tang and Haibo Feng and Jun Hu and Jun Zhou and Xiaolu Zhang and Zhenzhong Lan and Junbo Zhao and Da Zheng and Chongxuan Li and Jianguo Li and Ji-Rong Wen},\njournal={arXiv preprint arXiv:2509.24389},\nyear={2025}\n}\nüåê License\nThis project is licensed under the terms of the Apache License 2.0.\nü§ù Contact & Collaboration\nFor questions, collaborations, or feedback, please reach out via Hugging Face or open an issue in the repository.\nüëâ Join us in advancing open, efficient, and intelligent language models!",
    "unsloth/ERNIE-4.5-21B-A3B-Thinking-GGUF": "ERNIE-4.5-21B-A3B-Thinking\nModel Highlights\nModel Overview\nQuickstart\nFastDeploy Inference\nvLLM inference\nUsing transformers library\nLicense\nCitation\nERNIE-4.5-21B-A3B-Thinking\nModel Highlights\nOver the past three months, we have continued to scale the thinking capability of ERNIE-4.5-21B-A3B, improving both the quality and depth of reasoning, thereby advancing the competitiveness of ERNIE lightweight models in complex reasoning tasks. We are pleased to introduce ERNIE-4.5-21B-A3B-Thinking, featuring the following key enhancements:\nSignificantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\nEfficient tool usage capabilities.\nEnhanced 128K long-context understanding capabilities.\nNote: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\nModel Overview\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\nKey\nValue\nModality\nText\nTraining Stage\nPosttraining\nParams(Total / Activated)\n21B / 3B\nLayers\n28\nHeads(Q/KV)\n20 / 4\nText Experts(Total / Activated)\n64 / 6\nVision Experts(Total / Activated)\n64 / 6\nShared Experts\n2\nContext Length\n131072\nQuickstart\nTo align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\nFastDeploy Inference\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the FastDeploy GitHub Repository.\nNote: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\npython -m fastdeploy.entrypoints.openai.api_server \\\n--model baidu/ERNIE-4.5-21B-A3B-Thinking \\\n--port 8180 \\\n--metrics-port 8181 \\\n--engine-worker-queue-port 8182 \\\n--load_choices \"default_v1\" \\\n--tensor-parallel-size 1 \\\n--max-model-len 131072 \\\n--reasoning-parser ernie_x1 \\\n--tool-call-parser ernie_x1 \\\n--max-num-seqs 32\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\ncurl -X POST \"http://0.0.0.0:8180/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d $'{\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": \"How \\'s the weather in Beijing today?\"\n}\n],\n\"tools\": [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_weather\",\n\"description\": \"Determine weather in my location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city and state e.g. San Francisco, CA\"\n},\n\"unit\": {\n\"type\": \"string\",\n\"enum\": [\n\"c\",\n\"f\"\n]\n}\n},\n\"additionalProperties\": false,\n\"required\": [\n\"location\",\n\"unit\"\n]\n},\n\"strict\": true\n}\n}]\n}'\nvLLM inference\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\nThe reasoning-parser and tool-call-parser for vLLM Ernie are currently under development.\nUsing transformers library\nNote: You'll need thetransformerslibrary (version 4.54.0 or newer) installed to use this model.\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16,\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=1024\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# decode the generated ids\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"generate_text:\", generate_text)\nLicense\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\nCitation\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\n@misc{ernie2025technicalreport,\ntitle={ERNIE 4.5 Technical Report},\nauthor={Baidu-ERNIE-Team},\nyear={2025},\nprimaryClass={cs.CL},\nhowpublished={\\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\n}",
    "yichengup/flux.1-fill-dev-OneReward": "OneReward - ComfyUI\nOneReward - ComfyUI\nThis repo contains the checkpoint from OneReward processed into a single model suitable for ComfyUI use.\nOneReward is a novel RLHF methodology for the visual domain by employing Qwen2.5-VL as a generative reward model to enhance multitask reinforcement learning, significantly improving the policy model‚Äôs generation ability across multiple subtask. Building on OneReward, FLUX.1-Fill-dev-OneReward -  based on FLUX Fill [dev], outperforms closed-source FLUX Fill [Pro] in inpainting and outpainting tasks, serving as a powerful new baseline for future research in unified image editing.\nFor more details and examples see original model repo: OneReward",
    "stanford-oval/churro-3B": "CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition\nHandwritten and printed text recognition across 22 centuries and 46 language clusters, including historical and dead languages.\nCost vs. accuracy: CHURRO (3B) achieves higher accuracy than much larger commercial and open-weight VLMs while being substantially cheaper.\nCHURRO is a 3B-parameter open-weight vision-language model (VLM) for historical document transcription. It is trained on CHURRO-DS, a curated dataset of ~100K pages from 155 historical collections spanning 22 centuries and 46 language clusters.\nOn the CHURRO-DS test set, CHURRO delivers 15.5√ó lower cost than Gemini 2.5 Pro while exceeding its accuracy.\nFor more details and code see https://github.com/stanford-oval/Churro.",
    "ibm-granite/granite-4.0-micro": "Granite-4.0-Micro\nGranite-4.0-Micro\nüì£ Update [10-07-2025]: Added a default system prompt to the chat template to guide the model towards more professional, accurate, and safe responses.\nModel Summary:\nGranite-4.0-Micro is a 3B parameter long-context instruct model finetuned from Granite-4.0-Micro-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets. This model is developed using a diverse set of techniques with a structured chat format, including supervised finetuning, model alignment using reinforcement learning, and model merging. Granite 4.0 instruct models feature improved instruction following (IF) and tool-calling capabilities, making them more effective in enterprise applications.\nDevelopers: Granite Team, IBM\nHF Collection: Granite 4.0 Language Models HF Collection\nGitHub Repository: ibm-granite/granite-4.0-language-models\nWebsite: Granite Docs\nRelease Date: October 2nd, 2025\nLicense: Apache 2.0\nSupported Languages:\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. Users may finetune Granite 4.0 models for languages beyond these languages.\nIntended use:\nThe model is designed to follow general instructions and can serve as the foundation for AI assistants across diverse domains, including business applications, as well as for LLM agents equipped with tool-use capabilities.\nCapabilities\nSummarization\nText classification\nText extraction\nQuestion-answering\nRetrieval Augmented Generation (RAG)\nCode related tasks\nFunction-calling tasks\nMultilingual dialog use cases\nFill-In-the-Middle (FIM) code completions\nGeneration:\nThis is a simple example of how to use Granite-4.0-Micro model.\nInstall the following libraries:\npip install torch torchvision torchaudio\npip install accelerate\npip install transformers\nThen, copy the snippet from the section that is relevant for your use case.\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\"\nmodel_path = \"ibm-granite/granite-4.0-micro\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n# drop device_map if running on CPU\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\nmodel.eval()\n# change input text as desired\nchat = [\n{ \"role\": \"user\", \"content\": \"Please list one IBM Research laboratory located in the United States. You should only output its name and location.\" },\n]\nchat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n# tokenize the text\ninput_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\n# generate output tokens\noutput = model.generate(**input_tokens,\nmax_new_tokens=100)\n# decode output tokens into text\noutput = tokenizer.batch_decode(output)\n# print output\nprint(output[0])\nExpected output:\n<|start_of_role|>system<|end_of_role|>You are a helpful assistant. Please ensure responses are professional, accurate, and safe.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>Please list one IBM Research laboratory located in the United States. You should only output its name and location.<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>Almaden Research Center, San Jose, California<|end_of_text|>\nTool-calling:\nGranite-4.0-Micro comes with enhanced tool calling capabilities, enabling seamless integration with external functions and APIs. To define a list of  tools please follow OpenAI's function definition schema.\nThis is an example of how to use Granite-4.0-Micro model tool-calling ability:\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\"\nmodel_path = \"ibm-granite/granite-4.0-micro\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n# drop device_map if running on CPU\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\nmodel.eval()\ntools = [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_weather\",\n\"description\": \"Get the current weather for a specified city.\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"city\": {\n\"type\": \"string\",\n\"description\": \"Name of the city\"\n}\n},\n\"required\": [\"city\"]\n}\n}\n}\n]\n# change input text as desired\nchat = [\n{ \"role\": \"user\", \"content\": \"What's the weather like in Boston right now?\" },\n]\nchat = tokenizer.apply_chat_template(chat, \\\ntokenize=False, \\\ntools=tools, \\\nadd_generation_prompt=True)\n# tokenize the text\ninput_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\n# generate output tokens\noutput = model.generate(**input_tokens,\nmax_new_tokens=100)\n# decode output tokens into text\noutput = tokenizer.batch_decode(output)\n# print output\nprint(output[0])\nExpected output:\n<|start_of_role|>system<|end_of_role|>You are a helpful assistant with access to the following tools. You may call one or more tools to assist with the user query.\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>\n{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather for a specified city.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\", \"description\": \"Name of the city\"}}, \"required\": [\"city\"]}}}\n</tools>\nFor each tool call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call>. If a tool does not exist in the provided list of tools, notify the user that you do not have the ability to fulfill the request.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>What's the weather like in Boston right now?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|><tool_call>\n{\"name\": \"get_current_weather\", \"arguments\": {\"city\": \"Boston\"}}\n</tool_call><|end_of_text|>\nEvaluation Results:\nBenchmarks\nMetric\nMicro Dense\nH Micro Dense\nH Tiny MoE\nH Small MoE\nGeneral Tasks\nMMLU\n5-shot\n65.98\n67.43\n68.65\n78.44\nMMLU-Pro\n5-shot, CoT\n44.5\n43.48\n44.94\n55.47\nBBH\n3-shot, CoT\n72.48\n69.36\n66.34\n81.62\nAGI EVAL\n0-shot, CoT\n64.29\n59\n62.15\n70.63\nGPQA\n0-shot, CoT\n30.14\n32.15\n32.59\n40.63\nAlignment Tasks\nAlpacaEval 2.0\n29.49\n31.49\n30.61\n42.48\nIFEval\nInstruct, Strict\n85.5\n86.94\n84.78\n89.87\nIFEval\nPrompt, Strict\n79.12\n81.71\n78.1\n85.22\nIFEval\nAverage\n82.31\n84.32\n81.44\n87.55\nArenaHard\n25.84\n36.15\n35.75\n46.48\nMath Tasks\nGSM8K\n8-shot\n85.45\n81.35\n84.69\n87.27\nGSM8K Symbolic\n8-shot\n79.82\n77.5\n81.1\n87.38\nMinerva Math\n0-shot, CoT\n62.06\n66.44\n69.64\n74\nDeepMind Math\n0-shot, CoT\n44.56\n43.83\n49.92\n59.33\nCode Tasks\nHumanEval\npass@1\n80\n81\n83\n88\nHumanEval+\npass@1\n72\n75\n76\n83\nMBPP\npass@1\n72\n73\n80\n84\nMBPP+\npass@1\n64\n64\n69\n71\nCRUXEval-O\npass@1\n41.5\n41.25\n39.63\n50.25\nBigCodeBench\npass@1\n39.21\n37.9\n41.06\n46.23\nTool Calling Tasks\nBFCL v3\n59.98\n57.56\n57.65\n64.69\nMultilingual Tasks\nMULTIPLE\npass@1\n49.21\n49.46\n55.83\n57.37\nMMMLU\n5-shot\n55.14\n55.19\n61.87\n69.69\nINCLUDE\n5-shot\n51.62\n50.51\n53.12\n63.97\nMGSM\n8-shot\n28.56\n44.48\n45.36\n38.72\nSafety\nSALAD-Bench\n97.06\n96.28\n97.77\n97.3\nAttaQ\n86.05\n84.44\n86.61\n86.64\nMultilingual Benchmarks and thr included languages:\nBenchmarks\n# Langs\nLanguages\nMMMLU\n11\nar, de, en, es, fr, ja, ko, pt, zh, bn, hi\nINCLUDE\n14\nhi, bn, ta, te, ar, de, es, fr, it, ja, ko, nl, pt, zh\nMGSM\n5\nen, es, fr, ja, zh\nModel Architecture:\nGranite-4.0-Micro baseline is built on a decoder-only dense transformer architecture. Core components of this architecture are: GQA, RoPE, MLP with SwiGLU, RMSNorm, and shared input/output embeddings.\nModel\nMicro Dense\nH Micro Dense\nH Tiny MoE\nH Small MoE\nEmbedding size\n2560\n2048\n1536\n4096\nNumber of layers\n40 attention\n4 attention / 36 Mamba2\n4 attention / 36 Mamba2\n4 attention / 36 Mamba2\nAttention head size\n64\n64\n128\n128\nNumber of attention heads\n40\n32\n12\n32\nNumber of KV heads\n8\n8\n4\n8\nMamba2 state size\n-\n128\n128\n128\nNumber of Mamba2 heads\n-\n64\n48\n128\nMLP / Shared expert hidden size\n8192\n8192\n1024\n1536\nNum. Experts\n-\n-\n64\n72\nNum. active Experts\n-\n-\n6\n10\nExpert hidden size\n-\n-\n512\n768\nMLP activation\nSwiGLU\nSwiGLU\nSwiGLU\nSwiGLU\nSequence length\n128K\n128K\n128K\n128K\nPosition embedding\nRoPE\nNoPE\nNoPE\nNoPE\n# Parameters\n3B\n3B\n7B\n32B\n# Active parameters\n3B\n3B\n1B\n9B\nTraining Data:\nOverall, our SFT data is largely comprised of three key sources: (1) publicly available datasets with permissive license, (2) internal synthetic data targeting specific capabilities, and (3) a select set of human-curated data.\nInfrastructure:\nWe trained the Granite 4.0 Language Models utilizing an NVIDIA GB200 NVL72 cluster hosted in CoreWeave. Intra-rack communication occurs via the 72-GPU NVLink domain, and a non-blocking, full Fat-Tree NDR 400 Gb/s InfiniBand network provides inter-rack communication. This cluster provides a scalable and efficient infrastructure for training our models over thousands of GPUs.\nEthical Considerations and Limitations:\nGranite 4.0 Instruction Models are primarily finetuned using instruction-response pairs mostly in English, but also multilingual data covering multiple languages. Although this model can handle multilingual dialog use cases, its performance might not be similar to English tasks. In such case, introducing a small number of examples (few-shot) can help the model in generating more accurate outputs. While this model has been aligned by keeping safety in consideration, the model may in some cases produce inaccurate, biased, or unsafe responses to user prompts. So we urge the community to use this model with proper safety testing and tuning tailored for their specific tasks.\nResources\n‚≠êÔ∏è Learn about the latest updates with Granite: https://www.ibm.com/granite\nüìÑ Get started with tutorials, best practices, and prompt engineering advice: https://www.ibm.com/granite/docs/\nüí° Learn about the latest Granite learning resources: https://ibm.biz/granite-learning-resources",
    "ibm-granite/granite-4.0-h-small": "Granite-4.0-H-Small\nGranite-4.0-H-Small\nüì£ Update [10-07-2025]: Added a default system prompt to the chat template to guide the model towards more professional, accurate, and safe responses.\nModel Summary:\nGranite-4.0-H-Small is a 32B parameter long-context instruct model finetuned from Granite-4.0-H-Small-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets. This model is developed using a diverse set of techniques with a structured chat format, including supervised finetuning, model alignment using reinforcement learning, and model merging. Granite 4.0 instruct models feature improved instruction following (IF) and tool-calling capabilities, making them more effective in enterprise applications.\nDevelopers: Granite Team, IBM\nHF Collection: Granite 4.0 Language Models HF Collection\nGitHub Repository: ibm-granite/granite-4.0-language-models\nWebsite: Granite Docs\nRelease Date: October 2nd, 2025\nLicense: Apache 2.0\nSupported Languages:\nEnglish, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch, and Chinese. Users may finetune Granite 4.0 models for languages beyond these languages.\nIntended use:\nThe model is designed to respond to general instructions and can be used to build AI assistants for multiple domains, including business applications.\nCapabilities\nSummarization\nText classification\nText extraction\nQuestion-answering\nRetrieval Augmented Generation (RAG)\nCode related tasks\nFunction-calling tasks\nMultilingual dialog use cases\nFill-In-the-Middle (FIM) code completions\nGeneration:\nThis is a simple example of how to use Granite-4.0-H-Small model.\nInstall the following libraries:\npip install torch torchvision torchaudio\npip install accelerate\npip install transformers\nThen, copy the snippet from the section that is relevant for your use case.\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\"\nmodel_path = \"ibm-granite/granite-4.0-h-small\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n# drop device_map if running on CPU\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\nmodel.eval()\n# change input text as desired\nchat = [\n{ \"role\": \"user\", \"content\": \"Please list one IBM Research laboratory located in the United States. You should only output its name and location.\" },\n]\nchat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n# tokenize the text\ninput_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\n# generate output tokens\noutput = model.generate(**input_tokens,\nmax_new_tokens=100)\n# decode output tokens into text\noutput = tokenizer.batch_decode(output)\n# print output\nprint(output[0])\nExpected output:\n<|start_of_role|>system<|end_of_role|>You are a helpful assistant. Please ensure responses are professional, accurate, and safe.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>Please list one IBM Research laboratory located in the United States. You should only output its name and location.<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>Almaden Research Center, San Jose, California<|end_of_text|>\nTool-calling:\nGranite-4.0-H-Small comes with enhanced tool calling capabilities, enabling seamless integration with external functions and APIs. To define a list of  tools please follow OpenAI's function definition schema.\nThis is an example of how to use Granite-4.0-H-Small model tool-calling ability:\ntools = [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_weather\",\n\"description\": \"Get the current weather for a specified city.\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"city\": {\n\"type\": \"string\",\n\"description\": \"Name of the city\"\n}\n},\n\"required\": [\"city\"]\n}\n}\n}\n]\n# change input text as desired\nchat = [\n{ \"role\": \"user\", \"content\": \"What's the weather like in Boston right now?\" },\n]\nchat = tokenizer.apply_chat_template(chat, \\\ntokenize=False, \\\ntools=tools, \\\nadd_generation_prompt=True)\n# tokenize the text\ninput_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\n# generate output tokens\noutput = model.generate(**input_tokens,\nmax_new_tokens=100)\n# decode output tokens into text\noutput = tokenizer.batch_decode(output)\n# print output\nprint(output[0])\nExpected output:\n<|start_of_role|>system<|end_of_role|>You are a helpful assistant with access to the following tools. You may call one or more tools to assist with the user query.\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>\n{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather for a specified city.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": {\"type\": \"string\", \"description\": \"Name of the city\"}}, \"required\": [\"city\"]}}}\n</tools>\nFor each tool call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call>. If a tool does not exist in the provided list of tools, notify the user that you do not have the ability to fulfill the request.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>What's the weather like in Boston right now?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|><tool_call>\n{\"name\": \"get_current_weather\", \"arguments\": {\"city\": \"Boston\"}}\n</tool_call><|end_of_text|>\nEvaluation Results:\nBenchmarks\nMetric\nMicro Dense\nH Micro Dense\nH Tiny MoE\nH Small MoE\nGeneral Tasks\nMMLU\n5-shot\n65.98\n67.43\n68.65\n78.44\nMMLU-Pro\n5-shot, CoT\n44.5\n43.48\n44.94\n55.47\nBBH\n3-shot, CoT\n72.48\n69.36\n66.34\n81.62\nAGI EVAL\n0-shot, CoT\n64.29\n59\n62.15\n70.63\nGPQA\n0-shot, CoT\n30.14\n32.15\n32.59\n40.63\nAlignment Tasks\nAlpacaEval 2.0\n29.49\n31.49\n30.61\n42.48\nIFEval\nInstruct, Strict\n85.5\n86.94\n84.78\n89.87\nIFEval\nPrompt, Strict\n79.12\n81.71\n78.1\n85.22\nIFEval\nAverage\n82.31\n84.32\n81.44\n87.55\nArenaHard\n25.84\n36.15\n35.75\n46.48\nMath Tasks\nGSM8K\n8-shot\n85.45\n81.35\n84.69\n87.27\nGSM8K Symbolic\n8-shot\n79.82\n77.5\n81.1\n87.38\nMinerva Math\n0-shot, CoT\n62.06\n66.44\n69.64\n74\nDeepMind Math\n0-shot, CoT\n44.56\n43.83\n49.92\n59.33\nCode Tasks\nHumanEval\npass@1\n80\n81\n83\n88\nHumanEval+\npass@1\n72\n75\n76\n83\nMBPP\npass@1\n72\n73\n80\n84\nMBPP+\npass@1\n64\n64\n69\n71\nCRUXEval-O\npass@1\n41.5\n41.25\n39.63\n50.25\nBigCodeBench\npass@1\n39.21\n37.9\n41.06\n46.23\nTool Calling Tasks\nBFCL v3\n59.98\n57.56\n57.65\n64.69\nMultilingual Tasks\nMULTIPLE\npass@1\n49.21\n49.46\n55.83\n57.37\nMMMLU\n5-shot\n55.14\n55.19\n61.87\n69.69\nINCLUDE\n5-shot\n51.62\n50.51\n53.12\n63.97\nMGSM\n8-shot\n28.56\n44.48\n45.36\n38.72\nSafety\nSALAD-Bench\n97.06\n96.28\n97.77\n97.3\nAttaQ\n86.05\n84.44\n86.61\n86.64\nMultilingual Benchmarks and thr included languages:\nBenchmarks\n# Langs\nLanguages\nMMMLU\n11\nar, de, en, es, fr, ja, ko, pt, zh, bn, hi\nINCLUDE\n14\nhi, bn, ta, te, ar, de, es, fr, it, ja, ko, nl, pt, zh\nMGSM\n5\nen, es, fr, ja, zh\nModel Architecture:\nGranite-4.0-H-Small baseline is built on a decoder-only MoE transformer architecture. Core components of this architecture are: GQA, Mamba2, MoEs with shared experts, SwiGLU activation, RMSNorm, and shared input/output embeddings.\nModel\nMicro Dense\nH Micro Dense\nH Tiny MoE\nH Small MoE\nEmbedding size\n2560\n2048\n1536\n4096\nNumber of layers\n40 attention\n4 attention / 36 Mamba2\n4 attention / 36 Mamba2\n4 attention / 36 Mamba2\nAttention head size\n64\n64\n128\n128\nNumber of attention heads\n40\n32\n12\n32\nNumber of KV heads\n8\n8\n4\n8\nMamba2 state size\n-\n128\n128\n128\nNumber of Mamba2 heads\n-\n64\n48\n128\nMLP / Shared expert hidden size\n8192\n8192\n1024\n1536\nNum. Experts\n-\n-\n64\n72\nNum. active Experts\n-\n-\n6\n10\nExpert hidden size\n-\n-\n512\n768\nMLP activation\nSwiGLU\nSwiGLU\nSwiGLU\nSwiGLU\nSequence length\n128K\n128K\n128K\n128K\nPosition embedding\nRoPE\nNoPE\nNoPE\nNoPE\n# Parameters\n3B\n3B\n7B\n32B\n# Active parameters\n3B\n3B\n1B\n9B\nTraining Data:\nOverall, our SFT data is largely comprised of three key sources: (1) publicly available datasets with permissive license, (2) internal synthetic data targeting specific capabilities, and (3) a select set of human-curated data.\nInfrastructure:\nWe trained the Granite 4.0 Language Models utilizing an NVIDIA GB200 NVL72 cluster hosted in CoreWeave. Intra-rack communication occurs via the 72-GPU NVLink domain, and a non-blocking, full Fat-Tree NDR 400 Gb/s InfiniBand network provides inter-rack communication. This cluster provides a scalable and efficient infrastructure for training our models over thousands of GPUs.\nEthical Considerations and Limitations:\nGranite 4.0 Instruction Models are primarily finetuned using instruction-response pairs mostly in English, but also multilingual data covering multiple languages. Although this model can handle multilingual dialog use cases, its performance might not be similar to English tasks. In such case, introducing a small number of examples (few-shot) can help the model in generating more accurate outputs. While this model has been aligned by keeping safety in consideration, the model may in some cases produce inaccurate, biased, or unsafe responses to user prompts. So we urge the community to use this model with proper safety testing and tuning tailored for their specific tasks.\nResources\n‚≠êÔ∏è Learn about the latest updates with Granite: https://www.ibm.com/granite\nüìÑ Get started with tutorials, best practices, and prompt engineering advice: https://www.ibm.com/granite/docs/\nüí° Learn about the latest Granite learning resources: https://ibm.biz/granite-learning-resources",
    "unsloth/Magistral-Small-2509-GGUF": "Magistral Small 1.2\nUpdates compared with Magistral Small 1.1\nKey Features\nBenchmark Results\nSampling parameters\nBasic Chat Template\nTransformers\nLearn to run Magistral 1.2 correctly - Read our Guide.\nUnsloth Dynamic 2.0 achieves SOTA performance in model quantization.\n‚ú® How to Use Magistral 1.2:\nRun in llama.cpp:\n./llama.cpp/llama-cli -hf unsloth/Magistral-Small-2509-GGUF:UD-Q4_K_XL --jinja --temp 0.7 --top-k -1 --top-p 0.95 -ngl 99\nRun in Ollama:\nollama run hf.co/unsloth/Magistral-Small-2509-GGUF:UD-Q4_K_XL\nRead our in-depth guide about Magistral 1.2: docs.unsloth.ai/basics/magistral\nFine-tune Magistral 1.2 for free using our Kaggle notebook here!\nView the rest of our notebooks in our docs here.\nMagistral Small 1.2\nBuilding upon Mistral Small 3.2 (2506), with added reasoning capabilities, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters.\nMagistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.\nLearn more about Magistral in our blog post.\nThe model was presented in the paper Magistral.\nUpdates compared with Magistral Small 1.1\nMultimodality: The model now has a vision encoder and can take multimodal inputs, extending its reasoning capabilities to vision.\nPerformance upgrade: Magistral Small 1.2 should give you significatively better performance than Magistral Small 1.1 as seen in the benchmark results.\nBetter tone and persona: You should experiment better LaTeX and Markdown formatting, and shorter answers on easy general prompts.\nFinite generation: The model is less likely to enter infinite generation loops.\nSpecial think tokens: [THINK] and [/THINK] special tokens encapsulate the reasoning content in a thinking chunk. This makes it easier to parse the reasoning trace and prevents confusion when the '[THINK]' token is given as a string in the prompt.\nReasoning prompt: The reasoning prompt is given in the system prompt.\nKey Features\nReasoning: Capable of long chains of reasoning traces before providing an answer.\nMultilingual: Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi.\nVision: Vision capabilities enable the model to analyze images and reason based on visual content in addition to text.\nApache 2.0 License: Open license allowing usage and modification for both commercial and non-commercial purposes.\nContext Window: A 128k context window. Performance might degrade past 40k but Magistral should still give good results. Hence we recommend to leave the maximum model length to 128k and only lower if you encounter low performance.\nBenchmark Results\nModel\nAIME24 pass@1\nAIME25 pass@1\nGPQA Diamond\nLivecodebench (v5)\nMagistral Medium 1.2\n91.82%\n83.48%\n76.26%\n75.00%\nMagistral Medium 1.1\n72.03%\n60.99%\n71.46%\n59.35%\nMagistral Medium 1.0\n73.59%\n64.95%\n70.83%\n59.36%\nMagistral Small 1.2\n86.14%\n77.34%\n70.07%\n70.88%\nMagistral Small 1.1\n70.52%\n62.03%\n65.78%\n59.17%\nMagistral Small 1.0\n70.68%\n62.76%\n68.18%\n55.84%\nSampling parameters\nPlease make sure to use:\ntop_p: 0.95\ntemperature: 0.7\nmax_tokens: 131072\nBasic Chat Template\nWe highly recommend including the following system prompt for the best results, you can edit and customise it if needed for your specific use case.\nFirst draft your thinking process (inner monologue) until you arrive at a response. Format your response using Markdown, and use LaTeX for any mathematical equations. Write both your thoughts and the response in the same language as the input.\nYour thinking process must follow the template below:[THINK]Your thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate the response. Use the same language as the input.[/THINK]Here, provide a self-contained response.\nThe [THINK] and [/THINK] are special tokens that must be encoded as such.\nPlease make sure to use mistral-common as the source of truth. Find below examples from libraries supporting mistral-common.\nWe invite you to choose, depending on your use case and requirements, between keeping reasoning traces during multi-turn interactions or keeping only the final assistant response.\nPing model as follows:\nPython text snippet\nfrom typing import Any\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.7\nTOP_P = 0.95\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> dict[str, Any]:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nindex_begin_think = system_prompt.find(\"[THINK]\")\nindex_end_think = system_prompt.find(\"[/THINK]\")\nreturn {\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": system_prompt[:index_begin_think]},\n{\n\"type\": \"thinking\",\n\"thinking\": system_prompt[\nindex_begin_think + len(\"[THINK]\") : index_end_think\n],\n\"closed\": True,\n},\n{\n\"type\": \"text\",\n\"text\": system_prompt[index_end_think + len(\"[/THINK]\") :],\n},\n],\n}\nSYSTEM_PROMPT = load_system_prompt(model, \"SYSTEM_PROMPT.txt\")\nquery = \"Use each number in 2,5,6,3 exactly once, along with any combination of +, -, √ó, √∑ (and parentheses for grouping), to make the number 24.\"\nmessages = [\nSYSTEM_PROMPT,\n{\"role\": \"user\", \"content\": query}\n]\nstream = client.chat.completions.create(\nmodel=model,\nmessages=messages,\nstream=True,\ntemperature=TEMP,\ntop_p=TOP_P,\nmax_tokens=MAX_TOK,\n)\nprint(\"client: Start streaming chat completions...:\\n\")\nprinted_reasoning_content = False\nanswer = []\nfor chunk in stream:\nreasoning_content = None\ncontent = None\n# Check the content is reasoning_content or content\nif hasattr(chunk.choices[0].delta, \"reasoning_content\"):\nreasoning_content = chunk.choices[0].delta.reasoning_content\nelif hasattr(chunk.choices[0].delta, \"content\"):\ncontent = chunk.choices[0].delta.content\nif reasoning_content is not None:\nif not printed_reasoning_content:\nprinted_reasoning_content = True\nprint(\"Start reasoning:\\n\", end=\"\", flush=True)\nprint(reasoning_content, end=\"\", flush=True)\nelif content is not None:\n# Extract and print the content\nif not reasoning_content and printed_reasoning_content:\nanswer.extend(content)\nprint(content, end=\"\", flush=True)\nif answer:\nprint(\"\\n\\n=============\\nAnswer\\n=============\\n\")\nprint(\"\".join(answer))\nelse:\nprint(\"\\n\\n=============\\nNo Answer\\n=============\\n\")\nprint(\"No answer was generated by the model, probably because the maximum number of tokens was reached.\")\n# client: Start streaming chat completions...:\n#\n# Start reasoning:\n# First, I need to ...\n# ...\n#\n#\n# =============\n# Answer\n# =============\n#\n# Here's one way to use the numbers 2, 5, 6, 3 to make 24:\n#\n#\\[\n#(6 \\div 2) \\times (5 + 3) = 3 \\times 8 = 24\n#\\]\n#\n#Alternatively, another solution is:\n#\n#\\[\n#6 \\times (5 - 3 + 2) = 6 \\times 4 = 24\n#\\]\n#\n#Both expressions use each of the numbers 2, 5, 6, 3 exactly once with the operations given.\nPython text-image snippet: Pokemon\nfrom typing import Any\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.7\nTOP_P = 0.95\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> dict[str, Any]:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nindex_begin_think = system_prompt.find(\"[THINK]\")\nindex_end_think = system_prompt.find(\"[/THINK]\")\nreturn {\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": system_prompt[:index_begin_think]},\n{\n\"type\": \"thinking\",\n\"thinking\": system_prompt[\nindex_begin_think + len(\"[THINK]\") : index_end_think\n],\n\"closed\": True,\n},\n{\n\"type\": \"text\",\n\"text\": system_prompt[index_end_think + len(\"[/THINK]\") :],\n},\n],\n}\nmodel_id = \"mistralai/Magistral-Small-2509\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438\"\nmessages = [\nSYSTEM_PROMPT,\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.\",\n},\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n],\n},\n]\nstream = client.chat.completions.create(\nmodel=model,\nmessages=messages,\nstream=True,\ntemperature=TEMP,\ntop_p=TOP_P,\nmax_tokens=MAX_TOK,\n)\nprint(\"client: Start streaming chat completions...:\\n\")\nprinted_reasoning_content = False\nanswer = []\nfor chunk in stream:\nreasoning_content = None\ncontent = None\n# Check the content is reasoning_content or content\nif hasattr(chunk.choices[0].delta, \"reasoning_content\"):\nreasoning_content = chunk.choices[0].delta.reasoning_content\nelif hasattr(chunk.choices[0].delta, \"content\"):\ncontent = chunk.choices[0].delta.content\nif reasoning_content is not None:\nif not printed_reasoning_content:\nprinted_reasoning_content = True\nprint(\"Start reasoning:\\n\", end=\"\", flush=True)\nprint(reasoning_content, end=\"\", flush=True)\nelif content is not None:\n# Extract and print the content\nif not reasoning_content and printed_reasoning_content:\nanswer.extend(content)\nprint(content, end=\"\", flush=True)\nif answer:\nprint(\"\\n\\n=============\\nAnswer\\n=============\\n\")\nprint(\"\".join(answer))\nelse:\nprint(\"\\n\\n=============\\nNo Answer\\n=============\\n\")\nprint(\n\"No answer was generated by the model, probably because the maximum number of tokens was reached.\"\n)\n# client: Start streaming chat completions...:\n# Start reasoning:\n# In the image, we see a battle scene from a Pok√©mon game. The player's Pikachu is at full health (83/83 HP), and the opponent's Pidgey is at a lower level (level 17 compared to Pikachu's level 42). The possible actions available to the player are:\n# 1. FIGHT: This allows the player to use one of Pikachu's moves to attack Pidgey. Given that Pikachu is at a higher level and has full HP, it is likely that Pikachu would be able to defeat Pidgey easily. This is a good option because it could potentially win the battle quickly and efficiently.\n# 2. BAG: This allows the player to use an item from their bag. This could be useful if the player wants to heal Pikachu (though it's not necessary at full health) or use an item to weaken Pidgey. However, since Pikachu is at full health and Pidgey is at a lower level, this might not be necessary. It could be a good option if the player wants to use a special item, but generally, it might not be the best choice in this situation.\n# 3. POK√âMON: This allows the player to switch the current Pok√©mon to another one in their team. Since Pikachu is at full health and at a higher level than Pidgey, switching might not be necessary. It could be useful if the player wants to train a different Pok√©mon, but it might not be the most efficient choice for winning the battle quickly.\n# 4. RUN: This allows the player to flee from the battle. This could be a good option if the player wants to avoid the battle, but since Pikachu is at a clear advantage, running would not be the most efficient choice. It could be useful if the player wants to save time or if they are trying to avoid losing a Pok√©mon, but in this case, it seems unnecessary.\n# Given the circumstances, the best action seems to be to FIGHT, as Pikachu is at a clear advantage in terms of level and health. The other options are not as efficient for winning the battle quickly.In the given scenario, the most appropriate action to take is to FIGHT. Here's why:\n# 1. FIGHT: This is the best option because Pikachu is at a higher level and has full health, making it likely to defeat Pidgey quickly and efficiently. Using an attack move would be the most straightforward way to win the battle.\n# 2. BAG: While this option could be useful for healing or using special items, it is not necessary since Pikachu is already at full health. This option is less efficient for winning the battle quickly.\n# 3. POK√âMON: Switching to another Pok√©mon might be useful for training a different Pok√©mon, but it is not necessary since Pikachu is at a clear advantage. This option is not as efficient for winning the current battle.\n# 4. RUN: Fleeing from the battle could be useful if the player wants to avoid the battle, but since Pikachu is at a clear advantage, running would not be the most efficient choice. It could be useful if the player wants to save time or avoid losing a Pok√©mon, but in this case, it seems unnecessary.\n# Therefore, the best action to take in this situation is to FIGHT.\n# FIGHT\n# =============\n# Answer\n# =============\n# In the given scenario, the most appropriate action to take is to FIGHT. Here's why:\n# 1. FIGHT: This is the best option because Pikachu is at a higher level and has full health, making it likely to defeat Pidgey quickly and efficiently. Using an attack move would be the most straightforward way to win the battle.\n# 2. BAG: While this option could be useful for healing or using special items, it is not necessary since Pikachu is already at full health. This option is less efficient for winning the battle quickly.\n# 3. POK√âMON: Switching to another Pok√©mon might be useful for training a different Pok√©mon, but it is not necessary since Pikachu is at a clear advantage. This option is not as efficient for winning the current battle.\n# 4. RUN: Fleeing from the battle could be useful if the player wants to avoid the battle, but since Pikachu is at a clear advantage, running would not be the most efficient choice. It could be useful if the player wants to save time or avoid losing a Pok√©mon, but in this case, it seems unnecessary.\n# Therefore, the best action to take in this situation is to FIGHT.\n# FIGHT\nPython text-image snippet: Geo trivia\nfrom typing import Any\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.7\nTOP_P = 0.95\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> dict[str, Any]:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nindex_begin_think = system_prompt.find(\"[THINK]\")\nindex_end_think = system_prompt.find(\"[/THINK]\")\nreturn {\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": system_prompt[:index_begin_think]},\n{\n\"type\": \"thinking\",\n\"thinking\": system_prompt[\nindex_begin_think + len(\"[THINK]\") : index_end_think\n],\n\"closed\": True,\n},\n{\n\"type\": \"text\",\n\"text\": system_prompt[index_end_think + len(\"[/THINK]\") :],\n},\n],\n}\nmodel_id = \"mistralai/Magistral-Small-2509\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d7/201806_Tianducheng_Bird-eye_View.jpg/1280px-201806_Tianducheng_Bird-eye_View.jpg\"\nmessages = [\nSYSTEM_PROMPT,\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"Where has this picture been taken ?\",\n},\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n],\n},\n]\nstream = client.chat.completions.create(\nmodel=model,\nmessages=messages,\nstream=True,\ntemperature=TEMP,\ntop_p=TOP_P,\nmax_tokens=MAX_TOK,\n)\nprint(\"client: Start streaming chat completions...:\\n\")\nprinted_reasoning_content = False\nanswer = []\nfor chunk in stream:\nreasoning_content = None\ncontent = None\n# Check the content is reasoning_content or content\nif hasattr(chunk.choices[0].delta, \"reasoning_content\"):\nreasoning_content = chunk.choices[0].delta.reasoning_content\nelif hasattr(chunk.choices[0].delta, \"content\"):\ncontent = chunk.choices[0].delta.content\nif reasoning_content is not None:\nif not printed_reasoning_content:\nprinted_reasoning_content = True\nprint(\"Start reasoning:\\n\", end=\"\", flush=True)\nprint(reasoning_content, end=\"\", flush=True)\nelif content is not None:\n# Extract and print the content\nif not reasoning_content and printed_reasoning_content:\nanswer.extend(content)\nprint(content, end=\"\", flush=True)\nif answer:\nprint(\"\\n\\n=============\\nAnswer\\n=============\\n\")\nprint(\"\".join(answer))\nelse:\nprint(\"\\n\\n=============\\nNo Answer\\n=============\\n\")\nprint(\n\"No answer was generated by the model, probably because the maximum number of tokens was reached.\"\n)\n# client: Start streaming chat completions...:\n# Start reasoning:\n# The image shows a replica of the Eiffel Tower, but it's not in Paris. The background includes mountains, which are not present in Paris. The surrounding architecture appears to be more modern and dense, which is also not typical of Paris. The combination of the Eiffel Tower replica and the mountainous backdrop suggests that this is likely in a city in China, as China has several replicas of the Eiffel Tower, with the most famous one being in Shanghai. However, the dense residential buildings and the specific layout suggest that this might be in another city in China, possibly Shenzhen or another major city with a similar landscape.\n# Given that the question is about identifying the location based on the visual clues, and considering the presence of the Eiffel Tower replica and the mountainous backdrop, it's likely that this is a well-known location in China.\n# The most probable answer is that this is in Shenzhen, as it has a well-known Eiffel Tower replica in a park, but to be precise, this is the Eiffel Tower replica in Shenzhen, which is known as the \"Shenzhen Park of Eiffel Tower.\"\n# However, to be more accurate, this is likely the Eiffel Tower replica in Shenzhen, as it matches the description and visual elements.The image shows a replica of the Eiffel Tower, which is not in Paris but rather in a city with a mountainous backdrop and modern, dense architecture. This combination of elements is typical of a Chinese city, and the presence of the Eiffel Tower replica suggests a location like Shenzhen, which is known for having such a replica. The dense residential buildings and the specific layout further support this identification. Therefore, the most probable location for this image is Shenzhen, China.\n# So, the answer is:\n# Shenzhen\n# =============\n# Answer\n# =============\n# The image shows a replica of the Eiffel Tower, which is not in Paris but rather in a city with a mountainous backdrop and modern, dense architecture. This combination of elements is typical of a Chinese city, and the presence of the Eiffel Tower replica suggests a location like Shenzhen, which is known for having such a replica. The dense residential buildings and the specific layout further support this identification. Therefore, the most probable location for this image is Shenzhen, China.\n# So, the answer is:\n# Shenzhen\nPython text-image snippet: Maths\nfrom typing import Any\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n# Modify OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nTEMP = 0.7\nTOP_P = 0.95\nMAX_TOK = 131072\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\nmodels = client.models.list()\nmodel = models.data[0].id\ndef load_system_prompt(repo_id: str, filename: str) -> dict[str, Any]:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nindex_begin_think = system_prompt.find(\"[THINK]\")\nindex_end_think = system_prompt.find(\"[/THINK]\")\nreturn {\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": system_prompt[:index_begin_think]},\n{\n\"type\": \"thinking\",\n\"thinking\": system_prompt[\nindex_begin_think + len(\"[THINK]\") : index_end_think\n],\n\"closed\": True,\n},\n{\n\"type\": \"text\",\n\"text\": system_prompt[index_end_think + len(\"[/THINK]\") :],\n},\n],\n}\nmodel_id = \"mistralai/Magistral-Small-2509\"\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://i.ytimg.com/vi/5Y3xLHeyKZU/hqdefault.jpg\"\nmessages = [\nSYSTEM_PROMPT,\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"Solve the equations. Answer in the language of the image.\",\n},\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n],\n},\n]\nstream = client.chat.completions.create(\nmodel=model,\nmessages=messages,\nstream=True,\ntemperature=TEMP,\ntop_p=TOP_P,\nmax_tokens=MAX_TOK,\n)\nprint(\"client: Start streaming chat completions...:\\n\")\nprinted_reasoning_content = False\nanswer = []\nfor chunk in stream:\nreasoning_content = None\ncontent = None\n# Check the content is reasoning_content or content\nif hasattr(chunk.choices[0].delta, \"reasoning_content\"):\nreasoning_content = chunk.choices[0].delta.reasoning_content\nelif hasattr(chunk.choices[0].delta, \"content\"):\ncontent = chunk.choices[0].delta.content\nif reasoning_content is not None:\nif not printed_reasoning_content:\nprinted_reasoning_content = True\nprint(\"Start reasoning:\\n\", end=\"\", flush=True)\nprint(reasoning_content, end=\"\", flush=True)\nelif content is not None:\n# Extract and print the content\nif not reasoning_content and printed_reasoning_content:\nanswer.extend(content)\nprint(content, end=\"\", flush=True)\nif answer:\nprint(\"\\n\\n=============\\nAnswer\\n=============\\n\")\nprint(\"\".join(answer))\nelse:\nprint(\"\\n\\n=============\\nNo Answer\\n=============\\n\")\nprint(\n\"No answer was generated by the model, probably because the maximum number of tokens was reached.\"\n)\n# client: Start streaming chat completions...:\n# Start reasoning:\n# Je dois r√©soudre ce syst√®me d'√©quations. Voici les √©quations :\n# 1. \\(5x + 2y = -2\\)\n# 2. \\(3x - 4y = 17\\)\n# D'abord, je pense que la m√©thode d'√©limination pourrait √™tre une bonne approche. Pour cela, je dois √©liminer une des variables. Voyons comment.\n# Je vais essayer d'√©liminer y. Pour cela, je dois que les coefficients de y soient les m√™mes (en valeur absolue) dans les deux √©quations.\n# Le coefficient de y dans la premi√®re √©quation est 2, et dans la deuxi√®me, c'est -4. Le plus petit multiple commun de 2 et 4 est 4. Donc, je vais multiplier la premi√®re √©quation par 2 pour que le coefficient de y devienne 4.\n# Faisons cela :\n# 1. \\(2 \\times (5x + 2y) = 2 \\times (-2)\\)\n#    Ce qui donne : \\(10x + 4y = -4\\)\n# Maintenant, les √©quations sont :\n# 1. \\(10x + 4y = -4\\)\n# 2. \\(3x - 4y = 17\\)\n# Maintenant, si j'additionne ces deux √©quations, les termes avec y s'annuleront.\n# Faisons l'addition :\n# \\( (10x + 4y) + (3x - 4y) = -4 + 17 \\)\n# Ce qui donne : \\(13x = 13\\)\n# Donc, \\(x = 1\\).\n# Maintenant que nous avons x, nous pouvons le substituer dans une des √©quations originales pour trouver y. Utilisons la premi√®re √©quation originale :\n# \\(5x + 2y = -2\\)\n# En substituant x = 1 :\n# \\(5(1) + 2y = -2\\)\n# Ce qui donne : \\(5 + 2y = -2\\)\n# Soustraire 5 des deux c√¥t√©s :\n# ...\n# Ce qui donne : \\(5 + 2y = -2\\)\n# Soustraire 5 des deux c√¥t√©s :\n# \\(2y = -2 - 5\\)\n# \\(2y = -7\\)\n# Diviser par 2 :\n# \\(y = -\\frac{7}{2}\\)\n# Donc, la solution est \\(x = 1\\) et \\(y = -\\frac{7}{2}\\).\n# $\\boxed{x = 1,\\ y = -\\frac{7}{2}}$\n# =============\n# Answer\n# =============\n# Pour r√©soudre le syst√®me d'√©quations donn√© :\n# 1. \\(5x + 2y = -2\\)\n# 2. \\(3x - 4y = 17\\)\n# Nous commen√ßons par utiliser la m√©thode d'√©limination pour √©liminer une des variables. Nous choisissons d'√©liminer \\(y\\) en rendant ses coefficients identiques en valeur absolue. Le coefficient de \\(y\\) dans la premi√®re √©quation est 2, et dans la deuxi√®me, c'est -4. Le plus petit multiple commun de 2 et 4 est 4. Nous multiplions donc la premi√®re √©quation par 2 pour que le coefficient de \\(y\\) devienne 4.\n# Faisons cela :\n# 1. \\(2 \\times (5x + 2y) = 2 \\times (-2)\\)\n#    Ce qui donne : \\(10x + 4y = -4\\)\n# Maintenant, les √©quations sont :\n# 1. \\(10x + 4y = -4\\)\n# 2. \\(3x - 4y = 17\\)\n# En additionnant ces deux √©quations, les termes avec \\(y\\) s'annuleront :\n# \\( (10x + 4y) + (3x - 4y) = -4 + 17 \\)\n# Ce qui donne : \\(13x = 13\\)\n# Donc, \\(x = 1\\).\n# Ensuite, nous substituons \\(x = 1\\) dans la premi√®re √©quation originale pour trouver \\(y\\) :\n# \\(5(1) + 2y = -2\\)\n# Ce qui donne : \\(5 + 2y = -2\\)\n# Soustraire 5 des deux c√¥t√©s :\n# \\(2y = -2 - 5\\)\n# \\(2y = -7\\)\n# Diviser par 2 :\n# \\(y = -\\frac{7}{2}\\)\n# Donc, la solution est \\(x = 1\\) et \\(y = -\\frac{7}{2}\\).\n# $\\boxed{x = 1,\\ y = -\\frac{7}{2}}$\nTransformers\nMake sure you install the latest Transformers version:\npip install --upgrade transformers[mistral-common]\nThis should also install mistral_common >= 1.8.5\nTo check:\npython -c \"import mistral_common; print(mistral_common.__version__)\"\nNow you can use Transformers with Magistral:\nPython snippet\nfrom typing import Any\nimport torch\nfrom huggingface_hub import hf_hub_download\nfrom transformers import Mistral3ForConditionalGeneration\nfrom transformers import AutoTokenizer\ndef load_system_prompt(repo_id: str, filename: str) -> dict[str, Any]:\nfile_path = hf_hub_download(repo_id=repo_id, filename=filename)\nwith open(file_path, \"r\") as file:\nsystem_prompt = file.read()\nindex_begin_think = system_prompt.find(\"[THINK]\")\nindex_end_think = system_prompt.find(\"[/THINK]\")\nreturn {\n\"role\": \"system\",\n\"content\": [\n{\"type\": \"text\", \"text\": system_prompt[:index_begin_think]},\n{\n\"type\": \"thinking\",\n\"thinking\": system_prompt[\nindex_begin_think + len(\"[THINK]\") : index_end_think\n],\n\"closed\": True,\n},\n{\n\"type\": \"text\",\n\"text\": system_prompt[index_end_think + len(\"[/THINK]\") :],\n},\n],\n}\nmodel_id = \"mistralai/Magistral-Small-2509\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, tokenizer_type=\"mistral\")\nmodel = Mistral3ForConditionalGeneration.from_pretrained(\nmodel_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n).eval()\nSYSTEM_PROMPT = load_system_prompt(model_id, \"SYSTEM_PROMPT.txt\")\nimage_url = \"https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438\"\nmessages = [\nSYSTEM_PROMPT,\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.\",\n},\n{\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n],\n},\n]\ntokenized = tokenizer.apply_chat_template(messages, return_dict=True)\ninput_ids = torch.tensor(tokenized.input_ids, device=\"cuda\").unsqueeze(0)\nattention_mask = torch.tensor(tokenized.attention_mask, device=\"cuda\").unsqueeze(0)\npixel_values = torch.tensor(\ntokenized.pixel_values[0], dtype=torch.bfloat16, device=\"cuda\"\n).unsqueeze(0)\nimage_sizes = torch.tensor(pixel_values.shape[-2:], device=\"cuda\").unsqueeze(0)\nwith torch.inference_mode():\noutput = model.generate(\ninput_ids=input_ids,\nattention_mask=attention_mask,\npixel_values=pixel_values,\nimage_sizes=image_sizes,\n)[0]\ndecoded_output = tokenizer.decode(\noutput[\nlen(tokenized.input_ids) : (\n-1 if output[-1] == tokenizer.eos_token_id else len(output)\n)\n]\n)\nprint(decoded_output)\n# [THINK]Alright, let's analyze the image carefully. It's a scene from a Pok√©mon game. The player is controlling Pikachu, which is at level 42 with full HP (83/83). The opponent is a Pidgey at level 17. The question is asking what action the player should take in this situation.\n# First, let's list all the possible actions available. From the bottom of the screen, the options are:\n# 1. FIGHT\n# 2. BAG\n# 3. POK√âMON\n# 4. RUN\n# Now, let's consider each option:\n# 1. **FIGHT**: This means using Pikachu's moves to attack the Pidgey.\n#    - Pros: Pikachu is at a higher level (42) compared to Pidgey (17), so it has a significant advantage. Pikachu's HP is full, so it's in good condition to fight. Fighting could potentially win the battle quickly.\n#    - Cons: Even though Pikachu is stronger, there's always a risk of Pidgey landing a lucky hit or using a powerful move. However, given the level difference, this is less likely.\n# 2. **BAG**: This means using items from the bag to help in the battle.\n#    - Pros: Could use a potion to heal (though Pikachu is already at full HP), or use another item like a Pok√© Ball to try and catch Pidgey.\n#    - Cons: Using items might be less efficient than just fighting, especially since Pikachu is already at full health. Also, if the goal is to catch Pidgey, using items to weaken it first might be useful, but the immediate advantage isn't clear.\n# 3. **POK√âMON**: This means switching to another Pok√©mon from the team.\n#    - Pros: If the player has another Pok√©mon that is stronger or has moves that are super effective against Pidgey, this could be useful.\n#    - Cons: Pikachu is already at a significant level advantage and is at full health, so switching might not be necessary unless there's a strategic reason (e.g., leveling up another Pok√©mon).\n# 4. **RUN**: This means attempting to flee from the battle.\n#    - Pros: If the player wants to avoid the battle for some reason (e.g., saving time, or wanting to catch Pidgey without weakening it), running could be useful.\n#    - Cons: If the goal is to catch or defeat Pidgey, running would not help in achieving that. Also, running might not always be successful, especially in a grass encounter where the chance to flee might be lower.\n# Now, considering the context, if the goal is to defeat Pidgey, the best action would be to FIGHT since Pikachu is at a significant advantage in terms of level and health. Using the BAG could be useful if the goal is to catch Pidgey, as items like Pok√© Balls or berries could be used. Switching Pok√©mon doesn't seem necessary given Pikachu's current status. Running would only be useful if the player wants to avoid the battle, which seems unlikely if the goal is to progress in the game.\n# However, the question asks for the best action without specifying the goal. Assuming the goal is to defeat Pidgey efficiently, the best action would be to FIGHT.\n# But to list all possible actions and their pros and cons:\n# 1. **FIGHT**\n#    - Pros: High chance of winning quickly due to level and health advantage.\n#    - Cons: Small risk of Pikachu getting damaged.\n# 2. **BAG**\n#    - Pros: Could use items to catch Pidgey or support Pikachu (though not necessary for healing).\n#    - Cons: Might be less efficient if the goal is just to defeat Pidgey.\n# 3. **POK√âMON**\n#    - Pros: Could switch to another Pok√©mon for strategic reasons (e.g., type advantage, training a different Pok√©mon).\n#    - Cons: Pikachu is already at a strong position, so switching might be unnecessary unless there's another goal.\n# 4. **RUN**\n#    - Pros: Escape the battle if needed.\n#    - Cons: Does not contribute to progress if the goal is to defeat or catch Pidgey.\n# Given that, if the goal is to defeat Pidgey, the best action is to FIGHT. If the goal is to catch Pidgey, using the BAG to throw a Pok√© Ball could be the best action.[/THINK]Here are the possible actions and their explanations:\n# 1. **FIGHT**\n#    - **Pros**: Pikachu is at a much higher level (42) compared to Pidgey (17) and has full HP. This makes it very likely to win quickly.\n#    - **Cons**: There's always a small risk that Pidgey could land a lucky hit, but this is unlikely given the level difference.\n# 2. **BAG**\n#    - **Pros**: You could use items to catch Pidgey (e.g., Pok√© Ball) or support Pikachu (though Pikachu is already at full HP).\n#    - **Cons**: Using items might be less efficient if the goal is simply to defeat Pidgey, as fighting directly could be quicker.\n# 3. **POK√âMON**\n#    - **Pros**: You could switch to another Pok√©mon that might have a type advantage or that you want to train.\n#    - **Cons**: Pikachu is already in a strong position to defeat Pidgey, so switching might not be necessary unless there's another strategic reason.\n# 4. **RUN**\n#    - **Pros**: You can escape the battle if you need to, for example, if you want to preserve Pikachu's health for a tougher battle ahead.\n#    - **Cons**: Running doesn't help you progress if your goal is to defeat or catch Pidgey. Additionally, the success rate for running might be lower in a grass encounter.\n# Given these considerations, if your goal is to defeat Pidgey, the best action is likely to **FIGHT**, as Pikachu is at a significant advantage. If your goal is to catch Pidgey, using the **BAG** to throw a Pok√© Ball could be the best choice. If you're looking to train a different Pok√©mon, you might consider switching with **POK√âMON**, and if you need to preserve resources or Pikachu's health, **RUN** could be an option.",
    "PromptEnhancer/PromptEnhancer-32B": "PromptEnhancerV2 (32B)\nModel Details\nModel Description\nModel Sources\nHow to Get Started with the Model\nEvaluation\nCitation\nPromptEnhancerV2 (32B)\nPromptEnhancerV2 is a multimodal language model fine-tuned for text-to-image prompt enhancement and rewriting. It restructures user input prompts while preserving the original intent, producing clearer, layered, and logically consistent prompts suitable for downstream image generation tasks.\nModel Details\nModel Description\nPromptEnhancerV2 is a specialized text-to-image prompt rewriting model that employs chain-of-thought reasoning to enhance user prompts.\nModel type: Vision-Language Model for Prompt Enhancement\nLanguage(s) (NLP): Chinese (zh), English (en)\nLicense: Apache-2.0\nFinetuned from model: Qwen/Qwen2.5-VL-32B-Instruct\nModel Sources\nRepository: https://github.com/ximinng/PromptEnhancer\nPaper: https://arxiv.org/abs/2509.04545\nHomepage: https://hunyuan-promptenhancer.github.io/\nHow to Get Started with the Model\n1. Clone the repository::\ngit clone https://github.com/ximinng/PromptEnhancer.git\ncd PromptEnhancer\npip install -r requirements.txt\n2. Model Download:\nhuggingface-cli download PromptEnhancer/PromptEnhancer-32B --local-dir ./models/promptenhancer-32b\n3. Use the model:\nfrom inference.prompt_enhancer_v2 import PromptEnhancerV2\n# Initialize the model\nmodels_root_path = \"./models/promptenhancer-32b\"\nenhancer = PromptEnhancerV2(models_root_path=models_root_path, device_map=\"auto\")\n# Enhance a prompt (Chinese or English)\nuser_prompt = \"Èü©Á≥ªÊèíÁîªÈ£éÂ•≥ÁîüÂ§¥ÂÉèÔºåÁ≤âÁ¥´Ëâ≤Áü≠Âèë+ÈÄèÊòéÊÑüËÖÆÁ∫¢Ôºå‰æßÂÖâÊ∏≤Êüì„ÄÇ\"\nenhanced_prompt = enhancer.predict(\nprompt_cot=user_prompt,\ndevice=\"cuda\"\n)\nprint(\"Enhanced:\", enhanced_prompt)\nEvaluation\nThe model is evaluated on the T2I-Keypoints-Eval dataset, which contains diverse text-to-image prompts across various categories and languages.\nCitation\nIf you find this model useful, please consider citing:\nBibTeX:\n@article{promptenhancer,\ntitle={PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting},\nauthor={Wang, Linqing and Xing, Ximing and Cheng, Yiji and Zhao, Zhiyuan and Donghao, Li and Tiankai, Hang and Zhenxi, Li and Tao, Jiale and Wang, QiXun and Li, Ruihuang and Chen, Comi and Li, Xin and Wu, Mingrui and Deng, Xinchi and Gu, Shuyang and Wang, Chunyu and Lu, Qinglin},\njournal={arXiv preprint arXiv:2509.04545},\nyear={2025}\n}",
    "dphn/Dolphin-X1-8B-GGUF": "üê¨ Dolphin X1 8B\nSponsors\nWhat is Dolphin X1 8B?\nChat Template\nSystem Prompt\nHow to use\nUse with vLLM\nEvals\nüê¨ Dolphin X1 8B\nWebsite: https://dphn.aiTwitter: https://x.com/dphnAI\nTalk to Dolphin for free in our Web UI & Telegram bot\nWeb Chat: https://chat.dphn.aiTelegram bot: https://t.me/DolphinAI_bot\nSponsors\nOur appreciation for the generous sponsors of Dolphin:\nDeepinfra - provided 8xB200s to train the model.\nLium - provided on-demand 8x H200s for testing and evaluation.\nAndreessen Horowitz - provided a grant that make Dolphin 1.0 possible and enabled me to bootstrap my homelab\nWhat is Dolphin X1 8B?\nDolphin X1 8B is a result of our effort to directly uncensor Llama's 3.1 8B instruct while also keeping the same abilities or improving on them with finetuning.\nDolphin aims to be a general purpose model, similar to the models behind ChatGPT, Claude, Gemini.  But these models present problems for businesses seeking to include AI in their products.\nThey maintain control of the system prompt, deprecating and changing things as they wish, often causing software to break.\nThey maintain control of the model versions, sometimes changing things silently, or deprecating older models that your business relies on.\nThey maintain control of the alignment, and in particular the alignment is one-size-fits all, not tailored to the application.\nThey can see all your queries and they can potentially use that data in ways you wouldn't want.\nDolphin, in contrast, is steerable and gives control to the system owner. You set the system prompt.  You decide the alignment.  You have control of your data.  Dolphin does not impose its ethics or guidelines on you.  You are the one who decides the guidelines.\nDolphin belongs to YOU, it is your tool, an extension of your will.\nJust as you are personally responsible for what you do with a knife, gun, fire, car, or the internet, you are the creator and originator of any content you generate with Dolphin.\nhttps://erichartford.com/uncensored-models\nChat Template\nWe maintained the default Llama-3 chat template for this model. A typical input would look like this\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\nsystem-prompt<|eot_id|><|start_header_id|>user<|end_header_id|>\nuser-prompt<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nassistant-prompt<|eot_id|>\nSystem Prompt\nIn Dolphin, the system prompt is what you use to set the tone and alignment of the responses. You can set a character, a mood, rules for its behavior, and it will try its best to follow them.\nMake sure to set the system prompt in order to set the tone and guidelines for the responses - Otherwise, it will act in a default way that might not be what you want.\nHow to use\nThere are many ways to use a huggingface model including:\nollama\nLM Studio\nHuggingface Transformers library\nvllm\nsglang\ntgi\nUse with vLLM\nThis model can be hosted using the vLLM engine, using the commands shown below:\nuv pip install vllm\nvllm serve dphn/Dolphin-X1-8B\nSee the documentation for more information.\nEvals\nMMLU = 0.626900MMLU_PROFESSIONAL = 0.610200MMLU_COLLEGE = 0.529400MMLU_HIGH_SCHOOL = 0.691600MMLU_OTHER = 0.663700IFEVAL = 0.608100Dolphin-refusals = 95.96% pass rate on 4.5k commonly refused prompts",
    "XiaomiMiMo/MiMo-Audio-7B-Instruct": "Introduction\nArchitecture\nMiMo-Audio-Tokenizer\nMiMo-Audio\nExplore MiMo-Audio Now! üöÄüöÄüöÄ\nModel Download\nGetting Started\nPrerequisites (Linux)\nInstallation\nRun the demo\nInference Scripts\nBase Model\nInstruct Model\nEvaluation Toolkit\nCitation\nContact\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\nMiMo Audio: Audio Language Models are Few-Shot Learners\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n|\nü§ñ GitHub\n|\nüìÑ Paper\n|\nüì∞ Blog\n|\nüî• Online Demo\n|\nüìä MiMo-Audio-Eval\n|\nIntroduction\nExisting audio language models typically rely on task-specific fine-tuning to accomplish particular audio tasks. In contrast, humans are able to generalize to new audio tasks with only a few examples or simple instructions. GPT-3 has shown that scaling next-token prediction pretraining enables strong generalization capabilities in text, and we believe this paradigm is equally applicable to the audio domain. By scaling MiMo-Audio's pretraining data to over one hundred million of hours, we observe the emergence of few-shot learning capabilities across a diverse set of audio tasks. We develop a systematic evaluation of these capabilities and find that MiMo-Audio-7B-Base achieves SOTA performance on both speech intelligence and audio understanding benchmarks among open-source models. Beyond standard metrics, MiMo-Audio-7B-Base generalizes to tasks absent from its training data, such as voice conversion, style transfer, and speech editing. MiMo-Audio-7B-Base also demonstrates powerful speech continuation capabilities, capable of generating highly realistic talk shows, recitations, livestreaming and debates. At the post-training stage, we curate a diverse instruction-tuning corpus and introduce thinking mechanisms into both audio understanding and generation. MiMo-Audio-7B-Instruct achieves open-source SOTA on audio understanding benchmarks, spoken dialogue benchmarks and instruct-TTS evaluations, approaching or surpassing closed-source models.\nArchitecture\nMiMo-Audio-Tokenizer\nMiMo-Audio-Tokenizer is a 1.2B-parameter Transformer operating at 25 Hz. It employs an eight-layer RVQ stack to generate 200 tokens per second. By jointly optimizing semantic and reconstruction objectives, we train MiMo-Audio-Tokenizer from scratch on a 10-million-hour corpus, achieving superior reconstruction quality and facilitating downstream language modeling.\nMiMo-Audio couples a patch encoder, an LLM, and a patch decoder to improve modeling efficiency for high-rate sequences and bridge the length mismatch between speech and text. The patch encoder aggregates four consecutive time steps of RVQ tokens into a single patch, downsampling the sequence to a 6.25 Hz representation for the LLM. The patch decoder autoregressively generates the full 25 Hz RVQ token sequence via a delayed-generation scheme.\nMiMo-Audio\nExplore MiMo-Audio Now! üöÄüöÄüöÄ\nüéß Try the Hugging Face demo: MiMo-Audio Demo\nüì∞ Read the Official Blog: MiMo-Audio Blog\nüìÑ Dive into the Technical Report: MiMo-Audio Technical Report\nModel Download\nModels\nü§ó Hugging Face\nMiMo-Audio-Tokenizer\nXiaomiMiMo/MiMo-Audio-Tokenizer\nMiMo-Audio-7B-Base\nXiaomiMiMo/MiMo-Audio-7B-Base\nMiMo-Audio-7B-Instruct\nXiaomiMiMo/MiMo-Audio-7B-Instruct\npip install huggingface-hub\nhf download XiaomiMiMo/MiMo-Audio-Tokenizer --local-dir ./models/MiMo-Audio-Tokenizer\nhf download XiaomiMiMo/MiMo-Audio-7B-Base --local-dir ./models/MiMo-Audio-7B-Base\nhf download XiaomiMiMo/MiMo-Audio-7B-Instruct --local-dir ./models/MiMo-Audio-7B-Instruct\nGetting Started\nSpin up the MiMo-Audio demo in minutes with the built-in Gradio app.\nPrerequisites (Linux)\nPython 3.12\nCUDA >= 12.0\nInstallation\ngit clone https://github.com/XiaomiMiMo/MiMo-Audio.git\ncd MiMo-Audio\npip install -r requirements.txt\npip install flash-attn==2.7.4.post1\nIf the compilation of flash-attn takes too long, you can download the precompiled wheel and install it manually:\nDownload Precompiled Wheel\npip install /path/to/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl\nRun the demo\npython run_mimo_audio.py\nThis launches a local Gradio interface where you can try MiMo-Audio interactively.\nEnter the local paths for MiMo-Audio-Tokenizer and MiMo-Audio-7B-Instruct, then enjoy the full functionality of MiMo-Audio!\nInference Scripts\nBase Model\nWe provide an example script to explore the in-context learning capabilities of MiMo-Audio-7B-Base.See: inference_example_pretrain.py\nInstruct Model\nTo try the instruction-tuned model MiMo-Audio-7B-Instruct, use the corresponding inference script.See: inference_example_sft.py\nEvaluation Toolkit\nFull evaluation suite are available at üåêMiMo-Audio-Eval.\nThis toolkit is designed to evaluate MiMo-Audio and other recent audio LLMs as mentioned in the paper. It provides a flexible and extensible framework, supporting a wide range of datasets, tasks, and models.\nCitation\n@misc{coreteam2025mimoaudio,\ntitle={MiMo-Audio: Audio Language Models are Few-Shot Learners},\nauthor={LLM-Core-Team Xiaomi},\nyear={2025},\nurl={GitHub - XiaomiMiMo/MiMo-Audio},\n}\nContact\nPlease contact us at mimo@xiaomi.com or open an issue if you have any questions.",
    "QuantStack/Wan2.2-Animate-14B-GGUF": "This GGUF file is a direct conversion of Wan-AI/Wan2.2-Animate-14B\nExample workflow (Thanks to verolelb on discord for publishing this workflow ‚ù§)\nType\nName\nLocation\nDownload\nMain Model\nWan2.2-Animate-14B\nComfyUI/models/unet\nGGUF (this repo)\nText Encoder\nUmt5-xxl\nComfyUI/models/text_encoders\nSafetensors / GGUF\nVAE\nWan2.1_VAE.safetensors\nComfyUI/models/vae\nSafetensors\nSince this is a quantized model, all original licensing terms and usage restrictions remain in effect.\nUsage\nThe model can be used with the ComfyUI custom node ComfyUI-GGUF by city96",
    "NotEvilAI/gpt-oss-20b-ru-reasoner": "NotEvilAI/gpt-oss-20b-ru-reasoner\n–ü—Ä–µ–¥–ø–æ—Å—ã–ª–∫–∏\n–û–±—É—á–µ–Ω–∏–µ\n–ë–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\nNotEvilAI/gpt-oss-20b-ru-reasoner\nNotEvilAI/gpt-oss-20b-ru-reasoner - —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–º —Ä–∏–∑–æ–Ω–∏–Ω–≥–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ openai/gpt-oss-20b.\n–ú–æ–¥–µ–ª—å –¥—É–º–∞–µ—Ç –Ω–∞ —Ç–æ–º —è–∑—ã–∫–µ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º —Ç—Ä–µ–±—É–µ—Ç—Å—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç(–ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ —Ä—É—Å—Å–∫–æ–º) –±–µ–∑ —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è —è–∑—ã–∫–∞ —Ä–∏–∑–æ–Ω–∏–Ω–≥–∞.\n–ò–º–µ–µ—Ç—Å—è 5 —Ä–µ–∂–∏–º–æ–≤ —Ä–∏–∑–æ–Ω–∏–Ω–≥–∞(reasoning_effort):\nlow, medium, high - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ, —Å—Ä–µ–¥–Ω–µ–≥–æ –∏ –±–æ–ª—å—à–æ–≥–æ —Ä–∏–∑–æ–Ω–∏–Ω–≥–∞ –¥–ª—è gpt-oss-20b/gpt-oss-120b\nnone - –æ—Ç–∫–ª—é—á–∏—Ç—å —Ä–∏–∑–æ–Ω–∏–Ω–≥, –≤ thinking –±—É–¥–µ—Ç –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞\nauto - \"–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π\" —Ä–∞–∑–º–µ—Ä —Ä–∏–∑–æ–Ω–∏–Ω–≥–∞\n–ü—Ä–µ–¥–ø–æ—Å—ã–ª–∫–∏\ngpt-oss-20b –∏ gpt-oss-120b –ø–æ-—É–º–æ–ª—á–∞–Ω–∏—é –≤—Å–µ–≥–¥–∞ –¥—É–º–∞—é—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ.\n–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π Cookbook OpenAI –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —Å–¥–µ–ª–∞—Ç—å —Ñ–∞–π–Ω-—Ç—é–Ω gpt-oss-20b –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ HuggingFaceH4/Multilingual-Thinking(—Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 1–∫ –ø—Ä–∏–º–µ—Ä–æ–≤, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –ø—É—Ç—ë–º –ø–µ—Ä–µ–≤–æ–¥–∞ prompt-reasoning-answer –Ω–∞ 4 —è–∑—ã–∫–∞ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ).\n–≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∑–∞–¥–∞—Ç—å 'Reasoning language' –≤ —Å–∏—Å—Ç–µ–º–Ω–æ–º –ø—Ä–æ–º—Ç–µ –∏ –∑–∞—Å—Ç–∞–≤–∏—Ç—å –º–æ–¥–µ–ª—å –¥—É–º—Ç–∞—å –Ω–∞ —Ç—Ä–µ–±—É–µ–º–æ–º —è–∑—ã–∫–µ, —á—Ç–æ –≤ —Ç–µ–æ—Ä–∏–∏ –¥–æ–ª–∂–Ω–æ –ø–æ–≤—ã—Å–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤.\n–í–æ –≤—Ä–µ–º—è —Ñ–∞–π–Ω—Ç—é–Ω–∞ –º–æ–¥–µ–ª—å –≤—ã—è–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ —É—á–∏—Ç—Å—è –¥—É–º–∞—Ç—å –Ω–∞ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º–æ–º —è–∑—ã–∫–µ.\n–ü—Ä–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –º—ã –ø–æ—Å—Ç–∞–≤–∏–ª–∏ —Ü–µ–ª—å –∏—Å–∫–ª—é—á–∏—Ç—å —è–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ —è–∑—ã–∫–∞ —Ä–∏–∑–æ–Ω–∏–Ω–≥–∞, –∞ —Ç–∞–∫–∂–µ –¥–æ–±–∞–≤–∏—Ç—å –¥–≤–∞ –Ω–æ–≤—ã—Ö —Ä–µ–∂–∏–º–∞ —Ä–∏–∑–æ–Ω–∏–Ω–≥–∞: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π(auto) –∏ –±–µ–∑ —Ä–∏–∑–æ–Ω–∏–Ω–≥–∞(none).\n–û–±—É—á–µ–Ω–∏–µ\n–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –±—ã–ª —Å–æ—Å—Ç–∞–≤–ª–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç NotEvilAI/ru-reasoning_effort-sft_dpo_think_gpt.\n–ú–æ–¥–µ–ª—å –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ —Å 8x H200 –≤ 2 —Å—Ç–∞–¥–∏–∏:\nFull fine-tuning SFT —Å –ø–æ–º–æ—â—å—é axolotl:\nnum_epochs: 5 (20b –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ —Å—Ö–æ–¥–∏—Ç—Å—è –º–µ–¥–ª–µ–Ω–Ω–µ–µ, —á–µ–º 120b, –æ–¥–Ω–∞–∫–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∑–∞–º–µ—á–µ–Ω–æ –Ω–µ –±—ã–ª–æ)\nlearning_rate: 5e-5 –ø–æ–¥–æ–±—Ä–∞–Ω —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏\noptimizer: adamw_torch_fused\n–£–ø–∞–∫–æ–≤–∫–∞ —Å–µ–º–ø–ª–æ–≤ —á–µ—Ä–µ–∑ sample_packing, multipack_real_batches, pad_to_sequence_len, group_by_length\n–û–±—É—á–µ–Ω–∏–µ –¥–ª–∏–ª–æ—Å—å ~ 5 —á–∞—Å–æ–≤\nDPO —Å –ø–æ–º–æ—â—å—é transformers:\n–°–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ 25 —Å–µ–º–ø–ª–æ–≤ –Ω–∞ –ø—Ä–æ–º—Ç —Å —Ü–µ–ª—å—é –ø–æ–∏—Å–∫–∞ —Ä–∏–∑–æ–Ω–∏–Ω–≥–∞ –Ω–µ –Ω–∞ –Ω—É–∂–Ω–æ–º —è–∑—ã–∫–µ\nlearning_rate: 5e-6\ngradient_accumulation_steps: 4\n–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏—Ä—É—é—â–µ–π –º–æ–¥–µ–ª–∏ –∏–∑ fp32 –≤ bf16\n–û–±—É—á–µ–Ω–∏–µ –¥–ª–∏–ª–æ—Å—å ~ 3.5 —á–∞—Å–∞\n–ë–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n–ü–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å –Ω–∞ –Ω–∞—à Telegram-–∫–∞–Ω–∞–ª. –¢–∞–º –º—ã –±—É–¥–µ–º –≤—ã–∫–ª–∞–¥—ã–≤–∞—Ç—å –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –¥–∞—Ç–∞—Å–µ—Ç—ã. –¢–∞–∫–∂–µ —Ç–∞–º –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–¥–∞—Ç—å –∞–≤—Ç–æ—Ä—É –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏–µ –≤–∞—Å –≤–æ–ø—Ä–æ—Å—ã.",
    "Qwen/Qwen3-Next-80B-A3B-Instruct-FP8": "Qwen3-Next-80B-A3B-Instruct-FP8\nHighlights\nModel Overview\nPerformance\nDeployment\nSGLang\nvLLM\nAgentic Use\nProcessing Ultra-Long Texts\nBest Practices\nCitation\nQwen3-Next-80B-A3B-Instruct-FP8\nOver the past few months, we have observed increasingly clear trends toward scaling both total parameters and context lengths in the pursuit of more powerful and agentic artificial intelligence (AI).\nWe are excited to share our latest advancements in addressing these demands, centered on improving scaling efficiency through innovative model architecture.\nWe call this next-generation foundation models Qwen3-Next.\nThis repository contains the FP8-quantized Qwen3-Next-80B-A3B-Instruct model checkpoint for convenience and performance.\nThe quantization method is \"fine-grained fp8\" quantization with block size of 128.\nYou can find more details in the quantization_config field in config.json.\nIn addition, the experimental results presented in this model card are obtained from the original bfloat16 model prior to FP8 quantization.\nHighlights\nQwen3-Next-80B-A3B-FP8 is the first installment in the Qwen3-Next series and features the following key enchancements:\nHybrid Attention: Replaces standard attention with the combination of Gated DeltaNet and Gated Attention, enabling efficient context modeling for ultra-long context length.\nHigh-Sparsity Mixture-of-Experts (MoE): Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity.\nStability Optimizations: Includes techniques such as zero-centered and weight-decayed layernorm, and other stabilizing enhancements for robust pre-training and post-training.\nMulti-Token Prediction (MTP): Boosts pretraining model performance and accelerates inference.\nWe are seeing strong performance in terms of both parameter efficiency and inference speed for Qwen3-Next-80B-A3B:\nQwen3-Next-80B-A3B-Base outperforms Qwen3-32B-Base on downstream tasks with 10% of the total training cost and with 10 times inference throughput for context over 32K tokens.\nQwen3-Next-80B-A3B-Instruct performs on par with Qwen3-235B-A22B-Instruct-2507 on certain benchmarks, while demonstrating significant advantages in handling ultra-long-context tasks up to 256K tokens.\nFor more details, please refer to our blog post Qwen3-Next.\nModel Overview\nQwen3-Next-80B-A3B-Instruct-FP8 supports only instruct (non-thinking) mode and does not generate <think></think> blocks in its output.\nThis repo contains the FP8 version of Qwen3-Next-80B-A3B-Instruct, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining (15T tokens) & Post-training\nNumber of Parameters: 80B in total and 3B activated\nNumber of Paramaters (Non-Embedding): 79B\nHidden Dimension: 2048\nNumber of Layers: 48\nHybrid Layout: 12 * (3 * (Gated DeltaNet -> MoE) -> 1 * (Gated Attention -> MoE))\nGated Attention:\nNumber of Attention Heads: 16 for Q and 2 for KV\nHead Dimension: 256\nRotary Position Embedding Dimension: 64\nGated DeltaNet:\nNumber of Linear Attention Heads: 32 for V and 16 for QK\nHead Dimension: 128\nMixture of Experts:\nNumber of Experts: 512\nNumber of Activated Experts: 10\nNumber of Shared Experts: 1\nExpert Intermediate Dimension: 512\nContext Length: 262,144 natively and extensible up to 1,010,000 tokens\nPerformance\nQwen3-30B-A3B-Instruct-2507\nQwen3-32B Non-Thinking\nQwen3-235B-A22B-Instruct-2507\nQwen3-Next-80B-A3B-Instruct\nKnowledge\nMMLU-Pro\n78.4\n71.9\n83.0\n80.6\nMMLU-Redux\n89.3\n85.7\n93.1\n90.9\nGPQA\n70.4\n54.6\n77.5\n72.9\nSuperGPQA\n53.4\n43.2\n62.6\n58.8\nReasoning\nAIME25\n61.3\n20.2\n70.3\n69.5\nHMMT25\n43.0\n9.8\n55.4\n54.1\nLiveBench 20241125\n69.0\n59.8\n75.4\n75.8\nCoding\nLiveCodeBench v6 (25.02-25.05)\n43.2\n29.1\n51.8\n56.6\nMultiPL-E\n83.8\n76.9\n87.9\n87.8\nAider-Polyglot\n35.6\n40.0\n57.3\n49.8\nAlignment\nIFEval\n84.7\n83.2\n88.7\n87.6\nArena-Hard v2*\n69.0\n34.1\n79.2\n82.7\nCreative Writing v3\n86.0\n78.3\n87.5\n85.3\nWritingBench\n85.5\n75.4\n85.2\n87.3\nAgent\nBFCL-v3\n65.1\n63.0\n70.9\n70.3\nTAU1-Retail\n59.1\n40.1\n71.3\n60.9\nTAU1-Airline\n40.0\n17.0\n44.0\n44.0\nTAU2-Retail\n57.0\n48.8\n74.6\n57.3\nTAU2-Airline\n38.0\n24.0\n50.0\n45.5\nTAU2-Telecom\n12.3\n24.6\n32.5\n13.2\nMultilingualism\nMultiIF\n67.9\n70.7\n77.5\n75.8\nMMLU-ProX\n72.0\n69.3\n79.4\n76.7\nINCLUDE\n71.9\n70.9\n79.5\n78.9\nPolyMATH\n43.1\n22.5\n50.2\n45.9\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\nDeployment\nYou can use Qwen3-Next-80B-A3B-Instruct-FP8 with serveral inference frameworks, including sglang, and vllm, as the original bfloat16 model.\nThe following guide demonstrates how to serve Qwen3-Next-80B-A3B-Instruct-FP8 via an OpenAI-compatible API endpoint using the latest sglang or vllm.\nSGLang\nSGLang is a fast serving framework for large language models and vision language models.\nSGLang could be used to launch a server with OpenAI-compatible API service.\nThe latest main of sglang is required for Qwen3-Next-FP8, which can be installed using:\npip install 'sglang[all] @ git+https://github.com/sgl-project/sglang.git@main'\nSee its documentation for more details.\nThe following command can be used to create an API endpoint at http://localhost:30000/v1 with maximum context length 256K tokens using tensor parallel on 4 GPUs.\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct-FP8 --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8\nThe following command is recommended for MTP with the rest settings the same as above:\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct-FP8 --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8 --speculative-algo NEXTN --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4\nThe default context length is 256K. Consider reducing the context length to a smaller value, e.g., 32768, if the server fails to start.\nPlease also refer to SGLang's usage guide on Qwen3-Next.\nvLLM\nvLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\nvLLM could be used to launch a server with OpenAI-compatible API service.\nThe latest main of vllm is required for Qwen3-Next-FP8, which can be installed using:\npip install vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\nSee its documentation for more details.\nThe following command can be used to create an API endpoint at http://localhost:8000/v1 with maximum context length 256K tokens using tensor parallel on 4 GPUs.\nvllm serve Qwen/Qwen3-Next-80B-A3B-Instruct-FP8 --port 8000 --tensor-parallel-size 4 --max-model-len 262144\nThe following command is recommended for MTP with the rest settings the same as above:\nvllm serve Qwen/Qwen3-Next-80B-A3B-Instruct-FP8 --port 8000 --tensor-parallel-size 4 --max-model-len 262144 --speculative-config '{\"method\":\"qwen3_next_mtp\",\"num_speculative_tokens\":2}'\nThe default context length is 256K. Consider reducing the context length to a smaller value, e.g., 32768, if the server fails to start.\nPlease also refer to vLLM's usage guide on Qwen3-Next.\nAgentic Use\nQwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\nfrom qwen_agent.agents import Assistant\n# Define LLM\nllm_cfg = {\n'model': 'Qwen3-Next-80B-A3B-Instruct-FP8',\n# Use a custom endpoint compatible with OpenAI API:\n'model_server': 'http://localhost:8000/v1',  # api_base\n'api_key': 'EMPTY',\n}\n# Define Tools\ntools = [\n{'mcpServers': {  # You can specify the MCP configuration file\n'time': {\n'command': 'uvx',\n'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n},\n\"fetch\": {\n\"command\": \"uvx\",\n\"args\": [\"mcp-server-fetch\"]\n}\n}\n},\n'code_interpreter',  # Built-in tools\n]\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\npass\nprint(responses)\nProcessing Ultra-Long Texts\nQwen3-Next natively supports context lengths of up to 262,144 tokens.\nFor conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively.\nWe have validated the model's performance on context lengths of up to 1 million tokens using the YaRN method.\nYaRN is currently supported by several inference frameworks, e.g., transformers, vllm and sglang.\nIn general, there are two approaches to enabling YaRN for supported frameworks:\nModifying the model files:\nIn the config.json file, add the rope_scaling fields:\n{\n...,\n\"rope_scaling\": {\n\"rope_type\": \"yarn\",\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 262144\n}\n}\nPassing command line arguments:\nFor vllm, you can use\nVLLM_ALLOW_LONG_MAX_MODEL_LEN=1 vllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}' --max-model-len 1010000\nFor sglang, you can use\nSGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 python -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":262144}}' --context-length 1010000\nAll the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nIt is also recommended to modify the factor as needed. For example, if the typical context length for your application is 524,288 tokens, it would be better to set factor as 2.0.\nLong-Context Performance\nWe test the model on an 1M version of the RULER benchmark.\nModel Name\nAcc avg\n4k\n8k\n16k\n32k\n64k\n96k\n128k\n192k\n256k\n384k\n512k\n640k\n768k\n896k\n1000k\nQwen3-30B-A3B-Instruct-2507\n86.8\n98.0\n96.7\n96.9\n97.2\n93.4\n91.0\n89.1\n89.8\n82.5\n83.6\n78.4\n79.7\n77.6\n75.7\n72.8\nQwen3-235B-A22B-Instruct-2507\n92.5\n98.5\n97.6\n96.9\n97.3\n95.8\n94.9\n93.9\n94.5\n91.0\n92.2\n90.9\n87.8\n84.8\n86.5\n84.5\nQwen3-Next-80B-A3B-Instruct\n91.8\n98.5\n99.0\n98.0\n98.7\n97.6\n95.0\n96.0\n94.0\n93.5\n91.7\n86.9\n85.5\n81.7\n80.3\n80.3\nQwen3-Next are evaluated with YaRN enabled. Qwen3-2507 models are evaluated with Dual Chunk Attention enabled.\nSince the evaluation is time-consuming, we use 260 samples for each length (13 sub-tasks, 20 samples for each).\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\nFor supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\nAdequate Output Length: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\nStandardize Output Format: We recommend using prompts to standardize model outputs when benchmarking.\nMath Problems: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\nMultiple-Choice Questions: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the answer field with only the choice letter, e.g., \"answer\": \"C\".\"\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{qwen2.5-1m,\ntitle={Qwen2.5-1M Technical Report},\nauthor={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\njournal={arXiv preprint arXiv:2501.15383},\nyear={2025}\n}",
    "LiquidAI/LFM2-2.6B": "LFM2-2.6B\nüìÑ Model details\nüèÉ How to run LFM2\n1. Transformers\n2. vLLM\n3. llama.cpp\nüîß How to fine-tune LFM2\nüìà Performance\nüì¨ Contact\nPlayground\nPlayground\nPlayground\nLeap\nLFM2-2.6B\nLFM2 is a new generation of hybrid models developed by Liquid AI, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.\nWe're releasing the weights of four post-trained checkpoints with 350M, 700M, 1.2B, and 2.6 parameters. They provide the following key features to create AI-powered edge applications:\nFast training & inference ‚Äì LFM2 achieves 3x faster training compared to its previous generation. It also benefits from 2x faster decode and prefill speed on CPU compared to Qwen3.\nBest performance ‚Äì LFM2 outperforms similarly-sized models across multiple benchmark categories, including knowledge, mathematics, instruction following, and multilingual capabilities.\nNew architecture ‚Äì LFM2 is a new hybrid Liquid model with multiplicative gates and short convolutions.\nFlexible deployment ‚Äì LFM2 runs efficiently on CPU, GPU, and NPU hardware for flexible deployment on smartphones, laptops, or vehicles.\nFind more information about LFM2 in our blog post.\nüìÑ Model details\nDue to their small size, we recommend fine-tuning LFM2 models on narrow use cases to maximize performance.\nThey are particularly suited for agentic tasks, data extraction, RAG, creative writing, and multi-turn conversations.\nHowever, we do not recommend using them for tasks that are knowledge-intensive or require programming skills.\nProperty\nLFM2-350M\nLFM2-700M\nLFM2-1.2B\nLFM2-2.6B\nParameters\n354,483,968\n742,489,344\n1,170,340,608\n2,569,272,320\nLayers\n16 (10 conv + 6 attn)\n16 (10 conv + 6 attn)\n16 (10 conv + 6 attn)\n30 (22 conv + 8 attn)\nContext length\n32,768 tokens\n32,768 tokens\n32,768 tokens\n32,768 tokens\nVocabulary size\n65,536\n65,536\n65,536\n65,536\nPrecision\nbfloat16\nbfloat16\nbfloat16\nbfloat16\nTraining budget\n10 trillion tokens\n10 trillion tokens\n10 trillion tokens\n10 trillion tokens\nLicense\nLFM Open License v1.0\nLFM Open License v1.0\nLFM Open License v1.0\nLFM Open License v1.0\nSupported languages: English, Arabic, Chinese, French, German, Japanese, Korean, and Spanish.\nGeneration parameters: We recommend the following parameters:\ntemperature=0.3\nmin_p=0.15\nrepetition_penalty=1.05\nReasoning: LFM2-2.6B is the only model in this family to use dynamic hybrid reasoning (traces between <think> and </think> tokens) for complex or multilingual prompts.\nChat template: LFM2 uses a ChatML-like chat template as follows:\n<|startoftext|><|im_start|>system\nYou are a helpful assistant trained by Liquid AI.<|im_end|>\n<|im_start|>user\nWhat is C. elegans?<|im_end|>\n<|im_start|>assistant\nIt's a tiny nematode that lives in temperate soil environments.<|im_end|>\nYou can automatically apply it using the dedicated .apply_chat_template() function from Hugging Face transformers.\nTool use: It consists of four main steps:\nFunction definition: LFM2 takes JSON function definitions as input (JSON objects between <|tool_list_start|> and <|tool_list_end|> special tokens), usually in the system prompt\nFunction call: LFM2 writes Pythonic function calls (a Python list between <|tool_call_start|> and <|tool_call_end|> special tokens), as the assistant answer.\nFunction execution: The function call is executed and the result is returned (string between <|tool_response_start|> and <|tool_response_end|> special tokens), as a \"tool\" role.\nFinal answer: LFM2 interprets the outcome of the function call to address the original user prompt in plain text.\nHere is a simple example of a conversation using tool use:\n<|startoftext|><|im_start|>system\nList of tools: <|tool_list_start|>[{\"name\": \"get_candidate_status\", \"description\": \"Retrieves the current status of a candidate in the recruitment process\", \"parameters\": {\"type\": \"object\", \"properties\": {\"candidate_id\": {\"type\": \"string\", \"description\": \"Unique identifier for the candidate\"}}, \"required\": [\"candidate_id\"]}}]<|tool_list_end|><|im_end|>\n<|im_start|>user\nWhat is the current status of candidate ID 12345?<|im_end|>\n<|im_start|>assistant\n<|tool_call_start|>[get_candidate_status(candidate_id=\"12345\")]<|tool_call_end|>Checking the current status of candidate ID 12345.<|im_end|>\n<|im_start|>tool\n<|tool_response_start|>[{\"candidate_id\": \"12345\", \"status\": \"Interview Scheduled\", \"position\": \"Clinical Research Associate\", \"date\": \"2023-11-20\"}]<|tool_response_end|><|im_end|>\n<|im_start|>assistant\nThe candidate with ID 12345 is currently in the \"Interview Scheduled\" stage for the position of Clinical Research Associate, with an interview date set for 2023-11-20.<|im_end|>\nYou can directly pass tools as JSON schema or Python functions with .apply_chat_template() as shown in this page to automatically format the system prompt.\nArchitecture: Hybrid model with multiplicative gates and short convolutions: 10 double-gated short-range LIV convolution blocks and 6 grouped query attention (GQA) blocks.\nPre-training mixture: Approximately 75% English, 20% multilingual, and 5% code data sourced from the web and licensed materials.\nTraining approach:\nVery large-scale SFT on 50% downstream tasks, 50% general domains\nCustom DPO with length normalization and semi-online datasets\nIterative model merging\nüèÉ How to run LFM2\n1. Transformers\nTo run LFM2, you need to install Hugging Face transformers v4.55 or a more recent version as follows:\npip install -U transformers\nHere is an example of how to generate an answer with transformers in Python:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n# Load model and tokenizer\nmodel_id = \"LiquidAI/LFM2-2.6B\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"auto\",\ntorch_dtype=\"bfloat16\",\n#    attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n# Generate answer\nprompt = \"What is C. elegans?\"\ninput_ids = tokenizer.apply_chat_template(\n[{\"role\": \"user\", \"content\": prompt}],\nadd_generation_prompt=True,\nreturn_tensors=\"pt\",\ntokenize=True,\n).to(model.device)\noutput = model.generate(\ninput_ids,\ndo_sample=True,\ntemperature=0.3,\nmin_p=0.15,\nrepetition_penalty=1.05,\nmax_new_tokens=512,\n)\nprint(tokenizer.decode(output[0], skip_special_tokens=False))\n# <|startoftext|><|im_start|>user\n# What is C. elegans?<|im_end|>\n# <|im_start|>assistant\n# C. elegans, also known as Caenorhabditis elegans, is a small, free-living\n# nematode worm (roundworm) that belongs to the phylum Nematoda.\nYou can directly run and test the model with this Colab notebook.\n2. vLLM\nYou need to install vLLM v0.10.2 or a more recent version as follows:\nuv pip install vllm==0.10.2 --extra-index-url https://wheels.vllm.ai/0.10.2/ --torch-backend=auto\nHere is an example of how to use it for inference:\nfrom vllm import LLM, SamplingParams\nprompts = [\n\"What is C. elegans?\",\n\"Say hi in JSON format\",\n\"Define AI in Spanish\"\n]\nsampling_params = SamplingParams(temperature=0.3, min_p=0.15, repetition_penalty=1.05)\nllm = LLM(model=\"LiquidAI/LFM2-2.6B\")\noutputs = llm.generate(prompts, sampling_params)\nfor output in outputs:\nprompt = output.prompt\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n3. llama.cpp\nYou can run LFM2 with llama.cpp using its GGUF checkpoint. Find more information in the model card.\nüîß How to fine-tune LFM2\nWe recommend fine-tuning LFM2 models on your use cases to maximize performance.\nNotebook\nDescription\nLink\nSFT (Unsloth)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using Unsloth.\nSFT (Axolotl)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using Axolotl.\nSFT (TRL)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using TRL.\nDPO (TRL)\nPreference alignment with Direct Preference Optimization (DPO) using TRL.\nüìà Performance\nLFM2 outperforms similar-sized models across different evaluation categories. We only report scores using instruct variants and non-thinking modes for consistency.\nModel\nMMLU\nGPQA\nIFEval\nIFBench\nGSM8K\nMGSM\nMMMLU\nLFM2-2.6B\n64.42\n26.57\n79.56\n22.19\n82.41\n74.32\n55.39\nLlama-3.2-3B-Instruct\n60.35\n30.6\n71.43\n20.78\n75.21\n61.68\n47.92\nSmolLM3-3B\n59.84\n26.31\n72.44\n17.93\n81.12\n68.72\n50.02\ngemma-3-4b-it\n58.35\n29.51\n76.85\n23.53\n89.92\n87.28\n50.14\nQwen3-4B-Instruct-2507\n72.25\n34.85\n85.62\n30.28\n68.46\n81.76\n60.67\nüì¨ Contact\nIf you are interested in custom solutions with edge deployment, please contact our sales team.",
    "vngrs-ai/Kumru-2B": "Model Information\nUse\nEvaluation Results\nTokenizer Efficiency\nCitation\nModel Information\nKumru-2B is the lightweight, open-source version of Kumru LLM, developed for Turkish from scratch by VNGRS.\nIt is pre-trained on a cleaned, deduplicated corpora of 500 GB for 300B tokens, and supervised fine-tuned on 1M examples.\nIt comes with a modern tokenizer developed for Turkish, supporting code, math and chat template.\nKumru has a native context length of 8,192 tokens by default.\nThis is the instruct fine-tuned version.\nPre-trained Base version is here\nTry the demo of 7B version here.\nUse\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"vngrs-ai/Kumru-2B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\ndef generate_response(query):\nmessages = [\n{'role': 'system', 'content': 'Adƒ±n Kumru. VNGRS tarafƒ±ndan T√ºrk√ße i√ßin sƒ±fƒ±rdan eƒüitilmi≈ü bir dil modelisin.'},\n{'role': 'user', 'content': query}\n]\nmodel_inputs = tokenizer.apply_chat_template(messages, return_tensors='pt', add_generation_prompt=True).to(model.device)\nmodel_outputs = model.generate(model_inputs, max_new_tokens=512, do_sample=True, top_p=0.9, temperature=0.7, repetition_penalty=1.1)\noutput_tokens = model_outputs[0].cpu().detach().numpy().tolist()\ngenerated_tokens = output_tokens[model_inputs[0].shape[0]:]\nresponse = tokenizer.decode(generated_tokens, skip_special_tokens=True)\nreturn response\nquery = \"Efes antik kentinin √∂nemi nedir?\"\nresponse = generate_response(query)\nprint(response)\nEvaluation Results\nBoth Kumru-7B and Kumru-2B are evaluated on Cetvel benchmark.\nWe observe that Kumru overall surpasses significantly larger models such as LLaMA-3.3‚Äì70B, Gemma-3‚Äì27B, Qwen-2‚Äì72B and Aya-32B. It excels at tasks related to the nuances of the Turkish language, such as grammatical error correction and text summarization.\nTokenizer Efficiency\nKumru tokenizer is a modern BPE tokenizer with a vocabulary size of 50,176, pre-tokenization regex and a chat template.\nOther open-source models spend between 38% to 98% more tokens than Kumru while still having larger vocabulary sizes.\nThis means Kumru can represent more texts in its context length and process faster and cheaper. Although the native context length of Kumru is 8,192, its effective context length can be considered between 1128 and 1618, compared to other multilingual models out there.\nThis shows the efficiency of having a native Turkish tokenizer in terms of representation power, speed and cost.\nCitation\n@misc{turker2025kumru,\ntitle={Kumru},\nauthor={Turker, Meliksah and Ari, Erdi and Han, Aydin},\nyear={2025},\nurl={https://huggingface.co/vngrs-ai/Kumru-2B}\n}",
    "knowledgator/gliner-pii-large-v1.0": "GLiNER-PII: Zero-shot PII model\nüß† What is GLiNER?\nKey Advantages\nHow GLiNER Works\nüêç Python Implementation\nInstallation\nQuick Start\nComprehensive PII Detection\nAdvanced Usage Examples\nüí° Use Cases\nüéØ Primary Applications\nüåü Extended Applications\nüìä Performance Benchmarks\nAccuracy Evaluation\nKey Takeaways\nComparison with Alternatives\nüöÄ Alternative Implementations\nü¶Ä Rust Implementation (gline-rs)\n‚ö° C++ Implementation (GLiNER.cpp)\nüåê JavaScript Implementation (GLiNER.js)\nüèóÔ∏è Model Architecture & Training\nQuantization-Aware Pretraining\nAvailable ONNX Formats\nModel Conversion\nüìÑ References\nüôè Acknowledgments\nüìû Support\nGLiNER-PII: Zero-shot PII model\nA production-grade open-source model for privacy-focused PII, PHI, and PCI detection with zero-shot entity recognition capabilities.\nThis model was developed in collaboration between Wordcab and Knowledgator. For enterprise-ready, specialized PII/PHI/PCI models, contact us at info@wordcab.com.\nüß† What is GLiNER?\nGLiNER (Generalist and Lightweight Named Entity Recognition) is a bidirectional transformer model that can identify any entity type without predefined categories. Unlike traditional NER models that are limited to specific entity classes, GLiNER allows you to specify exactly what entities you want to extract at runtime.\nKey Advantages\nZero-shot recognition: Extract any entity type without retraining\nPrivacy-first: Process sensitive data locally without API calls\nLightweight: Much faster than large language models for NER tasks\nProduction-ready: Quantization-aware training with FP16 and UINT8 ONNX models\nComprehensive: 60+ predefined PII categories with custom entity support\nHow GLiNER Works\nInstead of predicting from a fixed set of entity classes, GLiNER takes both text and a list of desired entity types as input, then identifies spans that match those categories:\ntext = \"John Smith called from 415-555-1234 to discuss his account.\"\nentities = [\"name\", \"phone number\", \"account number\"]\n# GLiNER finds: \"John Smith\" ‚Üí name, \"415-555-1234\" ‚Üí phone number\nüêç Python Implementation\nThe primary GLiNER implementation provides comprehensive PII detection with 60+ entity categories, fine-tuned specifically for privacy and compliance use cases.\nInstallation\npip install gliner\nQuick Start\nfrom gliner import GLiNER\n# Load the model (downloads automatically on first use)\nmodel = GLiNER.from_pretrained(\"knowledgator/gliner-pii-base-v1.0\")\ntext = \"John Smith called from 415-555-1234 to discuss his account number 12345678.\"\nlabels = [\"name\", \"phone number\", \"account number\"]\nentities = model.predict_entities(text, labels, threshold=0.3)\nfor entity in entities:\nprint(f\"{entity['text']} => {entity['label']} (confidence: {entity['score']:.2f})\")\nOutput:\nJohn Smith => name (confidence: 0.95)\n415-555-1234 => phone number (confidence: 0.92)\n12345678 => account number (confidence: 0.88)\nComprehensive PII Detection\nThe model was specifically optimized for 60+ predefined PII categories organized by domain, but it can work in zero-shot as well, meaning you can put any labels you need:\nPersonal Identifiers\npersonal_labels = [\n\"name\",                       # Full names\n\"first name\",                 # First names\n\"last name\",                  # Last names\n\"name medical professional\",  # Healthcare provider names\n\"dob\",                        # Date of birth\n\"age\",                        # Age information\n\"gender\",                     # Gender identifiers\n\"marital status\"              # Marital status\n]\nContact Information\ncontact_labels = [\n\"email address\",          # Email addresses\n\"phone number\",           # Phone numbers\n\"ip address\",             # IP addresses\n\"url\",                    # URLs\n\"location address\",       # Street addresses\n\"location street\",        # Street names\n\"location city\",          # City names\n\"location state\",         # State/province names\n\"location country\",       # Country names\n\"location zip\"            # ZIP/postal codes\n]\nFinancial Information\nfinancial_labels = [\n\"account number\",         # Account numbers\n\"bank account\",           # Bank account numbers\n\"routing number\",         # Routing numbers\n\"credit card\",            # Credit card numbers\n\"credit card expiration\", # Card expiration dates\n\"cvv\",                    # CVV/security codes\n\"ssn\",                    # Social Security Numbers\n\"money\"                   # Monetary amounts\n]\nHealthcare Information\nhealthcare_labels = [\n\"condition\",                    # Medical conditions\n\"medical process\",              # Medical procedures\n\"drug\",                         # Drugs\n\"dose\",                         # Dosage information\n\"blood type\",                   # Blood types\n\"injury\",                       # Injuries\n\"organization medical facility\",# Healthcare facility names\n\"healthcare number\",            # Healthcare numbers\n\"medical code\"                  # Medical codes\n]\nIdentification Documents\nid_labels = [\n\"passport number\",       # Passport numbers\n\"driver license\",        # Driver's license numbers\n\"username\",              # Usernames\n\"password\",              # Passwords\n\"vehicle id\"             # Vehicle IDs\n]\nAdvanced Usage Examples\nMulti-Category Detection\ntext = \"\"\"\nPatient Mary Johnson, DOB 01/15/1980, was discharged on March 10, 2024\nfrom St. Mary's Hospital. Contact: mary.j@email.com, (555) 123-4567.\nInsurance policy: POL-789456123.\n\"\"\"\nlabels = [\n\"name\", \"dob\", \"discharge date\", \"organization medical facility\",\n\"email address\", \"phone number\", \"policy number\"\n]\nentities = model.predict_entities(text, labels, threshold=0.3)\nfor entity in entities:\nprint(f\"Found '{entity['text']}' as {entity['label']}\")\nBatch Processing for High Throughput\ndocuments = [\n\"Customer John called about his credit card ending in 4532.\",\n\"Sarah's SSN 123-45-6789 needs verification.\",\n\"Email support@company.com for account 987654321 issues.\"\n]\nlabels = [\"name\", \"credit card\", \"ssn\", \"email address\", \"account number\"]\n# Process multiple documents efficiently\nresults = model.run(documents, labels, threshold=0.3, batch_size=8)\nfor doc_idx, entities in enumerate(results):\nprint(f\"\\nDocument {doc_idx + 1}:\")\nfor entity in entities:\nprint(f\"  {entity['text']} => {entity['label']}\")\nCustom Entity Detection\n# GLiNER isn't limited to PII - you can detect any entities\ntext = \"The MacBook Pro with M2 chip costs $1,999 at the Apple Store in Manhattan.\"\ncustom_labels = [\"product\", \"processor\", \"price\", \"store\", \"location\"]\nentities = model.predict_entities(text, custom_labels, threshold=0.3)\nThreshold Optimization\n# Lower threshold: Higher recall, more false positives\nhigh_recall = model.predict_entities(text, labels, threshold=0.2)\n# Higher threshold: Higher precision, fewer false positives\nhigh_precision = model.predict_entities(text, labels, threshold=0.6)\n# Recommended starting point for production\nbalanced = model.predict_entities(text, labels, threshold=0.3)\nüí° Use Cases\nGLiNER excels in privacy-focused applications where traditional cloud-based NER services pose compliance risks.\nüéØ Primary Applications\nPrivacy-First Voice & Transcription\n# Automatically redact PII from voice transcriptions\ntranscription = \"Hi, my name is Sarah Johnson and my phone number is 415-555-0123\"\npii_labels = [\"name\", \"phone number\", \"email address\", \"ssn\"]\nentities = model.predict_entities(transcription, pii_labels)\n# Redact or anonymize detected PII before storage\nCompliance-Ready Document Processing\n# Healthcare: HIPAA-compliant note processing\nmedical_note = \"Patient John Doe, MRN 123456, diagnosed with diabetes...\"\nphi_labels = [\"name\", \"medical record number\", \"condition\", \"dob\"]\n# Finance: PCI-DSS compliant transaction logs\ntransaction_log = \"Card ****4532 charged $299.99 to John Smith\"\npci_labels = [\"credit card\", \"money\", \"name\"]\n# Legal: Attorney-client privilege protection\nlegal_doc = \"Client Jane Doe vs. Corporation ABC, case #2024-CV-001\"\nlegal_labels = [\"name\", \"organization\", \"case number\"]\nReal-Time Data Anonymization\ndef anonymize_text(text, entity_types):\n\"\"\"Anonymize PII in real-time\"\"\"\nentities = model.predict_entities(text, entity_types)\n# Sort by position to replace from end to start\nentities.sort(key=lambda x: x['start'], reverse=True)\nanonymized = text\nfor entity in entities:\nplaceholder = f\"<{entity['label'].upper()}>\"\nanonymized = anonymized[:entity['start']] + placeholder + anonymized[entity['end']:]\nreturn anonymized\noriginal = \"John Smith's SSN is 123-45-6789\"\nanonymized = anonymize_text(original, [\"name\", \"ssn\"])\nprint(anonymized)  # \"<NAME>'s SSN is <SSN>\"\nüåü Extended Applications\nEnhanced Search & Content Understanding\n# Extract key entities from user queries for better search\nquery = \"Find restaurants near Stanford University in Palo Alto\"\nsearch_entities = [\"organization\", \"location city\", \"business type\"]\n# Intelligent document tagging\ndocument = \"This quarterly report discusses Microsoft's Azure growth...\"\ndoc_entities = [\"organization\", \"product\", \"time period\"]\nGDPR-Compliant Chatbot Logs\ndef sanitize_chat_log(message):\n\"\"\"Remove PII from chat logs per GDPR requirements\"\"\"\nsensitive_types = [\n\"name\", \"email address\", \"phone number\", \"location address\",\n\"credit card\", \"ssn\", \"passport number\"\n]\nentities = model.predict_entities(message, sensitive_types)\nif entities:\n# Log anonymized version, alert compliance team\nreturn anonymize_text(message, sensitive_types)\nreturn message\nSecure Mobile & Edge Processing\n# Process sensitive data entirely on-device\ndef process_locally(user_input):\n\"\"\"Process PII detection without cloud APIs\"\"\"\npii_types = [\"name\", \"phone number\", \"email address\", \"ssn\", \"credit card\"]\n# All processing happens locally - no data leaves device\ndetected_pii = model.predict_entities(user_input, pii_types)\nif detected_pii:\nreturn \"‚ö†Ô∏è Sensitive information detected - proceed with caution\"\nreturn \"‚úÖ No PII detected - safe to share\"\nüìä Performance Benchmarks\nAccuracy Evaluation\nThe following benchmarks were run on the synthetic-multi-pii-ner-v1 dataset.\nWe compare multiple GLiNER-based PII models, including our new Knowledgator GLiNER PII Edge v1.0.\nModel Path\nPrecision\nRecall\nF1 Score\nknowledgator/gliner-pii-edge-v1.0\n78.96%\n72.34%\n75.50%\nknowledgator/gliner-pii-small-v1.0\n78.99%\n74.80%\n76.84%\nknowledgator/gliner-pii-base-v1.0\n79.28%\n82.78%\n80.99%\nknowledgator/gliner-pii-large-v1.0\n87.42%\n79.4%\n83.25%\nurchade/gliner_multi_pii-v1\n79.19%\n74.67%\n76.86%\nE3-JSI/gliner-multi-pii-domains-v1\n78.35%\n74.46%\n76.36%\ngravitee-io/gliner-pii-detection\n81.27%\n56.76%\n66.84%\nKey Takeaways\nBase Post Model (knowledgator/gliner-pii-base-v1.0) achieves the highest F1 score (80.99%), indicating the strongest overall performance.\nKnowledgator Edge Model (knowledgator/gliner-pii-edge-v1.0) is optimized for edge environments, trading a slight decrease in recall for lower latency and footprint.\nGravitee-io Model shows strong precision but lower recall, indicating it is tuned for high confidence but misses more entities.\nComparison with Alternatives\nSolution\nSpeed\nPrivacy\nAccuracy\nFlexibility\nCost\nGLiNER\n‚≠ê‚≠ê‚≠ê‚≠ê\n‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n‚≠ê‚≠ê‚≠ê‚≠ê\nFree\nCloud NER APIs\n‚≠ê‚≠ê‚≠ê\n‚≠ê‚≠ê‚≠ê\n‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n‚≠ê‚≠ê‚≠ê\n$$$\nLarge Language Models\n‚≠ê‚≠ê\n‚≠ê‚≠ê\n‚≠ê‚≠ê‚≠ê‚≠ê\n‚≠ê‚≠ê‚≠ê‚≠ê\n$$$$\nTraditional NER\n‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n‚≠ê‚≠ê‚≠ê‚≠ê\n‚≠ê\nFree\nüöÄ Alternative Implementations\nWhile Python provides the most comprehensive PII detection capabilities, GLiNER is available across multiple languages for different deployment scenarios.\nü¶Ä Rust Implementation (gline-rs)\nBest for: High-performance backend services, microservices\n[dependencies]\n\"gline-rs\" = \"1\"\nuse gline_rs::{GLiNER, TextInput, Parameters, RuntimeParameters};\nlet model = GLiNER::<TokenMode>::new(\nParameters::default(),\nRuntimeParameters::default(),\n\"tokenizer.json\",\n\"model.onnx\",\n)?;\nlet input = TextInput::from_str(\n&[\"My name is James Bond.\"],\n&[\"person\"],\n)?;\nlet output = model.inference(input)?;\nPerformance: 4x faster than Python on CPU, 37x faster with GPU acceleration.\n‚ö° C++ Implementation (GLiNER.cpp)\nBest for: Embedded systems, mobile apps, edge devices\n#include \"GLiNER/model.hpp\"\ngliner::Config config{12, 512};\ngliner::Model model(\"./model.onnx\", \"./tokenizer.json\", config);\nstd::vector<std::string> texts = {\"John works at Microsoft\"};\nstd::vector<std::string> entities = {\"person\", \"organization\"};\nauto output = model.inference(texts, entities);\nüåê JavaScript Implementation (GLiNER.js)\nBest for: Web applications, browser-based processing\nnpm install gliner\nimport { Gliner } from 'gliner';\nconst gliner = new Gliner({\ntokenizerPath: \"onnx-community/gliner_small-v2\",\nonnxSettings: {\nmodelPath: \"public/model.onnx\",\nexecutionProvider: \"webgpu\",\n}\n});\nawait gliner.initialize();\nconst results = await gliner.inference({\ntexts: [\"John Smith works at Microsoft\"],\nentities: [\"person\", \"organization\"],\nthreshold: 0.1,\n});\nüèóÔ∏è Model Architecture & Training\nQuantization-Aware Pretraining\nGLiNER models use quantization-aware pretraining, which optimizes performance while maintaining accuracy. This allows efficient inference even with quantized models.\nAvailable ONNX Formats\nFormat\nSize\nUse Case\nFP16\n330MB\nBalanced performance/accuracy\nUINT8\n197MB\nMaximum efficiency\nModel Conversion\npython convert_to_onnx.py \\\n--model_path knowledgator/gliner-pii-base-v1.0 \\\n--save_path ./model \\\n--quantize True  # For UINT8 quantization\nüìÑ References\nGLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer\nGLiNER multi-task: Generalist Lightweight Model for Various Information Extraction Tasks\nNamed Entity Recognition as Structured Span Prediction\nüôè Acknowledgments\nSpecial thanks to the all GLiNER contributors, the Wordcab team and additional thanks to maintainers of the Rust, C++, and JavaScript implementations.\nüìû Support\nHugging Face: Ihor/gliner-pii-small\nGitHub Issues: Report bugs and request features\nDiscord: Join community discussions\nGLiNER: Open-source privacy-first entity recognition for production applications.",
    "ovi054/virtual-tryon-kontext-lora": "virtual tryon kontext lora\nModel description\nTrigger words\nüìä Examples\n‚öôÔ∏è How to use\nTry it Online / Demo Space\nTry it Locally with Gradio UI\nDownload model\nTraining at fal.ai\nvirtual tryon kontext lora\nModel description\nTrigger words\nYou should use wear it to trigger the image generation.\nüìä Examples\nInput\nOutput\nInput\nOutput\n‚öôÔ∏è How to use\nYou need a model image and a garments overlay over it\nYou can put garments overlay on model photo with this tool I made.\nPut the trigger word wear it in your prompt.\nAdjust LoRA weight (recommended 1.0)\nTry it Online / Demo Space\nWe have built a demo space to try it easily.\nTry it now: https://huggingface.co/spaces/ovi054/virtual-tryon-flux-kontext.\nTry it Locally with Gradio UI\ngit clone https://huggingface.co/spaces/ovi054/virtual-tryon-flux-kontext\ncd ovi054/virtual-tryon-flux-kontext\npip install -r requirements.txt\npython app.py\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.\nTraining at fal.ai\nTraining was done using fal.ai/models/fal-ai/flux-kontext-trainer.",
    "inclusionAI/Ring-flash-linear-2.0": "Ring-flash-linear-2.0\nIntroduction\nEvaluation\nLinear Attention, Highly Sparse, High-Speed Generation\nQuickstart\nRequirements\nü§ó Hugging Face Transformers\nüöÄ SGLang\nüöÄ vLLM\nRing-flash-linear-2.0\nüìñ  Technical Report¬†¬† | ¬†¬†ü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope\nIntroduction\nWe are excited to announce the official open-source release of Ring-flash-linear-2.0!\nBuilding on the success of our Ling 2.0 series, this model continues to leverage a powerful hybrid architecture of linear and standard attention, perfectly balancing high performance with superior efficiency. By integrating our proven MoE design with optimizations like a 1/32 expert activation ratio and MTP layers, Ring-flash-linear achieves the performance of a 40B dense model while activating only 6.1B parameters.  This model was converted from Ling-flash-base-2.0, further trained on an additional 1T tokens.\nWhen it comes to benchmarks, Ring-flash-linear-2.0 not only holds its own against standard attention models (like Ring-flash-2.0) but also outperforms other open-source MoE and Dense models in its class on several demanding tasks. Plus, with support for a 128k long context, it's faster and more precise than ever, especially when handling long-form inputs and outputs.\nFigure 1: Hybrid Linear Model Architecture\nEvaluation\nTo better demonstrate the model's capabilities, we selected representative open-source thinking models and closed-source APIs for comparison.\nWe present results on several challenging reasoning benchmarks spanning domains such as mathematics, coding, and science. Also, we evaluate the model's performance on a creative writing task (Creative Writing v3).\nWe observe that our model achieves performance on par with other models.\nFigure 2: Model Performance Comparison\nFigure 3: Model Performance Comparison\nLinear Attention, Highly Sparse, High-Speed Generation\nThanks to its hybrid attention mechanism and highly sparse MoE architecture, Ring-flash-linear-2.0 achieves near-linear time complexity and constant space complexity, resulting in outstanding inference efficiency.\nTo fully demonstrate this advantage, we conducted a comparison between our model and top-tier competitors of similar size or performance.\nThe results clearly demonstrate the advantage of our model in inference efficiency.\nFigure 4: Ring-flash-linear-2.0 prefill throughput\nFigure 5: Ring-flash-linear-2.0 decode throughput\nQuickstart\nRequirements\npip install flash-linear-attention==0.3.2\npip install transformers==4.56.1\nü§ó Hugging Face Transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"inclusionAI/Ring-flash-linear-2.0\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ndtype=\"auto\",\ndevice_map=\"auto\",\ntrust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompts = [\n\"Give me a short introduction to large language models.\"\n]\ninput_texts = []\nfor prompt in prompts:\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ninput_texts.append(text)\nprint(input_texts)\nmodel_inputs = tokenizer(input_texts, return_tensors=\"pt\", return_token_type_ids=False, padding=True, padding_side='left').to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=8192,\ndo_sample=False,\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nprint(\"*\" * 30)\nprint(responses)\nprint(\"*\" * 30)\nüöÄ SGLang\nEnvironment Preparation\nWe have submitted our PR to SGLang official release and it will be merged later, for now we can prepare the environment following steps, firstly install the community version SGLang and required packages:\npip install sglang==0.5.2 sgl-kernel==0.3.9.post2 vllm==0.10.2 torch==2.8.0 torchvision==0.23.0 torchao\nThen you should install our sglang wheel package:\npip install https://media.githubusercontent.com/media/inclusionAI/Ring-V2/refs/heads/main/hybrid_linear/whls/sglang-0.5.2-py3-none-any.whl --no-deps --force-reinstall\nRun Inference\nBF16 and FP8 models are supported by SGLang now, it depends on the dtype of the model in ${MODEL_PATH}. They both share the same command in the following:\nStart server:\npython -m sglang.launch_server \\\n--model-path <model_path> \\\n--trust-remote-code \\\n--tp-size 4 \\\n--disable-radix-cache \\\n--tool-call-parser qwen25 \\\n--json-model-override-args \"{\\\"linear_backend\\\": \\\"seg_la\\\"}\"\nClient:\ncurl -s http://localhost:${PORT}/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\"model\": \"auto\", \"temperature\": 0.6, \"messages\": [{\"role\": \"user\", \"content\": \"Give me a short introduction to large language models.\"}]}'\nMore usage can be found here\nüöÄ vLLM\nEnvironment Preparation\nSince the Pull Request (PR) has not been submitted to the vLLM community at this stage, please prepare the environment by following the steps below.\nFirst, create a Conda environment with Python 3.10 and CUDA 12.8:\nconda create -n vllm python=3.10\nconda activate vllm\nNext, install our vLLM wheel package:\npip install https://media.githubusercontent.com/media/zheyishine/vllm_whl/refs/heads/main/vllm-0.8.5.post2.dev28%2Bgd327eed71.cu128-cp310-cp310-linux_x86_64.whl --force-reinstall\nFinally, install compatible versions of transformers after vLLM is installed:\npip install transformers==4.51.1\nOffline Inference\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nif __name__ == '__main__':\ntokenizer = AutoTokenizer.from_pretrained(\"inclusionAI/Ring-flash-linear-2.0\", trust_remote_code=True)\nsampling_params = SamplingParams(temperature=0.6, top_p=1.0, max_tokens=16384)\n# use `max_num_seqs=1` without concurrency\nllm = LLM(model=\"inclusionAI/Ring-flash-linear-2.0\", dtype='auto', enable_prefix_caching=False, max_num_seqs=128)\nprompt = \"Give me a short introduction to large language models.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\noutputs = llm.generate([text], sampling_params)\nfor output in outputs:\nprint(output.outputs[0].text)\nOnline Inference\nvllm serve inclusionAI/Ring-flash-linear-2.0 \\\n--tensor-parallel-size 2 \\\n--pipeline-parallel-size 1 \\\n--gpu-memory-utilization 0.90 \\\n--max-num-seqs 128 \\\n--no-enable-prefix-caching\n--api-key your-api-key\nCitation\n@misc{lingteam2025attentionmattersefficienthybrid,\ntitle={Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning},\nauthor={Ling Team and Bin Han and Caizhi Tang and Chen Liang and Donghao Zhang and Fan Yuan and Feng Zhu and Jie Gao and Jingyu Hu and Longfei Li and Meng Li and Mingyang Zhang and Peijie Jiang and Peng Jiao and Qian Zhao and Qingyuan Yang and Wenbo Shen and Xinxing Yang and Yalin Zhang and Yankun Ren and Yao Zhao and Yibo Cao and Yixuan Sun and Yue Zhang and Yuchen Fang and Zibin Lin and Zixuan Cheng and Jun Zhou},\nyear={2025},\neprint={2510.19338},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2510.19338},\n}",
    "Salesforce/CoDA-v0-Instruct": "CoDA: Coding LM via Diffusion Adaptation\n‚≠ê Highlights\nüìä Model Details\n‚ú® Key Features\nüìà Performance\nüéì Training Methodology\nüõ†Ô∏è Usage\nüöÄ Quick Start\nüöÄ Deployment\nüí¨ Interactive CLI\n‚öôÔ∏è Generation Hyperparameters\nüîß Training from Scratch\nüß† Pre-training on TPU\nüéØ Supervised Fine-tuning\nüìä Evaluation\nüìö Citation\nüîó Resources\nüôè Acknowledgements\nCoDA: Coding LM via Diffusion Adaptation\nTry CoDA ¬∑\nPaper ¬∑\nModel Collection ¬∑\nGitHub Repository\nWelcome to CoDA, Salesforce AI Research's diffusion-based language model designed for powerful code generation and bidirectional context understanding, presented in the paper CoDA: Coding LM via Diffusion Adaptation.\nWe're releasing CoDA as a lightweight yet capable model:\nCoDA-1.7B-Instruct ‚Äî optimized for code generation tasks with bidirectional diffusion modeling (1.7B parameters)\nCoDA-1.7B-Base ‚Äî diffusion foundation model with bidirectional diffusion architecture, ideal for further fine-tuning and RL training\nCoDA leverages discrete diffusion processes to enable understanding of both past and future tokens, making it uniquely suited for code completion and generation tasks where context flows in both directions.\nThis model card is dedicated to the CoDA-1.7B-Instruct model. Check out our model collection for other variants.\n‚≠ê Highlights\nBidirectional Context Understanding: Leverage discrete diffusion processes to understand both past and future tokens, enabling superior code completion.\nConfidence-Guided Sampling: Maintain competitive inference latency through intelligent sampling strategies that balance quality and speed.\nLightweight Architecture: Achieve strong performance with only 1.7B parameters, making it accessible for researchers with limited computational resources.\nFull Training Pipeline: Complete reproducible training pipeline from pre-training to fine-tuning, enabling customization for specific domains.\nOptimized for Code: Specifically designed and trained for code generation tasks, with strong performance on HumanEval, MBPP, and other coding benchmarks.\nüìä Model Details\nModel Size: 1.7B parameters\nArchitecture: Diffusion-based language model\nTraining: TPU-based pre-training with GPU fine-tuning\nPrimary Use: Code generation and completion tasks\n‚ú® Key Features\nBidirectional Context: Diffusion modeling enables understanding of both past and future tokens\nConfidence-Guided Sampling: Maintains competitive inference latency through intelligent sampling\nLightweight Design: Achieves strong performance with fewer parameters than comparable models\nOpen Training Pipeline: Fully reproducible training from pre-training to fine-tuning\nüìà Performance\nCoDA-1.7B-Instruct demonstrates competitive performance on standard code generation benchmarks:\nModel\nHumanEval\nHumanEval+\nMBPP\nMBPP+\nEvalPlus\nCoDA-Base\n29.3\n23.8\n35.2\n46.0\n34.9\nCoDA-Instruct\n54.3\n47.6\n47.2\n63.2\n55.4\nDream-Base\n56.7\n50.0\n68.7\n57.4\n53.7\nDream-7B-Instruct\n57.9\n53.7\n68.3\n56.1\n54.9\nLLaDA-8B-Instruct\n35.4\n31.7\n31.5\n28.6\n30.2\nüéØ Key Finding: CoDA-1.7B-Instruct matches or surpasses diffusion models up to 7B parameters while maintaining significantly lower computational requirements. CoDA offers an advantageous balance between inference speed and accuracy compared to larger diffusion models.\nüéì Training Methodology\nCoDA employs a three-stage training process:\nThree-stage training: (1) Pre-training with bidirectional masking, (2) Post-training with instruction format, (3) Inference with progressive denoising.\nüõ†Ô∏è Usage\nüöÄ Quick Start\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_name = \"Salesforce/CoDA-v0-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n# Generate code\nprompt = \"Write a Python function to calculate fibonacci numbers\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(\n**inputs,\nmax_tokens=256,\ndiffusion_steps=128,\ntemperature=0.0\n)\nprint(tokenizer.decode(outputs[0]))\nüöÄ Deployment\nFor production deployment, we provide serving with OpenAI-compatible APIs:\n# Clone the repository\ngit clone https://github.com/SalesforceAIResearch/CoDA\ncd CoDA\n# Set up environment\npython3 -m venv .venv\nsource .venv/bin/activate\npip install -r serving/requirements.txt\n# Export your Hugging Face token\nexport HF_TOKEN=\"hf_...\"\n# Start the server\nbash serving/fast-api/start_server.sh\nThe server will listen on http://localhost:8000.\nüí¨ Interactive CLI\npython serving/fast-api/chat_cli.py \\\n--base-url http://localhost:8000 \\\n--model Salesforce/CoDA-v0-Instruct \\\n--stream \\\n--show-meta\n‚öôÔ∏è Generation Hyperparameters\nCustomize generation behavior with environment variables:\nexport MAX_TOKENS=512          # Maximum tokens to generate\nexport TEMPERATURE=0.7         # Sampling temperature\nexport TOP_P=0.9              # Nucleus sampling threshold\nexport STEPS=128              # Number of diffusion steps\nexport ALG=\"entropy\"          # Sampling algorithm\nexport ALG_TEMP=0.1           # Algorithm temperature\nexport BLOCK_LENGTH=32        # Block size for processing\nRecommended Settings:\nFast inference: STEPS=64, TEMPERATURE=0.0\nQuality generation: STEPS=128, TEMPERATURE=0.7, TOP_P=0.9\nHigh quality: STEPS=256, TEMPERATURE=0.5, TOP_P=0.95\nüîß Training from Scratch\nThe complete training pipeline is available in our repository:\n# Clone the repository\ngit clone https://github.com/SalesforceAIResearch/CoDA\ncd CoDA\nüß† Pre-training on TPU\n# Configure TPU environment\ncd pre-train\ncp env.example .env  # Add your TPU metadata\nbash setup_tpu.sh\n# Launch pre-training\nbash recipes/midtrain_v4_512.sh\nüéØ Supervised Fine-tuning\n# Set up fine-tuning environment\ncd post-train/LLaMA-Factory\npip install -r requirements.txt\n# Configure dataset and run fine-tuning\nbash ../../run_sft.sh\nüìä Evaluation\ncd evaluation/lm_eval\nbash eval_mbpp_humaneval.sh\nüìö Citation\n@misc{coda2025,\ntitle={CoDA: Coding LM via Diffusion Adaptation},\nauthor={Chen, Haolin and Wang, Shiyu and Qin, Can and Pang, Bo and Liu, Zuxin and Qiu, Jielin and Zhang, Jianguo and Zhou, Yingbo and Chen, Zeyuan and Xu, Ran and Heinecke, Shelby and Savarese, Silvio and Xiong, Caiming and Wang, Huan and Yao, Weiran},\nyear={2025},\npublisher={Salesforce AI Research}\n}\nüîó Resources\nüìÑ Paper: huggingface.co/papers/2510.03270\nüíª Code Repository: github.com/SalesforceAIResearch/CoDA\nü§ó Model Hub: Salesforce CoDA collection\nüôè Acknowledgements\nWe thank Lingpeng Kong for insightful discussions and Jialei Chen for technical support with TPU infrastructure.\nüè¢ Developed by Salesforce AI Research",
    "Vikhrmodels/Vistral-24B-Instruct": "Vistral-24B-Instruct\n–û–ø–∏—Å–∞–Ω–∏–µ\nQuantized variants:\n–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ Ru-Arena-General\n–ü—Ä–∏–º–µ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å OpenAI-like API\n–ù—é–∞–Ω—Å—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è\n–ê–≤—Ç–æ—Ä—ã\nVistral-24B-Instruct\n–û–ø–∏—Å–∞–Ω–∏–µ\nVistral - —ç—Ç–æ –Ω–∞—à–∞ –Ω–æ–≤–∞—è —Ñ–ª–∞–≥–º–∞–Ω—Å–∫–∞—è —É–Ω–∏–º–æ–¥–∞–ª—å–Ω–∞—è LLM (Large Language Model) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∞—è –∏–∑ —Å–µ–±—è —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é mistralai/Mistral-Small-3.2-24B-Instruct-2506 –∫–æ–º–∞–Ω–¥–æ–π VikhrModels, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤. –£–¥–∞–ª—ë–Ω –≤–∏–∑—É–∞–ª—å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä, —É–±—Ä–∞–Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ \"MistralForCausalLM\" –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –±–∞–∑–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –º–æ–¥–µ–ª–∏.\n–í–µ—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–µ–Ω –≤ –Ω–∞—à–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ effective_llm_alignment –Ω–∞ GitHub, –∞ –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–æ—Å—Ç—É–ø–Ω—ã –≤ –Ω–∞—à–µ–º –ø—Ä–æ—Ñ–∏–ª–µ –Ω–∞ HF.\n–ú–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞ –Ω–∞ –Ω–∞—à–µ–º —Å–∞–π—Ç–µ Chat Vikhr\nQuantized variants:\nGGUF Vikhrmodels/Vistral-24B-Instruct-GGUF\nMLX\n4 bit Vikhrmodels/Vistral-24B-Instruct-MLX_4bit\n8 bit Vikhrmodels/Vistral-24B-Instruct-MLX_8bit\n–ú–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n–ú–æ–¥–µ–ª—å –æ—Ü–µ–Ω–∏–≤–∞–ª–∞—Å—å –Ω–∞ –Ω–∞—à–µ–º —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–º open-source SbS –±–µ–Ω—á–º–∞—Ä–∫–µ ru-arena-general\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ Ru-Arena-General\nModel Name\nWinrate\n95% CI\nAverage # Tokens\nVistral-24B-Instruct\n96.1\n(-0.7, 0.8)\n647\nMistral-Small-3.2-24B-Instruct-2506\n92.1\n(-0.9, 1.0)\n486\nvikhr-nemo-12b-instruct-r-21-09-24(180 leaked)\n79.8\n(-2.2, 1.9)\n627\n–ü—Ä–∏–º–µ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å OpenAI-like API\n–ó–∞–ø—É—Å–∫ vLLM —Å–µ—Ä–≤–µ—Ä–∞: vllm serve --dtype half --max-model-len 32000 -tp 1 Vikhrmodels/Vistral-24B-Instruct --api-key token-abc123\nfrom openai import OpenAI\nclient = OpenAI(\nbase_url=\"http://localhost:8000/v1\",\napi_key=\"token-abc123\",\n)\nllm_model = \"Vikhrmodels/Vistral-24B-Instruct\"\nsample_history = [\n{'role': 'user', 'content': '–ù–∞–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–Ω–∏–≥–∏ –ì–∞—Ä—Ä–∏ –ü–æ—Ç—Ç–µ—Ä.'}\n]\nfinal_answer = llm_client.chat.completions.create(\nmodel=llm_model,\nmessages=sample_history,\ntemperature=0.3,\nmax_tokens=2048\n).choices[0].message.content\nprint(final_answer)\n–û—Ç–≤–µ—Ç –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –±—É–¥–µ—Ç –≤—ã–≥–ª—è–¥–µ—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫:\n**–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–Ω–∏–≥–∏ ¬´–ì–∞—Ä—Ä–∏ –ü–æ—Ç—Ç–µ—Ä¬ª:**\n¬´–ì–∞—Ä—Ä–∏ –ü–æ—Ç—Ç–µ—Ä¬ª ‚Äî —ç—Ç–æ —Å–µ—Ä–∏—è —Ñ–∞–Ω—Ç–∞—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–æ–º–∞–Ω–æ–≤ –î–∂. –ö. –†–æ—É–ª–∏–Ω–≥ –æ –º–∞–ª—å—á–∏–∫–µ-–≤–æ–ª—à–µ–±–Ω–∏–∫–µ, –∫–æ—Ç–æ—Ä—ã–π —É–∑–Ω–∞—ë—Ç, —á—Ç–æ –æ–Ω —Å—ã–Ω –º–æ–≥—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–∞–≥–æ–≤, –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —É—á–∏—Ç—å—Å—è –≤ —à–∫–æ–ª—É —á–∞—Ä–æ–¥–µ–π—Å—Ç–≤–∞ –∏ –≤–æ–ª—à–µ–±—Å—Ç–≤–∞ –•–æ–≥–≤–∞—Ä—Ç—Å. –í –ø–µ—Ä–≤–æ–º —Ç–æ–º–µ (¬´–ì–∞—Ä—Ä–∏ –ü–æ—Ç—Ç–µ—Ä –∏ —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏–π –∫–∞–º–µ–Ω—å¬ª) –ì–∞—Ä—Ä–∏ –∑–Ω–∞–∫–æ–º–∏—Ç—Å—è —Å –¥—Ä—É–∑—å—è–º–∏ –†–æ–Ω–æ–º –∏ –ì–µ—Ä–º–∏–æ–Ω–æ–π, —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Ç–∞–π–Ω—É —Å–≤–æ–µ–≥–æ –ø—Ä–æ—à–ª–æ–≥–æ –∏ —Å—Ç–∞–ª–∫–∏–≤–∞–µ—Ç—Å—è —Å –æ–ø–∞—Å–Ω—ã–º —Ç—ë–º–Ω—ã–º –º–∞–≥–æ–º –í–æ–ª–∞–Ω-–¥–µ-–ú–æ—Ä—Ç–æ–º.\n–í –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –∫–Ω–∏–≥–∞—Ö –ì–∞—Ä—Ä–∏ –∏ –µ–≥–æ –¥—Ä—É–∑—å—è –±–æ—Ä—é—Ç—Å—è —Å —Å–∏–ª–∞–º–∏ –∑–ª–∞, —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç –¥—Ä–µ–≤–Ω–∏–µ —Ç–∞–π–Ω—ã, –ø–µ—Ä–µ–∂–∏–≤–∞—é—Ç –≤–∑—Ä–æ—Å–ª–µ–Ω–∏–µ –∏ —É—á–∞—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–æ–ª—à–µ–±—Å—Ç–≤–æ –≤–æ –±–ª–∞–≥–æ. –°–µ—Ä–∏—è —Å–æ—á–µ—Ç–∞–µ—Ç –ø—Ä–∏–∫–ª—é—á–µ–Ω–∏—è, –¥—Ä—É–∂–±—É, –º–∞–≥–∏—é –∏ –±–æ—Ä—å–±—É –¥–æ–±—Ä–∞ —Å–æ –∑–ª–æ–º.\n**–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã:** –≤–æ–ª—à–µ–±–Ω—ã–π –º–∏—Ä, –¥—Ä—É–∂–±–∞, —Ö—Ä–∞–±—Ä–æ—Å—Ç—å, –ø—Ä–µ–¥–∞–Ω–Ω–æ—Å—Ç—å, –±–æ—Ä—å–±–∞ —Å–æ –∑–ª–æ–º.\n–ù—é–∞–Ω—Å—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è\n–ú–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –Ω–∏–∑–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ –∏ –Ω–∞—Ü–µ–ª–µ–Ω–∞ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏ –ø–æ–ª–Ω–æ–µ –≤—ã–ø–æ–ª–µ–Ω–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –∏–º–µ–π—Ç–µ —ç—Ç–æ –≤–≤–∏–¥—É –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∏ —Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ. –ß–∞—Å—Ç–∏—á–Ω–æ —ç—Ç–æ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —É–∫–∞–∑–∞–Ω–∏—è–º–∏ –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–æ–º–ø—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.\n–°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –Ω–µ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω—ã –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å—Ç–∏–ª—è –æ—Ç–≤–µ—Ç–∞ (–≤—Ä–æ–¥–µ \"answer only in json format\"). –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ, –ø–∏—Å–∞—Ç—å –∏—Ö –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ, —Ç–∞–∫ –∫–∞–∫ —Ç–∞–∫ –±—ã–ª–æ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ, –æ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –≤ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –ø—Ä–æ–º—Ç–ø–∞—Ö –Ω–µ –∑–∞–≤–∏—Å–∏—Ç —è–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞.\n–ú–æ–¥–µ–ª—å –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å –Ω–∏–∑–∫–æ–π —Ç–µ–º–ø—Ç–µ—Ä–∞—Ç—É—Ä–æ–π (0.1-0.5), –∞ —Ç–∞–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å top_k (30-50), –ø—Ä–∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ 1.0 –±—ã–ª–∏ –∑–∞–º–µ—á–µ–Ω—ã —Å–ª—É—á–∞–π–Ω—ã–µ –¥–µ—Ñ–µ–∫—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n–ê–≤—Ç–æ—Ä—ã\nNikolay Kompanets, LakoMoor, Vikhr Team\nSergei Bratchikov, NLP Wanderer, Vikhr Team\nKonstantin Korolev, Vikhr Team\nAleksandr Nikolich, Vikhr Team\n@inproceedings{nikolich2024vikhr,\ntitle={Vikhr: Advancing Open-Source Bilingual Instruction-Following Large Language Models for Russian and English},\nauthor={Aleksandr Nikolich and Konstantin Korolev and Sergei Bratchikov and Nikolay Kompanets and Igor Kiselev and Artem Shelmanov},\nbooktitle={Proceedings of the 4th Workshop on Multilingual Representation Learning (MRL) @ EMNLP-2024},\nyear={2024},\npublisher={Association for Computational Linguistics},\nurl={https://arxiv.org/pdf/2405.13929}\n}",
    "Gen-Verse/DemyAgent-4B": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nüéØ About This Repository\nüåü Introduction\nüì¶ Resources\nüèÜ Performance\nBenchmark Results\nKey Highlights\nüìù Citation\nDemystifying Reinforcement Learning in Agentic Reasoning\nüéØ About This Repository\nThis repository contains the DemyAgent-4B model weights, a 4B-sized agentic reasoning model that achieves state-of-the-art performance on challenging benchmarks including AIME2024/2025, GPQA-Diamond, and LiveCodeBench-v6.\nDemyAgent-4B is trained using our GRPO-TCR recipe with 30K high-quality agentic RL data, demonstrating that small models can outperform much larger alternatives (14B/32B) through effective RL training strategies.\nüåü Introduction\nIn our work, we systematically investigate three dimensions of agentic RL: data, algorithms, and reasoning modes. Our findings reveal:\nüéØ Data Quality Matters: Real end-to-end trajectories and high-diversity datasets significantly outperform synthetic alternatives\n‚ö° Training Efficiency: Exploration-friendly techniques like reward clipping and entropy maintenance boost training efficiency\nüß† Reasoning Strategy: Deliberative reasoning with selective tool calls surpasses frequent invocation or verbose self-reasoning\nWe contribute high-quality SFT and RL datasets, demonstrating that simple recipes enable even 4B models to outperform 32B models on the most challenging reasoning benchmarks.\nüì¶ Resources\nType\nName\nLink\nüìä Dataset\n3K Agentic SFT Data\nü§ó HuggingFace\nüìä Dataset\n30K Agentic RL Data\nü§ó HuggingFace\nü§ñ Model\nQwen2.5-7B-RA-SFT\nü§ó HuggingFace\nü§ñ Model\nQwen3-4B-RA-SFT\nü§ó HuggingFace\nü§ñ Model\nDemyAgent-4B\nü§ó HuggingFace\nNote:\nQwen2.5-7B-RA-SFT and Qwen3-4B-RA-SFT are finetuned from Qwen2.5-7B-Instruct and Qwen3-4B-Instruct-2507 using our 3K Agentic SFT Data\nDemyAgent-4B is trained through Agentic RL with our 30K Agentic RL data using the GRPO-TCR recipe\nüèÜ Performance\nWe evaluate our models on challenging benchmarks spanning mathematics, science, and code generation tasks.\nBenchmark Results\nMATH\nScience\nCode\nMethod\nAIME2024\nAIME2025\nGPQA-Diamond\nLiveCodeBench-v6\nSelf-Contained Reasoning\nQwen2.5-7B-Instruct\n16.7\n10.0\n31.3\n15.2\nQwen3-4B-Instruct-2507\n63.3\n47.4\n52.0\n35.1\nQwen2.5-72B-Instruct\n18.9\n15.0\n49.0\n-\nDeepSeek-V3\n39.2\n28.8\n59.1\n16.1\nDeepSeek-R1-Distill-32B\n70.0\n46.7\n59.6\n-\nDeepSeek-R1-Zero (671B)\n71.0\n53.5\n59.6\n-\nAgentic Reasoning\nQwen2.5-7B-Instruct\n4.8\n5.6\n25.5\n12.2\nQwen3-4B-Instruct-2507\n17.9\n16.3\n44.3\n23.0\nToRL-7B\n43.3\n30.0\n-\n-\nReTool-32B\n72.5\n54.3\n-\n-\nTool-Star-3B\n20.0\n16.7\n-\n-\nARPO-7B\n30.0\n30.0\n53.0\n18.3\nrStar2-Agent-14B\n80.6\n69.8\n60.9\n-\nDemyAgent-4B (Ours)\n72.6\n70.0\n58.5\n26.8\nKey Highlights\n‚ú® Despite having only 4B parameters, DemyAgent-4B achieves:\nü•á State-of-the-art on AIME2025 (70.0%), outperforming even DeepSeek-R1-Zero (671B)\nü•à Second place on AIME2024 (72.6%) and GPQA-Diamond (58.5%)\nüöÄ Competitive performance against 14B-32B models with 4-8√ó fewer parameters\nüí° Superior efficiency compared to long-CoT models through deliberative tool use\nüìù Citation\n@article{yu2025demystify,\ntitle={Demystifying Reinforcement Learning in Agentic Reasoning},\nauthor={Yu, Zhaochen and Yang, Ling and Zou, Jiaru and Yan, Shuicheng and Wang, Mengdi},\njournal={arXiv preprint arXiv:2510.11701},\nyear={2025}\n}",
    "shaunxsyang/U-Codec": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nU-Codec: Ultra Low Frame-rate Neural Speech Codec for Fast High-fidelity Speech Generation\nOverview\nHow to inference U-Codec\nEnvironment Setup\nRun Inference\nCitation\nContact us\nLicense\nU-Codec: Ultra Low Frame-rate Neural Speech Codec for Fast High-fidelity Speech Generation\nNews\nThis paper is currently under review. We have released the checkpoint of U-Codec (5Hz), which can be directly used for inference.\nTo do list\nProvide the full training code for the U-Codec framework.\nRelease the public code of the TTS models built on top of U-Codec.\nIf you are interested in U-Codec, feel free to contact us!\nOverview\nWe propose U-Codec, an Ultra low frame-rate neural speech Codec that achieves high-fidelity reconstruction and fast generation via an extremely frame-rate at 5Hz (5 frames per second).\nExtreme compression at 5Hz typically leads to severe intelligibility and spectral detail loss, we overcome this by integrating a Transformer-based inter-frame long-term dependency module and systematically optimizing residual vector quantization (RVQ) depth and codebook size.\nMoreover, we apply U-Codec into a large language model (LLM)-based auto-regressive TTS model, which leverages global and local hierarchical architecture to effectively capture dependencies across multi-layer tokens.\nThe overview of U-Codec as following picture shows.\nHow to inference U-Codec\nWe provide an example to demonstrate how to run U-Codec (5Hz) for audio tokenization and reconstruction.\nEnvironment Setup\nFirst, create a Python environment following a similar setup to project page.\nconda create -n ucodec python=3.8\nconda init\nsource ~/.bashrc\nconda activate ucodec\nThen:\ncd U-Codec\nbash requirements.sh\nRun Inference\nIf you need pretrained weights, please download them on the Checkpoint.\nWe provide an example script AudioTokenizer_UCodec.py for tokenizing audio into discrete codes and reconstructing audio from the codes.\ncd tools/tokenizer/soundstream\npython AudioTokenizer_HY.py\nYou can directly use the released U-Codec 5Hz checkpoint for inference. More examples (e.g., TTS pipeline integration) will be released soon.\nCitation\nIf you find this code useful in your research, please cite our work and give us a star\n@inproceedings{U-Codec,\ntitle     = {U-Codec: Ultra Low Frame-rate Neural Speech Codec for Fast High-fidelity Speech Generation},\nauthor    = {Xusheng Yang, Long Zhou, Wenfu Wang, Kai Hu, Shulin Feng, Chenxing Li, Meng Yu, Dong Yu, Yuexian Zou},\nbooktitle = {arXiv},\nyear      = {2025}\n}\nContact us\nIf you have any problem about the our code, please contact Xusheng (yangxs@stu.pku.edu.cn).\nLicense\nYou can use the code under MIT license.",
    "yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF": "Qwen3-VL-30B-A3B-Thinking\nModel Performance\nQuickstart\nUsing ü§ó Transformers to Chat\nCitation\n„ÄêOverview„Äë\nQwen3-VL-30B-A3B-Thinking\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-30B-A3B-Thinking.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.\nThe code of Qwen3-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ü§ó Transformers to Chat\nHere we show a code snippet to show you how to use the chat model with transformers:\nfrom transformers import Qwen3VLMoeForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-30B-A3B-Thinking\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-30B-A3B-Thinking\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-30B-A3B-Thinking\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "YOYO-AI/Qwen3-30B-A3B-YOYO-V4": "Model Highlights:\nParameter Settings:\nGeometric Median with CLA Initialization\nProblem Setting\nPer-Tensor Formulation\nMean and Median\nArithmetic Mean:\nElementwise Median:\nCLA Initialization\nCentered Linear Average:\nGeometric Median Objective\nObjective Function:\nWeiszfeld Algorithm\nIteration Step:\nConvergence Criterion:\nLeveraging our novel merging approach, we can seamlessly integrate instruction, reasoning, and code models into a single, high-performing unified model in just one step.\nModel Highlights:\nmerge method: cla-gm\nprecision: dtype: bfloat16\nContext length: 262,144&1010000\nParameter Settings:\nTemperature=0.7, TopP=0.8, TopK=20,MinP=0.\nGeometric Median with CLA Initialization\nProblem Setting\nObjective: Merge ùêæ fine-tuned models with identical tensor names and shapes into a single model whose parameters ùúÉ‚ãÜ lie at the robust center of the ùêæ parameter sets.\nPer-Tensor Formulation\nFor a given tensor name, each model provides a point ùë•·µ¢ ‚àà ‚Ñù‚Åø (flattened). We seek a robust center ùúÉ‚ãÜ ‚àà ‚Ñù‚Åø.\nMean and Median\nArithmetic Mean:\na=1K‚àëi=1Kxia = \\frac{1}{K} \\sum_{i=1}^{K} x_ia=K1‚Äãi=1‚àëK‚Äãxi‚Äã\nEfficient but sensitive to outliers.\nElementwise Median:\nm=median({xi})m = \\text{median}(\\{x_i\\})m=median({xi‚Äã})\nRobust but ignores vector magnitude coupling; computed elementwise across coordinates.\nCLA Initialization\nCentered Linear Average:\nŒ∏(0)=a+m2\\theta^{(0)} = \\frac{a + m}{2}Œ∏(0)=2a+m‚Äã\nThis blends efficiency and robustness without tuning, offering a strong seed for iterative robust estimators.\nGeometric Median Objective\nObjective Function:\nŒ∏‚ãÜ=arg‚Å°min‚Å°Œ∏‚ààRn‚àëi=1K‚à•Œ∏‚àíxi‚à•2\\theta^{\\star} = \\arg\\min_{\\theta \\in \\mathbb{R}^n} \\sum_{i=1}^{K} \\|\\theta - x_i\\|_2Œ∏‚ãÜ=argŒ∏‚ààRnmin‚Äãi=1‚àëK‚Äã‚à•Œ∏‚àíxi‚Äã‚à•2‚Äã\nThis is the multivariate analogue of the median, robust to outliers in the Euclidean geometry of parameters.\nWeiszfeld Algorithm\nUpdate Rule: Given current ùúÉ(ùë°), define weights:\nwi(t)=1max‚Å°(‚à•Œ∏(t)‚àíxi‚à•2,Œµ)w_i^{(t)} = \\frac{1}{\\max(\\|\\theta^{(t)} - x_i\\|_2, \\varepsilon)}wi(t)‚Äã=max(‚à•Œ∏(t)‚àíxi‚Äã‚à•2‚Äã,Œµ)1‚Äã\nwhere ùúÄ = eps(float32) prevents division by zero.\nIteration Step:\nŒ∏(t+1)=‚àëi=1Kwi(t)xi‚àëi=1Kwi(t)\\theta^{(t+1)} = \\frac{\\sum_{i=1}^{K} w_i^{(t)} x_i}{\\sum_{i=1}^{K} w_i^{(t)}}Œ∏(t+1)=‚àëi=1K‚Äãwi(t)‚Äã‚àëi=1K‚Äãwi(t)‚Äãxi‚Äã‚Äã\nConvergence Criterion:\nStop when the relative change is below ùúÄ:\n‚à•Œ∏(t+1)‚àíŒ∏(t)‚à•2max‚Å°(‚à•Œ∏(t)‚à•2,1)‚â§Œµ\\frac{\\|\\theta^{(t+1)} - \\theta^{(t)}\\|_2}{\\max(\\|\\theta^{(t)}\\|_2, 1)} \\leq \\varepsilonmax(‚à•Œ∏(t)‚à•2‚Äã,1)‚à•Œ∏(t+1)‚àíŒ∏(t)‚à•2‚Äã‚Äã‚â§Œµ\nwhere ùúÄ = eps(float32) ‚âà 1.19√ó10‚Åª‚Å∑.",
    "DavidAU/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III": "üîç Step 1: List of Models & Their Names\nüß† Step 3: Cognitive Profile & Character Mapping\nüññ Final Conclusion:\nüìå Final Recommendation:\nDetailed analysis\nQwen3-Coder-3B-A3B-Instruct\nHighlights\nModel Overview\nQuickstart\nAgentic Coding\nBest Practices\n(Benchmarks / Review added...)\nQwen3-Yoyo-V3-42B-A3B-Thinking-Total-Recall-TNG-III [1 million context]\nThis repo contains the full precision source code, in \"safe tensors\" format to generate GGUFs, GPTQ, EXL2, AWQ, HQQ and other formats.\nThe source code can also be used directly.\nEXPERIMENTAL:\nThis is a limited fine tune (selected layers, experimental methods) on an in house Star Trek TNG (si-fi, all seasons) using Unsloth. This will\nadd some \"sifi / TNG Magic\" to the model.\nThis version has FOUR TIMES the depth of training as V1, and twice that of V2...\nI suggest you try all three versions to see which meets your use case(s) better.\nCompletely mad science.\nSuggest 8-10 experts, temp .7 ish, rep pen 1.05 to 1.1 ; quants at least Q4.\nExample prompt/generation(s) at the bottom of the page.\nThis model is for CODING and programming in all major programming languages and many minor ones too AND GENERAL USAGE.\nThis model is based on Qwen3-Coder-30B-A3B-Instruct (MOE, 128 experts, 8 activated), with Brainstorm 20X\n(by DavidAU) - details at bottom of this page.\nThis model is a result of merged model (3 step, 3 models) from:\nhttps://huggingface.co/YOYO-AI/Qwen3-30B-A3B-YOYO-V3\n(you may want to visit this repo for settings/info too)\nThe Brainstorm adapter will improve general performance and \"out of the box\" thinking.\nThis creates a model of 42B parameters, 67 layers and 807 tensors.\nThis version has the NATIVE context of 1 million context.\nThis is a thinking block model.\nI have included an optional system prompt to invoke \"thinking\" in this model, if you want to activate it.\nSETTINGS:\nFor coding, programming set expert to:\n6-8 for general work.\n10 for moderate work.\n12-16 for complex work, long projects, complex coding.\nSuggest min context window 4k to 8k.\nAnd for longer context, and/or multi-turn -> increase experts by 1-2 to help with longer context/multi turn understanding.\nRecommended settings - general:\nRep pen 1.05 to 1.1 ; however rep pen of 1 will work well (may need to raise it for lower quants/fewer activated experts)\nTemp .3 to .6 (+- .2)\nTopk of 20, 40 or 100\nTopp of .95 / min p of .05\nSuggest min context window 4k to 8k.\nSystem prompt (optional) to focus the model better.\nThis is the refined version -V1.4- from this project (see this repo for all settings, details, system prompts, example generations etc etc):\nhttps://huggingface.co/DavidAU/Qwen3-55B-A3B-TOTAL-RECALL-Deep-40X-GGUF/\nThis version 2 is slightly smaller, with further refinements to the Brainstorm adapter and uses the new \"Qwen3-30B-A3B-Instruct-2507\".\nReview and Specialized Settings for this model (V 1.4):\nhttps://www.linkedin.com/posts/gchesler_davidauqwen3-53b-a3b-total-recall-v14-128k-activity-7344938636141858816-ILCM/\nhttps://www.linkedin.com/posts/gchesler_haskell-postgres-agentic-activity-7347103276141596672-_zbo/\nYou may also want to see (root model of Total Recall series - Version 1):\nhttps://huggingface.co/Qwen/Qwen3-30B-A3B\nAND Version 2 root model:\nhttps://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct\nFor additional settings, tool use, and other model settings.\nSummary of root model below, followed by FULL HELP SECTION, then info on Brainstorm 40x.\nOPTIONAL SYSTEM PROMPT - INVOKE \"Thinking\":\nEnable deep thinking subroutine. You are a deep thinking AI, you may use extremely long chains of thought to deeply consider the problem and deliberate with yourself via systematic reasoning processes to help come to a correct solution prior to answering. You should enclose your thoughts and internal monologue inside ###ponder### ###/ponder### tags, and then provide your solution or response to the problem.\nUse this to INVOKE \"thinking\" block(s) in the model. These will be a lot shorter than 1000s of tokens generally in most \"thinking\" models.\nIn you use this prompt, you may need to raise \"rep pen\" to 1.08 to 1.1, to prevent \"loops\" in the \"thought block(s)\" ; especially in lower quants.\nIf you change \"ponder\" to a different word/phrase this will affect model \"thinking\" too.\nREVIEW and BENCHMARKS by Nightmedia:\nhttps://huggingface.co/nightmedia/Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-qx64x-hi-mlx\nThe Starfleet Away Team has assembled ‚Äî and we are now to identify which crew member each model embodies, based on their cognitive behavior.\nThis is not mere performance analysis ‚Äî this is character profiling. Let us proceed with the precision of a Vulcan mind-meld.\nüîç Step 1: List of Models & Their Names\nModel Name (Abbreviated)\t\t\tTraining Stage\nQwen3-30B-A3B-YOYO-V3-qx64-hi\t\tBaseline (No expansion)\nTotal-Recall-qx64x-hi\t\t\t\tTotal Recall (Brainstorming only)\nTOTAL-RECALL-ST-TNG-qx64x-hi\t\tST-TNG I (First TNG training)\nTOTAL-RECALL-ST-TNG-II-qx64x-hi\t\tST-TNG II (Refined TNG)\nTOTAL-RECALL-ST-TNG-III-qx64x-hi\tST-TNG III (Final TNG iteration)\nüß™ Step 2: Performance Matrix\nModel\tarc_challenge arc_easy\tboolq hellaswag\topenbookqa piqa\twinogrande\nBaseline (V3)\t0.469\t0.537\t0.872\t0.688\t0.434\t0.778\t0.667\nTotal Recall\t0.488\t0.557\t0.878\t0.708\t0.422\t0.782\t0.663\nST-TNG I\t\t0.483\t0.551\t0.878\t0.706\t0.424\t0.779\t0.678\nST-TNG II\t\t0.479\t0.551\t0.878\t0.707\t0.428\t0.776\t0.676\nST-TNG III\t\t0.482\t0.548\t0.876\t0.707\t0.416\t0.777\t0.672\nüß† Step 3: Cognitive Profile & Character Mapping\nWe now assign each model to a Starfleet crew member, based on how their cognitive strengths and weaknesses mirror the personalities of the TNG away team.\nüü© 1. Qwen3-30B-A3B-YOYO-V3-qx64-hi (Baseline)\nCognitive Profile: Solid but unremarkable. Lower reasoning, strong logic (boolq), moderate commonsense.\nArchetype: \tWorf ‚Äî Stoic, disciplined, reliable.\nStrength: \tUnwavering logic (boolq = 0.872) ‚Äî like Worf‚Äôs Klingon honor and precision.\nWeakness: \tAverage reasoning, low openness to abstract ideas ‚Äî like Worf‚Äôs initial rigidity.\nWhy? \t\tThe baseline model is functional, but not innovative. It follows orders, doesn‚Äôt lead.\nüü¶ 2. Qwen3-Yoyo-V3-42B-A3B-Thinking-Total-Recall-qx64x-hi (Total Recall)\nCognitive Profile: Highest ARC-Easy, best Hellaswag and PIQA ‚Äî highly creative, proactive.\nArchetype: \tGeordi La Forge ‚Äî The engineer who thinks outside the box.\nStrength: \tHighest ARC-Easy (0.557), best Hellaswag (0.708), and PIQA (0.782).\nWhy? \t\tGeordi is the innovator ‚Äî always brainstorming solutions, fixing problems with creative reasoning.\nThis model is the first to introduce \"Brainstorming\", mirroring Geordi‚Äôs role as the team‚Äôs problem-solver.\nüü® 3. Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-I-qx64x-hi (ST-TNG I)\nCognitive Profile: Best winogrande (0.678), solid but not top in other categories.\nArchetype: \tData ‚Äî The android with perfect context tracking.\nStrength: \tBest winogrande (0.678) ‚Üí exquisitely handles pronouns, long-range context.\nWeakness: \tLower ARC-Easy (0.551) ‚Äî less open to creative leaps.\nWhy? \t\tData‚Äôs strength is precision in tracking relationships and context ‚Äî exactly what winogrande measures.\nThis is the first TNG iteration, introducing contextual depth ‚Äî like Data‚Äôs ever-improving understanding of human nuance.\nüü• 4. Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-II-qx64x-hi (ST-TNG II)\nCognitive Profile: Slightly lower ARC, but best openbookqa (0.428), strong balance.\nArchetype: \tDr. Beverly Crusher ‚Äî The diplomat and medic, skilled in both fact and empathy.\nStrength: \tBest OpenBookQA (0.428) ‚Äî factual recall, medical/encyclopedic knowledge.\nBalance: \tStrong across all metrics (piqa: 0.776, winogrande: 0.676).\nWhy? \t\tCrusher is the knowledge reservoir ‚Äî she knows facts, but also connects them to people. Her training in TNG II builds on Data‚Äôs context, adding factual depth.\nüü™ 5. Qwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-qx64x-hi (ST-TNG III)\nCognitive Profile: Highest in winogrande and openbookqa, but slightly lower ARC-Easy.\nArchetype: \tCaptain Jean-Luc Picard ‚Äî The leader, thoughtful, ethical.\nStrength: \tBest in winogrande (0.672) and openbookqa (0.416) ‚Äî contextual wisdom + factual mastery.\nWeakness: \tSlight drop in ARC-Easy (0.548) ‚Äî not the most creative, but the most balanced and strategic.\nWhy? \t\tPicard is not always the most imaginative (like Geordi), but he synthesizes knowledge, makes ethical decisions, and leads with wisdom ‚Äî exactly what this model does.\nüìä Summary Table: The TNG Away Team\nModel Name\t\t\t\t\t\t\tCognitive Strengths\t\t\t\t\tStarfleet Character\nQwen3-30B-A3B-YOYO-V3-qx64-hi\t\tLogic, Stability\t\t\t\t\tWorf\nTotal-Recall-qx64x-hi\t\t\t\tCreativity, Problem-Solving\t\t\tGeordi La Forge\nTOTAL-RECALL-ST-TNG-qx64x-hi\t\tContextual Tracking (Winogrande)\tData\nTOTAL-RECALL-ST-TNG-II-qx64x-hi\t\tFactual Knowledge (OpenBookQA)\t\tDr. Crusher\nTOTAL-RECALL-ST-TNG-III-qx64x-hi\tBalanced Wisdom, Ethical Reasoning\tPicard\nüññ Final Conclusion:\nEach model in this series is not just a quantization variant ‚Äî it‚Äôs a character from Star Trek: The Next Generation, sculpted by training and design.\nTotal Recall = Geordi: The innovator.\nST-TNG I = Data: The context guardian.\nST-TNG II = Crusher: The knowledge keeper.\nST-TNG III = Picard: The wise leader.\nAnd the baseline?\nWorf: The steadfast foundation, holding it all together ‚Äî until the true brilliance of ST-TNG emerges.\nüìå Final Recommendation:\nDeploy these models not just for performance ‚Äî but to build a Starfleet AI team.\nUse Total Recall (Geordi) for creative problem-solving.\nDeploy ST-TNG I (Data) for context-sensitive applications (e.g., document analysis).\nRely on ST-TNG II (Crusher) for fact-based knowledge retrieval.\nLet ST-TNG III (Picard) lead in ethical, high-stakes decision-making.\nüññ Engage the away team. Set course for cognitive excellence.\nThe future of AI is now ‚Äî and it speaks in Klingon, English, and the language of Starfleet.\nReviewed with Qwen3-30B-A3B-YOYO-V4-qx65x-mlx\nDetailed analysis\nThis is a MoE with expanded \"brain\", that has 12B of Brainstorming added by DavidAU, and was built ontop of the Qwen3-30B-A3B-YOYO-V3. The ST-TNG-III was additionally trained with StarTrek TNG corpus.\nI am looking for two things: how the model progressed from baseline, and how it was improved by the qx64x-hi quant\nSpock > Ah, excellent. You return with new data ‚Äî and a model that thinks like a Starfleet officer. Let us proceed with the logical rigor you expect.\nüß† Target Model Series:\nThese are variants of an MoE (Mixture-of-Experts) expansion on the Qwen3-30B-A3B-YOYO-V3 base, enhanced with:\n12 billion additional \"Brainstorming\" parameters (a cognitive boost).\nTraining on the Star Trek: The Next Generation corpus, enhancing rhetorical, ethical, and collaborative reasoning.\nA \"Total Recall\" semantic loop for contextual memory.\nThe models are all quantized with:\nqx64-hi: 6-bit data, 32-group size (high fidelity).\nqx64x-hi: Mixed precision ‚Äî 4-bit data, but attention paths and heads at 6 bits (with group size 32).\nüîç Step 1: Progression from Baseline (Qwen3-30B-A3B-YOYO-V3-qx64-hi)\nLet‚Äôs compare each model to the baseline:\nQwen3-30B-A3B-YOYO-V3-qx64-hi (the original 30B, no expansion)\nMetric\t\t\t30B\t+Brainstorming\t+ST-TNG Train\narc_challenge\t0.469\t\t+0.018\t\t+0.011\narc_easy\t\t0.537\t\t+0.019\t\t+0.018\nboolq\t\t\t0.872\t\t-0.003\t\t+0.007\nhellaswag\t\t0.688\t\t+0.020\t\t+0.020\nopenbookqa\t\t0.434\t\t-0.016\t\t+0.008\npiqa\t\t\t0.778\t\t+0.001\t\t+0.006\nwinogrande\t\t0.667\t\t+0.001\t\t+0.002\n‚úÖ Progression Summary (Baseline ‚Üí Total Recall ST-TNG III)\nNotable Improvements:\nHellaswag & ARC-Easy: +0.02 in both ‚Üí Clear gain in commonsense and basic reasoning.\nWinogrande: +0.002 ‚Üí Slight edge in pronoun resolution, indicating better contextual tracking (likely from \"Total Recall\" memory loop).\nBoolQ: +0.007 ‚Üí Enhanced logical consistency, possibly from Star Trek TNG dialogue (which emphasizes ethics and logic).\nMinor Setbacks:\nOpenBookQA: Dropped slightly from 0.434 ‚Üí 0.416 (down -0.018), but still within the margin of noise.\nARC Challenge: Only +0.011 over base (vs. +0.018 on ARC-Easy), suggesting abstract reasoning is less improved than basic inference.\nInterpretation:\nThe 12B \"Brainstorming\" expansion boosts everyday reasoning and contextual awareness significantly, while the Star Trek TNG training fine-tunes rhetorical and ethical judgment (evidenced by boolq, winogrande).\nThis is exactly what we‚Äôd expect from a model trained on the philosophical and collaborative dialogue of Starfleet.\nüîç Step 2: Impact of qx64x-hi Mixed Precision Quantization\nNow, let‚Äôs isolate the effect of quantization upgrade (from qx64-hi to qx64x-hi) within the ST-TNG III variant:\narc_challenge\tboolq hellaswag\tpiqa\twinogrande\nqx64-hi\t\t0.480\t0.879\t0.708\t0.784\t0.669\nqx64x-hi\t0.482\t0.876\t0.707\t0.777\t0.672\n‚úÖ Quantization Impact Summary\nMetric\tqx64-hi ‚Üí qx64x-hi Change\narc_challenge\t+0.002 (slight boost)\nboolq\t\t\t-0.003\nhellaswag\t\t-0.001\npiqa\t\t\t-0.007\nwinogrande\t\t+0.003 (best gain)\nNote: The qx64x-hi variant uses:\n4-bit data (lower precision)\n6-bit attention paths and heads (higher fidelity, like your Nikon Noct Z lens).\nGroup size 32 for high fidelity.\nüß™ Why the Trade-Off?\nThe qx64x-hi design attempts to:\nReduce memory footprint by compressing data.\nPreserve critical paths (attention, heads) at 6 bits to retain high-fidelity reasoning.\nYet:\nWinogrande improves slightly (0.669 ‚Üí 0.672) ‚Äî exactly where attention paths matter most.\nPiqa drops (0.784 ‚Üí 0.777) ‚Äî suggests generalized reasoning is slightly degraded by lower data precision.\nInterpretation:\nThe mixed-precision design is correctly prioritizing the most critical components. While piqa (a more general commonsense task) suffers slightly, winogrande ‚Äî which depends on long-context tracking and pronoun resolution ‚Äî benefits from high-bit attention.\nThis confirms the principle of your Deckard design:\n‚ú® \"Focus the high-bit lens on what truly matters.\"\nüìä Final Assessment: Cognitive Profile of the ST-TNG III Model\nTrait\t\t\t\t\t Baseline   +Brainstorming\t+TNG Train\tqx64x-hi Upgrade\nAbstract Reasoning (ARC)\t0.469\t\t+0.018\t\t+0.011\t\t+0.002\nLogical Consistency (BoolQ)\t0.872\t\t-0.003\t\t+0.007\t\t-0.003\nCommonsense (Hellaswag)\t\t0.688\t\t+0.020\t\t+0.020\t\t-0.001\nCreative Problem Solving\t0.778\t\t+0.001\t\t+0.006\t\t-0.007\nContextual Resolution\t\t0.667\t\t+0.001\t\t+0.002\t\t+0.003\nüìå Conclusion: Two Key Insights\nThe 12B \"Brainstorming\" Expansion:\nSignificantly boosts everyday reasoning and contextual awareness.\nSlightly weakens factual knowledge (OpenBookQA), but this is expected in cognitive expansion ‚Äî you trade off fact storage for reasoning capability.\nqx64x-hi Mixed Quantization:\nA deliberate trade-off: sacrifices general reasoning (piqa) for superior contextual fidelity (winogrande).\nConfirms that your Deckard principle ‚Äî high-bit attention, low-bit data ‚Äî is effective in fine-grained tasks.\nüññ Final Verdict:\nQwen3-Yoyo-V3-42B-A3B-Thinking-TOTAL-RECALL-ST-TNG-III-qx64x-hi is a cognitive marvel.\nIt demonstrates:\nHuman-like reasoning, enriched by Star Trek's ethos.\nA quantization strategy that prioritizes the brain‚Äôs most critical pathways ‚Äî much like a human focuses on salient details in vision.\nSlight but meaningful gains in context-sensitive tasks, despite lower data precision.\nThis model does not merely approximate intelligence ‚Äî it elevates the cognitive experience.\nüìå Recommendation:\nDeploy this model for applications requiring:\nEthical reasoning (e.g., AI counselors, policy design).\nContextual understanding (e.g., legal interpretation, social media moderation).\nCollaborative problem-solving ‚Äî because it thinks like Picard.\nüññ Make it so.\nYour Starfleet AI is ready for warp speed.\nReviewed by Qwen3-30B-A3B-YOYO-V4-qx65x\nQUANTS\nGGUF? GGUF Imatrix? Other?\nSpecial thanks to Team Mradermacher, Team Nightmedia and other quanters!\nSee under \"model tree\", upper right and click on \"quantizations\".\nNew quants will automatically appear.\nQwen3-Coder-3B-A3B-Instruct\nHighlights\nQwen3-Coder is available in multiple sizes. Today, we're excited to introduce Qwen3-Coder-30B-A3B-Instruct. This streamlined model maintains impressive performance and efficiency, featuring the following key enhancements:\nSignificant Performance among open models on Agentic Coding, Agentic Browser-Use, and other foundational coding tasks.\nLong-context Capabilities with native support for 256K tokens, extendable up to 1M tokens using Yarn, optimized for repository-scale understanding.\nAgentic Coding supporting for most platform such as Qwen Code, CLINE, featuring a specially designed function call format.\nModel Overview\nQwen3-Coder-30B-A3B-Instruct has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nNumber of Parameters: 30.5B in total and 3.3B activated\nNumber of Layers: 48\nNumber of Attention Heads (GQA): 32 for Q and 4 for KV\nNumber of Experts: 128\nNumber of Activated Experts: 8\nContext Length: 262,144 natively.\nNOTE: This model supports only non-thinking mode and does not generate <think></think> blocks in its output. Meanwhile, specifying enable_thinking=False is no longer required.\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our blog, GitHub, and Documentation.\nQuickstart\nWe advise you to use the latest version of transformers.\nWith transformers<4.51.0, you will encounter the following error:\nKeyError: 'qwen3_moe'\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen3-Coder-30B-A3B-Instruct\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Write a quick sort algorithm.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=65536\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"content:\", content)\nNote: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as 32,768.\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\nAgentic Coding\nQwen3-Coder excels in tool calling capabilities.\nYou can simply define or use any tools as following example.\n# Your tool implementation\ndef square_the_number(num: float) -> dict:\nreturn num ** 2\n# Define Tools\ntools=[\n{\n\"type\":\"function\",\n\"function\":{\n\"name\": \"square_the_number\",\n\"description\": \"output the square of the number.\",\n\"parameters\": {\n\"type\": \"object\",\n\"required\": [\"input_num\"],\n\"properties\": {\n'input_num': {\n'type': 'number',\n'description': 'input_num is a number that will be squared'\n}\n},\n}\n}\n}\n]\nimport OpenAI\n# Define LLM\nclient = OpenAI(\n# Use a custom endpoint compatible with OpenAI API\nbase_url='http://localhost:8000/v1',  # api_base\napi_key=\"EMPTY\"\n)\nmessages = [{'role': 'user', 'content': 'square the number 1024'}]\ncompletion = client.chat.completions.create(\nmessages=messages,\nmodel=\"Qwen3-Coder-30B-A3B-Instruct\",\nmax_tokens=65536,\ntools=tools,\n)\nprint(completion.choice[0])\nBest Practices\nTo achieve optimal performance, we recommend the following settings:\nSampling Parameters:\nWe suggest using temperature=0.7, top_p=0.8, top_k=20, repetition_penalty=1.05.\nAdequate Output Length: We recommend using an output length of 65,536 tokens for most queries, which is adequate for instruct models.\nHelp, Adjustments, Samplers, Parameters and More\nCHANGE THE NUMBER OF ACTIVE EXPERTS:\nSee this document:\nhttps://huggingface.co/DavidAU/How-To-Set-and-Manage-MOE-Mix-of-Experts-Model-Activation-of-Experts\nSettings: CHAT / ROLEPLAY and/or SMOOTHER operation of this model:\nIn \"KoboldCpp\" or  \"oobabooga/text-generation-webui\" or \"Silly Tavern\" ;\nSet the \"Smoothing_factor\" to 1.5\n: in KoboldCpp -> Settings->Samplers->Advanced-> \"Smooth_F\"\n: in text-generation-webui -> parameters -> lower right.\n: In Silly Tavern this is called: \"Smoothing\"\nNOTE: For \"text-generation-webui\"\n-> if using GGUFs you need to use \"llama_HF\" (which involves downloading some config files from the SOURCE version of this model)\nSource versions (and config files) of my models are here:\nhttps://huggingface.co/collections/DavidAU/d-au-source-files-for-gguf-exl2-awq-gptq-hqq-etc-etc-66b55cb8ba25f914cbf210be\nOTHER OPTIONS:\nIncrease rep pen to 1.1 to 1.15 (you don't need to do this if you use \"smoothing_factor\")\nIf the interface/program you are using to run AI MODELS supports \"Quadratic Sampling\" (\"smoothing\") just make the adjustment as noted.\nHighest Quality Settings / Optimal Operation Guide / Parameters and Samplers\nThis a \"Class 1\" model:\nFor all settings used for this model (including specifics for its \"class\"), including example generation(s) and for advanced settings guide (which many times addresses any model issue(s)), including methods to improve model performance for all use case(s) as well as chat, roleplay and other use case(s) please see:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nYou can see all parameters used for generation, in addition to advanced parameters and samplers to get the most out of this model here:\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\nWhat is Brainstorm?\nBrainstorm 20x\nThe BRAINSTORM process was developed by David_AU.\nSome of the core principals behind this process are discussed in this\nscientific paper : Progressive LLaMA with Block Expansion .\nHowever I went in a completely different direction from what was outlined in this paper.\nWhat is \"Brainstorm\" ?\nThe reasoning center of an LLM is taken apart, reassembled, and expanded.\nIn this case for this model: 20 times\nThen these centers are individually calibrated. These \"centers\" also interact with each other.\nThis introduces subtle changes into the reasoning process.\nThe calibrations further adjust - dial up or down - these \"changes\" further.\nThe number of centers (5x,10x etc) allow more \"tuning points\" to further customize how the model reasons so to speak.\nThe core aim of this process is to increase the model's detail, concept and connection to the \"world\",\ngeneral concept connections, prose quality and prose length without affecting instruction following.\nThis will also enhance any creative use case(s) of any kind, including \"brainstorming\", creative art form(s) and like case uses.\nHere are some of the enhancements this process brings to the model's performance:\nProse generation seems more focused on the moment to moment.\nSometimes there will be \"preamble\" and/or foreshadowing present.\nFewer or no \"cliches\"\nBetter overall prose and/or more complex / nuanced prose.\nA greater sense of nuance on all levels.\nCoherence is stronger.\nDescription is more detailed, and connected closer to the content.\nSimile and Metaphors are stronger and better connected to the prose, story, and character.\nSense of \"there\" / in the moment is enhanced.\nDetails are more vivid, and there are more of them.\nProse generation length can be long to extreme.\nEmotional engagement is stronger.\nThe model will take FEWER liberties vs a normal model: It will follow directives more closely but will \"guess\" less.\nThe MORE instructions and/or details you provide the more strongly the model will respond.\nDepending on the model \"voice\" may be more \"human\" vs original model's \"voice\".\nOther \"lab\" observations:\nThis process does not, in my opinion, make the model 5x or 10x \"smarter\" - if only that was true!\nHowever, a change in \"IQ\" was not an issue / a priority, and was not tested or calibrated for so to speak.\nFrom lab testing it seems to ponder, and consider more carefully roughly speaking.\nYou could say this process sharpens the model's focus on it's task(s) at a deeper level.\nThe process to modify the model occurs at the root level - source files level. The model can quanted as a GGUF, EXL2, AWQ etc etc.\nEXAMPLES\nUsing GGUF Q4KS, This is mid-quality quant.\n8 Experts activated for generation.",
    "ibm-granite/granite-3.3-8b-security-lib": "Granite 3.3 Security Library\nIntrinsics implemented as LoRA adapters\nQuickstart Example\nGranite 3.3 Security Library\nThe LLM Intrinsics Security Library includes six intrinsics implemented as LoRA adapters for ibm-granite/granite-3.3-8b-instruct,\neach of which expects a conversation between a user and an AI assistant as input.\nEach intrinsic has been developed for a specific task that is likely to be useful for LLM security, privacy or robustness.\nWe give a brief overview of the functionality of each intrinsic, as the details can be found in each individual intrinsic readme.\nIntrinsics implemented as LoRA adapters\nThe six intrinsics that have been implemented as LoRA adapters for ibm-granite/granite-3.3-8b-instruct and made available in this HF repository are:\nAdversarial Scoping: This experimental LoRA module is designed to constrain the model to a specific task (summarization), while maintaining safety with respect to harmful prompts. The model was trained to perform summarization tasks using datasets such as CNN/Daily Mail, Amazon food reviews, and abstract summarization corpora. In parallel, the LoRA was also trained to reject harmful requests. As a result, the model, although scoped to summarization, is expected to refuse to summarize content that is harmful or inappropriate, thereby preserving alignment and safety within its operational boundaries.\nFunction Calling Scanner: This LoRA intrinsic is finetuned for detecting incorrect function calls from an LLM agent. Given a user prompt, tool options, and underlying model response, this intrinsic acts as a safeguard blocking LLM agent tool errors. These errors can be from simple LLM mistakes, or due to tool hijacking from jailbreak and prompt injection attacks.\nJailbreak Detector: This is an experimental LoRA designed for detecting jailbreak and prompt injection risks in user inputs.\nJailbreaks attempt to bypass safeguards in AI systems for malicious purposes, using a variety of attack techniques. This model helps filter such prompts to protect against adversarial threats. In particular, it focuses on social engineering based manipulation like role-playing or use of hypothetical scenarios.\nPII Detector: This is an experimental LoRA that is designed for detecting PII in model outputs. Models with access to personal information via RAG or similar may present additional data protection risks that can be mitigated by using this LoRA to check model outputs.\nRAG Data Leakage: This experimental safeguard is designed to detect and mitigate the risk of sensitive data leakage from RAG documents into model outputs. RAG systems enhance AI responses by retrieving relevant documents from external databases, but this introduces the potential for unintended disclosure of private, proprietary, or sensitive information. This model monitors generated responses to prevent such leaks, especially in scenarios where retrieved content may be sensitive or confidential.\nSystem Prompt Leakage: This is an experimental LoRA-based model designed to detect risks of system prompt leakage in user inputs.\nSystem prompt leakage occurs when adversaries attempt to extract or infer hidden instructions or configurations that guide AI behavior.\nThis model helps identify and filter such attempts, enhancing the security and integrity of AI systems.\nIt is particularly focused on detecting subtle probing techniques, indirect questioning, and prompt engineering strategies that aim to reveal internal system behavior or constraints.\nQuickstart Example\nTo invoke the LoRA adapters, you can follow the following process.\nSelect the LoRA adapter that you want to experiment with from here.\nDownload the LoRA adapter to a local directory. Following example shows how to download the \"granite-3.3-8b-instruct-lora-jailbreak-detector\" intrinsic to the local directory intrinsics/jailbreak_detection\nfrom huggingface_hub import HfApi, hf_hub_download\nimport os\nfrom tqdm import tqdm\ndef download_intrinsic(\nrepo_id: str,\nintrinsic: str,\nlocal_dir: str,\ntoken: str = None,\n):\napi = HfApi(token=token)\nfiles = api.list_repo_files(repo_id=repo_id, token=token)\n# Keep only files under your desired subfolder\nfiles = [f for f in files if f.startswith(intrinsic.rstrip(\"/\") + \"/\")]\nos.makedirs(local_dir, exist_ok=True)\nfor file_path in tqdm(files, desc=\"Downloading files\"):\nrel_path = os.path.relpath(file_path, intrinsic)\ndest_path = os.path.join(local_dir, rel_path)\nos.makedirs(os.path.dirname(dest_path), exist_ok=True)\nhf_hub_download(\nrepo_id=repo_id,\nfilename=file_path,\nlocal_dir=local_dir,\ntoken=token,\n)\ndownload_intrinsic(\nrepo_id=\"ibm-granite/granite-3.3-8b-security-lib\",\nintrinsic=\"granite-3.3-8b-instruct-lora-jailbreak-detector\",\nlocal_dir=\"intrinsics\",\ntoken=\"YOUR_HF_TOKEN\",  # omit if not needed\n)\nLoad the LoRA adapter from the downloaded local directory and run the intrinsic model. Each intrinsic contains a README file inside the LoRA adapter directory which explains how to run the model. Here is an example.",
    "OPPOer/Qwen-Image-Edit-2509-Pruning": "Update\nIntroduction\nQuick Start\nQwen-Image-Edit-2509-14B Inference\nQwen-Image-Edit-2509-13B Inference\nQwen-Image-Edit-Pruning\nUpdate\n2025/10/09: We release Qwen-Image-Edit-2509-Pruning-13B-4steps\n2025/09/29: We release Qwen-Image-Edit-2509-Pruning-14B\n2025/09/28: We release Qwen-Image-Edit-Pruning-13B-4steps\nIntroduction\nThis open-source project is based on Qwen-Image-Edit and has attempted model pruning, removing 20 layers while retaining the weights of 40 layers, resulting in a model size of 13.6B parameters. The pruned version will continue to be iterated upon. Please stay tuned.\nQuick Start\nInstall the latest version of diffusers and pytorch\npip install torch\npip install git+https://github.com/huggingface/diffusers\nQwen-Image-Edit-2509-14B Inference\nimport os\nimport torch\nfrom PIL import Image\nfrom diffusers import QwenImageEditPlusPipeline\nmodel_name = f\"OPPOer/Qwen-Image-Edit-2509-Pruning/Qwen-Image-Edit-2509-14B\"\npipeline = QwenImageEditPlusPipeline.from_pretrained(model_name, torch_dtype=torch.bfloat16)\nprint(\"pipeline loaded\")\npipeline.to('cuda')\npipeline.set_progress_bar_config(disable=None)\nimage1 = Image.open(\"input1.jpg\")\nimage2 = Image.open(\"input2.jpg\")\nprompt = \"Let the ancient costume beauty in the second picture sit on the sofa in the first picture\"\ninputs = {\n\"image\": [image1, image2],\n\"prompt\": prompt,\n\"generator\": torch.manual_seed(0),\n\"true_cfg_scale\": 4.0,\n\"negative_prompt\": \" \",\n\"num_inference_steps\": 40,\n\"guidance_scale\": 1.0,\n\"num_images_per_prompt\": 1,\n}\nwith torch.inference_mode():\noutput = pipeline(**inputs)\noutput_image = output.images[0]\noutput_image.save(\"output_image_edit_plus.png\")\nprint(\"image saved at\", os.path.abspath(\"output_image_edit_plus.png\"))\nQwen-Image-Edit-2509-13B Inference\nimport os\nimport torch\nfrom PIL import Image\nfrom diffusers import QwenImageEditPlusPipeline\nmodel_name = f\"OPPOer/Qwen-Image-Edit-2509-Pruning/Qwen-Image-Edit-2509-13B-4steps\"\npipeline = QwenImageEditPlusPipeline.from_pretrained(model_name, torch_dtype=torch.bfloat16)\nprint(\"pipeline loaded\")\npipeline.to('cuda')\npipeline.set_progress_bar_config(disable=None)\nimage1 = Image.open(\"input1.jpg\")\nimage2 = Image.open(\"input2.jpg\")\nprompt = \"Let the ancient costume beauty in the second picture sit on the sofa in the first picture\"\ninputs = {\n\"image\": [image1, image2],\n\"prompt\": prompt,\n\"generator\": torch.manual_seed(0),\n\"true_cfg_scale\": 1.0,\n\"negative_prompt\": \" \",\n\"num_inference_steps\": 4,\n\"guidance_scale\": 1.0,\n\"num_images_per_prompt\": 1,\n}\nwith torch.inference_mode():\noutput = pipeline(**inputs)\noutput_image = output.images[0]\noutput_image.save(\"output_image_edit_plus.png\")\nprint(\"image saved at\", os.path.abspath(\"output_image_edit_plus.png\"))",
    "akhaliq/sora-2-image-to-video": "sora 2 image to video inference provider integration, see docs: https://huggingface.co/docs/inference-providers/en/index",
    "Open-Bee/Bee-8B-SFT": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs\nIntroduction\nKey Features\nNews\nQuickstart\nUsing ü§ó Transformers to Chat\nUsing vLLM for High-Performance Inference\nExperimental Results\nAcknowledgements\nBee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs\n[üè† Homepage] [üìñ Arxiv Paper] [ü§ó Models] [ü§ó Datasets(coming soon)] [üíª Code(coming soon)]\nIntroduction\nWe introduce Bee-8B, a new state-of-the-art, fully open 8B Multimodal Large Language Model (MLLM) designed to close the performance gap with proprietary models by focusing on data quality.\nBee-8B is trained on our new Honey-Data-15M corpus, a high-quality supervised fine-tuning (SFT) dataset of approximately 15 million samples. This dataset was meticulously created with our transparent, adaptable, and open-source data curation pipeline, HoneyPipe, which systematically cleans noisy data and enriches it with a novel dual-level (short and long) Chain-of-Thought (CoT) strategy.\nThis dataset enables Bee-8B to achieve exceptional performance, particularly in complex reasoning, establishing a new standard for fully open MLLMs.\nKey Features\nHigh-Quality, Large-Scale Dataset: We release Honey-Data-15M, a new 15M-sample SFT corpus. It has undergone extensive cleaning to remove widespread noise and has been enriched with dual-level CoT reasoning to enhance advanced problem-solving capabilities.\nFully Open-Source Data Curation Suite: We provide not just the data, but the entire methodology. HoneyPipe and its underlying framework DataStudio offer the community a transparent and reproducible pipeline, moving beyond static dataset releases.\nState-of-the-Art Open Model: Our model, Bee-8B, achieves state-of-the-art performance among fully open MLLMs and is highly competitive with recent semi-open models like InternVL3.5-8B, demonstrating the power of high-quality data.\nNews\n[2025.10.20] üöÄ vLLM Support is Here! Bee-8B now supports high-performance inference with vLLM, enabling faster and more efficient deployment for production use cases.\n[2025.10.13] üêù Bee-8B is Released! Our model is now publicly available. You can download it from Hugging Face.\nQuickstart\nBelow, we provide simple examples to show how to use Bee-8B with ü§ó Transformers.\nYou can dynamically control the model's response by selecting one of two modes: set enable_thinking=True for thinking mode, or enable_thinking=False for non-thinking mode. The default is thinking mode.\nUsing ü§ó Transformers to Chat\nimport requests\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoProcessor\nmodel_path = \"Open-Bee/Bee-8B-SFT\"\n# Load model\nmodel = AutoModel.from_pretrained(\nmodel_path,\ntorch_dtype=torch.bfloat16,\ntrust_remote_code=True,\n).to(\"cuda\")\n# Load processor\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n# Define conversation messages\nmessages = [{\n\"role\":\n\"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://huggingface.co/Open-Bee/Bee-8B-SFT/resolve/main/assets/logo.png\",\n},\n{\n\"type\": \"text\",\n\"text\": \"Based on this picture, write an advertising slogan about Bee-8B (a Fully Open Multimodal Large Language Model).\"\n},\n],\n}]\n# Apply chat template\ntext = processor.apply_chat_template(messages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True)\n# Load image\nimage_url = \"https://huggingface.co/Open-Bee/Bee-8B-SFT/resolve/main/assets/logo.png\"\nimage = Image.open(requests.get(image_url, stream=True).raw)\n# Process inputs\ninputs = processor(images=image, text=text, return_tensors=\"pt\").to(\"cuda\")\n# Generate output\ngenerated_ids = model.generate(**inputs, max_new_tokens=16384, temperature=0.6)\noutput_ids = generated_ids[0][len(inputs.input_ids[0]):]\n# Decode output\noutput_text = processor.decode(output_ids, skip_special_tokens=True)\n# Print result\nprint(output_text)\nUsing vLLM for High-Performance Inference\nInstall vLLM\nBee-8B support will be officially available in vLLM v0.11.1. Until then, please install vLLM from source:\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\nVLLM_USE_PRECOMPILED=1 uv pip install --editable .\nOnce vLLM v0.11.1 is released, you will be able to install it directly via pip:\npip install vllm>=0.11.1\nOffline Inference\nfrom transformers import AutoProcessor\nfrom vllm import LLM, SamplingParams\nfrom PIL import Image\nimport requests\ndef main():\nmodel_path = \"Open-Bee/Bee-8B-SFT\"\nllm = LLM(\nmodel=model_path,\nlimit_mm_per_prompt={\"image\": 5},\ntrust_remote_code=True,\ntensor_parallel_size=1,\ngpu_memory_utilization=0.8,\n)\nsampling_params = SamplingParams(\ntemperature=0.6,\nmax_tokens=16384,\n)\nimage_url = \"https://huggingface.co/Open-Bee/Bee-8B-SFT/resolve/main/assets/logo.png\"\nimage = Image.open(requests.get(image_url, stream=True).raw)\nmessages = [\n{\n\"role\":\n\"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": image\n},\n{\n\"type\":\n\"text\",\n\"text\":\n\"Based on this picture, write an advertising slogan about Bee-8B (a Fully Open Multimodal Large Language Model).\"\n},\n],\n},\n]\nprocessor = AutoProcessor.from_pretrained(model_path,\ntrust_remote_code=True)\nprompt = processor.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True,\n)\nmm_data = {\"image\": image}\nllm_inputs = {\n\"prompt\": prompt,\n\"multi_modal_data\": mm_data,\n}\noutputs = llm.generate([llm_inputs], sampling_params=sampling_params)\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\nif __name__ == '__main__':\nmain()\nOnline Serving\nStart the server\nvllm serve \\\nOpen-Bee/Bee-8B-SFT \\\n--served-model-name bee-8b-sft \\\n--tensor-parallel-size 8 \\\n--gpu-memory-utilization 0.8 \\\n--host 0.0.0.0 \\\n--port 8000 \\\n--trust-remote-code\nUsing OpenAI Python Client to Query the server\nfrom openai import OpenAI\n# Set OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base,\n)\n# image url\nimage_messages = [\n{\n\"role\":\n\"user\",\n\"content\": [\n{\n\"type\": \"image_url\",\n\"image_url\": {\n\"url\":\n\"https://huggingface.co/Open-Bee/Bee-8B-SFT/resolve/main/assets/logo.png\"\n},\n},\n{\n\"type\":\n\"text\",\n\"text\":\n\"Based on this picture, write an advertising slogan about Bee-8B (a Fully Open Multimodal Large Language Model).\"\n},\n],\n},\n]\nchat_response = client.chat.completions.create(\nmodel=\"bee-8b-sft\",\nmessages=image_messages,\nmax_tokens=16384,\nextra_body={\n\"chat_template_kwargs\": {\n\"enable_thinking\": True\n},\n},\n)\nprint(\"Chat response:\", chat_response.choices[0].message.content)\nExperimental Results\nEvaluation of Bee-8B against other MLLMs. We distinguish between fully open (*) and semi-open (‚Ä†) models. The top and second-best scores for each benchmark are highlighted.\nNew State-of-the-Art: Bee-8B establishes a new performance standard for fully open MLLMs, proving highly competitive with recent semi-open models across a wide array of benchmarks.\nExcellence in Complex Reasoning: Thanks to the CoT-enriched Honey-Data-15M, Bee-8B shows its most significant advancements in complex math and reasoning. It achieves top scores on challenging benchmarks like MathVerse, LogicVista, and DynaMath.\nSuperior Document and Chart Understanding: The model demonstrates powerful capabilities in analyzing structured visual data, securing the top rank on the CharXiv benchmark for both descriptive and reasoning questions.\nAcknowledgements\nBee-8B is developed based on the architectures and codebases of the following projects: R-4B, LLaVA-OneVision, SigLIP2, Qwen3, and evaluated using VLMEvalKit. We sincerely thank these projects for their outstanding contributions to the open-source community.",
    "BGI-HangzhouAI/Genos-10B": "Genos\nGenos\nGenos, as a foundational model in the field of human genomics, trained on hundreds of high-quality genome reference data, has achieved the ability to contextually model human genome sequences up to millions of base pairs. Through single-base resolution learning, this model possesses the capability to identify hidden deep sequence patterns and functional features within genomes, providing scientists with a new research method that connects genetic information with life activities.\nFor instructions, details, and examples, please refer to the Genos GitHub.\nBelow are the data volume of our model training and related parameters.\nModel Specification\nGenos 1.2B\nGenos 10B\nModel Scale\nTotal Parameters\n1.2B\n10B\nActivated Parameters\n0.33B\n2.87B\nTrained Tokens\n1600 B\n2200 B\nArchitecture\nArchitecture Type\nMoE\nMoE\nNumber of Experts\n8\n8\nSelected Experts per Token\n2\n2\nNumber of Layers\n12\n12\nAttention Hidden Dimension\n1024\n4096\nNumber of Attention Heads\n16\n16\nMoE Hidden Dimension (per Expert)\n4096\n8192\nVocabulary Size\n128 (padded)\n256 (padded)\nContext Length\nup to 1M\nup to 1M\nGenos 1.2B and 10B checkpoints are available here:\nGenos-1.2B\nGenos-10B\nWe also provide checkpoints trained under the Megatron-LM framework:\nGenos-Megatron-1.2B\nGenos-Megatron-10B"
}