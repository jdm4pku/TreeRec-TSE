{
    "apple/coreml-sam2.1-tiny": "SAM 2.1 Tiny Core ML\nDownload\nCitation\nSAM 2.1 Tiny Core ML\nSAM 2 (Segment Anything in Images and Videos), is a collection of foundation models from FAIR that aim to solve promptable visual segmentation in images and videos. See the SAM 2 paper for more information.\nThis is the Core ML version of SAM 2.1 Tiny, and is suitable for use with the SAM2 Studio demo app. It was converted in float16 precision using this fork of the original code repository.\nDownload\nInstall huggingface-cli\nbrew install huggingface-cli\nhuggingface-cli download --local-dir models apple/coreml-sam2.1-tiny\nCitation\nTo cite the paper, model, or software, please use the below:\n@article{ravi2024sam2,\ntitle={SAM 2: Segment Anything in Images and Videos},\nauthor={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\\\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\\'a}r, Piotr and Feichtenhofer, Christoph},\njournal={arXiv preprint arXiv:2408.00714},\nurl={https://arxiv.org/abs/2408.00714},\nyear={2024}\n}",
    "TheDrummer/Tiger-Gemma-9B-v3": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nTiger Gemma 9B v3\nTiger Gemma 9B v3\nDecensored Gemma 9B SPPO with a new dataset that removes the yapping and evil.\nGGUF: https://huggingface.co/TheDrummer/Tiger-Gemma-9B-v3-GGUF\nTiger lives on üòπ\nUpdate on his orange friend, now named Didi:\nI rescued Didi shortly before Tiger died from a car accident (they were both strays). He was lethargic and suffering from major respiratory problems. It has been 4 months since that fateful day and Didi is healthy and happier than ever. May Tiger live on through him!",
    "facebook/MEXMA": "Usage\nLicense\nTraining code\nPaper\nCitation\nCurrent pre-trained cross-lingual sentence encoders approaches use sentence-level objectives only. This can lead to loss of information, especially for tokens, which then degrades the sentence representation. We propose MEXMA, a novel approach that integrates both sentence-level and token-level objectives. The sentence representation in one language is used to predict masked tokens in another language, with both the sentence representation and all tokens directly updating the encoder. We show that adding token-level objectives greatly improves the sentence representation quality across several tasks. Our approach outperforms current pre-trained cross-lingual sentence encoders on bi-text mining as well as several downstream tasks. We also analyse the information encoded in our tokens, and how the sentence representation is built from them.\nUsage\nYou use this model as you would any other XLM-RoBERTa model, taking into account that the \"pooler\" has not been trained, so you should use the CLS the encoder outputs directly as your sentence representation:\nfrom transformers import AutoTokenizer, XLMRobertaModel\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/MEXMA\")\nmodel = XLMRobertaModel.from_pretrained(\"facebook/MEXMA\", add_pooling_layer=False)\nexample_sentences = ['Sentence1', 'Sentence2']\nexample_inputs = tokenizer(example_sentences, return_tensors='pt')\noutputs = model(**example_inputs)\nsentence_representation = outputs.last_hidden_state[:, 0]\nprint(sentence_representation.shape) # torch.Size([2, 1024])\nYou can also use this model with SentenceTransformers:\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"facebook/MEXMA\")\nexample_sentences = ['Sentence1', 'Sentence2']\nsentence_representation = model.encode(example_sentences)\nprint(sentence_representation.shape) # torch.Size([2, 1024])\nLicense\nThis model is released under the MIT license.\nTraining code\nFor the training code of this model, please check the official MEXMA repo.\nPaper\nMEXMA: Token-level objectives improve sentence representations\nCitation\nIf you use this model in your work, please cite:\n@inproceedings{janeiro-etal-2025-mexma,\ntitle = \"{MEXMA}: Token-level objectives improve sentence representations\",\nauthor = \"Janeiro, Jo{\\~a}o Maria  and\nPiwowarski, Benjamin  and\nGallinari, Patrick  and\nBarrault, Loic\",\neditor = \"Che, Wanxiang  and\nNabende, Joyce  and\nShutova, Ekaterina  and\nPilehvar, Mohammad Taher\",\nbooktitle = \"Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\nmonth = jul,\nyear = \"2025\",\naddress = \"Vienna, Austria\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://aclanthology.org/2025.acl-long.1168/\",\ndoi = \"10.18653/v1/2025.acl-long.1168\",\npages = \"23960--23995\",\nISBN = \"979-8-89176-251-0\",\nabstract = \"Cross-lingual sentence encoders (CLSE) create fixed-size sentence representations with aligned translations. Current pre-trained CLSE approaches use sentence-level objectives only. This can lead to loss of information, especially for tokens, which then degrades the sentence representation. We propose MEXMA, a novel approach that integrates both sentence-level and token-level objectives. The sentence representation in one language is used to predict masked tokens in another language, with both the sentence representation and *all tokens directly update the encoder*. We show that adding token-level objectives greatly improves the sentence representation quality across several tasks. Our approach outperforms current pre-trained cross-lingual sentence encoders on bitext mining as well as several downstream tasks. We also analyse the information encoded in our tokens, and how the sentence representation is built from them.\"\n}",
    "ZhengPeng7/BiRefNet-matting": "This repo holds the official weights of BiRefNet for general matting.\nTraining Sets:\nValidation Sets:\nPerformance:\nAcknowledgement:\nCitation\nBilateral Reference for High-Resolution Dichotomous Image Segmentation\nPeng Zheng 1,4,5,6,\nDehong Gao 2,\nDeng-Ping Fan 1*,\nLi Liu 3,\nJorma Laaksonen 4,\nWanli Ouyang 5,\nNicu Sebe 6\n1 Nankai University‚ÄÇ  2 Northwestern Polytechnical University‚ÄÇ  3 National University of Defense Technology‚ÄÇ 4 Aalto University‚ÄÇ  5 Shanghai AI Laboratory‚ÄÇ  6 University of Trento\nThis repo holds the official weights of BiRefNet for general matting.\nTraining Sets:\nP3M-10k (except TE-P3M-500-NP)\nTR-humans\nAM-2k\nAIM-500\nHuman-2k (synthesized with BG-20k)\nDistinctions-646 (synthesized with BG-20k)\nHIM2K\nPPM-100\nValidation Sets:\nTE-P3M-500-NP\nPerformance:\nDataset\nMethod\nSmeasure\nmaxFm\nmeanEm\nMSE\nmaxEm\nmeanFm\nwFmeasure\nadpEm\nadpFm\nHCE\nmBA\nmaxBIoU\nmeanBIoU\nTE-P3M-500-NP\nBiRefNet-matting--epoch_100\n.979\n.996\n.988\n.003\n.997\n.986\n.988\n.864\n.885\n.000\n.830\n.940\n.888\nCheck the main BiRefNet model repo for more info and how to use it:https://huggingface.co/ZhengPeng7/BiRefNet/blob/main/README.md\nAlso check the GitHub repo of BiRefNet for all things you may want:https://github.com/ZhengPeng7/BiRefNet\nAcknowledgement:\nMany thanks to @freepik for their generous support on GPU resources for training this model!\nCitation\n@article{zheng2024birefnet,\ntitle={Bilateral Reference for High-Resolution Dichotomous Image Segmentation},\nauthor={Zheng, Peng and Gao, Dehong and Fan, Deng-Ping and Liu, Li and Laaksonen, Jorma and Ouyang, Wanli and Sebe, Nicu},\njournal={CAAI Artificial Intelligence Research},\nvolume = {3},\npages = {9150038},\nyear={2024}\n}",
    "Lewdiculous/Llama-3.1-8B-ArliAI-Formax-v1.0-GGUF-IQ-ARM-Imatrix": "My quants for ArliAI/Llama-3.1-8B-ArliAI-Formax-v1.0.\n\"Formax is a model that specializes in following response format instructions. Tell it the format of it's response and it will follow it perfectly. Great for data processing and dataset creation tasks.\"\n\"It is also a highly uncensored model that will follow your instructions very well.\"\nChatML prompt format.",
    "MiniLLM/MiniPLM-Qwen-200M": "MinPLM-Qwen-200M\nEvaluation\nBaseline Models\nCitation\nMinPLM-Qwen-200M\npaper | code\nMiniPLM-Qwen-200M is a 200M model with Qwen achitecture pre-trained from scratch on the Pile using the MiniPLM knowledge distillation framework with the offcial Qwen1.5-1.8B as the teacher model.\nWe also open-source the pre-training corpus refined by Difference Sampling in MiniPLM for reproducibility.\nEvaluation\nMiniPLM models achieves better performance given the same computation and scales well across model sizes:\nBaseline Models\nConventional Pre-Training\nVanillaKD\nCitation\n@article{miniplm,\ntitle={MiniPLM: Knowledge Distillation for Pre-Training Language Models},\nauthor={Yuxian Gu and Hao Zhou and Fandong Meng and Jie Zhou and Minlie Huang},\njournal={arXiv preprint arXiv:2410.17215},\nyear={2024}\n}",
    "CohereLabs/aya-expanse-32b": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nBy submitting this form, you agree to the License Agreement  and acknowledge that the information you provide will be collected, used, and shared in accordance with Cohere‚Äôs Privacy Policy. You‚Äôll receive email updates about Cohere Labs and Cohere research, events, products and services. You can unsubscribe at any time.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Card for Aya-Expanse-32B\nModel Details\nEvaluation\nWhatsapp Integration\nModel Card Contact\nTerms of Use\nCite\nModel Card for Aya-Expanse-32B\nAya Expanse 32B is an open-weight research release of a model with highly advanced multilingual capabilities. It focuses on pairing a highly performant pre-trained Command family of models with the result of a year‚Äôs dedicated research from Cohere Labs, including data arbitrage, multilingual preference training, safety tuning, and model merging. The result is a powerful multilingual large language model serving 23 languages.\nThis model card corresponds to the 32-billion version of the Aya Expanse model. We also released an 8-billion version which you can find here.\nDeveloped by: Cohere Labs\nPoint of Contact: Cohere Labs\nLicense: CC-BY-NC, requires also adhering to Cohere Lab's Acceptable Use Policy\nModel: Aya Expanse 32B\nModel Size: 32 billion parameters\nSupported Languages\nWe cover 23 languages: Arabic, Chinese (simplified & traditional), Czech, Dutch, English, French, German, Greek, Hebrew, Hebrew, Hindi, Indonesian, Italian, Japanese, Korean, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Turkish, Ukrainian, and Vietnamese.\nTry it: Aya Expanse in Action\nUse the Cohere playground or our Hugging Face Space for interactive exploration.\nHow to Use Aya Expanse\nInstall the transformers library and load Aya Expanse 32B as follows:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_id = \"CohereLabs/aya-expanse-32b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n# Format message with the chat template\nmessages = [{\"role\": \"user\", \"content\": \"Anneme onu ne kadar sevdiƒüimi anlatan bir mektup yaz\"}]\ninput_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Anneme onu ne kadar sevdiƒüimi anlatan bir mektup yaz<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\ngen_tokens = model.generate(\ninput_ids,\nmax_new_tokens=100,\ndo_sample=True,\ntemperature=0.3,\n)\ngen_text = tokenizer.decode(gen_tokens[0])\nprint(gen_text)\nExample Notebooks\nFine-Tuning:\nDetailed Fine-Tuning Notebook.\nCommunity-Contributed Use Cases::\nThe following notebooks contributed by Cohere Labs Community members show how Aya Expanse can be used for different use cases:\nMulitlingual Writing Assistant\nAyaMCooking\nMultilingual Question-Answering System\nModel Details\nInput: Models input text only.\nOutput: Models generate text only.\nModel Architecture: Aya Expanse 32B is an auto-regressive language model that uses an optimized transformer architecture. Post-training includes supervised finetuning, preference training, and model merging.\nLanguages covered: The model is particularly optimized for multilinguality and supports the following languages: Arabic, Chinese (simplified & traditional), Czech, Dutch, English, French, German, Greek, Hebrew, Hindi, Indonesian, Italian, Japanese, Korean, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Turkish, Ukrainian, and Vietnamese\nContext length: 128K\nEvaluation\nWe evaluated Aya Expanse 32B against Gemma 2 27B, Llama 3.1 70B, Mixtral 8x22B, and Qwen 2.5 32B using the dolly_human_edited subset from the Aya Evaluation Suite dataset and m-ArenaHard, a dataset based on the Arena-Hard-Auto dataset and translated to the 23 languages we support in Aya Expanse. Win-rates were determined using gpt-4o-2024-08-06 as a judge. For a conservative benchmark, we report results from gpt-4o-2024-08-06, though gpt-4o-mini scores showed even stronger performance.\nThe m-ArenaHard dataset, used to evaluate Aya Expanse‚Äôs capabilities, is publicly available here.\nWhatsapp Integration\nYou can also talk to Aya Expanse through the popular messaging service WhatsApp. Use this link to open a WhatsApp chatbox with Aya Expanse.\nIf you don‚Äôt have WhatsApp downloaded on your machine you might need to do that, or, if you have it on your phone, you can follow the on-screen instructions to link your phone and WhatsApp Web.\nBy the end, you should see a text window which you can use to chat with the model.\nMore details about our whatsapp integration are available here.\nModel Card Contact\nFor errors or additional questions about details in this model card, contact labs@cohere.com\nTerms of Use\nWe hope that the release of this model will make community-based research efforts more accessible, by releasing the weights of a highly performant multilingual model to researchers all over the world. This model is governed by a CC-BY-NC, requires also adhering to Cohere Lab's Acceptable Use Policy\nCite\nYou can cite Aya Expanse using:\n@misc{dang2024ayaexpansecombiningresearch,\ntitle={Aya Expanse: Combining Research Breakthroughs for a New Multilingual Frontier},\nauthor={John Dang and Shivalika Singh and Daniel D'souza and Arash Ahmadian and Alejandro Salamanca and Madeline Smith and Aidan Peppin and Sungjin Hong and Manoj Govindassamy and Terrence Zhao and Sandra Kublik and Meor Amer and Viraat Aryabumi and Jon Ander Campos and Yi-Chern Tan and Tom Kocmi and Florian Strub and Nathan Grinsztajn and Yannis Flet-Berliac and Acyr Locatelli and Hangyu Lin and Dwarak Talupuru and Bharat Venkitesh and David Cairuz and Bowen Yang and Tim Chung and Wei-Yin Ko and Sylvie Shang Shi and Amir Shukayev and Sammie Bae and Aleksandra Piktus and Roman Castagn√© and Felipe Cruz-Salinas and Eddie Kim and Lucas Crawhall-Stein and Adrien Morisot and Sudip Roy and Phil Blunsom and Ivan Zhang and Aidan Gomez and Nick Frosst and Marzieh Fadaee and Beyza Ermis and Ahmet √úst√ºn and Sara Hooker},\nyear={2024},\neprint={2412.04261},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2412.04261},\n}",
    "DeZoomer/AngelinaJolie-FLuxLora": "Angelina Jolie | Flux\nModel description\nBackground\nDownload model\nAngelina Jolie | Flux\nPrompt\n-\nPrompt\n-\nPrompt\n-\nPrompt\n-\nPrompt\n-\nPrompt\n-\nModel description\nTrained locally with 20 publicly accessible images using AI-Toolkit (Flux.1 Dev).\nUse with LoRA strength between 0.8-1.2 and FluxGuidance between 3-4. No keywords needed.\nExample prompt (ComfyUI): Portrait photo of a woman in a garden.\nWant a custom/private LoRA? Good news‚Äîcommissions are open! Request yours here: https://ko-fi.com/de_zoomer/commissions.\nBackground\nI've been deeply exploring how to create LoRAs with 100% accuracy to the original character. My focus is on quality, which is why my files tend to be heavier than others.\nAfter creating over 100+ LoRAs for testing, using both Kohya and AI-Toolkit since day one, I've consistently stayed up to date with the latest releases, exchanging knowledge in their communities.\nMy expertise is mainly with characters, so I‚Äôm not as familiar with LoRAs for style or anime, although the process might not differ too much.\nIf you want your own custom LoRa, feel free to message me! Commissions are open‚Äîcheck out my Ko-fi link above.\nEnjoy using my LoRAs and have fun!\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.",
    "DeZoomer/ScarlettJohansson-FluxLora": "Scarlett Johansson | Flux\nModel description\nBackground\nDownload model\nScarlett Johansson | Flux\nPrompt\n-\nPrompt\n-\nPrompt\n-\nPrompt\n-\nPrompt\n-\nPrompt\n-\nModel description\nTrained locally with 20 publicly accessible images using AI-Toolkit (Flux.1 Dev).\nUse with LoRA strength between 0.8-1.2 and FluxGuidance between 3-4. No keywords needed.\nExample prompt (ComfyUI): Portrait photo of a woman in a garden.\nWant a custom/private LoRA? Good news‚Äîcommissions are open! Request yours here: https://ko-fi.com/de_zoomer/commissions.\nBackground\nI've been deeply exploring how to create LoRAs with 100% accuracy to the original character. My focus is on quality, which is why my files tend to be heavier than others.\nAfter creating over 100+ LoRAs for testing, using both Kohya and AI-Toolkit since day one, I've consistently stayed up to date with the latest releases, exchanging knowledge in their communities.\nMy expertise is mainly with characters, so I‚Äôm not as familiar with LoRAs for style or anime, although the process might not differ too much.\nIf you want your own custom LoRa, feel free to message me! Commissions are open‚Äîcheck out my Ko-fi link above.\nEnjoy using my LoRAs and have fun!\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.",
    "stabilityai/stable-diffusion-3.5-medium": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nBy clicking \"Agree\", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nStable Diffusion 3.5 Medium\nModel\nModel Description\nLicense\nModel Sources\nImplementation Details\nUsage & Limitations\nModel Performance\nFile Structure\nUsing with Diffusers\nQuantizing the model with diffusers\nFine-tuning\nUses\nIntended Uses\nOut-of-Scope Uses\nSafety\nIntegrity Evaluation\nRisks identified and mitigations:\nContact\nStable Diffusion 3.5 Medium\nModel\nStable Diffusion 3.5 Medium is a Multimodal Diffusion Transformer with improvements (MMDiT-X) text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency.\nPlease note: This model is released under the Stability Community License. Visit Stability AI to learn or contact us for commercial licensing details.\nModel Description\nDeveloped by: Stability AI\nModel type: MMDiT-X text-to-image generative model\nModel Description:  This model generates images based on text prompts. It is a Multimodal Diffusion Transformer\n(https://arxiv.org/abs/2403.03206) with improvements that use three fixed, pretrained text encoders, with QK-normalization to improve training stability, and dual attention blocks in the first 12 transformer layers.\nLicense\nCommunity License:  Free for research, non-commercial, and commercial use for organizations or individuals with less than $1M in total annual revenue. More details can be found in the Community License Agreement. Read more at https://stability.ai/license.\nFor individuals and organizations with annual revenue above $1M: please contact us to get an Enterprise License.\nModel Sources\nFor local or self-hosted use, we recommend ComfyUI for node-based UI inference, or diffusers or GitHub for programmatic use.\nComfyUI: Github, Example Workflow\nHuggingface Space: Space\nDiffusers: See below.\nGitHub: GitHub.\nAPI Endpoints:\nStability AI API\nImplementation Details\nMMDiT-X: Introduces self-attention modules in the first 13 layers of the transformer, enhancing multi-resolution generation and overall image coherence.\nQK Normalization: Implements the QK normalization technique to improve training Stability.\nMixed-Resolution Training:\nProgressive training stages: 256 ‚Üí 512 ‚Üí 768 ‚Üí 1024 ‚Üí 1440 resolution\nThe final stage included mixed-scale image training to boost multi-resolution generation performance\nExtended positional embedding space to 384x384 (latent) at lower resolution stages\nEmployed random crop augmentation on positional embeddings to enhance transformer layer robustness across the entire range of mixed resolutions and aspect ratios. For example, given a 64x64 latent image, we add a randomly cropped 64x64 embedding from the 192x192 embedding space during training as the input to the x stream.\nThese enhancements collectively contribute to the model's improved performance in multi-resolution image generation, coherence, and adaptability across various text-to-image tasks.\nText EncodersÔºö\nCLIPs: OpenCLIP-ViT/G, CLIP-ViT/L, context length 77 tokens\nT5: T5-xxl, context length 77/256 tokens at different stages of training\nTraining Data and Strategy:\nThis model was trained on a wide variety of data, including synthetic data and filtered publicly available data.\nFor more technical details of the original MMDiT architecture, please refer to the Research paper.\nUsage & Limitations\nWhile this model can handle long prompts, you may observe artifacts on the edge of generations when T5 tokens go over 256. Pay attention to the token limits when using this model in your workflow, and shortern prompts if artifacts becomes too obvious.\nThe medium model has a different training data distribution than the large model, so it may not respond to the same prompt similarly.\nWe recommend sampling with Skip Layer Guidance for better structure and anatomy coherency.\nModel Performance\nSee blog for our study about comparative performance in prompt adherence and aesthetic quality.\nFile Structure\nClick here to access the Files and versions tab\n‚îú‚îÄ‚îÄ text_encoders/\n‚îÇ   ‚îú‚îÄ‚îÄ README.md\n‚îÇ   ‚îú‚îÄ‚îÄ clip_g.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ clip_l.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ t5xxl_fp16.safetensors\n‚îÇ   ‚îî‚îÄ‚îÄ t5xxl_fp8_e4m3fn.safetensors\n‚îÇ\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ sd3.5_medium.safetensors\n‚îú‚îÄ‚îÄ SD3.5M_example_workflow.json\n‚îú‚îÄ‚îÄ SD3.5M_SLG_example_workflow.json\n‚îú‚îÄ‚îÄ SD3.5L_plus_SD3.5M_upscaling_example_workflow.json\n‚îî‚îÄ‚îÄ sd3_medium_demo.jpg\n** File structure below is for diffusers integration**\n‚îú‚îÄ‚îÄ scheduler/\n‚îú‚îÄ‚îÄ text_encoder/\n‚îú‚îÄ‚îÄ text_encoder_2/\n‚îú‚îÄ‚îÄ text_encoder_3/\n‚îú‚îÄ‚îÄ tokenizer/\n‚îú‚îÄ‚îÄ tokenizer_2/\n‚îú‚îÄ‚îÄ tokenizer_3/\n‚îú‚îÄ‚îÄ transformer/\n‚îú‚îÄ‚îÄ vae/\n‚îî‚îÄ‚îÄ model_index.json\nUsing with Diffusers\nUpgrade to the latest version of the üß® diffusers library\npip install -U diffusers\nand then you can run\nimport torch\nfrom diffusers import StableDiffusion3Pipeline\npipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3.5-medium\", torch_dtype=torch.bfloat16)\npipe = pipe.to(\"cuda\")\nimage = pipe(\n\"A capybara holding a sign that reads Hello World\",\nnum_inference_steps=40,\nguidance_scale=4.5,\n).images[0]\nimage.save(\"capybara.png\")\nQuantizing the model with diffusers\nReduce your VRAM usage and have the model fit on ü§è VRAM GPUs\npip install bitsandbytes\nfrom diffusers import BitsAndBytesConfig, SD3Transformer2DModel\nfrom diffusers import StableDiffusion3Pipeline\nimport torch\nmodel_id = \"stabilityai/stable-diffusion-3.5-medium\"\nnf4_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_quant_type=\"nf4\",\nbnb_4bit_compute_dtype=torch.bfloat16\n)\nmodel_nf4 = SD3Transformer2DModel.from_pretrained(\nmodel_id,\nsubfolder=\"transformer\",\nquantization_config=nf4_config,\ntorch_dtype=torch.bfloat16\n)\npipeline = StableDiffusion3Pipeline.from_pretrained(\nmodel_id,\ntransformer=model_nf4,\ntorch_dtype=torch.bfloat16\n)\npipeline.enable_model_cpu_offload()\nprompt = \"A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus, basking in a river of melted butter amidst a breakfast-themed landscape. It features the distinctive, bulky body shape of a hippo. However, instead of the usual grey skin, the creature's body resembles a golden-brown, crispy waffle fresh off the griddle. The skin is textured with the familiar grid pattern of a waffle, each square filled with a glistening sheen of syrup. The environment combines the natural habitat of a hippo with elements of a breakfast table setting, a river of warm, melted butter, with oversized utensils or plates peeking out from the lush, pancake-like foliage in the background, a towering pepper mill standing in for a tree.  As the sun rises in this fantastical world, it casts a warm, buttery glow over the scene. The creature, content in its butter river, lets out a yawn. Nearby, a flock of birds take flight\"\nimage = pipeline(\nprompt=prompt,\nnum_inference_steps=40,\nguidance_scale=4.5,\nmax_sequence_length=512,\n).images[0]\nimage.save(\"whimsical.png\")\nFine-tuning\nPlease see the fine-tuning guide here.\nUses\nIntended Uses\nIntended uses include the following:\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models, including understanding the limitations of generative models.\nAll uses of the model must be in accordance with our Acceptable Use Policy.\nOut-of-Scope Uses\nThe model was not trained to be factual or true representations of people or events.  As such, using the model to generate such content is out-of-scope of the abilities of this model.\nSafety\nAs part of our safety-by-design and responsible AI deployment approach, we take deliberate measures to ensure Integrity starts at the early stages of development. We implement safety measures throughout the development of our models. We have implemented safety mitigations that are intended to reduce the risk of certain harms, however we recommend that developers conduct their own testing and apply additional mitigations based on their specific use cases.For more about our approach to Safety, please visit our Safety page.\nIntegrity Evaluation\nOur integrity evaluation methods include structured evaluations and red-teaming testing for certain harms.  Testing was conducted primarily in English and may not cover all possible harms.\nRisks identified and mitigations:\nHarmful content:  We have used filtered data sets when training our models and implemented safeguards that attempt to strike the right balance between usefulness and preventing harm. However, this does not guarantee that all possible harmful content has been removed. TAll developers and deployers should exercise caution and implement content safety guardrails based on their specific product policies and application use cases.\nMisuse: Technical limitations and developer and end-user education can help mitigate against malicious applications of models. All users are required to adhere to our Acceptable Use Policy, including when applying fine-tuning and prompt engineering mechanisms. Please reference the Stability AI Acceptable Use Policy for information on violative uses of our products.\nPrivacy violations: Developers and deployers are encouraged to adhere to privacy regulations with techniques that respect data privacy.\nContact\nPlease report any issues with the model or contact us:\nSafety issues:  safety@stability.ai\nSecurity issues:  security@stability.ai\nPrivacy issues:  privacy@stability.ai\nLicense and general: https://stability.ai/license\nEnterprise license: https://stability.ai/enterprise",
    "ashllay/YOLO_Models": "No model card",
    "HuggingFaceTB/SmolLM2-360M": "SmolLM2\nTable of Contents\nModel Summary\nHow to use\nEvaluation\nBase Pre-Trained Model\nInstruction Model\nLimitations\nTraining\nModel\nHardware\nSoftware\nLicense\nCitation\nSmolLM2\nTable of Contents\nModel Summary\nLimitations\nTraining\nLicense\nCitation\nModel Summary\nSmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device. More details in our paper: https://arxiv.org/abs/2502.02737\nSmolLM2 demonstrates significant advances over its predecessor SmolLM1, particularly in instruction following, knowledge, reasoning. The 360M model was trained on 4 trillion tokens using a diverse dataset combination: FineWeb-Edu, DCLM, The Stack, along with new filtered datasets we curated and will release soon.  We developed the instruct version through supervised fine-tuning (SFT) using a combination of public datasets and our own curated datasets. We then applied Direct Preference Optimization (DPO) using UltraFeedback.\nThe instruct model additionally supports tasks such as text rewriting, summarization and function calling thanks to datasets developed by Argilla such as Synth-APIGen-v0.1.\nFor more details refer to: https://github.com/huggingface/smollm. You will find pre-training, post-training, evaluation and local inference code.\nHow to use\npip install transformers\nRunning the model on CPU/GPU/multi GPU\nUsing full precision\n# pip install transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"HuggingFaceTB/SmolLM2-360M\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\ninputs = tokenizer.encode(\"Gravity is\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\nUsing torch.bfloat16\n# pip install accelerate\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ncheckpoint = \"HuggingFaceTB/SmolLM2-360M\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for fp16 use `torch_dtype=torch.float16` instead\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16)\ninputs = tokenizer.encode(\"Gravity is\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\nMemory footprint: 723.56 MB\nEvaluation\nIn this section, we report the evaluation results of SmolLM2. All evaluations are zero-shot unless stated otherwise, and we use lighteval to run them.\nBase Pre-Trained Model\nMetrics\nSmolLM2-360M\nQwen2.5-0.5B\nSmolLM-360M\nHellaSwag\n54.5\n51.2\n51.8\nARC (Average)\n53.0\n45.4\n50.1\nPIQA\n71.7\n69.9\n71.6\nMMLU (cloze)\n35.8\n33.7\n34.4\nCommonsenseQA\n38.0\n31.6\n35.3\nTriviaQA\n16.9\n4.3\n9.1\nWinogrande\n52.5\n54.1\n52.8\nOpenBookQA\n37.4\n37.4\n37.2\nGSM8K (5-shot)\n3.2\n33.4\n1.6\nInstruction Model\nMetric\nSmolLM2-360M-Instruct\nQwen2.5-0.5B-Instruct\nSmolLM-360M-Instruct\nIFEval (Average prompt/inst)\n41.0\n31.6\n19.8\nMT-Bench\n3.66\n4.16\n3.37\nHellaSwag\n52.1\n48.0\n47.9\nARC (Average)\n43.7\n37.3\n38.8\nPIQA\n70.8\n67.2\n69.4\nMMLU (cloze)\n32.8\n31.7\n30.6\nBBH (3-shot)\n27.3\n30.7\n24.4\nGSM8K (5-shot)\n7.43\n26.8\n1.36\nLimitations\nSmolLM2 models primarily understand and generate content in English. They can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.\nTraining\nModel\nArchitecture: Transformer decoder\nPretraining tokens: 4T\nPrecision: bfloat16\nHardware\nGPUs: 128 H100\nSoftware\nTraining Framework: nanotron\nLicense\nApache 2.0\nCitation\n@misc{allal2025smollm2smolgoesbig,\ntitle={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model},\nauthor={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Mart√≠n Bl√°zquez and Guilherme Penedo and Lewis Tunstall and Andr√©s Marafioti and Hynek Kydl√≠ƒçek and Agust√≠n Piqueres Lajar√≠n and Vaibhav Srivastav and Joshua Lochner and Caleb Fahlgren and Xuan-Son Nguyen and Cl√©mentine Fourrier and Ben Burtenshaw and Hugo Larcher and Haojun Zhao and Cyril Zakka and Mathieu Morlon and Colin Raffel and Leandro von Werra and Thomas Wolf},\nyear={2025},\neprint={2502.02737},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2502.02737},\n}",
    "HuggingFaceTB/SmolLM2-360M-Instruct-GGUF": "ngxson/SmolLM2-360M-Instruct-Q8_0-GGUF\nUse with llama.cpp\nCLI:\nServer:\nngxson/SmolLM2-360M-Instruct-Q8_0-GGUF\nThis model was converted to GGUF format from HuggingFaceTB/SmolLM2-360M-Instruct using llama.cpp via the ggml.ai's GGUF-my-repo space.\nRefer to the original model card for more details on the model.\nUse with llama.cpp\nInstall llama.cpp through brew (works on Mac and Linux)\nbrew install llama.cpp\nInvoke the llama.cpp server or the CLI.\nCLI:\nllama-cli --hf-repo ngxson/SmolLM2-360M-Instruct-Q8_0-GGUF --hf-file smollm2-360m-instruct-q8_0.gguf -p \"The meaning to life and the universe is\"\nServer:\nllama-server --hf-repo ngxson/SmolLM2-360M-Instruct-Q8_0-GGUF --hf-file smollm2-360m-instruct-q8_0.gguf -c 2048\nNote: You can also use this checkpoint directly through the usage steps listed in the Llama.cpp repo as well.\nStep 1: Clone llama.cpp from GitHub.\ngit clone https://github.com/ggerganov/llama.cpp\nStep 2: Move into the llama.cpp folder and build it with LLAMA_CURL=1 flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).\ncd llama.cpp && LLAMA_CURL=1 make\nStep 3: Run inference through the main binary.\n./llama-cli --hf-repo ngxson/SmolLM2-360M-Instruct-Q8_0-GGUF --hf-file smollm2-360m-instruct-q8_0.gguf -p \"The meaning to life and the universe is\"\nor\n./llama-server --hf-repo ngxson/SmolLM2-360M-Instruct-Q8_0-GGUF --hf-file smollm2-360m-instruct-q8_0.gguf -c 2048",
    "MiaoshouAI/Florence-2-large-PromptGen-v2.0": "Florence-2-large-PromptGen v2.0\nFeatures:\nInstruction prompt:\nVersion History:\nHow to use:\nUse under MiaoshouAI Tagger ComfyUI\nFlorence-2-large-PromptGen v2.0\nThis upgrade is based on PromptGen 1.5 with some new features to the model:\nFeatures:\nImproved caption quality for <GENERATE_TAGS>, <DETAILED_CAPTION> and <MORE_DETAILED_CAPTION>.\nA new <ANALYZE> instruction, which helps the model to better understands the image composition of the input image.\nMemory efficient compare to other models! This is a really light weight caption model that allows you to use a little more than 1G of VRAM and produce lightening fast and high quality image captions.\nDesigned to handle image captions for Flux model for both T5XXL CLIP and CLIP_L, the Miaoshou Tagger new node called \"Flux CLIP Text Encode\" which eliminates the need to run two separate tagger tools for caption creation. You can easily populate both CLIPs in a single generation, significantly boosting speed when working with Flux models.\nInstruction prompt:\n<GENERATE_TAGS> generate prompt as danbooru style tags\n<CAPTION> a one line caption for the image\n<DETAILED_CAPTION> a structured caption format which detects the position of the subjects in the image\n<MORE_DETAILED_CAPTION> a very detailed description for the image\n<ANALYZE> image composition analysis mode\n<MIXED_CAPTION> a mixed caption style of more detailed caption and tags, this is extremely useful for FLUX model when using T5XXL and CLIP_L together. A new node in MiaoshouTagger ComfyUI is added to support this instruction.\n<MIXED_CAPTION_PLUS> Combine the power of mixed caption with analyze.\nVersion History:\nFor version 2.0, you will notice the following\n<ANALYZE> along with a beta node in ComfyUI for partial image analysis\nA new instruction for <MIXED_CAPTION_PLUS>\nA much improve accuracy for <GENERATE_TAGS>, <DETAILED_CAPTION> and <MORE_DETAILED_CAPTION>\nHow to use:\nTo use this model, you can load it directly from the Hugging Face Model Hub:\nmodel = AutoModelForCausalLM.from_pretrained(\"MiaoshouAI/Florence-2-large-PromptGen-v2.0\", trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"MiaoshouAI/Florence-2-large-PromptGen-v2.0\", trust_remote_code=True)\nprompt = \"<MORE_DETAILED_CAPTION>\"\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\ngenerated_ids = model.generate(\ninput_ids=inputs[\"input_ids\"],\npixel_values=inputs[\"pixel_values\"],\nmax_new_tokens=1024,\ndo_sample=False,\nnum_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nparsed_answer = processor.post_process_generation(generated_text, task=prompt, image_size=(image.width, image.height))\nprint(parsed_answer)\nUse under MiaoshouAI Tagger ComfyUI\nIf you just want to use this model, you can use it under ComfyUI-Miaoshouai-Tagger\nhttps://github.com/miaoshouai/ComfyUI-Miaoshouai-Tagger\nA detailed use and install instruction is already there.\n(If you have already installed MiaoshouAI Tagger, you need to update the node in ComfyUI Manager first or use git pull to get the latest update.)",
    "Qwen/Qwen2.5-Coder-32B-Instruct": "Qwen2.5-Coder-32B-Instruct\nIntroduction\nRequirements\nQuickstart\nProcessing Long Texts\nEvaluation & Performance\nCitation\nQwen2.5-Coder-32B-Instruct\nIntroduction\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\nSignificantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\nA more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\nLong-context Support up to 128K tokens.\nThis repo contains the instruction-tuned 32B Qwen2.5-Coder model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 32.5B\nNumber of Paramaters (Non-Embedding): 31.0B\nNumber of Layers: 64\nNumber of Attention Heads (GQA): 40 for Q and 8 for KV\nContext Length: Full 131,072 tokens\nPlease refer to this section for detailed instructions on how to deploy Qwen2.5 for handling long texts.\nFor more details, please refer to our blog, GitHub, Documentation, Arxiv.\nRequirements\nThe code of Qwen2.5-Coder has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"write a quick sort algorithm.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nProcessing Long Texts\nThe current config.json is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nFor deployment, we recommend using vLLM.\nPlease refer to our Documentation for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{hui2024qwen2,\ntitle={Qwen2. 5-Coder Technical Report},\nauthor={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\njournal={arXiv preprint arXiv:2409.12186},\nyear={2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "unsloth/Qwen2.5-Coder-14B-Instruct-GGUF": "Finetune Llama 3.2, Qwen2.5, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\n‚ú® Finetune for Free\nunsloth/Qwen2.5-Coder-14B-Instruct-GGUF\nIntroduction\nRequirements\nEvaluation & Performance\nCitation\nFinetune Llama 3.2, Qwen2.5, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!\nWe have a Qwen 2.5 (all model sizes) free Google Colab Tesla T4 notebook.\nAlso a Qwen 2.5 conversational style notebook.\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama-3.1 8b\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nPhi-3.5 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n50% less\nGemma-2 9b\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nMistral 7b\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nTinyLlama\n‚ñ∂Ô∏è Start on Colab\n3.9x faster\n74% less\nDPO - Zephyr\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\nunsloth/Qwen2.5-Coder-14B-Instruct-GGUF\nIntroduction\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\nSignificantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\nA more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\nThis repo contains the 0.5B Qwen2.5-Coder model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\nNumber of Parameters: 0.49B\nNumber of Paramaters (Non-Embedding): 0.36B\nNumber of Layers: 24\nNumber of Attention Heads (GQA): 14 for Q and 2 for KV\nContext Length: Full 32,768 tokens\nWe do not recommend using base language models for conversations. Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., or fill in the middle tasks on this model.\nFor more details, please refer to our blog, GitHub, Documentation, Arxiv.\nRequirements\nThe code of Qwen2.5-Coder has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{hui2024qwen2,\ntitle={Qwen2. 5-Coder Technical Report},\nauthor={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\njournal={arXiv preprint arXiv:2409.12186},\nyear={2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "autogluon/chronos-bolt-base": "Chronos-Bolt‚ö° (Base)\nPerformance\nUsage\nZero-shot inference with Chronos-Bolt in AutoGluon\nDeploying a Chronos-Bolt endpoint to SageMaker\nCitation\nLicense\nChronos-Bolt‚ö° (Base)\nüöÄ Update Feb 14, 2025: Chronos-Bolt models are now available on Amazon SageMaker JumpStart! Check out the tutorial notebook to learn how to deploy Chronos endpoints for production use in a few lines of code.\nChronos-Bolt is a family of pretrained time series forecasting models which can be used for zero-shot forecasting. It is based on the T5 encoder-decoder architecture and has been trained on nearly 100 billion time series observations. It chunks the historical time series context into patches of multiple observations, which are then input into the encoder. The decoder then uses these representations to directly generate quantile forecasts across multiple future steps‚Äîa method known as direct multi-step forecasting. Chronos-Bolt models are more accurate, up to 250 times faster and 20 times more memory-efficient than the original Chronos models of the same size.\nPerformance\nThe following plot compares the inference time of Chronos-Bolt against the original Chronos models for forecasting 1024 time series with a context length of 512 observations and a prediction horizon of 64 steps.\nChronos-Bolt models are not only significantly faster but also more accurate than the original Chronos models. The following plot reports the probabilistic and point forecasting performance of Chronos-Bolt in terms of the Weighted Quantile Loss (WQL) and the Mean Absolute Scaled Error (MASE), respectively, aggregated over 27 datasets (see the Chronos paper for details on this benchmark). Remarkably, despite having no prior exposure to these datasets during training, the zero-shot Chronos-Bolt models outperform commonly used statistical models and deep learning models that have been trained on these datasets (highlighted by *). Furthermore, they also perform better than other FMs, denoted by a +, which indicates that these models were pretrained on certain datasets in our benchmark and are not entirely zero-shot. Notably, Chronos-Bolt (Base) also surpasses the original Chronos (Large) model in terms of the forecasting accuracy while being over 600 times faster.\nChronos-Bolt models are available in the following sizes.\nModel\nParameters\nBased on\nchronos-bolt-tiny\n9M\nt5-efficient-tiny\nchronos-bolt-mini\n21M\nt5-efficient-mini\nchronos-bolt-small\n48M\nt5-efficient-small\nchronos-bolt-base\n205M\nt5-efficient-base\nUsage\nZero-shot inference with Chronos-Bolt in AutoGluon\nInstall the required dependencies.\npip install autogluon\nForecast with the Chronos-Bolt model.\nfrom autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\ndf = TimeSeriesDataFrame(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly/train.csv\")\npredictor = TimeSeriesPredictor(prediction_length=48).fit(\ndf,\nhyperparameters={\n\"Chronos\": {\"model_path\": \"autogluon/chronos-bolt-base\"},\n},\n)\npredictions = predictor.predict(df)\nFor more advanced features such as fine-tuning and forecasting with covariates, check out this tutorial.\nDeploying a Chronos-Bolt endpoint to SageMaker\nFirst, update the SageMaker SDK to make sure that all the latest models are available.\npip install -U sagemaker\nDeploy an inference endpoint to SageMaker.\nfrom sagemaker.jumpstart.model import JumpStartModel\nmodel = JumpStartModel(\nmodel_id=\"autogluon-forecasting-chronos-bolt-base\",\ninstance_type=\"ml.c5.2xlarge\",\n)\npredictor = model.deploy()\nNow you can send time series data to the endpoint in JSON format.\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\")\npayload = {\n\"inputs\": [\n{\"target\": df[\"#Passengers\"].tolist()}\n],\n\"parameters\": {\n\"prediction_length\": 12,\n}\n}\nforecast = predictor.predict(payload)[\"predictions\"]\nChronos-Bolt models can be deployed to both CPU and GPU instances. These models also support forecasting with covariates. For more details about the endpoint API, check out the example notebook.\nCitation\nIf you find Chronos or Chronos-Bolt models useful for your research, please consider citing the associated paper:\n@article{ansari2024chronos,\ntitle={Chronos: Learning the Language of Time Series},\nauthor={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\njournal={Transactions on Machine Learning Research},\nissn={2835-8856},\nyear={2024},\nurl={https://openreview.net/forum?id=gerNCVqqtR}\n}\nLicense\nThis project is licensed under the Apache-2.0 License.",
    "lldacing/flash-attention-windows-wheel": "Windows wheels of flash-attention\nBuild cuda wheel steps\nFirst clone code\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention\nSwitch tag branch, such as v2.7.0.post2 (you can get latest tag by git describe --tags or list all available tags by git tag -l)\ngit checkout -b v2.7.0.post2 v2.7.0.post2\nDownload WindowsWhlBuilder_cuda.bat into flash-attention\nTo build with MSVC, please open the \"Native Tools Command Prompt for Visual Studio\". The exact name may depend on your version of Windows, Visual Studio, and cpu architecture (in my case it was \"x64 Native Tools Command Prompt for VS 2022\".)\nMy Visual Studio Installer version\nSwitch python env and make sure the corresponding torch cuda version is installed\nStart task\n# Build with 1 parallel workers (I used 8 workers on i9-14900KF-3.20GHz-RAM64G, which took about 30 minutes.)\n# If you want reset workers, you should edit `WindowsWhlBuilder_cuda.bat` and modify `set MAX_JOBS=1`.(I tried to modify it by parameters, but failed)\nWindowsWhlBuilder_cuda.bat\n# Build with sm80 and sm120\nWindowsWhlBuilder_cuda.bat CUDA_ARCH=\"80;120\"\n# Enable cxx11abi\nWindowsWhlBuilder_cuda.bat CUDA_ARCH=\"80;120\" FORCE_CXX11_ABI=TRUE\nWheel file will be placed in the dist directory",
    "FallenMerick/MN-Violet-Lotus-12B": "MN-Violet-Lotus-12B\nRecommended ST Settings\nMerge Details\nMerge Method\nModels Merged\nConfiguration\nMN-Violet-Lotus-12B\nThis is the model I was trying to create when Chunky-Lotus emerged. Not only does this model score higher on my local EQ benchmarks (80.00 w/ 100% parsed @ 8-bit), but it does an even better job at roleplaying and producing creative outputs while still adhering to wide ranges of character personalities. The high levels of emotional intelligence are really quite noticeable as well.\nOnce again, models tend to score higher on my local tests when compared to their posted scores, but this has become the new high score for models I've personally tested.\nI really like the way this model writes, and I hope you'll enjoy using it as well!\nGGUF Quants:\nhttps://huggingface.co/backyardai/MN-Violet-Lotus-12B-GGUF\nhttps://huggingface.co/mradermacher/MN-Violet-Lotus-12B-GGUF\nhttps://huggingface.co/mradermacher/MN-Violet-Lotus-12B-i1-GGUF\nRecommended ST Settings\nSpecial thanks to @Zeldazachman for these amazing ST settings that I now wholeheartedly recommend!\nMerge Details\nThis is a merge of pre-trained language models created using mergekit.\nMerge Method\nThis model was merged using the Model Stock merge method.\nModels Merged\nThe following models were included in the merge:\nEpiculous/Violet_Twilight-v0.2\nNeverSleep/Lumimaid-v0.2-12B\nflammenai/Mahou-1.5-mistral-nemo-12B\nSao10K/MN-12B-Lyra-v4\nConfiguration\nThe following YAML configuration was used to produce this model:\nmodels:\n- model: FallenMerick/MN-Twilight-Maid-SLERP-12B #(unreleased)\n- model: Sao10K/MN-12B-Lyra-v4\n- model: flammenai/Mahou-1.5-mistral-nemo-12B\nmerge_method: model_stock\nbase_model: mistralai/Mistral-Nemo-Instruct-2407\nparameters:\nnormalize: true\ndtype: bfloat16\nIn this recipe, Violet Twilight and Lumimaid were first blended using the SLERP method to create a strong roleplaying foundation. Lyra v4 is then added to the mix for its great creativity and roleplaying performance, along with Mahou to once again curtail the outputs and prevent the resulting model from becoming too wordy. Model Stock was used for the final merge in order to really push the resulting weights in the proper direction while using Nemo Instruct as a strong anchor point.",
    "mradermacher/MN-Violet-Lotus-12B-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/FallenMerick/MN-Violet-Lotus-12B\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/MN-Violet-Lotus-12B-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n4.9\nGGUF\nQ3_K_S\n5.6\nGGUF\nQ3_K_M\n6.2\nlower quality\nGGUF\nQ3_K_L\n6.7\nGGUF\nIQ4_XS\n6.9\nGGUF\nQ4_0_4_4\n7.2\nfast on arm, low quality\nGGUF\nQ4_K_S\n7.2\nfast, recommended\nGGUF\nQ4_K_M\n7.6\nfast, recommended\nGGUF\nQ5_K_S\n8.6\nGGUF\nQ5_K_M\n8.8\nGGUF\nQ6_K\n10.2\nvery good quality\nGGUF\nQ8_0\n13.1\nfast, best quality\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "google/cxr-foundation": "Access CXR Foundation on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access CXR Foundation on Hugging Face, you're required to review and agree to Health AI Developer Foundation's terms of use. To do this, please ensure you're logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nCXR Foundation model card\nModel information\nDescription\nHow to use\nExamples\nModel architecture overview\nTechnical specifications\nPerformance and validation\nKey performance metrics\nInputs and outputs\nDataset details\nTraining dataset\nLabeling\nLicense\nData citation\nImplementation information\nSoftware\nUse and limitations\nIntended use\nBenefits\nLimitations\nCXR Foundation model card\nModel documentation:\nCXR Foundation\nResources:\nModel on Google Cloud Model Garden:\nCXR Foundation\nModel on Hugging Face:\ngoogle/cxr-foundation\nGitHub repository (supporting code, Colab notebooks, discussions, and\nissues): cxr-foundation\nQuick start notebook:\nnotebooks/quick_start\nSupport: See\nContact.\nTerms of use:\nHealth AI Developer Foundations terms of use\nAuthor: Google\nModel information\nThis section describes the CXR Foundation model and how to use it.\nDescription\nCXR Foundation is a machine learning model designed to accelerate AI development\nfor chest X-ray image analysis. It is pre-trained on large amounts of chest\nX-rays, to produce embeddings that capture dense features relevant for analyzing\nthese images. As a result, the embeddings CXR Foundation produces enable the\nefficient training of AI models with significantly less data and compute than\ntraditional methods. CXR Foundation offers two types of embeddings:\nELIXR v2.0: Produces 32x768 dimensional vectors, capturing detailed image\nfeatures relevant to X-ray analysis.\nELIXR-contrastive / v2.0 text: Generates 32x128 dimensional vectors and\nallows for projecting chest X-ray images and textual prompts into a shared\nembedding space. This enables powerful applications like semantic image\nretrieval and zero-shot classification.\nYou can read more about the research behind CXR Foundation in our manuscript:\nELIXR: Towards a general purpose X-ray artificial intelligence system through\nalignment of large language models and radiology vision\nencoders.\nHow to use\nFor getting started quickly with Hugging Face, refer to the Quick start notebook\nin the next section.\nIf you want to use the model at scale, we recommend that you create a production\nversion using\nModel Garden.\nExamples\nSee the following Colab notebooks for examples of how to use CXR Foundation:\nTo give the model a quick try, running it locally with weights from Hugging\nFace, see\nQuick start notebook in Colab.\nFor an example of how to use the model to train a linear classifier see\nLinear classifier notebook in Colab.\nFor an example of how to retrieve images from a database using text-image\nsimilarity see\nText retrieval notebook in Colab.\nFor an example of how to use the text embeddings to perform zero-shot\ninference see\nZero-shot inference notebook in Colab.\nModel architecture overview\nThe model uses the\nEfficientNet-L2 architecture and\nBERT architecture. It was trained on 821,544\nCXRs from India and the US using abnormal vs. normal labels, i.e. the image\ncontained any kind of abnormality, and the\nSupervised Contrastive loss as well as\naccompanying radiology reports and the\nCLIP loss and\nBLIP-2 losses. The abnormal vs. normal\nlabels were obtained from more granular labels (e.g. pneumothorax, fracture) as\nwell as\nregular expressions on radiology reports.\nYou can read more about the research behind CXR Foundation in our recent\npublication:\nSimplified Transfer Learning for Chest Radiography Models Using Less Data.\nTechnical specifications\nModel type: Convolutional neural network that produces embeddings\nKey publications:\nSimplified Transfer Learning for Chest Radiography Models Using Less\nData\nELIXR: Towards a general purpose X-ray artificial intelligence system\nthrough alignment of large language models and radiology vision\nencoders\nModel created: August 2, 2024\nModel version: Version: 2.0.0\nPerformance and validation\nCXR Foundation was evaluated across a range of different tasks for\ndata-efficient classification, zero-shot classification, semantic image\nretrieval, visual-question answering and report quality assurance.\nKey performance metrics\nData-efficient Classification: Mean AUCs of 0.898 (across atelectasis,\ncardiomegaly, consolidation, pleural effusion, and pulmonary edema) on\nCheXPert test\nZero-shot classification: Mean AUC of 0.846 across 13 findings on\nCheXpert test. Findings included: atelectasis, cardiomegaly, consolidation,\npleural effusion, and pulmonary edema, enlarged cardiomediastinum, pleural\nother, pneumothorax, support devices, airspace opacity, lung lesion,\npneumonia, and fracture.\nSemantic image retrieval: 0.76 normalized discounted cumulative gain\n(NDCG) @5 across 19 queries for semantic image retrieval, including\nperfect retrieval on 12 of them.\nReference: ELIXR: Towards a general purpose X-ray artificial intelligence\nsystem through alignment of large language models and radiology vision\nencoders\nInputs and outputs\nInput: Serialized tf.Example (with the bytes of a PNG written in the\nimage/encoded feature key).\nOutput: Embedding (a vector of floating points representing a projection\nof the original image into a compressed feature space)\nDataset details\nTraining dataset\nCXR Foundation was trained using the following de-identified datasets:\nMIMIC-CXR, comprising of 243,324 images of 60,523 unique patients (cited\nbelow);\nA private US dataset from an AMC in Illinois comprising of 165,182 images of\n12,988 unique patients; and\nA private Indian dataset from five hospitals comprising of 485,082 patients\nof 348,335 unique patients\nLabeling\nSupervised learning was used to label abnormal and normal human data from\nradiology reports.\nA medically tuned LLM, Med-Palm 2 29, was then applied to ensure that the labels\nwere consistent with the report, and a board certified thoracic radiologist (CL)\nadjudicated cases where the LLM results differed from the ground truth in\nMIMIC-CXR.\nAdditional information about data and labels used to evaluate CXR Foundation\nfor downstream tasks can be found in the following references:\nSellergren A, Chen C, et al. Simplified Transfer Learning for Chest\nRadiography Models Using Less Data. Radiology.\n2022.\nhttps://pubs.rsna.org/doi/10.1148/radiol.212482\n(Table 1, 2, 3)\nhttps://github.com/google-research/google-research/tree/master/supcon\nLicense\nThe use of CXR Foundation is governed by the\nHealth AI Developer Foundations terms of use.\nData citation\nMIMIC-CXR Johnson, A., Pollard, T., Mark, R., Berkowitz, S., & Horng, S.\n(2024). MIMIC-CXR Database (version 2.1.0).\nPhysioNet.\nJohnson, A.E.W., Pollard, T.J., Berkowitz, S.J. et al. MIMIC-CXR, a\nde-identified publicly available database of chest radiographs with\nfree-text reports. Sci Data 6, 317\n(2019).\nAvailable on Physionet Goldberger, A., Amaral, L., Glass, L., Hausdorff, J.,\nIvanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank,\nPhysioToolkit, and PhysioNet: Components of a new research resource for\ncomplex physiologic signals. Circulation [Online]. 101 (23), pp.\ne215‚Äìe220.\nImplementation information\nDetails about the model internals.\nSoftware\nTraining was done using JAX\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nUse and limitations\nIntended use\nCXR Foundation can reduce the training data, compute, and technical\nexpertise necessary to develop AI applications for radiographs. The model\nhas been optimized for chest X-rays, but researchers have reported success\nusing it for other types of X-rays, including X-rays of other body parts and\neven veterinary X-rays. Some example applications include:\nData-efficient classification:\nWith a low amount of labeled data, you can train a classifier model on top of\nCXR Foundation embeddings (ELIXR v2.0). Furthermore, each embedding can be used\ndownstream as an input for a variety of different classifiers, with very little\nadditional compute. Below are some example classification tasks:\nClinical findings like fracture or pneumothorax\nDetermining X-ray image quality\nDetermining the X-ray view or body part\nDetermining the presence of devices\nDiscovering misplaced tubes\nZero-shot classification\nBy using the contrastive mode (ELIXR-contrastive / v2.0 text), users can get a\nclassification score without any additional training data through\ntextual prompts. Zero-shot works by measuring the relative distance of the image\nembeddings from a positive e.g., \"pleural effusion present\", and negative text\nprompt e.g., \"normal X-ray\". The use cases are the same as data-efficient\nclassification but don't require data to train. The zero-shot method will\noutperform data-efficient classifications at low levels of training data, while\nthe data-efficient classification will tend to exceed zero-shot performance with\nlarger amounts of data. See ELIXR paper for\nmore details.\nSemantic image retrieval\nBy using the contrastive mode (ELIXR-contrastive / v2.0 text) users can rank a\nset of X-rays across a search query. Similar to Zero-shot classification,\nlanguage-based image retrieval relies on the distance between the embeddings of\nthe set of images and the text embeddings from the search query.\nBenefits\nCXR Foundation Embeddings can be used for efficient training of AI\ndevelopment for chest X-ray image analysis with significantly less data and\ncompute than traditional methods.\nBy leveraging the large set of pre-trained images CXR Foundation is trained\non, users need less data but can also build more generalizable models than\ntraining on more limited datasets.\nLimitations\nThe following are known factors that might limit the generalizability or\nusefulness of the model output for application in downstream tasks:\nThe model was trained using only de-identified data from the US and India\nand may not generalize well to data from other countries, patient\npopulations, or manufacturers not used in training.\nThe model has only been validated for a limited number of the many potential\ndownstream tasks involving chest radiographs.\nImage quality and min resolution. 1024x1024 recommended.\nThe model is only used to generate embeddings of user-provided data. It does\nnot generate any predictions or diagnosis on its own.\nTask-specific validation remains an important aspect of downstream model\ndevelopment by the end user.\nAs with any research, developers should ensure that any downstream\napplication is validated to understand performance using data that is\nappropriately representative of the intended use setting for the specific\napplication (e.g., age, sex, gender, condition, scanner, etc.).",
    "google/derm-foundation": "Access Derm Foundation on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Derm Foundation on Hugging Face, you're required to review and agree to Health AI Developer Foundation's terms of use. To do this, please ensure you're logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nDerm Foundation model card\nModel information\nDescription\nHow to use\nExamples\nModel architecture overview\nTechnical specifications\nPerformance and validation\nInputs and outputs\nDataset details\nTraining dataset\nLabeling\nLicense\nImplementation information\nSoftware\nUse and limitations\nIntended use\nBenefits\nLimitations\nDerm Foundation model card\nModel documentation:\nDerm Foundation\nResources:\nModel on Google Cloud Model Garden:\nDerm Foundation\nModel on Hugging Face:\ngoogle/derm-foundation\nGitHub repository (supporting code, Colab notebooks, discussions, and\nissues): derm-foundation\nQuick start notebook:\nnotebooks/quick_start\nSupport: See\nContact.\nTerms of use:\nHealth AI Developer Foundations terms of use\nAuthor: Google\nModel information\nThis section describes the Derm Foundation model and how to use it.\nDescription\nDerm Foundation is a machine learning model designed to accelerate AI\ndevelopment for skin image analysis for dermatology applications. It is\npre-trained on large amounts of labeled skin images to produce 6144 dimensional\nembeddings that capture dense features relevant for analyzing these images. As a\nresult, Derm Foundation‚Äôs embeddings enable the efficient training of AI models\nwith significantly less data and compute than traditional methods.\nHow to use\nFollowing are some example code snippets to help you quickly get started running\nthe model locally. If you want to use the model at scale, we recommend that you\ncreate a production version using\nModel Garden.\n# Download test image.\nfrom PIL import Image\nfrom io import BytesIO\nfrom IPython.display import Image as IPImage, display\nfrom huggingface_hub import from_pretrained_keras\nimport tensorflow as tf\n# Download sample image\n!wget -nc -q https://storage.googleapis.com/dx-scin-public-data/dataset/images/3445096909671059178.png\n# Load the image\nimg = Image.open(\"3445096909671059178.png\")\nbuf = BytesIO()\nimg.convert('RGB').save(buf, 'PNG')\nimage_bytes = buf.getvalue()\n# Format input\ninput_tensor= tf.train.Example(features=tf.train.Features(\nfeature={'image/encoded': tf.train.Feature(\nbytes_list=tf.train.BytesList(value=[image_bytes]))\n})).SerializeToString()\n# Load the model directly from Hugging Face Hub\nloaded_model = from_pretrained_keras(\"google/derm-foundation\")\n# Call inference\ninfer = loaded_model.signatures[\"serving_default\"]\noutput = infer(inputs=tf.constant([input_tensor]))\n# Extract the embedding vector\nembedding_vector = output['embedding'].numpy().flatten()\nExamples\nSee the following Colab notebooks for examples of how to use Derm Foundation:\nTo give the model a quick try, running it locally with weights from Hugging\nFace, see\nQuick start notebook in Colab.\nFor an example of how to use the model to train a linear classifier see\nLinear classifier notebook in Colab\nDERM12345 Embeddings and Demo includes a demo using Derm Foundation precomputed embeddings for\nDERM12345. Special thanks to Abdurrahim Yilmaz for providing this.\nModel architecture overview\nThe model is a BiT-M ResNet101x3.\nDerm Foundation was trained in two stages. The first pre-training stage used\ncontrastive learning to train on a large number of public image-text pairs from\nthe internet. The image component of this pre-trained model was then fine-tuned\nfor condition classification and a couple other downstream tasks using a number\nof clinical datasets (see below).\nTechnical specifications\nModel type: BiT-101x3 CNN (Convolutional Neural Network)\nKey publications:\nBiT:\nhttps://arxiv.org/abs/1912.11370\nConVIRT:\nhttps://arxiv.org/abs/2010.00747\nModel created: 2023-12-19\nModel version: Version: 1.0.0\nPerformance and validation\nDerm Foundation was evaluated for data-efficient accuracy across a range of\nskin-related classifications tasks. Training a linear classifier on\nDerm-Foundations embeddings were substantially more performant (10-15% increase\nin accuracy) than doing the same for a standard BiT-M model across different\nproportions of training data. See this\nHealth-specific embedding tools for dermatology and pathology\nfor more details.\nInputs and outputs\nInput: PNG image file 448 x 448 pixels\nOutput: Embedding vector of floating point values (Dimensions: 6144)\nDataset details\nTraining dataset\nDerm Foundation was trained in two stages. The first pre-training stage used\ncontrastive learning to train on a large number of public image-text pairs from\nthe internet. The image component of this pre-trained model was then fine-tuned\nfor condition classification and a couple of other downstream tasks using a\nnumber of clinical datasets (see below).\nBase model (pre-training): A large number of health-related image-text pairs\nfrom the public web\nSFT (supervised fine-tuned) model: tele-dermatology datasets from the United\nStates and Colombia, a skin cancer dataset from Australia, and additional\npublic images. The images come from a mix of device types, including images\nfrom smartphone cameras, other cameras, and dermatoscopes. The images also\nhave a mix of image takers; images may have been taken by clinicians during\nconsultations or self-captured by patients.\nLabeling\nLabeling sources vary by dataset. Examples include:\n(image, caption) pairs from the public web\nDermatology condition labels provided by dermatologists labelers funded by\nGoogle\nDermatology condition labels provided with a clinical dataset based on a\ntelehealth visit, an in-person visit, or a biopsy\nLicense\nThe use of Derm Foundation is governed by the\nHealth AI Developer Foundations terms of use.\nImplementation information\nDetails about the model internals.\nSoftware\nTraining was done using JAX\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nUse and limitations\nIntended use\nDerm Foundation can reduce the training data, compute, and technical\nexpertise necessary to develop task-specific models for skin image analysis.\nEmbeddings from the model can be used for a variety of user-defined\ndownstream tasks including, but not limited to:\nClassifying clinical conditions like psoriasis, melanoma or dermatitis\nScoring severity or progression of clinical conditions\nIdentifying the body part the skin is from\nDetermining image quality for dermatological assessment\nTo see how to use the model to train a classifier see this\nLinear classifier example\nBenefits\nDerm Foundation Embeddings can be used for efficient training of AI\ndevelopment for skin image analysis with significantly less data and compute\nthan traditional methods.\nBy leveraging the large set of pre-trained images Derm Foundation is trained\non, users need less data but can also build more generalizable models than\ntraining on more limited datasets.\nLimitations\nDerm Foundation is trained on images with various lightning and noise\nconditions captured in a real-world environment. However, its quality can\ndegrade in extreme conditions, such as photos that are too light or too\ndark.\nThe base model was trained using image-text pairs from the public web. These\nimages come from a variety of sources but may by noisy or low-quality. The\nSFT (supervised fine-tuned) model was trained data from a limited set of\ncountries (United States, Colombia, Australia, public images) and settings\n(mostly clinical). It may not generalize well to data from other countries,\npatient populations, or image types not used in training.\nThe model is only used to generate embeddings of user-provided data. It does\nnot generate any predictions or diagnosis on its own.\nAs with any research, developers should ensure any downstream application is\nvalidated to understand performance using data that is appropriately\nrepresentative of the intended use setting for the specific application\n(e.g., skin tone/type, age, sex, gender etc.).",
    "Marqo/nsfw-image-detection-384": "Model card for nsfw-image-detection-384\nModel Usage\nImage Classification with timm\nEvaluation\nThresholds and Precision vs Recall\nTraining Details\nArgs\nCitation\nModel card for nsfw-image-detection-384\nNOTE: Like all models, this one can make mistakes. NSFW content can be subjective and contextual, this model is intended to help identify this content, use at your own risk.\nMarqo/nsfw-image-detection-384 is a lightweight image classification model designed to identify NSFW images. The model is approximately 18‚Äì20x smaller than other open-source models and achieves a superior accuracy of 98.56% on our dataset. This model uses 384x384 pixel images for the input with 16x16 pixel patches.\nThis model was trained on a proprietary dataset of 220,000 images. The training set includes 100,000 NSFW examples and 100,000 SFW examples, while the test set contains 10,000 NSFW examples and 10,000 SFW examples. This dataset features a diverse range of content, including: real photos, drawings, Rule 34 material, memes, and AI-generated images. The definition of NSFW can vary and is sometimes contextual, our dataset was constructed to contain challenging examples however this definition may not be 100% aligned with every use case, as such we recommend experimenting and trying different thresholds to determine if this model is suitable for your needs.\nModel Usage\nImage Classification with timm\npip install timm\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimport torch\nimg = Image.open(urlopen(\n'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\nmodel = timm.create_model(\"hf_hub:Marqo/nsfw-image-detection-384\", pretrained=True)\nmodel = model.eval()\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\nwith torch.no_grad():\noutput = model(transforms(img).unsqueeze(0)).softmax(dim=-1).cpu()\nclass_names = model.pretrained_cfg[\"label_names\"]\nprint(\"Probabilities:\", output[0])\nprint(\"Class:\", class_names[output[0].argmax()])\nEvaluation\nThis model outperforms existing NSFW detectors on our dataset, here we provide an evaluation against AdamCodd/vit-base-nsfw-detector and Falconsai/nsfw_image_detection:\nThresholds and Precision vs Recall\nAdjusting the threshold for the NSFW probability can let you trade off precision, recall, and accuracy. This maybe be useful in different applications where different degrees of confidence are required.\nTraining Details\nThis model is a finetune of the timm/vit_tiny_patch16_384.augreg_in21k_ft_in1k model.\nArgs\nbatch_size: 256\ncolor_jitter: 0.2\ncolor_jitter_prob: 0.05\ncutmix: 0.1\ndrop: 0.1\ndrop_path: 0.05\nepoch_repeats: 0.0\nepochs: 20\ngaussian_blur_prob: 0.005\nhflip: 0.5\nlr: 5.0e-05\nmixup: 0.1\nmixup_mode: batch\nmixup_prob: 1.0\nmixup_switch_prob: 0.5\nmomentum: 0.9\nnum_classes: 2\nopt: adamw\nremode: pixel\nreprob: 0.5\nsched: cosine\nsmoothing: 0.1\nwarmup_epochs: 2\nwarmup_lr: 1.0e-05\nwarmup_prefix: false\nCitation\n@article{dosovitskiy2020vit,\ntitle={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\nauthor={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},\njournal={ICLR},\nyear={2021}\n}\n@misc{rw2019timm,\nauthor = {Ross Wightman},\ntitle = {PyTorch Image Models},\nyear = {2019},\npublisher = {GitHub},\njournal = {GitHub repository},\ndoi = {10.5281/zenodo.4414861},\nhowpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}",
    "heydariAI/persian-embeddings": "heydariAI/persian-embeddings\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nheydariAI/persian-embeddings\nMy Github: @heydaari\nMy Linkedin: Mohammad Hassan Heydari\nThis model is a fine-tuned version of xlm-roberta-base, specifically trained on a massive corpus of Persian data to create high-quality contextual embeddings for Persian sentences and paragraphs. It is designed to perform exceptionally well on tasks such as semantic search, clustering, and contextual similarity for Persian text, while also supporting multilingual tasks in English and Persian.\nThe fine-tuning process focused on adapting the pre-trained multilingual XLM-RoBERTa model to better capture Persian linguistic nuances, making it highly effective for tasks requiring embeddings tailored to the Persian language.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = ['What are Large Language Models?','ŸÖÿØŸÑ Ÿáÿß€å ÿ≤ÿ®ÿßŸÜ€å ÿ®ÿ≤ÿ±⁄Ø ⁄ÜŸá Ÿáÿ≥ÿ™ŸÜÿØÿü']\nmodel = SentenceTransformer('heydariAI/persian-embeddings')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['what are Large Language Models?', 'ŸÖÿØŸÑ Ÿáÿß€å ÿ≤ÿ®ÿßŸÜ€å ÿ®ÿ≤ÿ±⁄Ø ⁄ÜŸá Ÿáÿ≥ÿ™ŸÜÿØÿü']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('heydariAI/persian-embeddings')\nmodel = AutoModel.from_pretrained('heydariAI/persian-embeddings')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)",
    "stepfun-ai/GOT-OCR-2.0-hf": "Usage example\nPlain text inference\nPlain text inference batched\nFormatted text inference\nInference on multiple pages\nInference on cropped patches\nInference on a specific region\nInference on general OCR data example: sheet music\nCitation\nGeneral OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model - HF Transformers ü§ó implementation\nü§ó Spaces Demo | üåüGitHub | üìúPaper\nHaoran Wei*, Chenglong Liu*, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu,  Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, Chunrui Han, Xiangyu Zhang\nTips:\nGOT-OCR2 works on a wide range of tasks, including plain document OCR, scene text OCR, formatted document OCR, and even OCR for tables, charts, mathematical formulas, geometric shapes, molecular formulas and sheet music. While this implementation of the model will only output plain text, the outputs can be further processed to render the desired format, with packages like pdftex, mathpix, matplotlib, tikz, verovio or pyecharts.\nThe model can also be used for interactive OCR, where the user can specify the region to be recognized by providing the coordinates or the color of the region's bounding box.\nThis model was contributed by yonigozlan.\nThe original code can be found here.\nUsage example\nPlain text inference\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n>>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/image_ocr.jpg\"\n>>> inputs = processor(image, return_tensors=\"pt\").to(device)\n>>> generate_ids = model.generate(\n...     **inputs,\n...     do_sample=False,\n...     tokenizer=processor.tokenizer,\n...     stop_strings=\"<|im_end|>\",\n...     max_new_tokens=4096,\n... )\n>>> processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\"R&D QUALITY IMPROVEMENT\\nSUGGESTION/SOLUTION FORM\\nName/Phone Ext. : (...)\"\nPlain text inference batched\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n>>> image1 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/multi_box.png\"\n>>> image2 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/image_ocr.jpg\"\n>>> inputs = processor([image1, image2], return_tensors=\"pt\").to(device)\n>>> generate_ids = model.generate(\n...     **inputs,\n...     do_sample=False,\n...     tokenizer=processor.tokenizer,\n...     stop_strings=\"<|im_end|>\",\n...     max_new_tokens=4,\n... )\n>>> processor.batch_decode(generate_ids[:, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n[\"Reducing the number\", \"R&D QUALITY\"]\nFormatted text inference\nGOT-OCR2 can also generate formatted text, such as markdown or LaTeX. Here is an example of how to generate formatted text:\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n>>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/latex.png\"\n>>> inputs = processor(image, return_tensors=\"pt\", format=True).to(device)\n>>> generate_ids = model.generate(\n...     **inputs,\n...     do_sample=False,\n...     tokenizer=processor.tokenizer,\n...     stop_strings=\"<|im_end|>\",\n...     max_new_tokens=4096,\n... )\n>>> processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\"\\\\author{\\nHanwen Jiang* quad\\\\quadquad Arjun Karpur daggerquad{ }^{\\\\dagger} \\\\quaddaggerquad Bingyi Cao daggerquad{ }^{\\\\dagger} \\\\quaddaggerquad (...)\"\nInference on multiple pages\nAlthough it might be reasonable in most cases to use a ‚Äúfor loop‚Äù for multi-page processing, some text data with formatting across several pages make it necessary to process all pages at once. GOT introduces a multi-page OCR (without ‚Äúfor loop‚Äù) feature, where multiple pages can be processed by the model at once, whith the output being one continuous text.\nHere is an example of how to process multiple pages at once:\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n>>> image1 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/page1.png\"\n>>> image2 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/page2.png\"\n>>> inputs = processor([image1, image2], return_tensors=\"pt\", multi_page=True, format=True).to(device)\n>>> generate_ids = model.generate(\n...     **inputs,\n...     do_sample=False,\n...     tokenizer=processor.tokenizer,\n...     stop_strings=\"<|im_end|>\",\n...     max_new_tokens=4096,\n... )\n>>> processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\"\\\\title{\\nGeneral OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model\\n}\\n\\\\author{\\nHaoran Wei (...)\"\nInference on cropped patches\nGOT supports a 1024√ó1024 input resolution, which is sufficient for most OCR tasks, such as scene OCR or processing A4-sized PDF pages. However, certain scenarios, like horizontally stitched two-page PDFs commonly found in academic papers or images with unusual aspect ratios, can lead to accuracy issues when processed as a single image. To address this, GOT can dynamically crop an image into patches, process them all at once, and merge the results for better accuracy with such inputs.\nHere is an example of how to process cropped patches:\n>>> import torch\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", torch_dtype=torch.bfloat16, device_map=device)\n>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n>>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/one_column.png\"\n>>> inputs = processor(image, return_tensors=\"pt\", format=True, crop_to_patches=True, max_patches=3).to(device)\n>>> generate_ids = model.generate(\n...     **inputs,\n...     do_sample=False,\n...     tokenizer=processor.tokenizer,\n...     stop_strings=\"<|im_end|>\",\n...     max_new_tokens=4096,\n... )\n>>> processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\"on developing architectural improvements to make learnable matching methods generalize.\\nMotivated by the above observations, (...)\"\nInference on a specific region\nGOT supports interactive OCR, where the user can specify the region to be recognized by providing the coordinates or the color of the region's bounding box. Here is an example of how to process a specific region:\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n>>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/multi_box.png\"\n>>> inputs = processor(image, return_tensors=\"pt\", color=\"green\").to(device) # or box=[x1, y1, x2, y2] for coordinates (image pixels)\n>>> generate_ids = model.generate(\n...     **inputs,\n...     do_sample=False,\n...     tokenizer=processor.tokenizer,\n...     stop_strings=\"<|im_end|>\",\n...     max_new_tokens=4096,\n... )\n>>> processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n\"You should keep in mind what features from the module should be used, especially \\nwhen you‚Äôre planning to sell a template.\"\nInference on general OCR data example: sheet music\nAlthough this implementation of the model will only output plain text, the outputs can be further processed to render the desired format, with packages like pdftex, mathpix, matplotlib, tikz, verovio or pyecharts.\nHere is an example of how to process sheet music:\n>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n>>> import verovio\n>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n>>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n>>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/sheet_music.png\"\n>>> inputs = processor(image, return_tensors=\"pt\", format=True).to(device)\n>>> generate_ids = model.generate(\n...     **inputs,\n...     do_sample=False,\n...     tokenizer=processor.tokenizer,\n...     stop_strings=\"<|im_end|>\",\n...     max_new_tokens=4096,\n... )\n>>> outputs = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n>>> tk = verovio.toolkit()\n>>> tk.loadData(outputs)\n>>> tk.setOptions(\n...     {\n...         \"pageWidth\": 2100,\n...         \"pageHeight\": 800,\n...         \"footer\": \"none\",\n...         \"barLineWidth\": 0.5,\n...         \"beamMaxSlope\": 15,\n...         \"staffLineWidth\": 0.2,\n...         \"spacingStaff\": 6,\n...     }\n... )\n>>> tk.getPageCount()\n>>> svg = tk.renderToSVG()\n>>> svg = svg.replace('overflow=\"inherit\"', 'overflow=\"visible\"')\n>>> with open(\"output.svg\", \"w\") as f:\n>>>     f.write(svg)\nCitation\nIf you find our work helpful, please consider citing our papers üìù and liking this project ‚ù§Ô∏èÔºÅ\n@article{wei2024general,\ntitle={General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model},\nauthor={Wei, Haoran and Liu, Chenglong and Chen, Jinyue and Wang, Jia and Kong, Lingyu and Xu, Yanming and Ge, Zheng and Zhao, Liang and Sun, Jianjian and Peng, Yuang and others},\njournal={arXiv preprint arXiv:2409.01704},\nyear={2024}\n}\n@article{liu2024focus,\ntitle={Focus Anywhere for Fine-grained Multi-page Document Understanding},\nauthor={Liu, Chenglong and Wei, Haoran and Chen, Jinyue and Kong, Lingyu and Ge, Zheng and Zhu, Zining and Zhao, Liang and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},\njournal={arXiv preprint arXiv:2405.14295},\nyear={2024}\n}\n@article{wei2023vary,\ntitle={Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models},\nauthor={Wei, Haoran and Kong, Lingyu and Chen, Jinyue and Zhao, Liang and Ge, Zheng and Yang, Jinrong and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},\njournal={arXiv preprint arXiv:2312.06109},\nyear={2023}\n}",
    "strangerzonehf/Flux-Midjourney-Mix2-LoRA": "Model description for MJv6 Mix2 LoRA\nBest Dimensions\nSetting Up\nTrigger words\nSample Prompts for MJv6 Mix2 LoRA\nDownload model\nPrompt\nMJ v6, Portrait photography of a woman in a red dress, in the style of unsplash photography, street photography, dark green background --ar 47:64 --v 6.0 --style raw\nPrompt\nMJ v6, A portrait of a Bird in the dark, illuminated by an intense yellow light from above, with a soft blue gradient background. This scene evokes a sense of mystery or contemplation, highlighting the beauty of the subjects features against the contrasting backdrop, lens glossy effect, high contrast, star bokeh\nPrompt\nMJ v6, A photo of an attractive man in his thirties, wearing a black coat and yellow scarf with a brown pattern inside a building talking on a phone standing near a modern glass skyscraper in London, shot from below looking up at him in the style of street photography, cinematic.  --ar 85:128 --v 6.0 --style raw\nPrompt\nMJ v6, banana bread with chocolate chips and pecans, in the style of tabletop photography, y2k aesthetic, spiky mounds, flawless line work, schlieren photography, 8k, natural fibers, minimal  --ar 123:185 --v 5\nPrompt\nMJ v6, A portrait of Woman, fashion photography, big shapes in the background, on top of colorful squares with stars, in the style of retro vintage photography, pastel colors, soft purple and yellow\nPrompt\nMJ v6, delicious dipped chocolate pastry japo gallery, white background, in the style of dark brown, close-up intensity, duckcore, rounded, high resolution --ar 2:3 --v 5\nModel description for MJv6 Mix2 LoRA\n[ Best for Realism, Modeling, Demonstration, Close-Up Shots ]\nImage Processing Parameters\nParameter\nValue\nParameter\nValue\nLR Scheduler\nconstant\nNoise Offset\n0.03\nOptimizer\nAdamW8\nMultires Noise Discount\n0.1\nNetwork Dim\n64\nMultires Noise Iterations\n10\nNetwork Alpha\n32\nRepeat & Steps\n25 & 3660\nEpoch\n28\nSave Every N Epochs\n1\nLabeling: florence2-en(natural language & English)\nTotal Images Used for Training : 36\nBest Dimensions\n768 x 1024 (Best)\n1024 x 1024 (Default)\nSetting Up\nimport torch\nfrom pipelines import DiffusionPipeline\nbase_model = \"black-forest-labs/FLUX.1-dev\"\npipe = DiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.bfloat16)\nlora_repo = \"strangerzonehf/Flux-Midjourney-Mix2-LoRA\"\ntrigger_word = \"MJ v6\"\npipe.load_lora_weights(lora_repo)\ndevice = torch.device(\"cuda\")\npipe.to(device)\nTrigger words\nYou should use MJ v6 to trigger the image generation.\nSample Prompts for MJv6 Mix2 LoRA\nPrompt\nDescription\nMJ v6, Portrait photography of a woman in a red dress, in the style of unsplash photography, street photography, dark green background --ar 47:64 --v 6.0 --style raw\nA portrait of a woman in a red dress, photographed in the street photography style with a dark green background, capturing the raw and natural aesthetics of Unsplash-style imagery.\nMJ v6, A portrait of a Bird in the dark, illuminated by an intense yellow light from above, with a soft blue gradient background. Lens glossy effect, high contrast, star bokeh\nA mysterious and contemplative bird portrait illuminated by yellow light with a blue gradient background. Features include high contrast and a star bokeh effect to enhance the atmosphere.\nMJ v6, banana bread with chocolate chips and pecans, in the style of tabletop photography, y2k aesthetic, spiky mounds, flawless line work, schlieren photography, 8k, natural fibers, minimal  --ar 123:185 --v 5\nA close-up image of banana bread with chocolate chips and pecans, styled with a Y2K aesthetic. The photography emphasizes texture, line work, and high resolution, with natural materials enhancing the minimalistic approach.\nMJ v6, delicious dipped chocolate pastry japo gallery, white background, in the style of dark brown, close-up intensity, duckcore, rounded, high resolution --ar 2:3 --v 5\nA close-up of a chocolate-dipped pastry on a white background, featuring a rich brown color palette and soft, rounded forms. High-resolution imagery enhances the details and texture of the subject.\nMJ v6, A portrait of Woman, fashion photography, big shapes in the background, on top of colorful squares with stars, in the style of retro vintage photography, pastel colors, soft purple and yellow\nA retro-vintage style portrait of a woman with a whimsical background of large shapes and colorful squares with stars. The pastel tones of purple and yellow create a soft and nostalgic mood.\nMJ v6, Captured at eye-level, a close-up shot of a young woman with long dark brown hair, wearing a green bikini top adorned with yellow and orange flowers. The woman's body is partially submerged in a body of water, her eyes are slightly open. The background is blurred, with a stone wall visible behind her. The sun is shining on the right side of the image, casting a shadow on the wall.\nA vibrant and summery close-up of a young woman partially submerged in water, wearing a floral green bikini top. The image captures natural lighting, with the background blurred to enhance the subject's focus.\nMJ v6, a woman with long dark brown hair stands in front of a stark white wall. She is dressed in a sleeveless black and white dress, adorned with a checkered pattern. Her eyes are a deep blue, and her lips are pursed. Her hair cascades over her shoulders, adding a touch of warmth to her face. The lighting is subdued, creating a stark contrast to the woman's outfit.\nA stark, minimalist portrait of a woman in a checkered dress. The subdued lighting and simple white background emphasize her expressive features and contrast with her bold outfit.\nMJ v6, a beautiful young woman with long brown hair is seated in a field of lavender flowers. She is dressed in a cream-colored bra with a red belt tied around her waist. Her bra is tied in a knot at the center of her chest. Her eyes are closed and her lips are pursed. Her hair is pulled back in a ponytail, adding a pop of color to her face. The backdrop is a lush green hillside.\nA serene and dreamy image of a woman in a lavender field. The cream-colored attire and red accents create a harmonious blend with the lush green and vibrant purple surroundings.\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.",
    "Qwen/QwQ-32B-Preview": "QwQ-32B-Preview\nIntroduction\nRequirements\nQuickstart\nCitation\nQwQ-32B-Preview\nIntroduction\nQwQ-32B-Preview is an experimental research model developed by the Qwen Team, focused on advancing AI reasoning capabilities. As a preview release, it demonstrates promising analytical abilities while having several important limitations:\nLanguage Mixing and Code-Switching: The model may mix languages or switch between them unexpectedly, affecting response clarity.\nRecursive Reasoning Loops: The model may enter circular reasoning patterns, leading to lengthy responses without a conclusive answer.\nSafety and Ethical Considerations: The model requires enhanced safety measures to ensure reliable and secure performance, and users should exercise caution when deploying it.\nPerformance and Benchmark Limitations: The model excels in math and coding but has room for improvement in other areas, such as common sense reasoning and nuanced language understanding.\nSpecification:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 32.5B\nNumber of Paramaters (Non-Embedding): 31.0B\nNumber of Layers: 64\nNumber of Attention Heads (GQA): 40 for Q and 8 for KV\nContext Length: Full 32,768 tokens\nFor more details, please refer to our blog. You can also check Qwen2.5 GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/QwQ-32B-Preview\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"How many r in strawberry.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwq-32b-preview,\ntitle = {QwQ: Reflect Deeply on the Boundaries of the Unknown},\nurl = {https://qwenlm.github.io/blog/qwq-32b-preview/},\nauthor = {Qwen Team},\nmonth = {November},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "EPlus-LLM/EPlus-LLMv2": "EPlus-LLM\nüéâ News\nüöÄ Key Features\nüèóÔ∏è Target Users\nüöÄ Quick Start\nüìù Citation\nEPlus-LLM\nEPlus-LLM series, natural language for auto-building energy modeling via LLM\nüéâ News\n‚ö†Ô∏è [2025/05/15] (update #5): A bug has been fixed and the model has been updated. Many thanks to the user for the feedback! Appreciated!!\nüìÑ [2025/04/18] (update #4): The paper related to the EPlus-LLMv2 platform has been accepted for publication in Automation in Construction.\nPaper here.\n‚ö°Ô∏è [2025/01/15] (update #3): We release EPlus-LLMv2, successfully addressing the challenge of auto-building energy modeling (ABEM) in complex scenarios. The new version of the platform supports a wide range of modeling scenarios encountered in real-world building applications, significantly enhancing its breadth and flexibility. Based on comprehensive datasets and a large-scale LLM, we integrate techniques such as LoRA, mixed precision training, and model quantification to reduce computational burden and achieve efficient fine-tuning (without compensating performance).\nPaper coming soon.\nüìÑ [2025/01/14] (update #2): Our paper on using prompt engineering to inform LLMs for automated building energy modeling has been accepted by Energy.\nPaper here.\nüî• [2024/05/016] (update #1): We first successfully implement natural language-based auto-building modeling by fine-tuning a large language model (LLM).\nPaper here.\nüöÄ Key Features\nScalability: Auto-generates complex EnergyPlus models, including varying geometries, materials, thermal zones, hourly schedules, and more.\nAccuracy & Efficiency: Achieves 100% modeling accuracy while reducing manual modeling time by over 98%.\nInteraction & Automation: A user-friendly human-AI interface for seamless model creation and customization.\nA user-friendly human-AI interface for EPlus-LLMv2.\nFlexible Design Scenarios:\n‚úÖ Geometry: square, L-, T-, U-, and hollow-square-shaped buildings‚úÖ Roof types: flat, gable, hip ‚Äì customizable attic/ridge height‚úÖ Orientation & windows: custom WWR, window placement, facade-specific controls‚úÖ Walls & materials: thermal properties, insulation types‚úÖ Internal loads: lighting, equipment, occupancy, infiltration/ventilation, schedules, heating/cooling setpoints‚úÖ Thermal zoning: configurable multi-zone layouts with core & perimeter zones\nThe relationship between the prompt and the model.\nüèóÔ∏è Target Users\nThis current platform is designed for engineers, architects, and researchers working in building performance, sustainability, and resilience. It is especially useful during early-stage conceptual design when modeling decisions have the greatest impact.\nEXample scenarios of EPlus-LLMv2.\nüöÄ Quick Start\nHere provides a code snippet to show you how to load the EPlus-LLM and auto-generate building energy models.\n# ‚ö†Ô∏è Please make sure you have adequate GPU memory.\n# ‚ö†Ô∏è Please make sure your EnergyPlus version is 9.6 for successful running.\n# ‚ö†Ô∏è Download the v2_nextpart.idf file from the EPlus-LLMv2 repo and place it in your current working directory.\n# ! pip install -U bitsandbytes -q # pip this repo at your first run\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch\nfrom peft import PeftModel, PeftConfig\n# Load the EPlus-LLMv2 config.\npeft_model_id = \"EPlus-LLM/EPlus-LLMv2\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\n# Load the base LLM, flan-t5-xxl, and tokenizer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xxl\", load_in_8bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n# Load the Lora model\nmodel = PeftModel.from_pretrained(model, peft_model_id)\n# Generation config\ngeneration_config = model.generation_config\ngeneration_config.max_new_tokens = 5000\ngeneration_config.temperature = 0.1\ngeneration_config.top_p = 0.1\ngeneration_config.num_return_sequences = 1\ngeneration_config.pad_token_id = tokenizer.eos_token_id\ngeneration_config.eos_token_id = tokenizer.eos_token_id\n# Please provide your input here ‚Äî a description of the desired building\n# For more details, please refer to the paper: https://doi.org/10.1016/j.autcon.2025.106223\ninput=f\"\"\"\nSimulate a U-shaped building that is 99.73 meters high, with a gable roof.\nThe horizontal segment is 732.31 meters long and 17.54 meters wide.\nThe left vertical segment is 256.31 meters long and 206.96 meters wide.\nThe right vertical segment is 431.54 meters long and 62 meters wide.\nThe roof ridge is 8.77 meters to the length side of the horizontal segment, and 128.16 meters, 215.77 meters to the width side of the vertical segments, respectively.\nThe attic height is 139.71 meters. The building orientation is 62 degrees to the north.\nThe building has 3 thermal zones with each segment as one thermal zone.\nThe window-to-wall ratio is 0.32. The window sill height is 33.91 meters, the window height is 65.82 meters, and the window jamb width is 0.01 meters.\nThe window U-factor is 6.36 W/m2K and the SHGC is 0.89.\nThe wall is made of wood, with a thickness of 0.48 meters and the wall insulation is RSI 1.6 m2K/W, U-factor 0.63 W/m2K.\nThe roof is made of metal, with a thickness of 0.09 meters and the roof insulation is RSI 5.4 m2K/W, U-factor 0.19 W/m2K.\nThe floor is made of concrete, covered with carpet. The ventilation rate is 2.32 ach. The infiltration rate is 0.55 ach.\nThe people density is 16.61 m2/person, the light density is 4.48 W/m2, and the electric equipment density is 22.63 W/m2.\nOccupancy starts at 7:00 and ends at 18:00. The occupancy rate is 1. The unoccupancy rate is 0.3.\nThe heating setpoint is 21.54 Celsius in occupancy period and 15.86 Celsius in unoccupancy period.\nThe cooling setpoint is 22.6 Celsius in occupancy period and 26.72 Celsius in unoccupancy period.\n\"\"\"\n# EPlus-LLM generating...\ninput_ids = tokenizer(input, return_tensors=\"pt\", truncation=False)\ngenerated_ids = model.generate(input_ids = input_ids.input_ids,\nattention_mask = input_ids.attention_mask,\ngeneration_config = generation_config)\ngenerated_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n# Default thermal zones setting\nzone_1 = \"\"\"ZoneHVAC:EquipmentConnections,Thermal Zone 1,Thermal Zone 1 Equipment,Thermal Zone 1 Ideal Loads Supply Inlet,,Thermal Zone 1 Zone Air Node,Thermal Zone 1 Return Outlet;\nZoneHVAC:EquipmentList,Thermal Zone 1 Equipment,SequentialLoad,ZoneHVAC:IdealLoadsAirSystem,Thermal Zone 1 Ideal Loads Air System,1,1,,;\nZoneHVAC:IdealLoadsAirSystem,Thermal Zone 1 Ideal Loads Air System,,Thermal Zone 1 Ideal Loads Supply Inlet,,,50,13,0.0156,0.0077,NoLimit,,,NoLimit,,,,,ConstantSensibleHeatRatio,0.7,None,,,None,NoEconomizer,None,0.7,0.65;\nZoneControl:Thermostat,Thermal Zone 1 Thermostat,Thermal Zone 1,Thermostat Schedule,ThermostatSetpoint:DualSetpoint,Thermostat Setpoint Dual Setpoint,,,,,,,0;\nSizing:Zone,Thermal Zone 1,SupplyAirTemperature,14,11.11,SupplyAirTemperature,40,11.11,0.0085,0.008,Ventilation,,,DesignDay,0,0.000762,0,0,DesignDay,0,0.002032,0.1415762,0.3,,No;\"\"\"\nzone_2 = \"\"\"ZoneHVAC:EquipmentConnections,Thermal Zone 2,Thermal Zone 2 Equipment,Thermal Zone 2 Ideal Loads Supply Inlet,,Thermal Zone 2 Zone Air Node,Thermal Zone 2 Return Outlet;\nZoneHVAC:EquipmentList,Thermal Zone 2 Equipment,SequentialLoad,ZoneHVAC:IdealLoadsAirSystem,Thermal Zone 2 Ideal Loads Air System,1,1,,;\nZoneHVAC:IdealLoadsAirSystem,Thermal Zone 2 Ideal Loads Air System,,Thermal Zone 2 Ideal Loads Supply Inlet,,,50,13,0.0156,0.0077,NoLimit,,,NoLimit,,,,,ConstantSensibleHeatRatio,0.7,None,,,None,NoEconomizer,None,0.7,0.65;\nZoneControl:Thermostat,Thermal Zone 2 Thermostat,Thermal Zone 2,Thermostat Schedule,ThermostatSetpoint:DualSetpoint,Thermostat Setpoint Dual Setpoint,,,,,,,0;\nSizing:Zone,Thermal Zone 2,SupplyAirTemperature,14,11.11,SupplyAirTemperature,40,11.11,0.0085,0.008,Ventilation,,,DesignDay,0,0.000762,0,0,DesignDay,0,0.002032,0.1415762,0.3,,No;\"\"\"\nzone_3 = \"\"\"ZoneHVAC:EquipmentConnections,Thermal Zone 3,Thermal Zone 3 Equipment,Thermal Zone 3 Ideal Loads Supply Inlet,,Thermal Zone 3 Zone Air Node,Thermal Zone 3 Return Outlet;\nZoneHVAC:EquipmentList,Thermal Zone 3 Equipment,SequentialLoad,ZoneHVAC:IdealLoadsAirSystem,Thermal Zone 3 Ideal Loads Air System,1,1,,;\nZoneHVAC:IdealLoadsAirSystem,Thermal Zone 3 Ideal Loads Air System,,Thermal Zone 3 Ideal Loads Supply Inlet,,,50,13,0.0156,0.0077,NoLimit,,,NoLimit,,,,,ConstantSensibleHeatRatio,0.7,None,,,None,NoEconomizer,None,0.7,0.65;\nZoneControl:Thermostat,Thermal Zone 3 Thermostat,Thermal Zone 3,Thermostat Schedule,ThermostatSetpoint:DualSetpoint,Thermostat Setpoint Dual Setpoint,,,,,,,0;\nSizing:Zone,Thermal Zone 3,SupplyAirTemperature,14,11.11,SupplyAirTemperature,40,11.11,0.0085,0.008,Ventilation,,,DesignDay,0,0.000762,0,0,DesignDay,0,0.002032,0.1415762,0.3,,No;\"\"\"\nzone_4 = \"\"\"ZoneHVAC:EquipmentConnections,Thermal Zone 4,Thermal Zone 4 Equipment,Thermal Zone 4 Ideal Loads Supply Inlet,,Thermal Zone 4 Zone Air Node,Thermal Zone 4 Return Outlet;\nZoneHVAC:EquipmentList,Thermal Zone 4 Equipment,SequentialLoad,ZoneHVAC:IdealLoadsAirSystem,Thermal Zone 4 Ideal Loads Air System,1,1,,;\nZoneHVAC:IdealLoadsAirSystem,Thermal Zone 4 Ideal Loads Air System,,Thermal Zone 4 Ideal Loads Supply Inlet,,,50,13,0.0156,0.0077,NoLimit,,,NoLimit,,,,,ConstantSensibleHeatRatio,0.7,None,,,None,NoEconomizer,None,0.7,0.65;\nZoneControl:Thermostat,Thermal Zone 4 Thermostat,Thermal Zone 4,Thermostat Schedule,ThermostatSetpoint:DualSetpoint,Thermostat Setpoint Dual Setpoint,,,,,,,0;\nSizing:Zone,Thermal Zone 4,SupplyAirTemperature,14,11.11,SupplyAirTemperature,40,11.11,0.0085,0.008,Ventilation,,,DesignDay,0,0.000762,0,0,DesignDay,0,0.002032,0.1415762,0.3,,No;\"\"\"\nzone_5 = \"\"\"ZoneHVAC:EquipmentConnections,Thermal Zone 5,Thermal Zone 5 Equipment,Thermal Zone 5 Ideal Loads Supply Inlet,,Thermal Zone 5 Zone Air Node,Thermal Zone 5 Return Outlet;\nZoneHVAC:EquipmentList,Thermal Zone 5 Equipment,SequentialLoad,ZoneHVAC:IdealLoadsAirSystem,Thermal Zone 5 Ideal Loads Air System,1,1,,;\nZoneHVAC:IdealLoadsAirSystem,Thermal Zone 5 Ideal Loads Air System,,Thermal Zone 5 Ideal Loads Supply Inlet,,,50,13,0.0156,0.0077,NoLimit,,,NoLimit,,,,,ConstantSensibleHeatRatio,0.7,None,,,None,NoEconomizer,None,0.7,0.65;\nZoneControl:Thermostat,Thermal Zone 5 Thermostat,Thermal Zone 5,Thermostat Schedule,ThermostatSetpoint:DualSetpoint,Thermostat Setpoint Dual Setpoint,,,,,,,0;\nSizing:Zone,Thermal Zone 5,SupplyAirTemperature,14,11.11,SupplyAirTemperature,40,11.11,0.0085,0.008,Ventilation,,,DesignDay,0,0.000762,0,0,DesignDay,0,0.002032,0.1415762,0.3,,No;\"\"\"\ngenerated_output = generated_output.replace(\";\",\";\\n\")\ngenerated_output = generated_output.replace(\"Ideal Load System Setting for Thermal Zone 1;\", zone_1)\ngenerated_output = generated_output.replace(\"Ideal Load System Setting for Thermal Zone 2;\", zone_2)\ngenerated_output = generated_output.replace(\"Ideal Load System Setting for Thermal Zone 3;\", zone_3)\ngenerated_output = generated_output.replace(\"Ideal Load System Setting for Thermal Zone 4;\", zone_4)\ngenerated_output = generated_output.replace(\"Ideal Load System Setting for Thermal Zone 5;\", zone_5)\n# Load the rest port of IDF file.\nfile_path = \"v2_nextpart.idf\" # File is in the repo. Please download.\noutput_path = \"v2_final.idf\"\n# Output the building energy model in IDF file\nwith open(file_path, 'r', encoding='utf-8') as file:\nnextpart = file.read()\nfinal_text = nextpart + \"\\n\\n\" + generated_output\nwith open(output_path, 'w', encoding='utf-8') as f:\nf.write(final_text)\nprint(f\"Building Energy Model Auto-Generated: {output_path}\")\nüìù Citation\nIf you find our work helpful, feel free to give us a cite.\n@article{jiang2025EPlus-LLMv2,\nauthor    = {Gang Jiang and Jianli Chen},\ntitle     = {Efficient fine-tuning of large language models for automated building energy modeling in complex cases},\njournal   = {Automation in Construction},\nvolume    = {175},\npages     = {106223},\nyear      = {2025},\nmonth     = {July},\ndoi       = {https://doi.org/10.1016/j.autcon.2025.106223}}\n@article{jiang2025prompting,\nauthor    = {Gang Jiang and Zhihao Ma and Liang Zhang and Jianli Chen},\ntitle     = {Prompt engineering to inform large language models in automated building energy modeling},\njournal   = {Energy},\nvolume    = {316},\npages     = {134548},\nyear      = {2025},\nmonth     = {Feb},\ndoi       = {https://doi.org/10.1016/j.energy.2025.134548}}\n@article{jiang2025EPlus-LLM,\nauthor    = {Gang Jiang and Zhihao Ma and Liang Zhang and Jianli Chen},\ntitle     = {EPlus-LLM: A large language model-based computing platform for automated building energy modeling},\njournal   = {Applied Energy},\nvolume    = {367},\npages     = {123431},\nyear      = {2024},\nmonth     = {Aug},\ndoi       = {https://doi.org/10.1016/j.apenergy.2024.123431}}",
    "MarsupialAI/Monstral-123B-v2": "Monstral 123B v2\nPrompt Format\nBraggadocio\nMonstral 123B v2\nA Mistral-Large merge\nThis model is a hybrid merge of Behemoth 1.2, Tess, and Magnum V4.  The intention was to do a three-way slerp merge, which is technically\nnot possible.  To simulate the effeect of a menage-a-slerp, I slerped B1.2 with tess, then separately did B1.2 with magnum.  I then did a\nmodel stock merge of those two slerps using B1.2 as the base.  Somehow, it worked out spectacularly well.  Sometimes dumb ideas pay off.\nMergefuel:\nTheDrummer/Behemoth-123B-v1.2\nanthracite-org/magnum-v4-123b\nmigtissera/Tess-3-Mistral-Large-2-123B\nSee recipe.txt for full details.\nImprovements over Monstral v1:  Drummer's 1.2 tune of behemoth is a marked improvement over the original, and the addition ot tess to the\nmix really makes the creativity pop.  I seem to have dialed out the rapey magnum influence, without stripping it of the ability to get mean\nand/or dirty when the situation actually calls for it.  The RP output of this model shows a lot more flowery and \"literary\" description of\nscenes and activities.  It's more colorful and vibrant.  Repitition is dramatically reduced, as is slop (though to a lesser extent).  The\nannoying tendency to double-describe things with \"it was X, almost Y\" is virtually gone.  Do you like a slow-burn story that builds over\ntime?  Well good fucking news, because v2 excels at that.\nThe only complaint I've received is occasional user impersonation with certain cards.  I've not seen this myself on any of my cards, so I\nhave to assume it's down to the specific formatting on specific cards.  I don't want to say it's a skill issue, but...\nThis model is uncensored and perfectly capable of generating objectionable material.  I have not observed it injecting NSFW content into\nSFW scenarios, but no guarentees can be made.  As with any LLM, no factual claims made by the model should be taken at face value.  You\nknow that boilerplate safety disclaimer that most professional models have? Assume this has it too.  This model is for entertainment\npurposes only.\nGGUFs:  https://huggingface.co/MarsupialAI/Monstral-123B-v2_GGUF\nPrompt Format\nMetharme seems to work flawlessly.  In theory, mistral V3 or possibly even chatml should work to some extent, but meth was providing such\nhigh quality output that I couldn't even be bothered to test the others.  Just do meth, kids.\nIf you really want to kick it up a notch, use Konnect's methception prompt.  It's available as an all-in-one sillytavern preset, and as an\nabridged plaintext prompt to use as a sysprompt or character card insertion. https://huggingface.co/Konnect1221/Methception-Llamaception-SillyTavern-Preset\nBraggadocio\nAs of 1/14/25, this model is #4 on the UGI leaderboard overall, and #2 for open-weight models (just behind a 405b finetune).  Imagine how\nwell it would score if I knew what I was doing.",
    "TommyNgx/YOLOv10-Fire-and-Smoke-Detection": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nYOLOv10: Real-Time Fire and Smoke Detection\nModel Details\nHow to Use the Model\nInference Widget\nUsage with Python\nYOLOv10: Real-Time Fire and Smoke Detection\nThis repository contains a YOLOv10 model trained for real-time fire and smoke detection. The model uses the Ultralytics YOLO framework to perform object detection with high accuracy and efficiency. Users can adjust the confidence and IoU thresholds for optimal detection results.\nModel Details\nModel Type: YOLOv8 (adapted for YOLOv10 features)\nTask: Object Detection\nFramework: PyTorch\nInput Size: Adjustable (default: 640x640)\nClasses Detected: Fire, Smoke\nFile: best.pt\nHow to Use the Model\nThis model is hosted on Hugging Face and can be accessed via the Inference Widget or programmatically using the Hugging Face Transformers pipeline.\nInference Widget\nUpload an image to the widget below and adjust the following:\nConfidence Threshold: Minimum confidence level for predictions (default: 0.25).\nIoU Threshold: Minimum IoU level for object matching (default: 0.45).\nImage Size: Resize input image (default: 640x640).\nUsage with Python\nTo use the model programmatically:\nimport torch\nfrom ultralytics import YOLO\nfrom PIL import Image\n# Load the model\nmodel_path = \"pytorch_model.bin\"\nstate_dict = torch.load(model_path, map_location=\"cpu\")\n# Initialize the YOLO model\nmodel = YOLO()  # Replace with the correct YOLO class\nmodel.load_state_dict(state_dict)\n# Run inference\nimage = Image.open(\"path/to/image.jpg\")\nresults = model.predict(image, conf=0.25, iou=0.45)\nresults.show()"
}