{
    "noctrex/Qwen3-VL-30B-A3B-Instruct-MXFP4_MOE-GGUF": "This is a MXFP4_MOE quantization of the model Qwen3-VL-30B-A3B-Instruct\nOriginal model: https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Instruct\nThis GGUF quant has been made possible due to the excellent work from yairpatch and Thireus, and anyone else I forgot to mention\nAs of 2025-10-22 this is still experimental and should be treated as such.\nIn order to run it you must download a custom version of llama.cpp from here:\nhttps://github.com/Thireus/llama.cpp/releases/tag/tr-qwen3-vl-6-b7106-495c611",
    "noctrex/Qwen3-VL-30B-A3B-Thinking-MXFP4_MOE-GGUF": "This is a MXFP4_MOE quantization of the model Qwen3-VL-30B-A3B-Thinking\nOriginal model: https://huggingface.co/Qwen/Qwen3-VL-30B-A3B-Thinking\nThis GGUF quant has been made possible due to the excellent work from yairpatch and Thireus, and anyone else I forgot to mention\nAs of 2025-10-22 this is still experimental and should be treated as such.\nIn order to run it you must download a custom version of llama.cpp from here:\nhttps://github.com/Thireus/llama.cpp/releases/tag/tr-qwen3-vl-4-b7062-b471ef7",
    "mradermacher/Ling-flash-2.0-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/inclusionAI/Ling-flash-2.0\nFor a convenient overview and download list, visit our model page for this model.\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Ling-flash-2.0-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n37.8\nGGUF\nQ3_K_S\n44.7\nGGUF\nQ3_K_M\n49.4\nlower quality\nPART 1 PART 2\nQ3_K_L\n53.5\nPART 1 PART 2\nIQ4_XS\n55.6\nPART 1 PART 2\nQ4_K_S\n58.7\nfast, recommended\nPART 1 PART 2\nQ4_K_M\n62.5\nfast, recommended\nPART 1 PART 2\nQ5_K_S\n71.0\nPART 1 PART 2\nQ5_K_M\n73.3\nPART 1 PART 2\nQ6_K\n84.6\nvery good quality\nPART 1 PART 2 PART 3\nQ8_0\n109.5\nfast, best quality\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "QuantTrio/Qwen3-VL-32B-Instruct-AWQ": "Qwen3-VL-32B-Instruct-AWQ\n„ÄêDependencies / Installation„Äë\n„ÄêvLLM Startup Command„Äë\n„ÄêLogs„Äë\n„ÄêModel Files„Äë\n„ÄêModel Download„Äë\n„ÄêOverview„Äë\nQwen3-VL-32B-Instruct\nModel Performance\nQuickstart\nUsing ü§ó Transformers to Chat\nGeneration Hyperparameters\nCitation\nQwen3-VL-32B-Instruct-AWQ\nBase Model: Qwen/Qwen3-VL-32B-Instruct\n„ÄêDependencies / Installation„Äë\nAs of 2025-10-22, create a fresh Python environment and run:\nuv venv\nsource .venv/bin/activate\n# Install vLLM >=0.11.0\nuv pip install -U vllm\n# Install Qwen-VL utility library (recommended for offline inference)\nuv pip install qwen-vl-utils==0.0.14\nvLLM Installation Guide\n„ÄêvLLM Startup Command„Äë\nCONTEXT_LENGTH=32768\nvllm serve \\\nQuantTrio/Qwen3-VL-32B-Instruct-AWQ \\\n--served-model-name My_Model \\\n--swap-space 4 \\\n--max-num-seqs 8 \\\n--max-model-len $CONTEXT_LENGTH \\\n--gpu-memory-utilization 0.9 \\\n--tensor-parallel-size 2 \\\n--trust-remote-code \\\n--disable-log-requests \\\n--host 0.0.0.0 \\\n--port 8000\n„ÄêLogs„Äë\n2025-10-22\n1. Initial commit\n„ÄêModel Files„Äë\nFile Size\nLast Updated\n20 GiB\n2025-10-22\n„ÄêModel Download„Äë\nfrom huggingface_hub  import snapshot_download\nsnapshot_download('QuantTrio/Qwen3-VL-32B-Instruct-AWQ', cache_dir=\"your_local_path\")\n„ÄêOverview„Äë\nQwen3-VL-32B-Instruct\nMeet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless text‚Äìvision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.\nDeepStack: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.\nText‚ÄìTimestamp Alignment: Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-32B-Instruct.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.\nThe code of Qwen3-VL has been in the latest Hugging Face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ü§ó Transformers to Chat\nHere we show a code snippet to show how to use the chat model with transformers:\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-32B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-32B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-32B-Instruct\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\ninputs = inputs.to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.8\nexport top_k=20\nexport temperature=0.7\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport out_seq_length=16384\nText\nexport greedy='false'\nexport top_p=1.0\nexport top_k=40\nexport repetition_penalty=1.0\nexport presence_penalty=2.0\nexport temperature=1.0\nexport out_seq_length=32768\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "prithivMLmods/Qwen3-VL-32B-Instruct-abliterated": "Qwen3-VL-32B-Instruct-abliterated\nKey Highlights\nQuick Start with Transformers\nIntended Use\nLimitations\nQwen3-VL-32B-Instruct-abliterated\nQwen3-VL-32B-Instruct-abliterated is an abliterated (v1.0) variant of Qwen3-VL-32B-Instruct, designed for Abliterated Reasoning and Captioning.\nThis model is optimized to generate detailed, descriptive captions and reasoning outputs across a wide range of visual and multimodal contexts‚Äîincluding complex, sensitive, or nuanced content‚Äîwhile supporting diverse aspect ratios and resolutions.\nKey Highlights\nAbliterated / Uncensored Captioning ‚Äì Fine-tuned to bypass conventional content filters while preserving factual, descriptive, and reasoning-rich outputs.\nHigh-Fidelity Descriptions ‚Äì Generates comprehensive captions and reasoning for general, artistic, technical, abstract, or low-context images.\nRobust Across Aspect Ratios ‚Äì Performs consistently across wide, tall, square, and irregular image dimensions.\nVariational Detail Control ‚Äì Capable of producing outputs ranging from concise summaries to fine-grained, intricate descriptions and reasoning.\nFoundation on Qwen3-VL-32B Architecture ‚Äì Built upon Qwen3-VL-32B-Instruct‚Äôs advanced multimodal reasoning and instruction-following capabilities.\nMultilingual Output Capability ‚Äì Primarily optimized for English, with adaptability for multilingual prompts through prompt engineering.\nQuick Start with Transformers\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"prithivMLmods/Qwen3-VL-32B-Instruct-abliterated\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\nprocessor = AutoProcessor.from_pretrained(\"prithivMLmods/Qwen3-VL-32B-Instruct-abliterated\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Provide a detailed caption and reasoning for this image.\"},\n],\n}\n]\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n).to(\"cuda\")\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed,\nskip_special_tokens=True,\nclean_up_tokenization_spaces=False\n)\nprint(output_text)\nIntended Use\nThis model is suited for:\nGenerating detailed, uncensored captions and reasoning for general-purpose or artistic datasets.\nResearch in content moderation, red-teaming, and generative safety evaluation.\nEnabling descriptive captioning and reasoning for visual datasets typically excluded from mainstream models.\nCreative applications such as storytelling, art generation, or multimodal reasoning tasks.\nCaptioning and reasoning for non-standard aspect ratios and stylized visual content.\nLimitations\nMay produce explicit, sensitive, or offensive descriptions depending on the image content and prompts.\nNot recommended for production systems requiring strict content moderation.\nOutput style, tone, and reasoning may vary based on input phrasing.\nAccuracy can fluctuate for unfamiliar, synthetic, or highly abstract visual content.",
    "prithivMLmods/Qwen3-VL-32B-Thinking-abliterated": "Qwen3-VL-32B-Thinking-abliterated\nKey Highlights\nQuick Start with Transformers\nIntended Use\nLimitations\nQwen3-VL-32B-Thinking-abliterated\nQwen3-VL-32B-Thinking-abliterated is an abliterated (v1.0) variant of Qwen3-VL-32B-Thinking, designed for Abliterated Reasoning and Captioning.\nThis model is optimized to generate detailed, descriptive captions and reasoning outputs across a wide range of visual and multimodal contexts‚Äîincluding complex, sensitive, or nuanced content‚Äîwhile supporting diverse aspect ratios and resolutions.\nKey Highlights\nAbliterated / Uncensored Captioning ‚Äì Fine-tuned to bypass conventional content filters while preserving factual, descriptive, and reasoning-rich outputs.\nHigh-Fidelity Descriptions ‚Äì Generates comprehensive captions and reasoning for general, artistic, technical, abstract, or low-context images.\nRobust Across Aspect Ratios ‚Äì Performs consistently across wide, tall, square, and irregular image dimensions.\nVariational Detail Control ‚Äì Capable of producing outputs ranging from concise summaries to fine-grained, intricate descriptions and reasoning.\nFoundation on Qwen3-VL-32B Architecture ‚Äì Built upon Qwen3-VL-32B-Thinking‚Äôs advanced multimodal reasoning and instruction-following capabilities.\nMultilingual Output Capability ‚Äì Primarily optimized for English, with adaptability for multilingual prompts through prompt engineering.\nQuick Start with Transformers\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"prithivMLmods/Qwen3-VL-32B-Thinking-abliterated\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\nprocessor = AutoProcessor.from_pretrained(\"prithivMLmods/Qwen3-VL-32B-Thinking-abliterated\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Provide a detailed caption and reasoning for this image.\"},\n],\n}\n]\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n).to(\"cuda\")\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed,\nskip_special_tokens=True,\nclean_up_tokenization_spaces=False\n)\nprint(output_text)\nIntended Use\nGenerating detailed, uncensored captions and reasoning for general-purpose or artistic datasets.\nResearch in content moderation, red-teaming, and generative safety evaluation.\nEnabling descriptive captioning and reasoning for visual datasets typically excluded from mainstream models.\nCreative applications such as storytelling, art generation, or multimodal reasoning tasks.\nCaptioning and reasoning for non-standard aspect ratios and stylized visual content.\nLimitations\nMay produce explicit, sensitive, or offensive descriptions depending on the image content and prompts.\nNot recommended for production systems requiring strict content moderation.\nOutput style, tone, and reasoning may vary based on input phrasing.\nAccuracy can fluctuate for unfamiliar, synthetic, or highly abstract visual content.",
    "prithivMLmods/Qwen3-VL-2B-Thinking-abliterated": "Qwen3-VL-2B-Thinking-abliterated\nKey Highlights\nQuick Start with Transformers\nIntended Use\nLimitations\nQwen3-VL-2B-Thinking-abliterated\nQwen3-VL-2B-Thinking-abliterated is an abliterated (v1.0) variant of Qwen3-VL-2B-Thinking, designed for Abliterated Reasoning and Captioning.\nThis model is optimized to generate detailed, descriptive captions and reasoning outputs across a wide range of visual and multimodal contexts‚Äîincluding complex, sensitive, or nuanced content‚Äîwhile supporting diverse aspect ratios and resolutions.\nKey Highlights\nAbliterated / Uncensored Captioning ‚Äì Fine-tuned to bypass conventional content filters while preserving factual, descriptive, and reasoning-rich outputs.\nHigh-Fidelity Descriptions ‚Äì Generates comprehensive captions and reasoning for general, artistic, technical, abstract, or low-context images.\nRobust Across Aspect Ratios ‚Äì Performs consistently across wide, tall, square, and irregular image dimensions.\nVariational Detail Control ‚Äì Capable of producing outputs ranging from concise summaries to fine-grained, intricate descriptions and reasoning.\nFoundation on Qwen3-VL-2B-Thinking Architecture ‚Äì Built upon Qwen3-VL-2B-Thinking‚Äôs strong multimodal reasoning and instruction-following capabilities.\nMultilingual Output Capability ‚Äì Primarily optimized for English, with adaptability for multilingual prompts through prompt engineering.\nQuick Start with Transformers\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"prithivMLmods/Qwen3-VL-2B-Thinking-abliterated\",\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\nprocessor = AutoProcessor.from_pretrained(\"prithivMLmods/Qwen3-VL-2B-Thinking-abliterated\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Provide a detailed caption and reasoning for this image.\"},\n],\n}\n]\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n).to(\"cuda\")\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed,\nskip_special_tokens=True,\nclean_up_tokenization_spaces=False\n)\nprint(output_text)\nIntended Use\nThis model is suited for:\nGenerating detailed, uncensored captions and reasoning for general-purpose or artistic datasets.\nResearch in content moderation, red-teaming, and generative safety evaluation.\nEnabling descriptive captioning and reasoning for visual datasets typically excluded from mainstream models.\nCreative applications such as storytelling, art generation, or multimodal reasoning tasks.\nCaptioning and reasoning for non-standard aspect ratios and stylized visual content.\nLimitations\nMay produce explicit, sensitive, or offensive descriptions depending on the image content and prompts.\nNot recommended for production systems requiring strict content moderation.\nOutput style, tone, and reasoning may vary based on input phrasing.\nAccuracy can fluctuate for unfamiliar, synthetic, or highly abstract visual content.",
    "Nanbeige/Nanbeige4-3B-Thinking-2510": "1. Introduction\n2. Model Summary\n3. Model Performance\n4. Quickstart\n5. Limitations\n6. Citation\n7. Contact\n1. Introduction\nNanbeige4-3B-Thinking is a 3B-parameter reasoning model within the fourth-generation Nanbeige LLM family.\nIt showcases that even compact models can achieve advanced reasoning abilities through continuous enhancements in data quality and training methodologies.\nTo support research and technological advancement in the open-source community, we have open-sourced the Nanbeige4-3B-Thinking model together with its technical methodology.\n2. Model Summary\nPre-Training\nWe constructed a comprehensive 23T-tokens training corpus from web texts, books, code, and papers, meticulously filtered through a hybrid strategy of tagging-based scoring and retrieval-based recalling.\nThis foundation was then augmented with knowledge-dense and reasoning-intensive synthetic data, including Q&A pairs, textbooks, and Long-COTs, which significantly benefited the downstream task performance.\nWe designed an innovative FG-WSD (Fine-Grained Warmup-Stable-Decay) training scheduler, meticulously refining the conventional WSD approach.\nThis scheduler was implemented with a fine-grained, quality-progressive data curriculum, dividing the Stable stage into multiple phases with progressively improved data mixtures. Compared to the vanilla WSD, our method achieved notable performance gains. During the Decay stage, we increased the proportion of math, code, synthetic QA, and synthetic Long-COT data to further enhance reasoning capabilities.\nStage\nTraining Tokens\nLearning Rate\nWarmup Stage\n0.1T\n0 ‚Äî‚Äî> 4.5e-4\nDiversity-Enriched Stable Stage\n12.4T\nConstant 4.5e-4\nHigh-Quality Stable Stage\n6.5T\nConstant 4.5e-4\nDecay and Long-Context Stage\n4T\n4.5e-4 ‚Äî‚Äî> 1.5e-6\nPost-Training\nSFT phase. We constructed a collection of over 30 million high-quality Long Chain-of-Thought (Long-CoT) samples to support multi-stage curriculum learning.\nBy integrating both rule-based and model-based verification methods, we not only ensured sample accuracy but also enhanced the comprehensiveness and instructional value of each training example compared to alternative candidates.\nThis rich diversity in instructions and high response quality equipped the model to achieve outstanding performance across a variety of benchmarks.\nDistill. Following SFT, we employed the Nanbeige flagship reasoning model as the teacher model to distill the Nanbeige4-3B-Thinking model, and further enhanced the performance.\nWe observed that on-policy distillation provides greater benefits for mathematical reasoning tasks, while off-policy distillation is more effective for general tasks such as human-preference alignment.\nRL phase. We then advanced to a multi-stage, on-policy reinforcement learning phase.\nThis approach leverages verifiable rewards to enhance reasoning capability and a preference reward model to improve alignment, utilizing a carefully filtered blend of real-world and synthetic data calibrated for appropriate difficulty.\n3. Model Performance\nFor model performance comparison, we benchmark our model against recent reasoning LLMs from the Qwen3 series.\nAll models are evaluated under identical configurations to ensure fairness.\nThe results show that our model outperforms the baselines across a range of mainstream benchmarks, including math, science, creative writing, tool use, and human preference alignment.\nModel\nAIME24\nAIME25\nGPQA\nSuper-GPQA\nScience-QA\nWriting-Bench\nBFCL-V4-Agentic\nArena-hard2\nQwen3-8B-Thinking-2504\n76.0\n67.3\n62.0\n39.1\n24.8\n74.8\n14.4\n26.4\nQwen3-14B-Thinking-2504\n79.3\n70.4\n64.0\n46.8\n23.2\n77.2\n17.0\n40.5\nQwen3-4B-Thinking-2507\n83.3\n81.3\n67.2\n46.7\n24.4\n84.3\n14.3\n37.7\nNanbeige4-3B-Thinking-2510\n87.5\n81.7\n77.2\n51.4\n26.0\n85.5\n17.2\n42.9\n4. Quickstart\nFor the chat scenario:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n'Nanbeige/Nanbeige4-3B-Thinking-2510',\nuse_fast=False,\ntrust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n'Nanbeige/Nanbeige4-3B-Thinking-2510',\ntorch_dtype='auto',\ndevice_map='auto',\ntrust_remote_code=True\n)\nmessages = [\n{'role': 'user', 'content': 'Which number is bigger, 9.11 or 9.8?'}\n]\nprompt = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\ntokenize=False\n)\ninput_ids = tokenizer(prompt, add_special_tokens=False, return_tensors='pt').input_ids\noutput_ids = model.generate(input_ids.to('cuda'), eos_token_id=166101)\nresp = tokenizer.decode(output_ids[0][len(input_ids[0]):], skip_special_tokens=True)\nprint(resp)\nFor the tool use scenario:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n'Nanbeige/Nanbeige4-3B-Thinking-2510',\nuse_fast=False,\ntrust_remote_code=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n'Nanbeige/Nanbeige4-3B-Thinking-2510',\ntorch_dtype='auto',\ndevice_map='auto',\ntrust_remote_code=True\n)\nmessages = [\n{'role': 'user',  'content': 'Help me check the weather in Beijing now'}\n]\ntools = [{'type': 'function',\n'function': {'name': 'SearchWeather',\n'description': 'Find out current weather in a certain place on a certain day.',\n'parameters': {'type': 'dict',\n'properties': {'location': {'type': 'string',\n'description': 'A city in china.'},\n'required': ['location']}}}}]\nprompt = tokenizer.apply_chat_template(\nmessages,\ntools,\nadd_generation_prompt=True,\ntokenize=False\n)\ninput_ids = tokenizer(prompt, add_special_tokens=False, return_tensors='pt').input_ids\noutput_ids = model.generate(input_ids.to('cuda'), eos_token_id=166101)\nresp = tokenizer.decode(output_ids[0][len(input_ids[0]):], skip_special_tokens=True)\nprint(resp)\n5. Limitations\nWhile we place great emphasis on the safety of the model during the training process, striving to ensure that its outputs align with ethical and legal requirements, it may not completely avoid generating unexpected outputs due to the model's size and probabilistic nature. These outputs may include harmful content such as bias or discrimination. Please don't propagate such content. We do not assume any responsibility for the consequences resulting from the dissemination of inappropriate information.\n6. Citation\nIf you find our model useful or want to use it in your projects, please kindly cite this Huggingface project.\n7. Contact\nIf you have any questions, please raise an issue or contact us at nanbeige@126.com.",
    "12bitmisfit/Qwen3-Coder-30B-A3B-Instruct_Pruned_REAP-15B-A3B-GGUF": "No model card",
    "azeem23/seqtrack": "README.md exists but content is empty.",
    "MarrLab/DinoBloom": "DinoBloom: A Foundation Model for Generalizable Cell Embeddings in Hematology\nüß† Model Variants\nüöÄ Usage\nüìä Model Performance\nüîß Requirements\nüìö Citation\nüìñ Related Work\nüìÑ License\nDinoBloom: A Foundation Model for Generalizable Cell Embeddings in Hematology\nDinoBloom builds upon DINOv2 (Meta AI) and is trained on 13 diverse publicly available datasets of single cells from peripheral blood and bone marrow.\nüìÑ Paper ‚Ä¢\nüíª GitHub ‚Ä¢\nüì¶ Zenodo\nüß† Model Variants\nDinoBloom is available in four sizes:\nModel\nFeature Dim\nParameters\nCheckpoint\nDinoBloom-S\n384\n22M\npytorch_model_s.bin\nDinoBloom-B\n768\n86M\npytorch_model_b.bin\nDinoBloom-L\n1024\n304M\npytorch_model_l.bin\nDinoBloom-G\n1536\n1136M\npytorch_model_g.bin\nüöÄ Usage\nfrom huggingface_hub import hf_hub_download\nimport torch\nimport torch.nn as nn\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Choose variant: \"s\", \"b\", \"l\", or \"g\"\nvariant = \"b\"\n# Configuration\nvariant_config = {\n\"s\": (\"dinov2_vits14\", 384),\n\"b\": (\"dinov2_vitb14\", 768),\n\"l\": (\"dinov2_vitl14\", 1024),\n\"g\": (\"dinov2_vitg14\", 1536),\n}\ndinov2_model, embed_dim = variant_config[variant]\n# Load base DINOv2 model\nmodel = torch.hub.load(\"facebookresearch/dinov2\", dinov2_model)\n# Download DinoBloom weights\nckpt_path = hf_hub_download(\nrepo_id=\"MarrLab/DinoBloom\",\nfilename=f\"pytorch_model_{variant}.bin\"\n)\nckpt = torch.load(ckpt_path, map_location=\"cpu\")\nnum_tokens = int(1 + (224 / 14) ** 2)\nmodel.pos_embed = nn.Parameter(torch.zeros(1, num_tokens, embed_dim))\nmodel.load_state_dict(ckpt, strict=True)\nmodel.to(device)\nmodel.eval()\n# Get transforms\nfrom torchvision import transforms\ntransform = transforms.Compose([\ntransforms.Resize((224,224)),\ntransforms.ToTensor(),\ntransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n# Apply to image\nfrom PIL import Image\nimg = Image.open(\"path/to/cell_image\")\nimg_tensor = transform(img).unsqueeze(0).to(device)\n# Get features\nwith torch.no_grad():\nfeatures = model(img_tensor)\nprint(f\"Features shape: {features.shape}\")  # [1, 768] for DinoBloom-B\nüìä Model Performance\nDinoBloom outperforms existing medical and non-medical vision models in:\nLinear probing and k-nearest neighbor evaluations for cell-type classification\nWeakly supervised multiple-instance learning (MIL) for acute myeloid leukemia subtyping\nSee our paper for detailed benchmarks.\nüîß Requirements\npip install torch torchvision huggingface_hub\nüìö Citation\nIf you use DinoBloom in your research, please cite:\n@inproceedings{koch2024dinobloom,\ntitle={DinoBloom: a foundation model for generalizable cell embeddings in hematology},\nauthor={Koch, Valentin and Wagner, Sophia J and Kazeminia, Salome and Sancar, Ece and Hehr, Matthias and Schnabel, Julia A and Peng, Tingying and Marr, Carsten},\nbooktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},\npages={520--530},\nyear={2024},\norganization={Springer}\n}\nüìñ Related Work\nDinoBloom builds upon:\nDINOv2 - Self-supervised vision transformers\nOriginal DinoBloom Paper - MICCAI 2024\nüìÑ License\nApache 2.0 - See LICENSE file for details.\nFor questions or issues, please open an issue on GitHub or contact the authors.",
    "thenexthub/OpenModel-1T-A50B-Instruct": "üß† OpenModel-1T-A50B-Instruct\nüîç Overview\n‚öôÔ∏è Key Features\nüìä Evaluation\nüß© Benchmark Results\nüß¨ Design Philosophy\nüß¨ Pre-Training at Trillion Scale\nüí° Applications\nüõ°Ô∏è Responsible AI\nüì¶ Technical Specs\nüß≠ Citation\nüß† OpenModel-1T-A50B-Instruct\nRepository: thenexthub/OpenModel-1T-A50B-Instruct\nOrganization: NeXTHub\nModel Type: Mixture-of-Experts (MoE) Large Language Model\nParameters: 1 Trillion total | 50 Billion active per forward pass\nContext Length: 128K tokens\nArchitecture: Evo-CoT MoE Transformer (Evolutionary Chain-of-Thought)\nTraining Tokens: 20+ Trillion reasoning-dense, high-quality tokens\nüîç Overview\nOpenModel-1T-A50B-Instruct represents a major leap in NeXTHub‚Äôs pursuit of scalable, efficient, and deeply reasoning general-purpose AI.\nThe model blends trillion-scale architecture with a Mixture-of-Experts (MoE) system, where 50 billion active parameters are dynamically routed per token ‚Äî balancing raw power and energy efficiency.\nAt its core, OpenModel-1T leverages an Evolutionary Chain-of-Thought (Evo-CoT) process across mid-training and post-training phases, allowing reasoning patterns to ‚Äúevolve‚Äù across checkpoints rather than merely optimize static objectives. This enables emergent meta-reasoning, recursive planning, and adaptive self-correction ‚Äî a new standard in interpretability and coherence.\n‚öôÔ∏è Key Features\nüß© 1T Total | 50B Active MoE Design: Trillion-parameter scale with sparse activation for exceptional throughput efficiency.\nüß† Evo-CoT Training: Evolutionary chain-of-thought reinforcement ‚Äî model learns to reason about its own reasoning.\nüìö 20T+ Token Corpus: Pre-trained on a curated, reasoning-dense dataset spanning code, math, science, multilingual text, and human reasoning.\n‚è±Ô∏è 128K Context Window: Long-context comprehension for entire projects, books, or datasets.\nüßÆ Reasoning-Optimized Objective: Curriculum emphasizing precision in long-form logic and mathematical reasoning.\nüß© Cross-Domain Instruction Tuning: Fine-tuned for professional reasoning, code synthesis, mathematics, and complex dialogue.\nüìä Evaluation\nOpenModel-1T-A50B-Instruct was evaluated against both open-source and closed-source state-of-the-art models, including:\nDeepSeek-V3.1-Terminus\nKimi-K2-Instruct-0905\nGPT-5-main (API)\nGemini-2.5-Pro (API)\nüß© Benchmark Results\nDomain\nBenchmark\nOpenModel-1T-A50B-Instruct\nSOTA Comparison\nMathematics (Competition-Level)\nAIME-25\nExtended Pareto frontier of reasoning length vs. accuracy\n‚úì Superior\nProfessional Math\nMATH-500\nOutperforms by +6.2% over DeepSeek-V3.1\n‚úì Superior\nLogical Reasoning\nARC-C / GPQA\nDemonstrates state-of-the-art coherence and low hallucination rate\n‚úì Superior\nCode Generation\nHumanEval+ / MBPP+\nOutperforms Kimi-K2-Instruct by ~8% pass@1\n‚úì Superior\nGeneral Dialogue\nMT-Bench\nComparable to GPT-5-main; improved factual grounding\n‚úì On Par / Better in Logic Depth\nüß¨ Design Philosophy\nOpenModel-1T was built not just to scale intelligence, but to evolve it.\nThe Evo-CoT process simulates intellectual growth ‚Äî allowing reasoning pathways to mutate, recombine, and self-select under performance feedback, akin to neural evolution.\nThis architecture fuses cognitive diversity with efficiency, enabling the model to ‚Äúthink deeper, not longer.‚Äù\nüß¨ Pre-Training at Trillion Scale\nThe OpenModel architecture was engineered for trillion-scale efficiency ‚Äî ensuring stability and scalability across 1e25‚Äì1e26 FLOPs of compute.\nArchitectural Innovations\n‚öôÔ∏è 1 T total / 50 B active parameters with 1/32 MoE activation ratio\nüß© MTP Layers ‚Äì enhanced compositional reasoning\nüöÄ Aux-loss-free, sigmoid-scoring expert routing with zero-mean updates\nüß† QK Normalization ‚Äì fully stable convergence at scale\nüí° Applications\nAutonomous code generation and debugging\nAI-assisted scientific research\nComplex data analytics and mathematical modeling\nMulti-agent collaboration and orchestration\nEducational tutoring and theorem proving\nüõ°Ô∏è Responsible AI\nOpenModel-1T was trained with strict filtering of unsafe, biased, or synthetic low-fidelity data.\nSafety layers include prompt-level moderation, reasoning self-checks, and toxicity filters.\nThe model does not produce or endorse harmful, biased, or illegal content.\nüì¶ Technical Specs\nSpecification\nDetail\nTotal Parameters\n1 Trillion\nActive Parameters\n50 Billion\nArchitecture\nTransformer-MoE with Evo-CoT\nTraining Tokens\n20+ Trillion\nContext Length\n128K\nPrecision\nFP8 / BF16 hybrid\nLicense\nApache-2.0 with AI-Responsible Use Addendum\nüß≠ Citation\nIf you use OpenModel-1T in your research or products, please cite:\n@misc{thenexthub-openmodel-1t-a50b,\ntitle={OpenModel-1T-A50B-Instruct: Open Source, Trillion-Scale MoE Model with Evolutionary Chain-of-Thought},\nauthor={NeXTHub},\nyear={2025},\nhowpublished={\\url{https://huggingface.co/thenexthub/OpenModel-1T-A50B-Instruct}},\n}",
    "mradermacher/olmOCR-2-7B-1025-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/allenai/olmOCR-2-7B-1025\nFor a convenient overview and download list, visit our model page for this model.\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/olmOCR-2-7B-1025-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nmmproj-Q8_0\n1.0\nmulti-modal supplement\nGGUF\nmmproj-f16\n1.5\nmulti-modal supplement\nGGUF\nQ2_K\n3.1\nGGUF\nQ3_K_S\n3.6\nGGUF\nQ3_K_M\n3.9\nlower quality\nGGUF\nQ3_K_L\n4.2\nGGUF\nIQ4_XS\n4.4\nGGUF\nQ4_K_S\n4.6\nfast, recommended\nGGUF\nQ4_K_M\n4.8\nfast, recommended\nGGUF\nQ5_K_S\n5.4\nGGUF\nQ5_K_M\n5.5\nGGUF\nQ6_K\n6.4\nvery good quality\nGGUF\nQ8_0\n8.2\nfast, best quality\nGGUF\nf16\n15.3\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "mlx-community/olmOCR-2-7B-1025-bf16": "mlx-community/olmOCR-2-7B-1025-bf16\nUse with mlx\nmlx-community/olmOCR-2-7B-1025-bf16\nThis model was converted to MLX format from allenai/olmOCR-2-7B-1025 using mlx-vlm version 0.3.4.\nRefer to the original model card for more details on the model.\nUse with mlx\npip install -U mlx-vlm\npython -m mlx_vlm.generate --model mlx-community/olmOCR-2-7B-1025-bf16 --max-tokens 100 --temperature 0.0 --prompt \"Describe this image.\" --image <path_to_image>",
    "Kwai-Klear/Klear-AgentForge-8B-SFT": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nREADME.md exists but content is empty.",
    "jd-opensource/JSL-joysafety-v2": "",
    "CodeGoat24/UnifiedReward-Edit-qwen-3b": "UnifiedReward-Edit-qwen-7B\nCitation\nUnifiedReward-Edit-qwen-7B\n[2025/10/23] üî•üî•üî• We release UnifiedReward-Edit-3b, a unified reward model for both Text-to-Image and Image-to-Image generation!!\nFor image editing reward task, our models support:\nPairwise Rank ‚Äî directly judge which of two edited images is better.\nPairwise Score ‚Äî assign a separate score to each image in a pair.\nPointwise Score ‚Äî rate a single image on two axes: instruction-following and overall image quality.\nüöÄ The image editing reward inference code is available at UnifiedReward-Edit/ directory, while T2I inference code is unchanged from previous models. The editing training data is preprocessed from EditScore and EditReward and will be released soon. We sincerely appreciate all contributors!!\nFor further details, please refer to the following resources:\nüì∞ Paper: https://arxiv.org/pdf/2503.05236\nü™ê Project Page: https://codegoat24.github.io/UnifiedReward/\nü§ó Model Collections: https://huggingface.co/collections/CodeGoat24/unifiedreward-models-67c3008148c3a380d15ac63a\nü§ó Dataset Collections: https://huggingface.co/collections/CodeGoat24/unifiedreward-training-data-67c300d4fd5eff00fa7f1ede\nüëã Point of Contact: Yibin Wang\nCitation\n@article{unifiedreward,\ntitle={Unified reward model for multimodal understanding and generation},\nauthor={Wang, Yibin and Zang, Yuhang and Li, Hao and Jin, Cheng and Wang, Jiaqi},\njournal={arXiv preprint arXiv:2503.05236},\nyear={2025}\n}",
    "Cogent-ai/cogent-csp-15m": "",
    "mradermacher/UnifiedReward-Edit-qwen-7b-GGUF": "",
    "noctrex/Tongyi-DeepResearch-30B-A3B-MXFP4_MOE-GGUF": "",
    "beyoru/EvolLLM-Linh": "",
    "small-models-for-glam/Qwen3-VL-2B-catmus": "",
    "mradermacher/Huihui-Ling-mini-2.0-abliterated-i1-GGUF": "",
    "Volkopat/DeepSeek-DeepEncoder": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nDeepEncoder (Extracted from DeepSeek-OCR)\nOverview\nModel Files\nArchitecture\nUsage\nTraining\nSource\nDeepEncoder (Extracted from DeepSeek-OCR)\nOverview\nThis directory contains the encoder components extracted from DeepSeek-OCR.\nModel Files\nsam_encoder.pth: SAM ViT-B encoder (95,569,152 params, 364.6 MB)\nclip_encoder.pth: CLIP-Large encoder (303,177,728 params, 1156.6 MB)\nprojector.pth: Linear projector (2,622,720 params, 10.0 MB)\nconfig.json: Model configuration\nTotal: 401,369,600 parameters\nArchitecture\nImage (1024√ó1024) ‚Üí SAM (95M) ‚Üí 16√ó Conv ‚Üí CLIP (303M) ‚Üí Projector (3M) ‚Üí 256 vision tokens\nUsage\nimport torch\nfrom deepencoder import build_sam_vit_b, build_clip_l, MlpProjector\nfrom easydict import EasyDict as adict\n# Load models\nsam = build_sam_vit_b(checkpoint=None)\nsam.load_state_dict(torch.load('sam_encoder.pth'))\nclip = build_clip_l()\nclip.load_state_dict(torch.load('clip_encoder.pth'))\nprojector_cfg = adict({'projector_type': 'linear', 'input_dim': 2048, 'n_embed': 1280})\nprojector = MlpProjector(projector_cfg)\nprojector.load_state_dict(torch.load('projector.pth'))\n# Run encoder\nvision_tokens = encode(image)  # [1, 256, 1280]\nTraining\nThese weights are:\nInitialized from pretrained SAM (SA-1B) + CLIP (LAION-2B)\nFine-tuned together on optical compression/OCR tasks\nOptimized for text preservation in compressed form\nSource\nExtracted from: deepseek-ai/DeepSeek-OCR",
    "Luke-Bergen/Mineral-Nano-1": "A newer version of this model is available:\nPaddlePaddle/PaddleOCR-VL\nMineral Nano 1 Vision\nModel Details\nArchitecture\nLanguage Model\nVision Encoder\nUsage\nInstallation\nBasic Image Understanding\nMultiple Images\nChat with Images\nLocal Images\nTraining Details\nCapabilities\nLimitations\nIntended Use\nImage Preprocessing\nPerformance Tips\nLicense\nCitation\nContact\nAcknowledgments\nMineral Nano 1 Vision\nMineral Nano 1 Vision is a compact, efficient vision-language model designed for fast inference and low-resource environments with multimodal capabilities.\nModel Details\nModel Name: mineral-nano-1\nModel Type: Vision-Language Model (VLM)\nParameters: ~110M parameters\nContext Length: 2048 tokens\nArchitecture: Transformer-based decoder with vision encoder (12 layers)\nPrecision: BFloat16\nImage Resolution: 224x224\nModalities: Text + Images\nArchitecture\nLanguage Model\nHidden Size: 768\nIntermediate Size: 3072\nAttention Heads: 12\nLayers: 12\nVocabulary Size: 32,000 tokens\nPositional Encoding: RoPE (Rotary Position Embeddings)\nVision Encoder\nImage Size: 224x224\nPatch Size: 16x16\nHidden Size: 768\nLayers: 12\nImage Tokens: 196 per image\nArchitecture: ViT-style encoder\nUsage\nInstallation\npip install transformers pillow torch\nBasic Image Understanding\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom PIL import Image\nimport requests\nmodel_name = \"Luke-Bergen/mineral-nano-1\"\nprocessor = AutoProcessor.from_pretrained(model_name)\nmodel = AutoModelForVision2Seq.from_pretrained(model_name)\n# Load an image\nurl = \"https://example.com/image.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n# Prepare inputs\nprompt = \"<image>What is in this image?\"\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n# Generate response\noutputs = model.generate(**inputs, max_new_tokens=100)\nresponse = processor.decode(outputs[0], skip_special_tokens=True)\nprint(response)\nMultiple Images\nfrom PIL import Image\nimages = [\nImage.open(\"image1.jpg\"),\nImage.open(\"image2.jpg\")\n]\nprompt = \"<image>Describe the first image. <image>Now describe the second image.\"\ninputs = processor(text=prompt, images=images, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=200)\nprint(processor.decode(outputs[0], skip_special_tokens=True))\nChat with Images\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\"},\n{\"type\": \"text\", \"text\": \"What objects are in this image?\"}\n]\n}\n]\n# Apply chat template\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = processor(text=text, images=image, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=150, temperature=0.7)\nprint(processor.decode(outputs[0], skip_special_tokens=True))\nLocal Images\nfrom PIL import Image\n# Load local image\nimage = Image.open(\"path/to/your/image.jpg\")\nprompt = \"<image>Describe what you see in detail.\"\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=256)\nprint(processor.decode(outputs[0], skip_special_tokens=True))\nTraining Details\nFramework: PyTorch with Transformers\nTraining Data: Text + Image pairs\nTraining Duration: [Specify training time]\nHardware: [Specify GPUs used]\nVision Encoder: Pretrained ViT encoder fine-tuned with language model\nCapabilities\n‚úÖ Image description and captioning\n‚úÖ Visual question answering\n‚úÖ Object detection and recognition\n‚úÖ Scene understanding\n‚úÖ Multi-image reasoning\n‚úÖ OCR and text extraction from images\nLimitations\nLimited to 224x224 resolution images\nContext window of 2048 tokens including image tokens\nMay struggle with fine-grained details\nBest for general image understanding tasks\nCompact size means reduced capabilities compared to larger VLMs\nLimited multilingual vision capabilities\nIntended Use\nThis model is designed for:\nEducational purposes and learning VLM architectures\nPrototyping multimodal applications\nLow-resource deployment scenarios\nFast inference with vision capabilities\nMobile and edge device applications\nPersonal projects requiring image understanding\nImage Preprocessing\nImages are automatically:\nResized to 224x224\nNormalized with CLIP-style statistics\nConverted to RGB\nSplit into 16x16 patches (196 total patches)\nPerformance Tips\nUse square images when possible for best results\nEnsure images are clear and well-lit\nKeep prompts concise and specific\nUse batch processing for multiple images\nEnable use_cache=True for faster generation\nLicense\n[Specify your license - e.g., MIT, Apache 2.0, etc.]\nCitation\n@misc{mineral-nano-1-vision,\nauthor = {Luke Bergen},\ntitle = {Mineral Nano 1 Vision: A Compact Vision-Language Model},\nyear = {2025},\npublisher = {HuggingFace},\nurl = {https://huggingface.co/Luke-Bergen/mineral-nano-1}\n}\nContact\nFor questions or issues, please open an issue on the model repository.\nAcknowledgments\nThis model builds upon research in vision transformers and multimodal learning.",
    "remyxai/SpaceQwen3-VL-2B-Thinking": "Model Card for SpaceQwen3-VL-2B-Thinking\nCitation\nModel Card for SpaceQwen3-VL-2B-Thinking\nFinetuned Qwen3-VL-2B-Thinking by Low-Rank Adapters using the SpaceOm dataset created with VQASynth, an open-source multimodal data synthesis pipeline inspired by SpatialVLM\nCitation\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{chen2024spatialvlm,\ntitle = {SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities},\nauthor = {Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brian and Driess, Danny and Florence, Pete and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},\njournal = {arXiv preprint arXiv:2401.12168},\nyear = {2024},\nurl = {https://arxiv.org/abs/2401.12168},\n}",
    "Volkopat/Qwen-VLM-Optical-Encoder": "README.md exists but content is empty.",
    "Shiva7706/RxStruct-Gemma-1B": "",
    "uva-cv-lab/FrameINO_CogVideoX_Stage2_MotionINO_v1.0": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nREADME.md exists but content is empty.",
    "uva-cv-lab/FrameINO_CogVideoX_Stage1_Motion_v1.0": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nREADME.md exists but content is empty."
}