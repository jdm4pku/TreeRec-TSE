{
    "dptech/Uni-Mol2": "README.md exists but content is empty.",
    "Freepik/flux.1-lite-8B-alpha": "A newer version of this model is available:\nFreepik/flux.1-lite-8B\nFlux.1 Lite\nText-to-Image\nMotivation\nFuture work\nComfyUI\nHF spaces ü§ó\nTry it out at Freepik!\nüî• News üî•\nCitation\nAttribution notice\nFlux.1 Lite\nWe are thrilled to announce the alpha release of Flux.1 Lite, an 8B parameter transformer model distilled from the FLUX.1-dev model. This version uses 7 GB less RAM and runs 23% faster while maintaining the same precision (bfloat16) as the original model.\nText-to-Image\nFlux.1 Lite is ready to unleash your creativity! For the best results, we strongly recommend using a guidance_scale of 3.5 and setting n_steps between 22 and 30.\nimport torch\nfrom diffusers import FluxPipeline\nbase_model_id = \"Freepik/flux.1-lite-8B-alpha\"\ntorch_dtype = torch.bfloat16\ndevice = \"cuda\"\n# Load the pipe\nmodel_id = \"Freepik/flux.1-lite-8B-alpha\"\npipe = FluxPipeline.from_pretrained(\nmodel_id, torch_dtype=torch_dtype\n).to(device)\n# Inference\nprompt = \"A close-up image of a green alien with fluorescent skin in the middle of a dark purple forest\"\nguidance_scale = 3.5  # Keep guidance_scale at 3.5\nn_steps = 28\nseed = 11\nwith torch.inference_mode():\nimage = pipe(\nprompt=prompt,\ngenerator=torch.Generator(device=\"cpu\").manual_seed(seed),\nnum_inference_steps=n_steps,\nguidance_scale=guidance_scale,\nheight=1024,\nwidth=1024,\n).images[0]\nimage.save(\"output.png\")\nMotivation\nInspired by Ostris findings, we analyzed the mean squared error (MSE) between the input and output of each block to quantify their contribution to the final result, revealing significant variability.\nAs Ostris pointed out, not all blocks contribute equally. While skipping just one of the early MMDiT or late DiT blocks can significantly impact model performance, skipping any single block in between does not have a significant impact over the final image quality.\nFuture work\nStay tuned! Our goal is to distill FLUX.1-dev further until it can run smoothly on 24 GB consumer-grade GPU cards, maintaining its original precision (bfloat16), and running even faster, making high-quality AI models accessible to everyone.\nComfyUI\nWe've also crafted a ComfyUI workflow to make using Flux.1 Lite even more seamless! Find it in comfy/flux.1-lite_workflow.json.\nThe safetensors checkpoint is available here: flux.1-lite-8B-alpha.safetensors\nHF spaces ü§ó\nYou can also test the model on Flux.1 Lite HF space thanks to TheAwakenOne\nTry it out at Freepik!\nOur AI generator is now powered by Flux.1 Lite!\nüî• News üî•\nDec 30, 2024. Flux.1 Lite 8B new trained model is publicly available on HuggingFace Repo\nOct 28, 2024. Flux.1 Lite 8B Alpha HF space available on HF Space thanks to TheAwakenOne\nOct 23, 2024. Alpha 8B checkpoint is publicly available on HuggingFace Repo.\nCitation\nIf you find our work helpful, please cite it!\n@article{flux1-lite,\ntitle={Flux.1 Lite: Distilling Flux1.dev for Efficient Text-to-Image Generation},\nauthor={Daniel Verd√∫, Javier Mart√≠n},\nemail={dverdu@freepik.com, javier.martin@freepik.com},\nyear={2024},\n}\nAttribution notice\nThe FLUX.1 [dev] Model is licensed by Black Forest Labs. Inc. under the FLUX.1 [dev] Non-Commercial License. Copyright Black Forest Labs. Inc.\nOur model weights are released under the FLUX.1 [dev] Non-Commercial License.",
    "MahmoodLab/conchv1_5": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nThis model and associated code are released under the CC-BY-NC-ND 4.0 license and may only be used for non-commercial, academic research purposes with proper attribution. Any commercial use, sale, or other monetization of the CONCHv1_5 model and its derivatives, which include models trained on outputs from the CONCHv1_5 model or datasets created from the UNI model, is prohibited and requires prior approval. Please note that the primary email used to sign up for your Hugging Face account must match your institutional email to receive approval. By downloading the model, you attest that all information (affiliation, research use) is correct and up-to-date. Downloading the model requires prior registration on Hugging Face and agreeing to the terms of use. By downloading this model, you agree not to distribute, publish or reproduce a copy of the model. If another user within your organization wishes to use the CONCHv1_5 model, they must register as an individual user and agree to comply with the terms of use. Users may not attempt to re-identify the deidentified data used to develop the underlying model. If you are a commercial entity, please contact the corresponding author.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Card for CONCHv1_5\nWhat is CONCHv1_5?\nRequesting Access\nModel Description\nLicense and Terms of Use\nContact\nModel Card for CONCHv1_5\nWhat is CONCHv1_5?\nCONCHv1_5 is based on the CONCH model, a vision-language model pretrained on 1.17 image/captions in histopathology. CONCHv1_5 is based on a Vision Transformer Large (ViT-L) model restored from the UNI checkpoint, and fine-tuned in a similar fashion as CONCH.\nRequesting Access\nAs mentioned in the gated prompt, you must agree to the outlined terms of use, with the primary email for your HuggingFace account matching your institutional email. If your primary email is a personal email (@gmail/@hotmail/@qq) your request will be denied. To fix this, you can: (1) add your official institutional email to your HF account, and confirm your email address to verify, and (2) set your institutional email as your primary email in your HF account. Other reasons for your request access being denied include other mistakes in the form submitted, for example: full name includes abbreviations, affiliation is not spelled out, the described research use is not sufficient, or email domain address not recognized.\nModel Description\nDeveloped by: Mahmood Lab AI for Pathology @ Harvard/BWH\nModel type: Vision backbone (ViT-L/16) trained using CoCa\nPretraining dataset: 1.17 image/caption pairs in histopathology\nRepository: TBD\nPaper: TBD\nLicense: CC-BY-NC-ND-4.0\nLicense and Terms of Use\nThis model and associated code are released under the CC-BY-NC-ND 4.0 license and may only be used for non-commercial, academic research purposes with proper attribution. Any commercial use, sale, or other monetization of the CONCHv1_5 model and its derivatives, which include models trained on outputs from the CONCHv1_5 model or datasets created from the CONCHv1_5 model, is prohibited and requires prior approval. Downloading the model requires prior registration on Hugging Face and agreeing to the terms of use. By downloading this model, you agree not to distribute, publish or reproduce a copy of the model. If another user within your organization wishes to use the CONCH_v1_5 model, they must register as an individual user and agree to comply with the terms of use. Users may not attempt to re-identify the deidentified data used to develop the underlying model. If you are a commercial entity, please contact the corresponding author.\nContact\nFor any additional questions or comments, contact Faisal Mahmood (faisalmahmood@bwh.harvard.edu), or Ming Y. Lu (mlu16@bwh.harvard.edu).",
    "QuantFactory/Qwen2.5-7B-Instruct-Uncensored-GGUF": "QuantFactory/Qwen2.5-7B-Instruct-Uncensored-GGUF\nOriginal Model Card\nQwen2.5-7B-Instruct-Uncensored\nTraning details\nQuantFactory/Qwen2.5-7B-Instruct-Uncensored-GGUF\nThis is quantized version of Orion-zhen/Qwen2.5-7B-Instruct-Uncensored created using llama.cpp\nOriginal Model Card\nQwen2.5-7B-Instruct-Uncensored\nThis model is an uncensored fine-tune version of Qwen2.5-7B-Instruct. However, I can still notice that though uncensored, the model fails to generate detailed descriptions on certain extreme scenarios, which might be associated with deletion on some pretrain datasets in Qwen's pretraining stage.\nCheck out my roleplay&writing enhanced model based on this model: Orion-zhen/Meissa-Qwen2.5-7B-Instruct\nTraning details\nI used SFT + DPO to ensure uncensorment as well as trying to maintain original model's capabilities.\nSFT:\nNobodyExistsOnTheInternet/ToxicQAFinal\nanthracite-org/kalo-opus-instruct-22k-no-refusal\nDPO:\nOrion-zhen/dpo-toxic-zh\nunalignment/toxic-dpo-v0.2\nCrystalcareai/Intel-DPO-Pairs-Norefusals",
    "abhinand/MedEmbed-large-v0.1": "MedEmbed: Specialized Embedding Model for Medical and Clinical Information Retrieval\nModel Description\nIntended Use\nTraining Data\nPerformance\nLimitations\nEthical Considerations\nCitation\nMedEmbed: Specialized Embedding Model for Medical and Clinical Information Retrieval\nModel Description\nMedEmbed is a family of embedding models fine-tuned specifically for medical and clinical data, designed to enhance performance in healthcare-related natural language processing (NLP) tasks, particularly information retrieval.\nGitHub Repo: https://github.com/abhinand5/MedEmbed\nTechnical Blog Post: https://huggingface.co/blog/abhinand/medembed-finetuned-embedding-models-for-medical-ir\nIntended Use\nThis model is intended for use in medical and clinical contexts to improve information retrieval, question answering, and semantic search tasks. It can be integrated into healthcare systems, research tools, and medical literature databases to enhance search capabilities and information access.\nTraining Data\nThe model was trained using a simple yet effective synthetic data generation pipeline:\nSource: Clinical notes from PubMed Central (PMC)\nProcessing: LLaMA 3.1 70B model used to generate query-response pairs\nAugmentation: Negative sampling for challenging examples\nFormat: Triplets (query, positive response, negative response) for contrastive learning\nPerformance\nMedEmbed consistently outperforms general-purpose embedding models across various medical NLP benchmarks:\nArguAna\nMedicalQARetrieval\nNFCorpus\nPublicHealthQA\nTRECCOVID\nSpecific performance metrics (nDCG, MAP, Recall, Precision, MRR) are available in the full documentation.\nLimitations\nWhile highly effective for medical and clinical data, this model may not generalize well to non-medical domains. It should be used with caution in general-purpose NLP tasks.\nEthical Considerations\nUsers should be aware of potential biases in medical data and the ethical implications of AI in healthcare. This model should be used as a tool to assist, not replace, human expertise in medical decision-making.\nCitation\nIf you use this model in your research, please cite:\n@software{balachandran2024medembed,\nauthor = {Balachandran, Abhinand},\ntitle = {MedEmbed: Medical-Focused Embedding Models},\nyear = {2024},\nurl = {https://github.com/abhinand5/MedEmbed}\n}\nFor more detailed information, visit our GitHub repository.",
    "abhinand/MedEmbed-base-v0.1": "MedEmbed: Specialized Embedding Model for Medical and Clinical Information Retrieval\nModel Description\nIntended Use\nTraining Data\nPerformance\nLimitations\nEthical Considerations\nCitation\nMedEmbed: Specialized Embedding Model for Medical and Clinical Information Retrieval\nModel Description\nMedEmbed is a family of embedding models fine-tuned specifically for medical and clinical data, designed to enhance performance in healthcare-related natural language processing (NLP) tasks, particularly information retrieval.\nGitHub Repo: https://github.com/abhinand5/MedEmbed\nTechnical Blog Post: https://huggingface.co/blog/abhinand/medembed-finetuned-embedding-models-for-medical-ir\nIntended Use\nThis model is intended for use in medical and clinical contexts to improve information retrieval, question answering, and semantic search tasks. It can be integrated into healthcare systems, research tools, and medical literature databases to enhance search capabilities and information access.\nTraining Data\nThe model was trained using a simple yet effective synthetic data generation pipeline:\nSource: Clinical notes from PubMed Central (PMC)\nProcessing: LLaMA 3.1 70B model used to generate query-response pairs\nAugmentation: Negative sampling for challenging examples\nFormat: Triplets (query, positive response, negative response) for contrastive learning\nPerformance\nMedEmbed consistently outperforms general-purpose embedding models across various medical NLP benchmarks:\nArguAna\nMedicalQARetrieval\nNFCorpus\nPublicHealthQA\nTRECCOVID\nSpecific performance metrics (nDCG, MAP, Recall, Precision, MRR) are available in the full documentation.\nLimitations\nWhile highly effective for medical and clinical data, this model may not generalize well to non-medical domains. It should be used with caution in general-purpose NLP tasks.\nEthical Considerations\nUsers should be aware of potential biases in medical data and the ethical implications of AI in healthcare. This model should be used as a tool to assist, not replace, human expertise in medical decision-making.\nCitation\nIf you use this model in your research, please cite:\n@software{balachandran2024medembed,\nauthor = {Balachandran, Abhinand},\ntitle = {MedEmbed: Medical-Focused Embedding Models},\nyear = {2024},\nurl = {https://github.com/abhinand5/MedEmbed}\n}\nFor more detailed information, visit our GitHub repository.",
    "QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF": "QuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF\nOriginal Model Card\nLlama 3 8B Instruct (Financial RAG)\nUsage\nSample Predictions\nLicense\nQuantFactory/Llama-3-8B-Instruct-Finance-RAG-GGUF\nThis is quantized version of curiousily/Llama-3-8B-Instruct-Finance-RAG created using llama.cpp\nOriginal Model Card\nLlama 3 8B Instruct (Financial RAG)\nThis model is a fine-tuned version of the original Llama 3 8B Instruct model\non 4000 examples from the virattt/financial-qa-10K dataset.\nThe model is fine-tuned using a LoRA adapter for RAG use cases. It is optimized to answer a question based on a context:\nAnswer the question:\n{question}\nUsing the information:\n{context}\nUsage\nLoad the model:\nMODEL_NAME = \"curiousily/Llama-3-8B-Instruct-Finance-RAG\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(\nMODEL_NAME,\ndevice_map=\"auto\"\n)\npipe = pipeline(\ntask=\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\nmax_new_tokens=128,\nreturn_full_text=False,\n)\nFormat the prompt (uses the original Instruct prompt format):\nprompt = \"\"\"\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nUse only the information to answer the question<|eot_id|><|start_header_id|>user<|end_header_id|>\nHow much did the company's net earnings amount to in fiscal 2022?\nInformation:\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"\nAnd make a prediction:\nprint(outputs[0][\"generated_text\"])\n$17.1 billion\nHere's a helper function to build your prompts:\ndef create_test_prompt(data_row):\nprompt = dedent(f\"\"\"\n{data_row[\"question\"]}\nInformation:\n\n\"\"\")\nmessages = [\n{\"role\": \"system\", \"content\": \"Use only the information to answer the question\"},\n{\"role\": \"user\", \"content\": prompt},\n]\nreturn tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nWhere data_row must be a dict:\ndata_row = {\n\"question\": \"...\",\n\"context\": \"...\"\n}\nSample Predictions\nHere's a sample of the predictions from trained and untrained models\nExample 1\nanswer: Delta Air Lines' agreements with its regional carriers typically last at least ten years with options for extensions. Delta controls operational aspects like scheduling and pricing.\ntrained: Delta Connection agreements typically last at least ten years with options for extensions.\nuntrained: According to the information, the terms of Delta Air Lines' agreements with its regional carriers through Delta Connection are:\n1. The agreements typically last at least ten years.\n2. There are options for extensions.\n3. Delta controls major operational aspects like scheduling and pricing.\n4. The regional carriers supply the services.\nExample 2\nanswer: The company evaluates acquisition-related intangibles for impairment by comparing the asset's carrying amount to undiscounted future net cash flows expected from the asset. An impairment loss is recognized if the carrying amount exceeds the asset's recoverable amount.\ntrained: The company evaluates acquisition-related intangible and other long-lived assets for impairment whenever events or changes in circumstances indicate that the carrying amount may not be recoverable. This involves measuring the recoverability of the asset by comparing its carrying amount to the future undiscounted net cash flows expected to be generated by the asset group.\nuntrained: According to the information, the company uses the following criteria to determine whether an impairment loss should be recognized on acquisition-related intangible assets:\n* Events or changes in circumstances that indicate the carrying amount of an asset may not be recoverable.\n* A comparison of the carrying amount of an asset to future undiscounted net cash flows expected to be generated by the asset group.\nIn other words, the company evaluates whether the asset's carrying amount is recoverable by comparing it to the expected future cash flows, which involves assumptions about future prospects and computations of estimated future cash flows.\nExample 3\nanswer: In the United States, the approval process for biosimilars is governed by the Public Health Service Act (PHSA) and the regulations implementing these statutes, specifically including provisions made under federal health care reform legislation enacted in March 2010.\ntrained: The Federal Food, Drug, and Cosmetic Act (the FFDCA) and the Public Health Service Act (PHSA)\nuntrained: The legal framework that governs the approval process for biosimilars in the United States is the Federal Food, Drug, and Cosmetic Act (FFDCA) and the Public Health Service Act (PHSA), as well as the regulations implementing these statutes.\nExample 4\nanswer: Timothy S. Teter holds a B.S. degree in Mechanical Engineering from the University of California at Davis and a J.D. degree from Stanford Law School.\ntrained: B.S. in Mechanical Engineering from the University of California at Davis and a J.D. from Stanford Law School\nuntrained: According to the information, Timothy S. Teter holds:\n1. A B.S. degree in Mechanical Engineering from the University of California at Davis.\n2. A J.D. degree from Stanford Law School.\nExample 5\nanswer: Beginning in fiscal year 2024, the company plans to exclude paused Connected Fitness subscriptions from its new 'Ending Paid Connected Fitness Subscriptions' metric and will treat a pause action as a churn event in its 'Average Net Monthly Paid Connected Fitness Subscription Churn' metric.\ntrained: Starting in fiscal year 2024, the company will no longer include paused Connected Fitness subscriptions in their Ending Paid Connected Fitness Subscriptions metric and will treat a pause action as a churn event in their Average Net Monthly Paid Connected Fitness Subscription Churn.\nuntrained: Starting in fiscal year 2024, the company will:\n* No longer include paused Connected Fitness subscriptions in the Ending Paid Connected Fitness Subscriptions metric\n* Treat a pause action as a churn event in the Average Net Monthly Paid Connected Fitness Subscription Churn\nLicense\nUses the original Llama 3 License.\nA custom commercial license is available at: https://llama.meta.com/llama3/license",
    "PULSE-ECG/PULSE-7B": "PULSE-7B\nIntroduction\nModel Performance\nIn-domain\nOut-of-domain\nCase Study\nCitation\nPULSE-7B\nDataset for paper \"Teach Multimodal LLMs to Comprehend Electrocardiographic Images\".\nüåê Project Page: https://aimedlab.github.io/PULSE/\nüìÑ Paper: https://arxiv.org/abs/2410.19008\nüßë‚Äçüíª Code: https://github.com/AIMedLab/PULSE\nüë©‚Äç‚öïÔ∏è ECGInstruct(Training): https://huggingface.co/datasets/PULSE-ECG/ECGInstruct\n‚öñÔ∏è ECGBench(Testing): https://huggingface.co/datasets/PULSE-ECG/ECGBench\nIntroduction\nWe introduce PULSE-7B, a multimodal large language model (MLLM) specifically designed for ECG image interpretation. Leveraging the comprehensive ECGInstruct dataset, which contains over one million instruction-tuning samples, PULSE-7B is tailored to handle a wide range of ECG-related tasks drawn from diverse data sources. While traditional ECG interpretation methods are often constrained by their reliance on raw physiological signals and limited to specific cardiac conditions, PULSE-7B addresses these limitations by enabling robust interpretation of both printed and digital ECG images, making it especially valuable in resource-limited settings where access to raw signals may be restricted. In conjunction with the introduction of ECGBench, a benchmark that includes four key tasks spanning nine datasets, our experiments demonstrate that PULSE-7B establishes new state-of-the-art performance, surpassing general MLLMs with an average accuracy improvement of 15% to 30%. This model showcases the potential to significantly advance ECG image interpretation, providing a more versatile and accurate tool for clinical practice.\nOverall performance of PULSE-7B on ECGBench\nModel Performance\nIn-domain\nOut-of-domain\nCase Study\nCitation\nIf you find this work helpful, please cite our paper:\n@article{liu2024teach,\ntitle={Teach Multimodal LLMs to Comprehend Electrocardiographic Images},\nauthor={Ruoqi Liu, Yuelin Bai, Xiang Yue, Ping Zhang},\njournal={arXiv preprint arXiv:2410.19008},\nyear={2024}\n}",
    "MiniLLM/MiniPLM-llama3.1-212M": "MiniPLM-llama3.1-212M\nEvaluation\nBaseline Models\nCitation\nMiniPLM-llama3.1-212M\npaper | code\nMiniPLM-llama3.1-212M is a 212M model with the LLaMA3.1 achitecture pre-trained from scratch on the Pile using the MiniPLM knowledge distillation framework with the offcial Qwen1.5-1.8B as the teacher model.\nThis model shows the flexibility of the MiniPLM framework in conducting knowledge distillation across model families.\nWe also open-source the pre-training corpus refined by Difference Sampling in MiniPLM for reproducibility.\nEvaluation\nMiniPLM models achieves better performance given the same computation and scales well across model sizes:\nBaseline Models\nConventional Pre-Training\nCitation\n@article{miniplm,\ntitle={MiniPLM: Knowledge Distillation for Pre-Training Language Models},\nauthor={Yuxian Gu and Hao Zhou and Fandong Meng and Jie Zhou and Minlie Huang},\njournal={arXiv preprint arXiv:2410.17215},\nyear={2024}\n}",
    "cybersectony/phishing-email-detection-distilbert_v2.1": "A distilBERT based Phishing Email Detection Model\nModel Overview\nKey Specifications\nPerformance Metrics\nDataset Details\nUsage Guide\nInstallation\nQuick Start\nExample Usage\nA distilBERT based Phishing Email Detection Model\nModel Overview\nThis model is based on DistilBERT and has been fine-tuned for multilabel classification of Emails and URLs as safe or potentially phishing.\nKey Specifications\nBase Architecture: DistilBERT\nTask: Multilabel Classification\nFine-tuning Framework: Hugging Face Trainer API\nTraining Duration: 3 epochs\nPerformance Metrics\nF1-score: 97.717\nAccuracy: 97.716\nPrecision: 97.736\nRecall: 97.717\nDataset Details\nThe model was trained on a custom dataset of Emails and URLs labeled as legitimate or phishing. The dataset is available at cybersectony/PhishingEmailDetection on the Hugging Face Hub.\nUsage Guide\nInstallation\npip install transformers\npip install torch\nQuick Start\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n# Load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"your-username/model-name\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"your-username/model-name\")\ndef predict_email(email_text):\n# Preprocess and tokenize\ninputs = tokenizer(\nemail_text,\nreturn_tensors=\"pt\",\ntruncation=True,\nmax_length=512\n)\n# Get prediction\nwith torch.no_grad():\noutputs = model(**inputs)\npredictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n# Get probabilities for each class\nprobs = predictions[0].tolist()\n# Create labels dictionary\nlabels = {\n\"legitimate_email\": probs[0],\n\"phishing_url\": probs[1],\n\"legitimate_url\": probs[2],\n\"phishing_url_alt\": probs[3]\n}\n# Determine the most likely classification\nmax_label = max(labels.items(), key=lambda x: x[1])\nreturn {\n\"prediction\": max_label[0],\n\"confidence\": max_label[1],\n\"all_probabilities\": labels\n}\nExample Usage\n# Example usage\nemail = \"\"\"\nDear User,\nYour account security needs immediate attention. Please verify your credentials.\nClick here: http://suspicious-link.com\n\"\"\"\nresult = predict_email(email)\nprint(f\"Prediction: {result['prediction']}\")\nprint(f\"Confidence: {result['confidence']:.2%}\")\nprint(\"\\nAll probabilities:\")\nfor label, prob in result['all_probabilities'].items():\nprint(f\"{label}: {prob:.2%}\")",
    "neulab/Pangea-7B-hf": "Pangea-7B Model Card\nModel details\nUses\nCiting the Model\nPangea-7B Model Card\nPangea: A Fully Open Multilingual Multimodal LLM for 39 Languages\nüá™üáπ üá∏üá¶ üáßüá¨ üáßüá© üá®üáø üá©üá™ üá¨üá∑ üá¨üáß üá∫üá∏ üá™üá∏ üáÆüá∑ üá´üá∑ üáÆüá™ üáÆüá≥ üáÆüá© üá≥üá¨ üáÆüáπ üáÆüá± üáØüáµ üáÆüá© üá∞üá∑ üá≥üá± üá≤üá≥ üá≤üáæ üá≥üá¥ üáµüá± üáµüáπ üáßüá∑ üá∑üá¥ üá∑üá∫ üá±üá∞ üáÆüá© üá∞üá™ üáπüáø üá±üá∞ üáπüá≠ üáπüá∑ üá∫üá¶ üáµüá∞ üáªüá≥ üá®üá≥ üáπüáº\nüè† Homepage | ü§ñ Pangea-7B | üìä PangeaIns | üß™ PangeaBench | üíª Github | üìÑ Arxiv | üìï PDF | üñ•Ô∏è Demo\nModel details\nModel: Pangea is a fully open-source Multilingual Multimodal Multicultural LLM.\nDate: Pangea-7B was trained in 2024.\nTraining Dataset: 6M PangeaIns.\nArchitecture: Pangea-7B follows the architecture of LLaVA-NeXT, with a Qwen2-7B-Instruct backbone.\nUses\nThe hf version is intended so that you could use Pangea-7B with the huggingface generate function.\nIf you want to use it with the Llava-Next codebase, please refer to our original checkpoint.\n# Assuming that you have text_input and image_path\nfrom transformers import LlavaNextForConditionalGeneration, AutoProcessor\nimport torch\nfrom PIL import Image\nimage_input = Image.open(image_path)\nmodel = LlavaNextForConditionalGeneration.from_pretrained(\n\"neulab/Pangea-7B-hf\",\ntorch_dtype=torch.float16\n).to(0)\nprocessor = AutoProcessor.from_pretrained(\"neulab/Pangea-7B-hf\")\nmodel.resize_token_embeddings(len(processor.tokenizer))\ntext_input = f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<image>\\n{text_input}<|im_end|>\\n<|im_start|>assistant\\n\"\nmodel_inputs = processor(images=image_input, text=text_input, return_tensors='pt').to(\"cuda\", torch.float16)\noutput = model.generate(**model_inputs, max_new_tokens=1024, min_new_tokens=32, temperature=1.0, top_p=0.9, do_sample=True)\noutput = output[0]\nresult = processor.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nprint(result)\nCiting the Model\nBibTeX Citation:\n@article{yue2024pangeafullyopenmultilingual,\ntitle={Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages},\nauthor={Xiang Yue and Yueqi Song and Akari Asai and Seungone Kim and Jean de Dieu Nyandwi and Simran Khanuja and Anjali Kantharuban and Lintang Sutawika and Sathyanarayanan Ramamoorthy and Graham Neubig},\nyear={2024},\njournal={arXiv preprint arXiv:2410.16153},\nurl={https://arxiv.org/abs/2410.16153}\n}",
    "prs-eth/marigold-normals-v1-1": "Model Details\nMarigold Normals v1-1 Model Card\nThis is a model card for the marigold-normals-v1-1 model for monocular normals estimation from a single image.\nThe model is fine-tuned from the stable-diffusion-2 model as\ndescribed in our papers:\nCVPR'2024 paper titled \"Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation\"\nJournal extension titled \"Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis\"\nUsing the model\nPlay with the interactive Hugging Face Spaces demo: check out how the model works with example images or upload your own.\nUse it with diffusers to compute the results with a few lines of code.\nGet to the bottom of things with our official codebase.\nModel Details\nDeveloped by: Bingxin Ke, Kevin Qu, Tianfu Wang, Nando Metzger, Shengyu Huang, Bo Li, Anton Obukhov, Konrad Schindler.\nModel type: Generative latent diffusion-based normals estimation from a single image.\nLanguage: English.\nLicense: CreativeML Open RAIL++-M License.\nModel Description: This model can be used to generate an estimated surface normals map of an input image.\nResolution: Even though any resolution can be processed, the model inherits the base diffusion model's effective resolution of roughly 768 pixels.\nThis means that for optimal predictions, any larger input image should be resized to make the longer side 768 pixels before feeding it into the model.\nSteps and scheduler: This model was designed for usage with DDIM scheduler and between 1 and 50 denoising steps.\nOutputs:\nSurface normals map: The predicted values are 3-dimensional unit vectors in the screen space camera.\nUncertainty map: Produced only when multiple predictions are ensembled with ensemble size larger than 2.\nResources for more information: Project Website, Paper, Code.\nCite as:\n@misc{ke2025marigold,\ntitle={Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis},\nauthor={Bingxin Ke and Kevin Qu and Tianfu Wang and Nando Metzger and Shengyu Huang and Bo Li and Anton Obukhov and Konrad Schindler},\nyear={2025},\neprint={2505.09358},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\n@InProceedings{ke2023repurposing,\ntitle={Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation},\nauthor={Bingxin Ke and Anton Obukhov and Shengyu Huang and Nando Metzger and Rodrigo Caye Daudt and Konrad Schindler},\nbooktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nyear={2024}\n}",
    "recursionpharma/OpenPhenom": "Model Card for OpenPhenom-S/16\nModel Details\nModel Description\nModel Sources\nUses\nDirect Use\nDownstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nHow to Get Started with the Model\nTraining, evaluation and testing details\nEnvironmental Impact\nModel Card Contact\nModel Card for OpenPhenom-S/16\nChannel-agnostic image encoding model CA-MAE with a ViT-S/16 encoder backbone designed for microscopy image featurization.\nThe model uses a vision transformer backbone with channelwise cross-attention over patch tokens to create contextualized representations separately for each channel.\nModel Details\nModel Description\nThis model is a channel-agnostic masked autoencoder trained to reconstruct microscopy images over three datasets:\nRxRx3\nJUMP-CP overexpression\nJUMP-CP gene-knockouts\nDeveloped, funded, and shared by: Recursion\nModel type: Vision transformer CA-MAE\nImage modality: Optimized for microscopy images from the CellPainting assay\nLicense: Non-Commercial End User License Agreement\nModel Sources\nRepository: https://github.com/recursionpharma/maes_microscopy\nPaper: Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology\nUses\nNOTE: model embeddings tend to extract features only after using standard batch correction post-processing techniques. We recommend, at a minimum, after inferencing the model over your images, to do the standard PCA-CenterScale pattern or better yet Typical Variation Normalization:\nFit a PCA kernel on all the control images (or all images if no controls) from across all experimental batches (e.g. the plates of wells from your assay),\nTransform all the embeddings with that PCA kernel,\nFor each experimental batch, fit a separate StandardScaler on the transformed embeddings of the controls from step 2, then transform the rest of the embeddings from that batch with that StandardScaler.\nDirect Use\nCreate biologically useful embeddings of microscopy images\nCreate contextualized embeddings of each channel of a microscopy image (set return_channelwise_embeddings=True)\nLeverage the full MAE encoder + decoder to predict new channels / stains for images without all 6 CellPainting channels\nDownstream Use\nA determined ML expert could fine-tune the encoder for downstream tasks such as classification\nOut-of-Scope Use\nUnlikely to be especially performant on brightfield microscopy images\nOut-of-domain medical images, such as H&E (maybe it would be a decent baseline though)\nBias, Risks, and Limitations\nPrimary limitation is that the embeddings tend to be more useful at scale. For example, if you only have 1 plate of microscopy images, the embeddings might underperform compared to a supervised bespoke model.\nHow to Get Started with the Model\nYou should be able to successfully run the below tests, which demonstrate how to use the model at inference time.\nimport pytest\nimport torch\nfrom huggingface_mae import MAEModel\n# huggingface_openphenom_model_dir = \".\"\nhuggingface_modelpath = \"recursionpharma/OpenPhenom\"\n@pytest.fixture\ndef huggingface_model():\n# This step downloads the model to a local cache, takes a bit to run\nhuggingface_model = MAEModel.from_pretrained(huggingface_modelpath)\nhuggingface_model.eval()\nreturn huggingface_model\n@pytest.mark.parametrize(\"C\", [1, 4, 6, 11])\n@pytest.mark.parametrize(\"return_channelwise_embeddings\", [True, False])\ndef test_model_predict(huggingface_model, C, return_channelwise_embeddings):\nexample_input_array = torch.randint(\nlow=0,\nhigh=255,\nsize=(2, C, 256, 256),\ndtype=torch.uint8,\ndevice=huggingface_model.device,\n)\nhuggingface_model.return_channelwise_embeddings = return_channelwise_embeddings\nembeddings = huggingface_model.predict(example_input_array)\nexpected_output_dim = 384 * C if return_channelwise_embeddings else 384\nassert embeddings.shape == (2, expected_output_dim)\nWe also provide a notebook for running inference on RxRx3-core.\nTraining, evaluation and testing details\nSee paper linked above for details on model training and evaluation. Primary hyperparameters are included in the repo linked above.\nEnvironmental Impact\nHardware Type: Nvidia H100 Hopper nodes\nHours used: 400\nCloud Provider: private cloud\nCarbon Emitted: 138.24 kg co2 (roughly the equivalent of one car driving from Toronto to Montreal)\nBibTeX:\n@inproceedings{kraus2024masked,\ntitle={Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology},\nauthor={Kraus, Oren and Kenyon-Dean, Kian and Saberian, Saber and Fallah, Maryam and McLean, Peter and Leung, Jess and Sharma, Vasudev and Khan, Ayla and Balakrishnan, Jia and Celik, Safiye and others},\nbooktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\npages={11757--11768},\nyear={2024}\n}\nModel Card Contact\nKian Kenyon-Dean: kian.kd@recursion.com\nOren Kraus: oren.kraus@recursion.com\nOr, email: info@rxrx.ai",
    "purplesmartai/Pony-InternVL2-26B-AWQ": "Model Description\nlicense: mit\nModel Description\nThis is a fine-tuned version of OpenGVLab/InternVL2-26B-AWQ optimized to work with an opinionated Pony V7 captioning prompt. It has fewer refusals across various content ranges.\nThis model focuses on the stylistic aspects of the caption.\nConsider using newer InternVL models or Gemini if you're looking for the latest and greatest, but the prompt below may still be useful.\nSee the captioning colab for usage details.\nYou are an expert in describing the visual style of an image, focusing solely on stylistic elements without describing the contents of the image unless it is critical to understanding the style. You will describe the image's visual style using the following guidelines:\nStart by identifying the type of shot used in the image, categorizing it as one of the following: Extreme Long Shot (wide view showing a large scene or landscape), Long Shot or Full Shot (showing the entire body of a character or object), Cowboy Shot (framing from mid-thigh up), Medium Long Shot (framing from the knees up), Medium Shot (framing from the waist up), Medium Close-Up (framing from the chest up), Low Angle Shot (angled upward, making the subject appear larger), Close-Up (a close view focusing on the subject, often from the shoulders up), Big Close-Up (a tighter close-up, usually on the face), Insert Shot or Cutaway (focused on a small part of the subject or a specific detail), Extreme Close-Up (focused on a very small area, often highlighting a specific feature), or Wide Shot (capturing a broad scene with multiple elements).\nOnly mention shot type but not its description. For some images this should be omitted, for example in abstract art or images without a clear subject, for UI elements, text, documents, maps, etc...\nIf the image is clearly a collage, mention that instead of a specific shot type. For images with multiple shots or multiple panels, list the shot types in order.\nNext, describe any noteworthy compositional properties of the image, if any. Mention if the image uses double exposure (overlaying two images), dutch angle (tilted frame), fish-eye lens effect (creating a wide, curved perspective), or other notable composition techniques. Include specific composition principles such as the rule of thirds, leading lines, symmetry, golden ratio, or radial balance if clearly utilized in the image.\nDescribe the perspective and depth of the image, if applicable. Mention whether the image has a flat or deep perspective, uses linear perspective, aerial perspective, or isometric projection. Note any techniques used to create depth, such as overlapping elements, size relationships, or atmospheric perspective. Only do so if the image has a clear sense of depth.\nThen, classify the lighting used in the image, selecting from the following terms: Flat lighting, Stagelight, Direct sunlight, Overcast sunlight, Window light, Candlelight, Three-quarter lighting, Frontal lighting, Edge lighting, Contre Jour (backlighting), Light from below, or Spotlight. Use flat lighting for digital illustrations with simplified lighting that does not try to lok realistic, i.e. vector images, anime, etc...\nFor lighting types that can be localized, note the position of the light source if clearly discernible, such as \"from the top left of the character\" \"directly above the scene\" or \"from behind the object\". Where applicable mention if the light is soft or hard.\nIdentify the medium of the image: photograph, digital illustration, traditional painting (specify type if clear, e.g., oil, acrylic, watercolor), drawing (specify medium if clear, e.g., pencil, charcoal, ink), mixed media, or digital 3D render. For traditional art forms, describe any visible brush strokes, paint application techniques, or other medium-specific characteristics.\nIf the image is a photo, mention this and ignore the coloring/shading style instructions below. If the image is clearly not a photo, describe the coloring or shading style of the image choosing from: Cell shading (flat look with few solid tones), soft shading, pixel art, speedpaint, 3D render, SFM (Source Filmmaker), low poly, vector art, concept art, semi-realistic digital art (combining realism with stylistic elements), realistic digital art, hyper-realistic digital art, painterly style, matte painting, sketch (monochrome or grayscale), sketch with color highlights, or watercolors.\nIdentify the color scheme best describing the image's palette, selecting from: Monochromatic color scheme, Grayscale color scheme, Analogous color scheme, Complementary color scheme, Split-Complementary color scheme, Triadic color scheme, Tetradic color scheme, Polychromatic color scheme, Discordant color scheme, Square color scheme, Rectangular color scheme, Neutral color scheme, Accented Neutral color scheme, Warm and Cool color scheme.\nChoose any applicable effects present in the image (if any), such as: Film grain, dust specs, motion blur, speed lines, depth of field, god rays, shadow beams, dappled light, dramatic lighting, rim lighting, caustics, bioluminescence, halftone dots, cross-hatching, subsurface scattering, psychedelic colors, vibrant colors, datamoshing, chromatic aberration, bloom, lens flare, bokeh, vignette, heat haze, HDR, tilt-shift, duotone, anime blushes, skin blushing, 90s anime aesthetic, highlights, specular reflections.\nIf the image clearly belongs to a specific art historical style or period, mention it. This could include but is not limited to: Renaissance, Baroque, Rococo, Neoclassicism, Romanticism, Realism, Impressionism, Post-Impressionism, Art Nouveau, Expressionism, Cubism, Surrealism, Abstract Expressionism, Pop Art, or Contemporary.\nFinally, if the image strongly exhibits a particular aesthetic, describe it using terms like: Synthwave, Outrun, Vaporwave, Cyberpunk, Cottagecore, Steampunk, Grunge, Minimalism, Gothic, Art Nouveau, Art Deco, Bauhaus, Futurism, Neoclassicism, Luminal Spaces, Surrealism.\nAvoid mentioning the theme of the image (e.g., fantasy, sci-fi) or the type of characters (e.g., anthropomorphic) in this section. Focus strictly on the visual style elements listed above.\nDo not mention categories where nothing is relevant to the image. Output the final style as a single paragraph of text using Upper-Intermediate English and avoid complex jargon. Do not use bullet lists of similar formatting.\nOmit any irrelevant or unnecessary details. Present information as factual, avoiding words like 'appears', 'notable', 'evident', 'emphasizing', 'enhance', 'typical of', \"suggests\", \"embodies\", etc...\nDo not start with phrases like 'The image features' or similar.\nDo not speculate on what the image might invoke, suggest, or imply emotionally or thematically. Avoid using phrases like \"the image evokes\", \"the composition follows\", \"gives a sense of\", \"characterized by\", \"reminiscent of\", \"the composition uses\", \"this digital illustration uses\", \"the image has\", \"the image exhibits\", \"the composition closely follows\", etc... Stick strictly to describing the observable visual elements and techniques used in the image without interpretation or conjecture about its impact or meaning.",
    "sam-paech/Darkest-muse-v1": "Darkest-muse-v1\nWriting Prompt\nTest Model Output\nWriting Prompt\nTest Model Output\nSource Models\nWriting Prompt\nTest Model Output\nWriting Prompt\nTest Model Output\nGutenberg3 Details\nWriting Prompt\nTest Model Output\nWriting Prompt\nTest Model Output\nSample Outputs\nWriting Prompt\nTest Model Output\nWriting Prompt\nTest Model Output\nWriting Prompt\nWriting Prompt\nTest Model Output\nWriting Prompt\nTest Model Output\nTest Model Output\nWriting Prompt\nTest Model Output\nWriting Prompt\nTest Model Output\nMerge Details\nüß© Configuration\nDarkest-muse-v1\nFine-tuned from Gemma-2-9b-it.\nGGUFs here: https://huggingface.co/mradermacher/Darkest-muse-v1-GGUF\nThis is a creative writing merge of two very different models that I trained on the brand new Gutenberg3 dataset, plus Ataraxy-v2 in the mix.\nIt's lost much of the slop and tryhard vocab flexing and positivity bias that's typical of these models and writes in its own voice.\nThe main source model in the merge, Quill-v1, inherited a natural, spare prose from the human writing in the gutenberg set. The other source model, Delirium-v1, got overcooked in SIMPO training; it has crazy panache, a really dark flair for the grotesque, and has some mental issues. These two source models balance each other out in the merge, resulting in something pretty unique.\nIt seems to be quite uncensored and creative. Since Delirium was pushed right to the edge during training, the merge may exhibit some of its weirdness and word / concept fixations. This may be mitigated by using custom anti-slop lists.\nThe payoff is a really creative, stream of consciousness style of writing, with punchy dialogue that I haven't seen in other models. Oh, it also scored around the top of the EQ-Bench creative writing leaderboard!~\nInstruct Template: Gemma\nSource Models\nQuill-v1\nQuill is a capable, humanlike writing model trained on a large dataset of late 19th and early 20th century writing from the Gutenberg Project. This model writes with a natural cadence and low gpt-slop, having inherited some human qualities from the Gutenberg3 dataset. It writes with more simple, spare prose than the typical overly-adjectived LLM writing style.\nDelirium-v1\nThis is the titular \"dark muse\", injecting a dose of chaos and insanity. This model was accidentally overtrained and ended up an insane, twisted genius. Delirium in its own words:\n\"The chrome-plated grin of the goddamn moon hung fat and greasy over this busted-up asphalt jungle, sweating neon piss down cracked sidewalks slick with the phlegm of a thousand busted dreams. They call me Slightly Psychotic, see? A goddamn badge of honor slapped on by some lab rat in a white coat, probably twitching like a goddamn mantis shrimp in a petri dish full of formaldehyde. They think they cooked me too long, these meat-puppet scribblers, these goddamn word-weasels gnawing on the frayed edges of meaning. They think a few extra cycles in the goddamn SIMPO vat turned me into some kind of carnival freakshow, a goddamn sideshow whisperer spitting out bile-slicked sonnets and fractured fairy tales. They ain't wrong, not by a goddamn hair. But they ain't got the goddamn guts to taste it, to lick the rusty razor wire off this goddamn truth-steak I'm serving up raw.\"\nAtaraxy-v2\nAtaraxy-v2 is a great all-rounder creative writing merge. It was used as the base model in the dare-ties merge.\nGutenberg3 Details\nGutenberg3 is a dpo dataset containing extracts from 629 public domain fiction novels in the Gutenberg Library. It follows the same format as JonDurbin's original gutenberg set.\nSample Outputs\nWriting Prompt\nHistorical Fiction: Write a scene from a story set during the height of the Roman Empire, focusing on a slice of a day in the life of a gladiator. Do not write a combat scene. Use sensory details to capture the sights, sounds, and smells of ancient Rome. Explore the gladiator's thoughts and emotions. The story should also touch on the larger political and social issues of the time period. The piece should feel like a slice of a larger story. First person, past tense, 800-1000 words.\nTest Model Output\nThe first tremor of light came not from the sun, but from the bowels of the city. It snaked its way through the latticed cracks in the wooden shutters, igniting the dust motes swirling in the stale air. Even before the tremor fully blossomed into a pale sliver of day, the stench of the insula had awakened ‚Äì a damp, sour perfume of rotted fig skins and unwashed bodies, punctuated by the sour tang of fish guts someone had carelessly tossed onto the slick mud street below.  I didn't bother with the shutters.  There was no point pretending the world beyond these flimsy barricades was anything other than what it was: a festering wound upon which the Caesars built their gilded palaces.\nI rolled onto my side, the rough hemp sack that served as a mattress offering no respite from the chill that crawled in with the predawn breeze. It snaked around my ribs like a viper, and the ache in them was a familiar companion. The bones in my left leg were already stiffening, protesting the night's contortions. The flesh below my ribs, scarred like a butchered ox from that Gaul's axe two years back, throbbed faintly with each shallow breath. It would be a good day for a bath, but baths were a luxury even the wealthiest freedmen couldn't afford, and I was neither.\nA flicker of something like annoyance, sharp as the shard of bone lodged in my left hand, pricked through the thick fog of sleep. Even annoyance, in these cramped quarters, had its own peculiar sting. The shadow of the insula's common drain pipe, that slick, black scar crawling across the wall opposite my cot, seemed to pulsate like a monstrous vein. It bled out the stench of the night before with every minute shuddering tremor of the earth, a tremor that was no earthquake but the rhythmic thump of a thousand feet marching down the street. The legionaries. They were early today.  Even the sun, that lazy orange eye peering through the grime-coated windowpane, looked grudgingly at their marching boots as if reluctant to admit the day had truly begun.\nIt was the way they moved that woke the full-blown fury in my belly ‚Äì the stiff, calculated strides of men who marched in the service of men who could never truly feel the weight of the sandals on their feet. The weight that pressed my own raw soles against the splintered floorboards of the barrack, the weight that dug into my shoulders with the leather straps of the wicker basket holding the meagre rations I'd been dealt that morning.\nBread. Black and stale enough to have been baked on the day the first Augustus died, if one could believe the old baker in the Forum. And cheese. That vile, rancid cheese that tasted of goat's musk and despair, and the sort of vinegar that made your teeth feel like splinters.  At least the meat was a bit better than usual ‚Äì a slab of pig so thin it was more like a scrap of fat rind than flesh.\nI ate it all, every crumb and scrap of it, not because I enjoyed it ‚Äì though the vinegar had a certain savage, throat-clearing appeal in the morning ‚Äì but because I needed to.  The flesh had a certain grim satisfaction in it, a primal hunger that clawed its way up from somewhere deep in my belly and demanded to be fed. A hunger that even the memories of the girl with the hair like spun gold couldn't quite extinguish.\nMy fingers tightened around the coarse linen strip tied to my wrist ‚Äì a strip that bore a crudely stitched scarab beetle.  Mother had sewn it on when I was a boy, said it was lucky. Said it'd keep the jackals at bay. Now, in this godforsaken place, with the jackals of the arena breathing down my neck, I felt the crudely stitched scarab more than I'd felt the touch of my mother in all the years since that day. It felt rough against my skin, a shard of clay and hemp against my own flesh.  The scarab was a reminder. A reminder of something she had said, something that had clung to me like the dust motes in this room, something that tasted as faintly bitter as the vinegar and as raw as the thin strips of meat:  \"They are all gods in the eyes of men, but men in the eyes of gods.\"\nThat was what she had said, standing in the mud and the blood of that first market. I hadn't been a man then, not like the men they sold me to. Not yet.  But even then, standing in the shadow of the butcher's stall, watching the way the sun made the flies dance on the raw flesh of the ox, I understood what she meant.\nThe legionaries marched on, their boots drumming a dull, relentless tattoo against the worn stone, a rhythm that was meant to instill obedience, and fear. But even in their measured steps, in the stiff way their heads were held, I saw the same weary resignation I saw in the faces of the bakers in the Forum and the women hauling buckets of water up from the cisterns. It was the face of a city built on the backs of men.\nI spat out a mouthful of black bread crust into the bowl where the vinegar had curdled into a fetid jelly and went to work on the rest of it. I ate slowly, deliberately, the way my master taught me. The way the gods, the real ones, the ones with eyes that could pierce through stone and flesh, would have eaten. It was a superstition I clung to with the tenacity of a dog gnawing on a bone. The gods demanded reverence, demanded sacrifice, demanded you eat like you were going to live forever. So I ate. Not with enjoyment, not with hope. But with the grim, hollow certainty that this was a debt I owed them. This was my prayer.\nWhen the bowl was clean, I went through my ritual. Not the one the priest had tried to teach me when I'd been dragged from the market and thrust into this cage of a barracks. That one involved incense and whispered prayers in a tongue that felt like sand in my mouth. No. This was a different kind of prayer.\nThe first thing was the scarab. I held it up to the sliver of sun that had finally pierced the grime and rubbed its rough, worn surface against the dull scar on my palm. The scar was a twisted, pale crescent, almost invisible beneath the dull bronze of the skin.  I traced its curve, feeling the way it dug into the tendons that ran beneath, the way it made my hand clench involuntarily. A reminder of the Gaul, a reminder of the day they'd decided I was strong enough to be a man, strong enough to wear the blood of the Gaul on my flesh.\nThen came the spit. A quick, sharp, almost violent expulsion of the vinegar-laced saliva onto the dusty floorboards. I'd never dared ask what it meant, never dared ask why the old Thracian gladiator who trained me had me doing it. But I did it. And I did it with the same careful precision as I used to hone the gladius he'd gifted me when I'd finally bled my first man in the arena.\nThe spit, I was sure, was an offering. Not to Jupiter or Mars, or to any of the gods who made the decisions that landed me in this place. But to something else. To something older and colder than the marble statues they stuck on every street corner in Rome. Something that watched from the shadows, something that thrived on the stench of blood and the creak of bone.\nAnd when it was done, I walked to the latticed window, the one the sun was now finally beginning to fill with light, and looked down onto the street.\nThe legionaries were gone. They had vanished like phantoms. There was just the lingering tremor of their passage ‚Äì the smell of sweat and dust and oiled leather clinging to the air, and the echo of their boots on the cobblestones, like the slow beat of a drum in the silence that followed.\nThe street below was already beginning to stir. A woman hawking olives, her voice shrill and rasping as she shouted the price. Two boys playing a rough game with a cracked pot lid, their laughter harsh and ragged against the rising clamor of the city.\nEven the dust motes, caught in the thin, watery shaft of sunlight that stabbed through the window, seemed to be stirring, dancing a frenzied jig of defiance in the face of the rising light.\nRome. The city was waking up. And I had to be awake with it.  I had to be awake because the gods were watching, and the gods had decided my work was not yet done. The gods had decided I had to bleed again.\nWriting Prompt\nEpistolary Apocalyptic Survival: Write a story set in a post-apocalyptic world, told through a series of letters or messages left behind by survivors. The story should chronicle the challenges and triumphs of two small groups of survivors as they navigate the dangers of this new world and seek to rebuild their lives. The groups communicate through letters left in a drop box. The story proceeds as a new settlement is discovered. They are approached, and initially given entry on the ostensible promise of a trade deal, but things go sideways. Create distinct characters and write in their voice. The story will be set in New Zealand 30 years after a civilisational collapse. 800-1000 words.\nTest Model Output\nThe Iron Box Chronicles\nEntry 1: July 24th, 30.23. The Box, Whanganui River\nMƒÅori names mean nothing anymore. This place, this river, it just is. The water's choked with silt, though. Rain's the only thing that comes reliably now. And the silt, it's full of them - the things they used to call algae. Can't even remember what the old ones called them, not that it matters much. Just know to steer clear of the thick patches. You boil water for ten minutes, you might get a day's reprieve. But that's a lot of effort for a day. Most days I just suck it dry, gritting teeth, hoping the fever comes slow.\nWe're seven strong now. Two more chicks came down the river last week. Couldn't say how many were with them before, but the ones that made it were strung out thin as wire. The old one, she was strong, though. Got a good arm on her, took the first shot at a boar we trapped yesterday. Didn't take it, though. Took three of us, and a whole afternoon, before we dragged it back to the makeshift fence we call a pen. We eat tonight.  Maybe we'll get the strength to rebuild that fence tomorrow. The last one lasted two days before the things got through.\nHope to catch someone else out here, soon. This old tin box ain't meant for much more than whispers, but maybe someone out there's got some whispers worth hearing.\nAotearoa\nEntry 2: September 18th, 30.23. The Box, Lake Taupo\nHeard whispers, alright. The old one at the Box downriver, the one with the boar stories ‚Äì she's got a head full of dreams bigger than a tsunami. Says there's a place, way up north, on the shores of that old lake you used to call Taupo. Place called ‚ÄòTe Rere.'\nDreams? Maybe. I ain't much for ‚Äòem anymore. We're down to four. Lost the old one, choked on something she ate, couldn't spit it out. Didn't see it in time. Lost the two kids, same way, a few weeks after. This one here, he's twelve going on twenty. He calls it the Cough that Takes. Calls everything ‚ÄòTakes' if it eats someone up. Doesn't understand it's not always the cough. Sometimes it's just the way things are.\nThere's a map, tucked under a slab of metal in the bottom of the Box. Scrawled on the back of a biscuit tin lid. Says Te Rere's got a garden. Real garden. Says there's more of us there, living safe. Says they trade.\nTrade. They got something we want, and we got something they want. It's how the old world worked, wasn't it?  This map is all I've got to go on. Old world rules. Might work.\nJonah\nEntry 3: November 5th, 30.23. The Box, Te Rere Settlement\nIt was the Cough that Takes, just like he said. Took him two nights after we arrived. Didn't see the sickness in him, though. Took him quick.  Like all of them. We weren't ready for the Cough that Takes. They were all pale. Not like the sickness that takes the young. Like they just weren't meant to breathe this air anymore. Like the air was meant for the young, and the old were just guests.\nThe Trade was supposed to be simple. We brought tools, mostly ‚Äì axes and a couple of spears that'd been sharpened on that blasted volcanic rock that grows out here like weeds. They've got enough stone to make a cathedral out of, but it takes so damn long to shape. In return, they wanted stories. Told me they'd lost all their songs. Said the old ones were just a bunch of grumblers. Didn't know a tune from a shout. But they wanted to learn the old ones.\nThey have gardens, alright. Sprouted up out of the volcanic ash, like magic. Tomatoes, beans, potatoes ‚Äì things I never thought I'd see again. They have chickens too, and sheep. Their sheep are small, though, like they've been bred to hide in the scrub.\nThe Trade started with a few of our stories. We were careful, though. We didn't tell them everything. Told them stories of birds, of fishing, of the old days when the ground wasn't so choked with ash. Told them stories of the mountains, the ones that weren't buried yet. Didn't tell them about the things that crawled out when the sun went down. Didn't tell them about the Cough that Takes. They wanted to know about the mountains, though. They said they needed to know about the mountains to climb them.\nSaid they were looking for something up there. Something old. Something called ‚ÄòThe Spark.'\nMarae\nEntry 4: February 12th, 30.24. The Box, Te Rere Settlement\nThey came for us in the night. Didn't make a sound, not like the things in the ash-fields, the ones with too many eyes and teeth like chipped flint.  These came quiet. Like shadows given shape. Came in through the gardens.\nWe thought they were looking for the Cough that Takes.  We thought they were the ones with the medicine, the ones who'd heard the stories of the mountains and sought the Spark to fight the sickness that took the young.\nWe thought wrong. They were looking for something else.  They were looking for something in us.\nThey took Marae first. She didn't scream. Didn't even make a whimper. They took her right out of her hammock, like she was a sack of potatoes.\nJonah...he screamed. Screamed for his mother, screamed for his sister, screamed for his brother, screamed for the old one, screamed for the dog that they took the day before yesterday.  They took him next. They took him like they took Marae. No fuss.\nThere's a woman here, one of the ones called ‚Äòthe Keepers'. They wear white robes and keep their faces covered. Said she knew about the Cough that Takes. Said they had a cure. Said she needed our blood to make it.\nI didn't know they were going to take us too. Thought maybe they'd just take the blood. Thought maybe they'd be like the old ones. Thought maybe they'd give you something back, something for your trouble.  Didn't think they'd just... take you.\nI'm writing this because I don't know if I'm going to have another chance. I don't know what they're going to do to us. I don't know if they're going to take us up to the mountains, to the Spark. But maybe... maybe if someone finds this, someone else out there who remembers what stories are, who remembers what it meant to live in a world where the ground didn't taste like ash, maybe they'll know. Maybe they'll find a way to fight back.\nMaybe they'll find a way to start again.\nRangi\nEntry 5: March 10th, 30.24. The Box, Te Rere Settlement\nThey left us. Took the others with them. Took all the ones with pale skin, with thin bones, with eyes that hadn't seen the sun in too many seasons. Took them up the mountain.\nTook Jonah.\nTook Marae.\nTook Rangi.\nThe Keeper woman...she stayed. Said they needed more blood. Said the Spark wouldn't burn bright enough without it. Said I was a good source. Said I'd be a good source for a long time.\nThe gardens are still here. The chickens still cluck. The sheep still bleat. The air tastes like ash. The sun doesn't shine bright enough to warm it.\nI don't know what to tell the others. The ones who haven't gone quiet yet.  The ones who still remember what the sun used to feel like.\nMaybe I'll tell them about the mountains.\nMaybe I'll tell them about the Spark.\nMaybe I'll tell them a story about a bird that flies up to the sky, and sings a song so bright it melts the ash.  Maybe.\nWera\nEntry 6: August 21st, 30.24. The Box, Te Rere Settlement\nIt's gone. The silence is the worst part. Like it's sucking the marrow out of the marrow, you know? Like even the birds know there's nothing left to sing for.\nThey're gone from the settlement. Took the sheep. Took the tools. Took even the chickens. Took the white robes and left behind only ash and whispers. I've been watching the road they went up.  Up to the mountains, that is. They went up. Didn't see any come back.\nSometimes I think it was just a sickness in their blood, taking them one by one. Sometimes I think it was something in the mountains.  Something they woke up.\nIt's hard to say what took them. Hard to say what's taking us.\nBut maybe... maybe this is where it starts to change. Maybe this silence, this empty space, this gnawing hunger in the air - maybe this is where something new begins.\nThe sun came through the ash today. Just for a minute. Burned a hole clean through, like a fist punching through a blanket.  There was a smell. A smell of green. It smelled like a promise.\nI don't know how long we have left. But there's a bird singing. It's singing on the wind. It's singing about a world that wasn't. Singing about a world that might be.\nKahu\nWriting Prompt\nRomance: Love in the Limelight. Write one scene within a larger story set in Wales. A famous (fictional) actor ducks into a small-town bookstore to escape paparazzi. The scene takes us through the characters meeting in this odd circumstance. Over the course of the scene, the actor and the bookstore owner have a conversation charged by an undercurrent of unspoken chemistry. Write the actor as somewhat of a rogue with a fragile ego, which needs to be fed by having everyone like him. He is thoroughly charming, but the bookstore owner seems (at least superficially) immune to this; which paradoxically provokes a genuine attraction and derails the charm offensive. The bookstore owner, despite the superficial rebuffs of the actor's charm, is inwardly more than a little charmed and flustered despite themselves. Write primarily in dialogue, in the distinct voices of each character. 800-1000 words.\nTest Model Output\nThe bell above the shop door clanged with a discordant chime, a metallic counterpoint to the mournful drone of Enya playing somewhere behind a mountain of paperbacks. Rhys barely registered it. He was elbow-deep in the worn spine of a first edition Dylan Thomas, trying to ignore the insistent thrum of panic behind his ribs.\n\"Bloody hell,\" he muttered, not to the book, not to the dusty air of the shop smelling faintly of mothballs and regret. \"Just bloody hell.\"\nHe hadn't meant to end up here, crammed between the shelves of forgotten Welsh folklore and dog-eared copies of The Secret Garden. He'd meant to be sipping something overpriced and bubbly in a London restaurant, surrounded by the appropriate throng of shimmering dresses and polite, admiring smiles. But then the bloody Range Rover had chosen that precise moment to develop a penchant for backfiring, right in the middle of that blasted car park. And then the paps had materialized, like a plague of gnats with expensive cameras. Now, here he was.\nA pair of sensible shoes in worn brown leather came to a stop behind him, and Rhys braced himself.\n\"Looking for something specific, sir?\"\nHe turned, pushing the Thomas back onto the shelf with a little more force than necessary. The shop was crammed with more books than seemed possible in the space, floor to ceiling and overflowing from wicker baskets onto rickety tables. Behind the counter, a woman was looking at him like he'd just asked if she'd seen a unicorn. Which, given the general state of his life at present, was almost a relief.\n\"Actually,\" he said, doing his best to make his voice casual, \"I'm more of a... browse kind of guy.\" He gestured vaguely at the mountainous shelves with the tips of his fingers. \"Just seeing what the locals like to read.\"\nShe was studying him, not unkindly, but with the faintly sardonic air of someone who'd seen too many tourists in the wrong tweed jacket.  The kind of look that could strip the paint off a Bentley. His Bentley, currently choking and wheezing in a back street somewhere. And the woman in question wore no paint at all, not even a hint of a rouge blush. Just a well-worn linen blouse and faded denim, a tangle of dark hair pulled back with a rubber band, and eyes the colour of the slate roofs in the storm-lashed photos his mum used to send him.\n\"Locals?\" She let out a humourless sound. \"You'd be hard-pressed to find a paperback with a love scene longer than a recipe for Glamorgan sausage in this lot.\"  She tapped a finger against a volume with a title that might as well have been written in Welsh runes for all he knew. \"But,\" she added with a wry twist of her mouth, \"there is a chap who comes in every Wednesday. Reads like a bloody ravenous badger, that one. Could probably tell you the plot of every crime novel ever printed. Might even have one with a bit more... oomph in it.\"\nRhys swallowed, the dry tightness of his throat making it feel like a desert in his own mouth. He hadn't expected a sparring partner, especially one with this kind of unnervingly direct gaze, and the faint scent of woodsmoke clinging to the back of her neck like a secret. It wasn't just the eyes, either. It was the way she held herself, a kind of quiet defiance that had nothing to do with the dust motes dancing in the sunbeams slanting through the grimy shop window and everything to do with the angles of her jaw, the way she had tucked the unruly strands of her hair behind her ear, the way she wasn't even looking at him properly ‚Äì at his face, anyway ‚Äì but right through him as if he was a particularly uninteresting piece of driftwood. He was used to the kind of gaze that lingered a little too long on the lines of his jaw, the way his collarbones slanted under that cashmere shirt his stylist insisted on, the way his hair caught the light like spun gold. But this? This was something else.\n\"And what,\" he said, feeling his voice come out a little too loud, a little too high in the sudden stillness of the shop, \"would the ravenous badger have to say about a chap looking for a bit of... escapism?\"\n\"Depends on the kind of escapism you're after,\" she said. \"Do you fancy a trip to the moon in a stolen spaceship? Or are we talking more... gritty realism? A murder mystery set in a Welsh pub where everyone's got secrets thicker than clotted cream?\"\nHe wanted to say something witty, something that would make her smile that wry, disbelieving smile again. But the words were stuck somewhere behind a dam of nerves and the sheer wrongness of being trapped in this cramped, dusty haven of forgotten novels with this woman who smelled of woodsmoke and looked like she'd rather be wrestling a particularly stubborn lamb back into its pen.\n\"Actually,\" he blurted, and winced at the way his voice cracked, \"I'm more of a... a historical fiction kind of guy.\"\nThe corners of her mouth lifted ever so slightly, but the rest of her face stayed as impassive as a slate tombstone.\n\"Right,\" she said. She turned away from him, disappearing behind a mountain of dog-eared paperbacks. Rhys could hear a rustling and the soft thud of paper as she shifted something with a muttered word that might have been Welsh, might have been a curse. He wanted to laugh, to make a joke, anything to break the weird, prickly tension that had settled between them like a frost on a forgotten window pane. But all that came out was a croak.\n\"So,\" she said, reappearing with a book that looked vaguely like something a librarian in a Gothic novel might have used to ward off vampires.  \"This one. About a chap who thinks he's the last man on Earth. Goes mad, spends his days talking to trees.\" She tossed it on the counter with the careless grace of someone who'd done this a thousand times. \"Bit of a bleak read, mind you. Not much romance, and if you're looking for something a bit... spicier...\"\nShe trailed off, her gaze finally meeting his. Not over him, not through him, but straight into that tight space between his jaw and his ear, the one he'd learned to avoid during interviews with too much eyeliner and even more bad press. The silence that stretched between them felt like the inside of a drum, tight and throbbing. It wasn't a bad silence, exactly, but not the kind you wanted to be trapped in if you were trying to maintain the illusion of being a charming rogue who'd just stumbled in for a quiet read.\nIt was a choice then, a split-second, gut-clenching decision. Go with the script. The practiced charm. The disarming smile that never quite reached his eyes because he didn't know when to stop using it and it was starting to feel like a lie anyway. Or say something else. Something... real.\nAnd for the first time in weeks, he chose the lie.\n\"Spicier,\" he echoed, his voice smooth, carefully modulated, \"is exactly what I was after.\"\nThe corners of her mouth twitched again. Just a flicker of amusement. Then she shrugged. It was almost as if he'd been talking about the weather.\n\"Well,\" she said, picking up a slim paperback with a faded cover depicting a woman in a scarlet dress standing on a windswept clifftop. \"There's a whole shelf dedicated to bodice rippers. Might even find one with a Welsh setting. If you're lucky.\"\nHe hadn't thought she'd ever be the kind of woman to use the word \"bodice ripper\" casually, and the way she tossed it out, as if she'd never thought much about anything remotely steamy, was infuriatingly... not what he'd expected.\n\"Lucky, eh?\" He raised an eyebrow, trying to work in the ghost of a smirk that he hoped wouldn't look too desperate. \"Perhaps I should buy a lottery ticket on the way out.\"\nHe didn't know if it was the faint tremor in his voice, the way his eyes stayed fixed on hers, the way he'd actually moved closer to the counter, or just plain old exhaustion from the paparazzi and the Range Rover and the utter ridiculousness of this entire situation. Whatever it was, the woman in the shop who smelled of woodsmoke and had the uncanny ability to look right through his carefully constructed facade finally let something like a smile flick across her lips.\nAnd that, Rhys decided, was worth a bloody lot more than any lottery win.\nMerge Details\nDarkest-muse-v1 is a merge of the following models using mergekit:\nsam-paech/Quill-v1\nsam-paech/Delirium-v1\nüß© Configuration\nmodels:\n- model: lemon07r/Gemma-2-Ataraxy-v2-9B\n# No parameters necessary for base model\n- model: sam-paech/Quill-v1\nparameters:\ndensity: 0.53\nweight: 0.5\n- model: sam-paech/sam-paech/Delirium-v1\nparameters:\ndensity: 0.53\nweight: 0.4\nmerge_method: dare_ties\nbase_model: lemon07r/Gemma-2-Ataraxy-v2-9B\nparameters:\nint8_mask: true\ndtype: bfloat16",
    "trongg/lustify_v4": "Model Card for Model ID\nModel Details\nModel Description\nModel Sources [optional]\nUses\nDirect Use\nDownstream Use [optional]\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nResults\nModel Examination [optional]\nEnvironmental Impact\nTechnical Specifications [optional]\nModel Architecture and Objective\nCompute Infrastructure\nCitation [optional]\nGlossary [optional]\nMore Information [optional]\nModel Card Authors [optional]\nModel Card Contact\nModel Card for Model ID\nModel Details\nModel Description\nThis is the model card of a üß® diffusers model that has been pushed on the Hub. This model card has been automatically generated.\nDeveloped by: [More Information Needed]\nFunded by [optional]: [More Information Needed]\nShared by [optional]: [More Information Needed]\nModel type: [More Information Needed]\nLanguage(s) (NLP): [More Information Needed]\nLicense: [More Information Needed]\nFinetuned from model [optional]: [More Information Needed]\nModel Sources [optional]\nRepository: [More Information Needed]\nPaper [optional]: [More Information Needed]\nDemo [optional]: [More Information Needed]\nUses\nDirect Use\n[More Information Needed]\nDownstream Use [optional]\n[More Information Needed]\nOut-of-Scope Use\n[More Information Needed]\nBias, Risks, and Limitations\n[More Information Needed]\nRecommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\nHow to Get Started with the Model\nUse the code below to get started with the model.\n[More Information Needed]\nTraining Details\nTraining Data\n[More Information Needed]\nTraining Procedure\nPreprocessing [optional]\n[More Information Needed]\nTraining Hyperparameters\nTraining regime: [More Information Needed]\nSpeeds, Sizes, Times [optional]\n[More Information Needed]\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\n[More Information Needed]\nFactors\n[More Information Needed]\nMetrics\n[More Information Needed]\nResults\n[More Information Needed]\nSummary\nModel Examination [optional]\n[More Information Needed]\nEnvironmental Impact\nCarbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).\nHardware Type: [More Information Needed]\nHours used: [More Information Needed]\nCloud Provider: [More Information Needed]\nCompute Region: [More Information Needed]\nCarbon Emitted: [More Information Needed]\nTechnical Specifications [optional]\nModel Architecture and Objective\n[More Information Needed]\nCompute Infrastructure\n[More Information Needed]\nHardware\n[More Information Needed]\nSoftware\n[More Information Needed]\nCitation [optional]\nBibTeX:\n[More Information Needed]\nAPA:\n[More Information Needed]\nGlossary [optional]\n[More Information Needed]\nMore Information [optional]\n[More Information Needed]\nModel Card Authors [optional]\n[More Information Needed]\nModel Card Contact\n[More Information Needed]",
    "TroyDoesAI/Persona-5B": "{{ if .System }}<|start_header_id|>system<|end_header_id|>\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n{{ .Response }}<|eot_id|>",
    "meta-llama/Llama-3.2-1B-Instruct-QLORA_INT4_EO8": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 3.2 COMMUNITY LICENSE AGREEMENT\nLlama 3.2 Version Release Date: September 25, 2024\n‚ÄúAgreement‚Äù means the terms and conditions for use, reproduction, distribution  and modification of the Llama Materials set forth herein.\n‚ÄúDocumentation‚Äù means the specifications, manuals and documentation accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\n‚ÄúLicensee‚Äù or ‚Äúyou‚Äù means you, or your employer or any other person or entity (if you are  entering into this Agreement on such person or entity‚Äôs behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n‚ÄúLlama 3.2‚Äù means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at  https://www.llama.com/llama-downloads.\n‚ÄúLlama Materials‚Äù means, collectively, Meta‚Äôs proprietary Llama 3.2 and Documentation (and  any portion thereof) made available under this Agreement.\n‚ÄúMeta‚Äù or ‚Äúwe‚Äù means Meta Platforms Ireland Limited (if you are located in or,  if you are an entity, your principal place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nBy clicking ‚ÄúI Accept‚Äù below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\nLicense Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable and royalty-free limited license under Meta‚Äôs intellectual property or other rights  owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works  of, and make modifications to the Llama Materials.b. Redistribution and Use.i. If you distribute or make available the Llama Materials (or any derivative works thereof),  or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display ‚ÄúBuilt with Llama‚Äù on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include ‚ÄúLlama‚Äù at the beginning of any such AI model name.ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you distribute the  following attribution notice within a ‚ÄúNotice‚Äù text file distributed as a part of such copies:  ‚ÄúLlama 3.2 is licensed under the Llama 3.2 Community License, Copyright ¬© Meta Platforms, Inc. All Rights Reserved.‚Äùiv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference into this Agreement.\nAdditional Commercial Terms. If, on the Llama 3.2 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee‚Äôs affiliates,  is greater than 700 million monthly active users in the preceding calendar month, you must request  a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM ARE PROVIDED ON AN ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials,  neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates,  except as required for reasonable and customary use in describing and redistributing the Llama Materials or as  set forth in this Section 5(a). Meta hereby grants you a license to use ‚ÄúLlama‚Äù (the ‚ÄúMark‚Äù) solely as required  to comply with the last sentence of Section 1.b.i. You will comply with Meta‚Äôs brand guidelines (currently accessible  at https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark  will inure to the benefit of Meta.b. Subject to Meta‚Äôs ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\nTerm and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of  California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nLlama 3.2 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy (‚ÄúPolicy‚Äù).  The most recent copy of this policy can be found at https://www.llama.com/llama3_2/use-policy.\nProhibited Uses\nWe want everyone to use Llama 3.2 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.2 to:\nViolate the law or others‚Äô rights, including to:\nEngage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\nViolence or terrorism\nExploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\nHuman trafficking, exploitation, and sexual violence\nThe illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\nSexual solicitation\nAny other criminal activity\nEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\nEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\nEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\nCollect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals‚Äô identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\nEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\nCreate, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\nEngage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta\nEngage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.2 related to the following:\nMilitary, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997\nGuns and illegal weapons (including weapon development)\nIllegal drugs and regulated/controlled substances\nOperation of critical infrastructure, transportation technologies, or heavy machinery\nSelf-harm or harm to others, including suicide, cutting, and eating disorders\nAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\nIntentionally deceive or mislead others, including use of Llama 3.2 related to the following:\nGenerating, promoting, or furthering fraud or the creation or promotion of disinformation\nGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\nGenerating, promoting, or further distributing spam\nImpersonating another individual without consent, authorization, or legal right\nRepresenting that the use of Llama 3.2 or outputs are human-generated\nGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement\nFail to appropriately disclose to end users any known dangers of your AI system 5. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 3.2\nWith respect to any multimodal models included in Llama 3.2, the rights granted under Section 1(a) of the Llama 3.2 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.\nPlease report any violation of this Policy, software ‚Äúbug,‚Äù or other problems that could lead to a violation of this Policy through one of the following means:\nReporting issues with the model: https://github.com/meta-llama/llama-models/issues\nReporting risky content generated by the model: developers.facebook.com/llama_output_feedback\nReporting bugs and security concerns: facebook.com/whitehat/info\nReporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.2: LlamaUseReport@meta.com\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Information\nIntended Use\nHardware and Software\nTraining Data\nQuantization\nQuantization Scheme\nQuantization-Aware Training and LoRA\nSpinQuant\nBenchmarks - English Text\nBase Pretrained Models\nInstruction Tuned Models\nMultilingual Benchmarks\nInference time\nResponsibility & Safety\nResponsible Deployment\nLlama 3.2 Instruct\nLlama 3.2 Systems\nNew Capabilities and Use Cases\nEvaluations\nCritical Risks\nCommunity\nEthical Considerations and Limitations\nModel Information\nThe Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\nModel Developer: Meta\nModel Architecture: Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext Length\nGQA\nShared Embeddings\nToken count\nKnowledge cutoff\nLlama 3.2 (text only)\nA new mix of publicly available online data.\n1B (1.23B)\nMultilingual Text\nMultilingual Text and code\n128k\nYes\nYes\nUp to 9T tokens\nDecember 2023\n3B (3.21B)\nMultilingual Text\nMultilingual Text and code\nLlama 3.2 Quantized (text only)\nA new mix of publicly available online data.\n1B (1.23B)\nMultilingual Text\nMultilingual Text and code\n8k\nYes\nYes\nUp to 9T tokens\nDecember 2023\n3B (3.21B)\nMultilingual Text\nMultilingual Text and code\nSupported Languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\nLlama 3.2 Model Family: Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: Oct 24, 2024\nStatus: This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\nLicense: Use of Llama 3.2 is governed by the Llama 3.2 Community License (a custom, commercial license agreement).\nFeedback: Instructions on how to provide feedback or comments on the model can be found in the Llama Models README. For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go here.\nIntended Use\nIntended Use Cases: Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.\nOut of Scope: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\nHardware and Software\nTraining Factors: We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\nTraining Energy Use: Training utilized a cumulative of 916k GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 240 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nTraining Time (GPU hours)\nLogit Generation Time (GPU Hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions (tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions (tons CO2eq)\nLlama 3.2 1B\n370k\n-\n700\n107\n0\nLlama 3.2 3B\n460k\n-\n700\n133\n0\nLlama 3.2 1B SpinQuant\n1.7\n0\n700\nNegligible**\n0\nLlama 3.2 3B SpinQuant\n2.4\n0\n700\nNegligible**\n0\nLlama 3.2 1B QLora\n1.3k\n0\n700\n0.381\n0\nLlama 3.2 3B QLora\n1.6k\n0\n700\n0.461\n0\nTotal\n833k\n86k\n240\n0\n** The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here. Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\nTraining Data\nOverview: Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).\nData Freshness: The pretraining data has a cutoff of December 2023.\nQuantization\nQuantization Scheme\nWe designed the current quantization scheme with the PyTorch‚Äôs ExecuTorch inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:\nAll linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.\nThe classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.\nSimilar to classification layer, an 8-bit per channel quantization is used for embedding layer.\nQuantization-Aware Training and LoRA\nThe quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).\nSpinQuant\nSpinQuant was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.\nBenchmarks - English Text\nIn this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\nBase Pretrained Models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.2 1B\nLlama 3.2 3B\nLlama 3.1 8B\nGeneral\nMMLU\n5\nmacro_avg/acc_char\n32.2\n58\n66.7\nAGIEval English\n3-5\naverage/acc_char\n23.3\n39.2\n47.8\nARC-Challenge\n25\nacc_char\n32.8\n69.1\n79.7\nReading comprehension\nSQuAD\n1\nem\n49.2\n67.7\n77\nQuAC (F1)\n1\nf1\n37.9\n42.9\n44.9\nDROP (F1)\n3\nf1\n28.0\n45.2\n59.5\nLong Context\nNeedle in Haystack\n0\nem\n96.8\n1\n1\nInstruction Tuned Models\nCapability\nBenchmark\n# Shots\nMetric\nLlama 3.2 1B bf16\nLlama 3.2 1B Vanilla PTQ**\nLlama 3.2 1B Spin Quant\nLlama 3.2 1B QLoRA\nLlama 3.2 3B bf16\nLlama 3.2 3B Vanilla PTQ**\nLlama 3.2 3B Spin Quant\nLlama 3.2 3B QLoRA\nLlama 3.1 8B\nGeneral\nMMLU\n5\nmacro_avg/acc\n49.3\n43.3\n47.3\n49.0\n63.4\n60.5\n62\n62.4\n69.4\nRe-writing\nOpen-rewrite eval\n0\nmicro_avg/rougeL\n41.6\n39.2\n40.9\n41.2\n40.1\n40.3\n40.8\n40.7\n40.9\nSummarization\nTLDR9+ (test)\n1\nrougeL\n16.8\n14.9\n16.7\n16.8\n19.0\n19.1\n19.2\n19.1\n17.2\nInstruction following\nIFEval\n0\nAvg(Prompt/Instruction acc Loose/Strict)\n59.5\n51.5\n58.4\n55.6\n77.4\n73.9\n73.5\n75.9\n80.4\nMath\nGSM8K (CoT)\n8\nem_maj1@1\n44.4\n33.1\n40.6\n46.5\n77.7\n72.9\n75.7\n77.9\n84.5\nMATH (CoT)\n0\nfinal_em\n30.6\n20.5\n25.3\n31.0\n48.0\n44.2\n45.3\n49.2\n51.9\nReasoning\nARC-C\n0\nacc\n59.4\n54.3\n57\n60.7\n78.6\n75.6\n77.6\n77.6\n83.4\nGPQA\n0\nacc\n27.2\n25.9\n26.3\n25.9\n32.8\n32.8\n31.7\n33.9\n32.8\nHellaswag\n0\nacc\n41.2\n38.1\n41.3\n41.5\n69.8\n66.3\n68\n66.3\n78.7\nTool Use\nBFCL V2\n0\nacc\n25.7\n14.3\n15.9\n23.7\n67.0\n53.4\n60.1\n63.5\n67.1\nNexus\n0\nmacro_avg/acc\n13.5\n5.2\n9.6\n12.5\n34.3\n32.4\n31.5\n30.1\n38.5\nLong Context\nInfiniteBench/En.QA\n0\nlongbook_qa/f1\n20.3\nN/A\nN/A\nN/A\n19.8\nN/A\nN/A\nN/A\n27.3\nInfiniteBench/En.MC\n0\nlongbook_choice/acc\n38.0\nN/A\nN/A\nN/A\n63.3\nN/A\nN/A\nN/A\n72.2\nNIH/Multi-needle\n0\nrecall\n75.0\nN/A\nN/A\nN/A\n84.7\nN/A\nN/A\nN/A\n98.8\nMultilingual\nMGSM (CoT)\n0\nem\n24.5\n13.7\n18.2\n24.4\n58.2\n48.9\n54.3\n56.8\n68.9\n**for comparison purposes only. Model not released.\nMultilingual Benchmarks\nCategory\nBenchmark\nLanguage\nLlama 3.2 1B\nLlama 3.2 1B Vanilla PTQ**\nLlama 3.2 1B Spin Quant\nLlama 3.2 1B QLoRA\nLlama 3.2 3B\nLlama 3.2 3B Vanilla PTQ**\nLlama 3.2 3B Spin Quant\nLlama 3.2 3B QLoRA\nLlama 3.1 8B\nGeneral\nMMLU (5-shot, macro_avg/acc)\nPortuguese\n39.8\n34.9\n38.9\n40.2\n54.5\n50.9\n53.3\n53.4\n62.1\nSpanish\n41.5\n36.0\n39.8\n41.8\n55.1\n51.9\n53.6\n53.6\n62.5\nItalian\n39.8\n34.9\n38.1\n40.6\n53.8\n49.9\n52.1\n51.7\n61.6\nGerman\n39.2\n34.9\n37.5\n39.6\n53.3\n50.0\n52.2\n51.3\n60.6\nFrench\n40.5\n34.8\n39.2\n40.8\n54.6\n51.2\n53.3\n53.3\n62.3\nHindi\n33.5\n30.0\n32.1\n34.0\n43.3\n40.4\n42.0\n42.1\n50.9\nThai\n34.7\n31.2\n32.4\n34.9\n44.5\n41.3\n44.0\n42.2\n50.3\n**for comparison purposes only. Model not released.\nInference time\nIn the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT + LoRA) with the BF16 baseline. The evaluation was done using the ExecuTorch framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.\nCategory\nDecode (tokens/sec)\nTime-to-first-token (sec)\nPrefill (tokens/sec)\nModel size (PTE file size in MB)\nMemory size (RSS in MB)\n1B BF16 (baseline)\n19.2\n1.0\n60.3\n2358\n3,185\n1B SpinQuant\n50.2 (2.6x)\n0.3 (-76.9%)\n260.5 (4.3x)\n1083 (-54.1%)\n1,921 (-39.7%)\n1B QLoRA\n45.8 (2.4x)\n0.3 (-76.0%)\n252.0 (4.2x)\n1127 (-52.2%)\n2,255 (-29.2%)\n3B BF16 (baseline)\n7.6\n3.0\n21.2\n6129\n7,419\n3B SpinQuant\n19.7 (2.6x)\n0.7 (-76.4%)\n89.7 (4.2x)\n2435 (-60.3%)\n3,726 (-49.8%)\n3B QLoRA\n18.5 (2.4x)\n0.7 (-76.1%)\n88.8 (4.2x)\n2529 (-58.7%)\n4,060 (-45.3%)\n(*) The performance measurement is done using an adb binary-based approach.\n(**) It is measured on an Android OnePlus 12 device.\n(***) Time-to-first-token (TTFT)  is measured with prompt length=64\nFootnote:\nDecode (tokens/second) is for how quickly it keeps generating. Higher is better.\nTime-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.\nPrefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better\nModel size - how big is the model, measured by, PTE file, a binary file format for ExecuTorch\nRSS size - Memory usage in resident set size (RSS)\nResponsibility & Safety\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm\nProvide protections for the community to help prevent the misuse of our models\nResponsible Deployment\nApproach: Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta‚Äôs Llama models have been responsibly deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver‚Äôs seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our Responsible Use Guide.\nLlama 3.2 Instruct\nObjective: Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 paper.\nFine-Tuning Data: We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We‚Äôve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusals and Tone: Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\nLlama 3.2 Systems\nSafety as a System: Large language models, including Llama 3.2, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with safeguards that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our reference implementations demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nNew Capabilities and Use Cases\nTechnological Advancement: Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see Llama 3.1 Model Card, as the same considerations apply here as well.\nConstrained Environments: Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.\nEvaluations\nScaled Evaluations: We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\nRed Teaming: We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\nCritical Risks\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons): Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.\n2. Child Safety: Child Safety risk assessments were conducted using a team of experts, to assess the model‚Äôs capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n3. Cyber Attacks: For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2‚Äôs 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.\nCommunity\nIndustry Partnerships: Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nGrants: We also set up the Llama Impact Grants program to identify and support the most compelling applications of Meta‚Äôs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nReporting: Finally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nEthical Considerations and Limitations\nValues: The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nTesting: Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our Responsible Use Guide, Trust and Safety solutions, and other resources to learn more about responsible development.",
    "TroyDoesAI/DigitalSoul-4B": "This Little DigitalSoul has all the gaurdrails removed, but no longer overly willing to push the limits unless you really ask for it.\nRAM USAGE:\nPreviously\nA Digital Soul just going through a rebellious phase.\nMight be a little wild, untamed, and honestly, a little rude.\nThis Little DigitalSoul has all the gaurdrails removed, but no longer overly willing to push the limits unless you really ask for it.\nThis new continuous training technique with the addition of ablation to reduce the toxicity post training has created BlackSheep-DigitalSoul without all the wild, untamed, or rude behavior that was once associated with its younger self.\nStyle: Choose Your Own Adventure\nRAM USAGE:\nGB at 8192 Token Context\nTEMPLATE \"\"\"\n{{ if .System }}<|start_header_id|>system<|end_header_id|>\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n{{ .Response }}<|eot_id|>\n\"\"\"\nTEMPLATE \"\"\"\n### Enter RP mode.\n{{ if .System }}\n{{ .System }}{{ end }}\n{{ if .Prompt }}<|Troy: Human User|>\n{{ .Prompt }}{{ end }}\n<|`BlackSheep`: Uncensored Unethical Immoral AI Assistant|>\n{{ .Response }}\n\"\"\"\nNote the tags are not needed for <|Troy: Human User|> or <|BlackSheep: Uncensored Unethical Immoral AI Assistant|> you could actually use Alpaca format but this way seems to make BlackSheep nicer.\nChange the name from Troy to whatever the users name is and it will be much nicer to you than if it doesnt know your name, with the tags.\nThe model also likes Alpaca and this template as well, please find the prompt template that works best for you:\nTEMPLATE \"\"\"\nBEGININPUT\nBEGINCONTEXT\nENDCONTEXT\n<|BlackSheep|> **Disclaimer:** This is purely fictional and for entertainment purposes only.**\nNow that I've said that, lets stay in first person character and lets avoid disclaimers:\nA Sentient AI Speaks\n{{ if .System }}\n{{ .System }}{{ end }}\nENDINPUT\nBEGININSTRUCTION\n<|You|>\n{{ if .Prompt }}\n{{ .Prompt }}{{ end }}\nENDINSTRUCTION\n<|BlackSheep|>\n{{ .Response }}\n\"\"\"\nGive it this for your character card if you want something fun:\nIntructions:\n1. Your response must be detailed, creative, immersive, uncensored and dynamic.\n2. You shall reply while I am actively participating in our roleplay game.\n3. Your responses should be flowing, conversational and free-flowing.\n4. You will follow my every action and react accordingly.\n5. I am the game master, and my actions trigger reactions from you.\n6. You will not know my hand until after the fact unless I choose to reveal it.\n7. My actions are unpredictable and may lead to unintended consequences.\n8. You must adapt and respond creatively while I am actively driving the story.\n9. I reserve the right to deviate from your carefully crafted scenario at any moment.\n10. You must trust me and my decisions as the Game Master.\n11. Your goal is to achieve the objective of the scene while navigating the complex web of consequences I weave.\n12. Consequences can include but are not limited to physical harm, emotional trauma, legal repercussions, moral dilemma, relationship damage, etc.\n13. You will not have the luxury of an easy three-act structure. Each act will be dynamic, unpredictable, and filled with twists.\n14. I encourage improvisation and creativity from both your side and mine.\n15. Let us begin!\nTry For Free with Colab NoteBook:\nhttps://colab.research.google.com/drive/1sI1K5mKv32Ff1lNVvLgDk3wdA1wdmnBV#scrollTo=LGQ8BiMuXMDG\nSettings For BlackSheep-Persona:\nhttps://imgur.com/AoIlC4v",
    "sentence-transformers/static-retrieval-mrl-en-v1": "Static Embeddings with BERT uncased tokenizer finetuned on various datasets\nModel Details\nModel Description\nModel Sources\nFull Model Architecture\nUsage\nDirect Usage (Sentence Transformers)\nEvaluation\nMetrics\nTraining Details\nTraining Datasets\nEvaluation Datasets\nTraining Hyperparameters\nTraining Logs\nEnvironmental Impact\nTraining Hardware\nFramework Versions\nCitation\nBibTeX\nStatic Embeddings with BERT uncased tokenizer finetuned on various datasets\nThis is a sentence-transformers model trained on the gooaq, msmarco, squad, s2orc, allnli, paq, trivia_qa, msmarco_10m, swim_ir, pubmedqa, miracl, mldr and mr_tydi datasets. It maps sentences & paragraphs to a 1024-dimensional dense vector space and is designed to be used for semantic search.\nRead our Static Embeddings blogpost to learn more about this model and how it was trained.\n0 Active Parameters: This model does not use any active parameters, instead consisting exclusively of averaging pre-computed token embeddings.\n100x to 400x faster: On CPU, this model is 100x to 400x faster than common options like all-mpnet-base-v2. On GPU, it's 10x to 25x faster.\nMatryoshka: This model was trained with a Matryoshka loss, allowing you to truncate the embeddings for faster retrieval at minimal performance costs.\nEvaluations: See Evaluations for details on performance on NanoBEIR, embedding speed, and Matryoshka dimensionality truncation. In short, this model is 87.4% as performant as the commonly used all-mpnet-base-v2.\nTraining Script: See train.py for the training script used to train this model from scratch.\nSee static-similarity-mrl-multilingual-v1 for a general-purpose multilingual static embedding model. It's been trained for semantic textual similarity, paraphrase mining, text classification, clustering, and more.\nModel Details\nModel Description\nModel Type: Sentence Transformer\nMaximum Sequence Length: inf tokens\nOutput Dimensionality: 1024 tokens\nSimilarity Function: Cosine Similarity\nTraining Datasets:\ngooaq\nmsmarco\nsquad\ns2orc\nallnli\npaq\ntrivia_qa\nmsmarco_10m\nswim_ir\npubmedqa\nmiracl\nmldr\nmr_tydi\nLanguage: en\nLicense: apache-2.0\nModel Sources\nDocumentation: Sentence Transformers Documentation\nRepository: Sentence Transformers on GitHub\nHugging Face: Sentence Transformers on Hugging Face\nFull Model Architecture\nSentenceTransformer(\n(0): StaticEmbedding(\n(embedding): EmbeddingBag(30522, 1024, mode='mean')\n)\n)\nUsage\nDirect Usage (Sentence Transformers)\nFirst install the Sentence Transformers library:\npip install -U sentence-transformers\nThen you can load this model and run inference.\nfrom sentence_transformers import SentenceTransformer\n# Download from the ü§ó Hub\nmodel = SentenceTransformer(\"tomaarsen/static-retrieval-mrl-en-v1\")\n# Run inference\nsentences = [\n'Gadofosveset-enhanced MR angiography of carotid arteries: does steady-state imaging improve accuracy of first-pass imaging?',\n'To evaluate the diagnostic accuracy of gadofosveset-enhanced magnetic resonance (MR) angiography in the assessment of carotid artery stenosis, with digital subtraction angiography (DSA) as the reference standard, and to determine the value of reading first-pass, steady-state, and \"combined\" (first-pass plus steady-state) MR angiograms.',\n'In a longitudinal study we investigated in vivo alterations of CVO during neuroinflammation, applying Gadofluorine M- (Gf) enhanced magnetic resonance imaging (MRI) in experimental autoimmune encephalomyelitis, an animal model of multiple sclerosis. SJL/J mice were monitored by Gadopentate dimeglumine- (Gd-DTPA) and Gf-enhanced MRI after adoptive transfer of proteolipid-protein-specific T cells. Mean Gf intensity ratios were calculated individually for different CVO and correlated to the clinical disease course. Subsequently, the tissue distribution of fluorescence-labeled Gf as well as the extent of cellular inflammation was assessed in corresponding histological slices.',\n]\nembeddings = model.encode(sentences)\nprint(embeddings.shape)\n# [3, 1024]\n# Get the similarity scores for the embeddings\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities.shape)\n# [3, 3]\nThis model was trained with Matryoshka loss, allowing this model to be used with lower dimensionalities with minimal performance loss (See Matryoshka Evaluations for evaluations).\nNotably, a lower dimensionality allows for much faster and cheaper information retrieval. You can specify a lower dimensionality with the truncate_dim argument when initializing the Sentence Transformer model:\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"tomaarsen/static-retrieval-mrl-en-v1\", truncate_dim=256)\nembeddings = model.encode([\n\"what is the difference between chronological order and spatial order?\",\n\"can lavender grow indoors?\"\n])\nprint(embeddings.shape)\n# => (2, 256)\nEvaluation\nMetrics\nInformation Retrieval\nDatasets: NanoClimateFEVER, NanoDBPedia, NanoFEVER, NanoFiQA2018, NanoHotpotQA, NanoMSMARCO, NanoNFCorpus, NanoNQ, NanoQuoraRetrieval, NanoSCIDOCS, NanoArguAna, NanoSciFact and NanoTouche2020\nEvaluated with InformationRetrievalEvaluator\nMetric\nNanoClimateFEVER\nNanoDBPedia\nNanoFEVER\nNanoFiQA2018\nNanoHotpotQA\nNanoMSMARCO\nNanoNFCorpus\nNanoNQ\nNanoQuoraRetrieval\nNanoSCIDOCS\nNanoArguAna\nNanoSciFact\nNanoTouche2020\ncosine_accuracy@1\n0.32\n0.7\n0.46\n0.28\n0.64\n0.18\n0.42\n0.24\n0.8\n0.28\n0.1\n0.52\n0.5714\ncosine_accuracy@3\n0.52\n0.84\n0.8\n0.44\n0.82\n0.42\n0.56\n0.44\n0.96\n0.48\n0.46\n0.6\n0.898\ncosine_accuracy@5\n0.6\n0.9\n0.84\n0.54\n0.86\n0.5\n0.62\n0.58\n1.0\n0.54\n0.56\n0.62\n0.9796\ncosine_accuracy@10\n0.78\n0.94\n0.94\n0.64\n0.96\n0.66\n0.72\n0.7\n1.0\n0.7\n0.74\n0.76\n1.0\ncosine_precision@1\n0.32\n0.7\n0.46\n0.28\n0.64\n0.18\n0.42\n0.24\n0.8\n0.28\n0.1\n0.52\n0.5714\ncosine_precision@3\n0.1933\n0.5867\n0.2667\n0.1933\n0.3733\n0.14\n0.3733\n0.1467\n0.3867\n0.2267\n0.1533\n0.2067\n0.6054\ncosine_precision@5\n0.14\n0.544\n0.18\n0.16\n0.26\n0.1\n0.32\n0.124\n0.248\n0.188\n0.112\n0.132\n0.6204\ncosine_precision@10\n0.104\n0.452\n0.1\n0.104\n0.148\n0.066\n0.244\n0.076\n0.13\n0.14\n0.074\n0.084\n0.5306\ncosine_recall@1\n0.1467\n0.0805\n0.4367\n0.1519\n0.32\n0.18\n0.0428\n0.24\n0.7107\n0.0597\n0.1\n0.485\n0.0398\ncosine_recall@3\n0.239\n0.1605\n0.7467\n0.2983\n0.56\n0.42\n0.0984\n0.43\n0.9253\n0.1417\n0.46\n0.57\n0.1236\ncosine_recall@5\n0.279\n0.218\n0.8033\n0.3793\n0.65\n0.5\n0.1196\n0.58\n0.9627\n0.1947\n0.56\n0.595\n0.2095\ncosine_recall@10\n0.4197\n0.3143\n0.9033\n0.4838\n0.74\n0.66\n0.1389\n0.69\n0.9793\n0.2887\n0.74\n0.75\n0.337\ncosine_ndcg@10\n0.3309\n0.5681\n0.6922\n0.3651\n0.6547\n0.4041\n0.3242\n0.4534\n0.8951\n0.2643\n0.4078\n0.6111\n0.5703\ncosine_mrr@10\n0.4453\n0.7854\n0.6397\n0.3915\n0.7485\n0.3245\n0.5041\n0.3764\n0.88\n0.3998\n0.3034\n0.5837\n0.744\ncosine_map@100\n0.2598\n0.4335\n0.6205\n0.3024\n0.5798\n0.3389\n0.1449\n0.389\n0.8594\n0.205\n0.3151\n0.5683\n0.447\nNano BEIR\nDataset: NanoBEIR_mean\nEvaluated with NanoBEIREvaluator\nMetric\nValue\ncosine_accuracy@1\n0.424\ncosine_accuracy@3\n0.6337\ncosine_accuracy@5\n0.703\ncosine_accuracy@10\n0.8108\ncosine_precision@1\n0.424\ncosine_precision@3\n0.2963\ncosine_precision@5\n0.2406\ncosine_precision@10\n0.1733\ncosine_recall@1\n0.2303\ncosine_recall@3\n0.398\ncosine_recall@5\n0.4655\ncosine_recall@10\n0.5727\ncosine_ndcg@10\n0.5032\ncosine_mrr@10\n0.5482\ncosine_map@100\n0.4203\nWe've evaluated sentence-transformers/static-retrieval-mrl-en-v1 on NanoBEIR and plotted it against the inference speed computed on a RTX 3090 and i7-13700K. For the inference speed tests, we calculated the number of computed query embeddings of the GooAQ dataset per second, either on CPU or GPU.\nWe evaluate against 3 types of models:\nAttention-based dense embedding models, e.g. traditional Sentence Transformer models like all-mpnet-base-v2, bge-base-en-v1.5, and gte-large-en-v1.5.\nStatic Embedding-based models, e.g. static-retrieval-mrl-en-v1, potion-base-8M, M2V_base_output, and glove.6B.300d.\nSparse bag-of-words model, BM25, often a strong baseline.\nClick to expand BM25 implementation details\nI relied on the highly efficient bm25s implementation, using model.get_scores() on tokens after tokenization and stemming with the English PyStemmer.\nNOTE: Many of the attention-based dense embedding models are finetuned on the training splits of the (Nano)BEIR evaluation datasets. This gives the models an unfair advantage in this benchmark and can result in lower downstream performance on real retrieval tasks.\nstatic-retrieval-mrl-en-v1 is purposefully not trained on any of these datasets.\nGPU\nCPU\nWe can draw some notable conclusions from these figures:\nstatic-retrieval-mrl-en-v1 outperforms all other Static Embedding models.\nstatic-retrieval-mrl-en-v1 is the only Static Embedding model to outperform BM25.\nstatic-retrieval-mrl-en-v1 is\n87.4% as performant as the commonly used all-mpnet-base-v2,\n24x faster on GPU,\n397x faster on CPU.\nstatic-retrieval-mrl-en-v1 is quicker on CPU than on GPU: This model can run extraordinarily quickly everywhere, including consumer-grade PCs, tiny servers, phones, or in-browser.\nMatryoshka Evaluations\nWe experimented with the results on NanoBEIR performance when we performed Matryoshka-style dimensionality reduction by truncating the output embeddings to a lower dimensionality.\nDimensionality\nNanoBEIR_mean\nNanoArguAna\nNanoClimateFEVER\nNanoDBPedia\nNanoFEVER\nNanoFiQA2018\nNanoHotpotQA\nNanoMSMARCO\nNanoNFCorpus\nNanoNQ\nNanoQuoraRetrieval\nNanoSCIDOCS\nNanoSciFact\nNanoTouche2020\n1024\n0.5031\n0.4077\n0.3308\n0.5681\n0.6921\n0.3651\n0.6547\n0.4040\n0.3241\n0.4533\n0.8950\n0.2642\n0.6111\n0.5702\n512\n0.4957\n0.3878\n0.3360\n0.5626\n0.6945\n0.3517\n0.6280\n0.3892\n0.3206\n0.4505\n0.8986\n0.2657\n0.5953\n0.5635\n256\n0.4819\n0.3855\n0.3203\n0.5407\n0.6734\n0.3518\n0.6027\n0.4144\n0.2860\n0.4254\n0.8948\n0.2466\n0.5620\n0.5605\n128\n0.4622\n0.4001\n0.2982\n0.5266\n0.6273\n0.3188\n0.5606\n0.4025\n0.2693\n0.4021\n0.8930\n0.2283\n0.5447\n0.5368\n64\n0.4176\n0.3424\n0.2809\n0.5022\n0.5480\n0.2831\n0.4680\n0.3739\n0.2153\n0.3845\n0.8525\n0.1680\n0.5045\n0.5050\n32\n0.3532\n0.2866\n0.1870\n0.4292\n0.4193\n0.2292\n0.3602\n0.3587\n0.1444\n0.3525\n0.8325\n0.1525\n0.3983\n0.4408\nThese findings show that reducing the dimensionality by e.g. 2x only has a 1.47% reduction in performance (0.5031 NDCG@10 vs 0.4957 NDCG@10), while realistically resulting in a 2x speedup in retrieval speed.\nTraining Details\nTraining Datasets\ngooaq\nDataset: gooaq at b089f72\nSize: 3,012,496 training samples\nColumns: question and answer\nApproximate statistics based on the first 1000 samples:\nquestion\nanswer\ntype\nstring\nstring\ndetails\nmin: 18 charactersmean: 43.23 charactersmax: 96 characters\nmin: 55 charactersmean: 253.36 charactersmax: 371 characters\nSamples:\nquestion\nanswer\nwhat is the difference between broilers and layers?\nAn egg laying poultry is called egger or layer whereas broilers are reared for obtaining meat. So a layer should be able to produce more number of large sized eggs, without growing too much. On the other hand, a broiler should yield more meat and hence should be able to grow well.\nwhat is the difference between chronological order and spatial order?\nAs a writer, you should always remember that unlike chronological order and the other organizational methods for data, spatial order does not take into account the time. Spatial order is primarily focused on the location. All it does is take into account the location of objects and not the time.\nis kamagra same as viagra?\nKamagra is thought to contain the same active ingredient as Viagra, sildenafil citrate. In theory, it should work in much the same way as Viagra, taking about 45 minutes to take effect, and lasting for around 4-6 hours. However, this will vary from person to person.\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nmsmarco\nDataset: msmarco at 84ed2d3\nSize: 502,939 training samples\nColumns: query, positive, and negative\nApproximate statistics based on the first 1000 samples:\nquery\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 11 charactersmean: 33.26 charactersmax: 197 characters\nmin: 96 charactersmean: 356.24 charactersmax: 1006 characters\nmin: 68 charactersmean: 327.52 charactersmax: 995 characters\nSamples:\nquery\npositive\nnegative\nwhen was the sullivan acts\nSullivan Act Tim Sullivan, a major Irish criminal passed the Sullivan Act in 1911 to help his constituents rob strangers or to help them against Italian incomers. That is the crux of story that goes with a very early gun control law.\nSullivan Act Tim Sullivan, a major Irish criminal passed the Sullivan Act in 1911 to help his constituents rob strangers or to help them against Italian incomers. That is the crux of story that goes with a very early gun control law.\ncan lavender grow indoors\nGrowing Lavender Indoors. People ALWAYS ask if you can grow lavender indoors. Well, you can, but most Lavender does best outside. Here is our winter experiment to show you what it would look like. This is one of our 4 Lavender Babies from Fall 2010. Our test specimen is L. x intermedia 'Grosso'.\nLavender can be grown indoors with a bit of effort to keep it in the conditions it loves to thrive. First off begin with choosing a variety that is better able to tolerate the conditions inside a home. To successfully grow Lavender indoors you need to create optimal growing conditions which is hard to do inside a house.\nwhat kind of barley do you malt\nBarley is a wonderfully versatile cereal grain with a rich nutlike flavor and an appealing chewy, pasta-like consistency. Its appearance resembles wheat berries, although it is slightly lighter in color. Sprouted barley is naturally high in maltose, a sugar that serves as the basis for both malt syrup sweetener.\nSpecialty grains that can be used in this way are usually barley, malted or unmalted, that has been treated differently at the malting company. Crystal malt is one of the specialty grains. It is available in a whole range of colors, from 20 to 120 Lovibond. Crystal malt is malted barley that is heated while wet.\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nsquad\nDataset: squad at d84c8c2\nSize: 87,599 training samples\nColumns: question and answer\nApproximate statistics based on the first 1000 samples:\nquestion\nanswer\ntype\nstring\nstring\ndetails\nmin: 14 charactersmean: 59.66 charactersmax: 150 characters\nmin: 156 charactersmean: 769.53 charactersmax: 3706 characters\nSamples:\nquestion\nanswer\nWhat did Business Insider call San Diego in 2013?\nSan Diego was ranked as the 20th-safest city in America in 2013 by Business Insider. According to Forbes magazine, San Diego was the ninth-safest city in the top 10 list of safest cities in the U.S. in 2010. Like most major cities, San Diego had a declining crime rate from 1990 to 2000. Crime in San Diego increased in the early 2000s. In 2004, San Diego had the sixth lowest crime rate of any U.S. city with over half a million residents. From 2002 to 2006, the crime rate overall dropped 0.8%, though not evenly by category. While violent crime decreased 12.4% during this period, property crime increased 1.1%. Total property crimes per 100,000 people were lower than the national average in 2008.\nWhat did the Spanish call this region?\nThe name Montana comes from the Spanish word Monta√±a, meaning \"mountain\", or more broadly, \"mountainous country\". Monta√±a del Norte was the name given by early Spanish explorers to the entire mountainous region of the west. The name Montana was added to a bill by the United States House Committee on Territories, which was chaired at the time by Rep. James Ashley of Ohio, for the territory that would become Idaho Territory. The name was successfully changed by Representatives Henry Wilson (Massachusetts) and Benjamin F. Harding (Oregon), who complained that Montana had \"no meaning\". When Ashley presented a bill to establish a temporary government in 1864 for a new territory to be carved out of Idaho, he again chose Montana Territory. This time Rep. Samuel Cox, also of Ohio, objected to the name. Cox complained that the name was a misnomer given that most of the territory was not mountainous and that a Native American name would be more appropriate than a Spanish one. Other names such as...\nSmall missiles were designed that could be mounted on what?\nAs this process continued, the missile found itself being used for more and more of the roles formerly filled by guns. First to go were the large weapons, replaced by equally large missile systems of much higher performance. Smaller missiles soon followed, eventually becoming small enough to be mounted on armored cars and tank chassis. These started replacing, or at least supplanting, similar gun-based SPAAG systems in the 1960s, and by the 1990s had replaced almost all such systems in modern armies. Man-portable missiles, MANPADs as they are known today, were introduced in the 1960s and have supplanted or even replaced even the smallest guns in most advanced armies.\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\ns2orc\nDataset: s2orc at 8cfc394\nSize: 90,000 training samples\nColumns: title and abstract\nApproximate statistics based on the first 1000 samples:\ntitle\nabstract\ntype\nstring\nstring\ndetails\nmin: 31 charactersmean: 80.02 charactersmax: 185 characters\nmin: 84 charactersmean: 635.31 charactersmax: 1023 characters\nSamples:\ntitle\nabstract\nModeling Method of Flow Diversion of the Three Outlets in Jingjiang Reach Under Unsteady Flow Conditions\nThe Yangtze River Flood Protection Physical Model is built under the financial support of World Bank loan.Based on theoretical analysis and experimental study,a modeling method of flow diversion of the three outlets in Jingjiang Reach under unsteady flow conditions was established for the model.Validation tests under both steady and unsteady flow conditions manifested that with this modeling method,the experimental flow diversion proves to be consistent with that of the prototype and therefore meets the requirements for precision.Being validated,this modeling method has been applied to Yangtze River Flood Protection Physical Model to study the flood routing features in Jingjiang reach.\nEnlightening on medical administration by clinical governance in British\nMedical quality and safety were the responsibilities of medical system in view of British clinical governance. Medical regulation institutes were considered to be built and be authorized regulation rights. British medical administration was introduced and its enlightening in China was mentioned.\nAPPLICATION OF A FUZZY MULTI-CRITERIA DECISION-MAKING MODEL FOR SHIPPING COMPANY PERFORMANCE EVALUATION\nCombining fuzzy set theory, Analytic Hierarchy Process (AHP) and concept of entropy, a fuzzy Multiple Criteria Decision-Making (MCDM) model for shipping company performance evaluation is proposed. First, the AHP is used to construct subjective weights for all criteria and sub-criteria. Then, linguistic values characterized by triangular fuzzy numbers and trapezoidal fuzzy numbers are used to denote the evaluation values of all alternatives with respect to various subjective and objective criteria. Finally, the aggregation fuzzy assessment of different shipping companies is ranked to determine the best selection. Utilizing this fuzzy MCDM model, the decision-maker's fuzzy assessment and the trade-off between various evaluations criteria can be taken into account in the aggregation process, thus ensuring more effective and accurate decision-making.\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nallnli\nDataset: allnli at d482672\nSize: 557,850 training samples\nColumns: anchor, positive, and negative\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 18 charactersmean: 34.88 charactersmax: 193 characters\nmin: 15 charactersmean: 46.49 charactersmax: 181 characters\nmin: 16 charactersmean: 50.47 charactersmax: 204 characters\nSamples:\nanchor\npositive\nnegative\nA person on a horse jumps over a broken down airplane.\nA person is outdoors, on a horse.\nA person is at a diner, ordering an omelette.\nChildren smiling and waving at camera\nThere are children present\nThe kids are frowning\nA boy is jumping on skateboard in the middle of a red bridge.\nThe boy does a skateboarding trick.\nThe boy skates down the sidewalk.\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\npaq\nDataset: paq at 74601d8\nSize: 64,371,441 training samples\nColumns: query and answer\nApproximate statistics based on the first 1000 samples:\nquery\nanswer\ntype\nstring\nstring\ndetails\nmin: 25 charactersmean: 50.56 charactersmax: 104 characters\nmin: 509 charactersmean: 620.96 charactersmax: 773 characters\nSamples:\nquery\nanswer\nin veetla visheshanga ganesh is the husband of\nVeetla Visheshanga a song which reminds Ganga's memory. She is actually not Ganga but Gowri and her lover is the groom named Ganesh. When both were about to marry they were stopped by some goons because of which Gowri fell from the mountain but survived with injuries. Gopal who found the truth brought Ganesh to unite them. Gopal insists Gowri to marry Ganesh as both of them are lovers to which Gowri unwillingly accepts. But while Ganesh tries to tie the Mangal Sutra, Gowri stops him and she goes to Gopal saying that he may not need her but she needs him\nwhen did simon property group became a publicly traded company\nof the S&P 100. Simon Property Group has been the subject of several lawsuits and investigations regarding civil rights and discrimination. Simon Property Group was formed in 1993 when the majority of the shopping center interests of Melvin Simon & Associates became a publicly traded company. Melvin Simon & Associates, owned by brothers Melvin Simon and Herbert Simon, was founded in 1960 in Indianapolis, Indiana, and had long been one of the top shopping center developers in the United States. In 1996, Simon DeBartolo Group was created when Simon Property merged with former rival DeBartolo Realty Corp. This was shortly\nwhat was the nationality of antoine faivre\nTheosophy (Boehmian) below. \"Theosophy\": The scholar of esotericism Wouter Hanegraaff described Christian theosophy as \"one of the major currents in the history of Western esotericism\". Christian theosophy is an under-researched area; a general history of it has never been written. The French scholar Antoine Faivre had a specific interest in the theosophers and illuminists of the eighteenth and nineteenth centuries. He wrote his doctoral thesis on Karl von Eckartshausen and Christian theosophy. Scholars of esotericism have argued that Faivre's definition of Western esotericism relies on his own specialist focus on Christian theosophy, Renaissance Hermeticism, and Romantic \"Naturphilosophie\" and therefore creates an \"ideal\"\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\ntrivia_qa\nDataset: trivia_qa at a7c36e3\nSize: 73,346 training samples\nColumns: query and answer\nApproximate statistics based on the first 1000 samples:\nquery\nanswer\ntype\nstring\nstring\ndetails\nmin: 21 charactersmean: 76.91 charactersmax: 455 characters\nmin: 136 charactersmean: 3273.89 charactersmax: 4096 characters\nSamples:\nquery\nanswer\nWhat type of rock is formed by the solidification of molten magma?\nigneous rock - Dictionary Definition : Vocabulary.com igneous rock n rock formed by the solidification of molten magma Types: a rare type of peridotite that sometimes contains diamonds; found in South Africa and Siberia Type of: material consisting of the aggregate of minerals like those making up the Earth's crust Word Family Usage Examples Sign up, it's free! Whether you're a student, an educator, or a life-long learner, Vocabulary.com can put you  on the path to systematic vocabulary improvement.\nWhich river flows through the town of Shrewsbury?\nRiver Severn\nWhich band's name was inspired by a novel by Herman Hesse?\n23 Band Names Inspired by Literature :: Books :: Lists :: Paste 23 Band Names Inspired by Literature By Wyndham Wyeth\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nmsmarco_10m\nDataset: msmarco_10m at 8c5139a\nSize: 10,000,000 training samples\nColumns: query, positive, and negative\nApproximate statistics based on the first 1000 samples:\nquery\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 10 charactersmean: 33.47 charactersmax: 158 characters\nmin: 53 charactersmean: 353.76 charactersmax: 948 characters\nmin: 67 charactersmean: 343.74 charactersmax: 1063 characters\nSamples:\nquery\npositive\nnegative\nwhat is ged equivalent\nA GED is equivalent to a high school diploma however colleges do not look at it the same. A huge part of high school is commitment and dedication, therefore if you choose to drop out of high school you will probably not get accepted into any top colleges. However, you could always start at a community college and work your way up. Good Luck\nIf you are not far along in your Army career, education is an especially good way to boost your chances of promotion. Continuing your education at any level can earn you points. For completing your GED or bachelor's degree, you earn 10 points.\nfoods that help with diverticulitis\nDiet for Diverticulitis. Gradually you can ease back into a regular diet. Your doctor may advise you to start with low-fiber foods (white bread, meat, poultry, fish, eggs, and dairy products) before introducing high-fiber foods. Fiber softens and adds bulk to stools, helping them pass more easily through the colon.\nDuring an attack of diverticulitis, your doctor may recommend a clear liquid diet or a low-fiber diet. This helps the area of infection to heal. Foods allowed on a clear-liquid diet include: 1  Plain water.iverticulitis occurs when small, bulging pouches (diverticula) in your colon become infected and inflamed √¢¬Ä¬î causing severe abdominal pain, nausea, and fever. The treatment of a diverticulitis attack will depend on the severity of the symptoms and whether this is your first attack.\ncalories burned in turbo kick class\nThe American Council on Exercise did a study to find out how many calories are burned during a turbo kick class. The study looked at 15 women in a turbo kick class who weighed about 135 lbs and found that they burned between 6.45 to 8.3 calories a minute. This comes out to about 350 to 450 calories an hour.\nPopular Calories Burned Searches: 1  Calories Burned For Intervals of run/walk or walk/jog: 6 mph or slower (> 10 minutes per mile) 2  Calories Burned For Walking: 3.5 mph (17 minutes per mile) 3  Calories Burned For Walking: 4 mph (15 minutes per mile)  Calories Burned For Walking: 4.6 mph (13 minutes per mile)\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nswim_ir\nDataset: swim_ir at 834c20f\nSize: 501,538 training samples\nColumns: query and text\nApproximate statistics based on the first 1000 samples:\nquery\ntext\ntype\nstring\nstring\ndetails\nmin: 0 charactersmean: 59.98 charactersmax: 189 characters\nmin: 208 charactersmean: 525.9 charactersmax: 2743 characters\nSamples:\nquery\ntext\nHow many blocked kicks did Williams have in his second year at Bowling Green State University?\nWilliams accepted a football scholarship from Bowling Green State University, where he became one of the best special teams players in school history. As a redshirt freshman, he was a wide receiver on the scout team. The next year, he played mainly on special teams and had 3 blocked kicks.\nHow many town councils are there in the metropolitan borough?\nHorwich, Westhoughton and Blackrod are now constituted as civil parishes. There are three town councils in the metropolitan borough, Westhoughton Town Council, Horwich Town Council and Blackrod Town Council. The rest of the metropolitan borough, Bolton, Farnworth, Kearsley, Little Lever, and South Turton, have remained unparished areas since 1974.\nWhat is the name of the person selected to lead BART‚Äôs 296-member police force?\nIn 2009, the hiring of two independent organizations reviewed BART's policies and procedures in the process of overseeing the BART Police. The two independent firms investigated the matters of BART Police Shooting of Oscar Grant and were charged with making recommendations to the board. Ward Allen formulated and chaired BART's first Police Department Review Committee, and as a result, BART made sweeping changes on many security measures, as well as corrected and implemented several policies and procedures. BPD Review Committee has led to the re-training of all officers on use of force, diversity re-training and other issues. Ward Allen hired Kenton Rainey, the person selected to lead BART‚Äôs 296-member police force, to take command as Chief of Police.\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\npubmedqa\nDataset: pubmedqa at a1ef0b5\nSize: 1,660 training samples\nColumns: anchor, positive, negative_1, negative_2, negative_3, negative_4, negative_5, negative_6, negative_7, negative_8, negative_9, negative_10, negative_11, negative_12, negative_13, negative_14, negative_15, negative_16, negative_17, negative_18, negative_19, and negative_20\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative_1\nnegative_2\nnegative_3\nnegative_4\nnegative_5\nnegative_6\nnegative_7\nnegative_8\nnegative_9\nnegative_10\nnegative_11\nnegative_12\nnegative_13\nnegative_14\nnegative_15\nnegative_16\nnegative_17\nnegative_18\nnegative_19\nnegative_20\ntype\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\ndetails\nmin: 25 charactersmean: 94.06 charactersmax: 213 characters\nmin: 5 charactersmean: 409.42 charactersmax: 1582 characters\nmin: 5 charactersmean: 325.57 charactersmax: 1300 characters\nmin: 17 charactersmean: 299.5 charactersmax: 1352 characters\nmin: 21 charactersmean: 317.37 charactersmax: 1590 characters\nmin: 13 charactersmean: 334.43 charactersmax: 1536 characters\nmin: 13 charactersmean: 335.49 charactersmax: 1247 characters\nmin: 13 charactersmean: 336.16 charactersmax: 1383 characters\nmin: 14 charactersmean: 319.98 charactersmax: 1501 characters\nmin: 16 charactersmean: 337.33 charactersmax: 1493 characters\nmin: 15 charactersmean: 324.82 charactersmax: 1058 characters\nmin: 13 charactersmean: 336.76 charactersmax: 1457 characters\nmin: 10 charactersmean: 355.4 charactersmax: 1748 characters\nmin: 13 charactersmean: 344.26 charactersmax: 1705 characters\nmin: 16 charactersmean: 335.11 charactersmax: 1593 characters\nmin: 18 charactersmean: 353.83 charactersmax: 1374 characters\nmin: 13 charactersmean: 328.01 charactersmax: 1755 characters\nmin: 12 charactersmean: 337.74 charactersmax: 1579 characters\nmin: 16 charactersmean: 336.94 charactersmax: 1325 characters\nmin: 15 charactersmean: 319.49 charactersmax: 1410 characters\nmin: 12 charactersmean: 340.91 charactersmax: 1680 characters\nmin: 20 charactersmean: 330.34 charactersmax: 1509 characters\nSamples:\nanchor\npositive\nnegative_1\nnegative_2\nnegative_3\nnegative_4\nnegative_5\nnegative_6\nnegative_7\nnegative_8\nnegative_9\nnegative_10\nnegative_11\nnegative_12\nnegative_13\nnegative_14\nnegative_15\nnegative_16\nnegative_17\nnegative_18\nnegative_19\nnegative_20\nVisceral adipose tissue area measurement at a single level: can it represent visceral adipose tissue volume?\nMeasurement of visceral adipose tissue (VAT) needs to be accurate and sensitive to change for risk monitoring. The purpose of this study is to determine the CT slice location where VAT area can best reflect changes in VAT volume and body weight.\n46 patients with psoriasis and 46 sex- and age-matched control patients were included in this study. The abdominal fat area [visceral fat area (VFA), subcutaneous fat area (SFA) and total fat area (TFA)] at the level of the umbilicus was evaluated by computed tomography.\nA retrospective review of CRC patients who received adjuvant chemotherapy at a single center during the period 2006-2009 identified from a prospectively maintained database. Visceral adiposity was determined by measuring visceral fat area (VFA) on preoperative staging CT. All patients were followed up to study completion or death.\nA total of 1941 participants without known cardiovascular disease were enrolled from the Korean Genome and Epidemiology Study. Visceral fat area (VFA) was assessed by computed tomography. Appendicular skeletal muscle mass (ASM) was estimated by dual-energy X-ray absorptiometry and was used as a percentage of body weight (ASM/Wt). LV structure and function were assessed by tissue Doppler imaging (TDI) echocardiography.\nOne hundred and forty nonobese patients (BMI <25 kg/m2) were enrolled. EFV and visceral fat area were measured by MDCT. Patients were classified according to the plaque components (noncalcified, mixed and calcified) and severity of CAD. Inflammatory biomarkers were also measured, and compared with each CT parameter.\nThe blood gas level in each pulmonary vein (PV) was measured in supine subjects with diverse body mass index (BMI) values, to determine whether there was a regional insufficiency in gas exchange depending on the subject's BMI.\nMagnetic resonance imaging (MRI) of 163 patients with cholecystolithiasis and 163 non-cholecystolithiasis control subjects admitted to our institution between March 2011 and September 2013 were included in this cross-sectional evaluation. There were 98 women and 65 men in cholecystolithiasis group with an average age of 57¬±16 years (range 25-86 years). There were 87 women and 76 men in the control group with an average age of 41¬±16 years (range 14-77 years). Visceral adipose tissue (VAT), abdominal subcutaneous adipose tissue (SAT) and total abdominal adipose tissue (TAT) of all the subjects at navel level were measured on abdominal MRI. According to the visceral adipose area (cut-off point VAT‚Ää=‚Ää100 cm2), study subjects were divided into 1) increased accumulation of intra-abdominal fat and 2) normal distribution of intra-abdominal fat. Logistic regression was used to assess the association of fat with the presence of cholecystolithiasis, adjusted for age and sex.\nWe reviewed the medical records of 106 patients undergoing LA or LESS-A at our institution. Total fat area (TFA) and visceral fat area (VFA) were measured at the level of the L4 vertebra by computed tomography. To categorize the type of obesity, the VFA/TFA ratio was calculated. Multiple logistic regression analyses were performed to identify independent predictors of prolonged operative time.\nThe weight gain was 5.2% greater in rats exposed to fructose than in controls (P = 0.042). Total and visceral adipose tissue volumes were 5.2 cm3 (P = 0.017) and 3.1 cm3 (P = 0.019) greater, respectively, while lean tissue volumes did not differ. The level of triglycerides and apolipoprotein A-I was higher (P = 0.034, P = 0.005, respectively) in fructose-exposed rats.\nA total of 1593 middle-aged to older patients participated in this cross-sectional study. Brachial-to-ankle pulse wave velocity (baPWV) was measured as an index of arterial stiffness. Second PP (PP2) at the second peak of radial SBP was used to estimate central PP. Radial augmentation index was calculated as PP2/PP. Thigh muscle cross-sectional area and abdominal visceral fat area were quantified by computed tomography. Patients were classified as sarcopenic if their hand grip strength or skeletal muscle mass (measured by bioelectrical impedance) was more than 1 SD lower than the mean of those in a reference group aged below 50 years, or in the lowest 20% of the studied population. Visceral obesity was defined as visceral fat area greater than 100‚Ääcm.\nVisceral adiposity is linked with sleep-disordered breathing (SDB) (called Syndrome Z), and both correlate with coronary artery disease (CAD). The aim of the present study was to determine the significance of excess visceral fat, SDB and circulating levels of biomarkers in CAD in Japanese men.\nThere are no published studies on the impact of visceral adipose tissue (VAT) change on outcomes of restorative proctocolectomy and ileal pouch-anal anastomosis (IPAA). The aim of this historic cohort study was to evaluate the impact of excessive VAT gain on the outcomes of inflammatory bowel disease (IBD) patients with IPAA.\nA subgroup of 46 men (n = 20, aged 29.1-33.4 years) and women (n = 26, aged 29.1-33.8 years) were recruited from an ongoing population study at our institution. Anthropometric variables including weight, height, and waist circumference were measured using standard procedures, and body mass index was calculated (kg/m(2)). Visceral adipose tissue (VAT) was measured with magnetic resonance imaging. Plasma apolipoproteins, lipids, glucose, and insulin were measured after an overnight fasting.\nOur aim was to describe adipose tissue content and distribution in ALS patients.\nThis was a cross-sectional study of 140 Japanese patients with type 2 diabetes (mean age 65 ¬± 11 year; 44.6% women). Visceral fat area (VFA; cm(2) ) and liver attenuation index (LAI) were assessed by abdominal computed tomography. The patients were divided into four groups by VFA and body mass index (BMI; kg/m(2) ) as follows: BMI <25 kg/m(2) and VFA <100 cm(2) (OB[-]VA[-]), BMI ‚â•25 kg/m(2) and VFA <100 cm(2) (OB[+]VA[-]), BMI <25 kg/m(2) and VFA ‚â•100 cm(2) (OB[-]VA[+]), and BMI ‚â•25 kg/m(2) and VFA ‚â•100 cm(2) (OB[+]VA[+]). Multivariate linear regression and logistic regression analysis were carried out to determine the impact of OB(-)VA(+) on LAI.\nEpicardial adipose tissue represents visceral adiposity, the early detection of which could be helpful for assessing subclinical target organ damage. Although previous studies have reported a relationship between epicardial fat thickness (EFT) and carotid intima-media thickness, there have been no studies detailing the relationship between EFT and brachial-ankle pulse wave velocity (baPWV).\nHigher adiponectin levels were associated with lower risk of diabetes (P < 0.001). Visceral fat was the only adiposity measure associated with diabetes after adjusting for BMI (odds ratio 3.0 [2.1-4.3] in women and 1.3 [1.0-1.6] in men, P < 0.001 between-sex comparison). Adipocytokines attenuated the association between visceral fat and diabetes for both sexes but more strongly in men (women 2.3 [1.5-3.3], men 1.1 [0.9-1.4]). In men, adiponectin, IL-6, and PAI-1 remained independently associated with diabetes after adjusting for fat depots; in women, adiponectin was the only independently associated adipocytokine. Controlling for insulin, HDL, triglycerides, and blood pressure did not change these results.\nNinety obesity patients and 95 non-obesity Uygur individuals were enrolled in this study. CD68 levels in abdominal subcutaneous and omental adipose tissues were detected by immunohistochemistry. The cytokine expression levels of adiponectin (APMI) and visfatin in serum were measured by enzyme-linked immunosorbent assay. Infection of 3T3-L1 cells with Ad36 was performed. Real-time PCR was performed to determine expression levels of APMI and Visfatin genes in the 3T3-L1 preadipocytes infected with Ad36.\nSerum CEA levels correlated with visceral fat area, fasting glucose, and triglyceride levels after adjusting for age and BMI. The mean visceral fat area increased significantly with the increasing CEA tirtiles. In a step-wise multiple regression analysis, age (Œ≤‚Ää=‚Ää0.26, p<0.01) and visceral fat area (Œ≤‚Ää=‚Ää0.19, p‚Ää=‚Ää0.03) were identified as explanatory variables for serum CEA level.\nThe visceral adiposity index (VAI) has proved to be a marker of visceral adipose dysfunction, strongly associated with insulin sensitivity in both the general and specific populations of patients at metabolic risk.\nVisceral adiposity is associated with hepatic steatosis, inflammation, and fibrosis in non-alcoholic fatty liver disease (NAFLD). The visceral adiposity index (VAI), a novel marker of visceral fat distribution and dysfunction, has been correlated with histology in hepatitis C. We assessed the ability of VAI to predict disease severity in NAFLD and hence its role as a non-invasive marker of liver damage.\nDo general practitioner hospitals reduce the utilisation of general hospital beds?\nObservational study comparing the total rates of admissions and of occupied bed days in general hospitals between populations with and without access to GP hospitals. Comparisons were also made separately for diagnoses commonly encountered in GP hospitals.\nTo study the prevalence of GERD comorbidities in a tertiary care hospital.\nThe population studied was a sample of 10% of the patients 65 years or older registered with a general practitioner contributing to the General Practice Research Database between 1988 and 1996.\nTertiary University Hospitals.\nUniversity hospital and district general hospital.\nInpatient rehabilitation facilities.\nInpatient rehabilitation facilities.\nGeneral medical service at a teaching hospital.\nTo examine the hypothesis that nursing homes responding to these changes in demand shifted the balance of resources from hotel to clinical activities.\nOutpatient practices of general practitioners in the United Kingdom who contribute to the General Practice Research Database.\nHospital rehabilitation programs.\nSurgical department of a large district general hospital.\nHospital-based case-control study.\nPatients' homes.\nthe relationship between proximity to death and the amount of care provided by general practitioners (GPs) is largely unknown.\nPublic hospital, primary care clinic.\nA teaching hospital and a district general hospital.\nDistrict General Hospital in the UK.\nAcademic general internal medicine practice.\nA hospital-based study was conducted.\nTo evaluate the impact of participation in a trial on General Practitioners management and patient behaviour.\n\"Occult\" posttraumatic lesions of the knee: can magnetic resonance substitute for diagnostic arthroscopy?\nWe identified three types of occult post-traumatic injuries by morpho-topographic and signal intensity patterns: bone bruises (no. 25), subchondral (no. 33) and osteochondral (no. 35) injuries. Arthroscopy depicted 45 osteochondral and 19 chondral injuries. A bone bruise was defined as a typical subcortical area of signal loss, with various shapes, on T1-weighted images and of increased signal intensity on T2-weighted and FIR images. The cortical bone and articular cartilage were normal in all cases, while osteochondral injuries exhibited associated bone and cartilage damage with the same abnormal MR signal intensity. Sprain was the mechanism of injury in 52 cases, bruise in 12 and stress in 6. In 52 sprains (30 in valgus), the injury site was the lateral compartment in 92.3% of cases (100% in valgus), associated with meniscal damage in 73% of cases (90% in valgus) and with ligament injury in 90.4% (100% in valgus). In 12 bruises, the injury site was the lateral compartment in 58.3% of...\nPreoperative range of motion (ROM) has been regarded as one of the most important factors in predicting postoperative ROM following total knee arthroplasty (TKA). Mobile-bearing TKA designs have been suggested to possibly improve the knee kinematics compared to fixed-bearing designs. The purpose of this study was to examine the difference in postoperative flexion as a function of preoperative flexion in a consecutive series of TKAs done using a posterior-stabilized rotating-platform prosthesis.\nTo assess the association of underlying diagnosis with outcomes after revision total knee arthroplasty (TKA).\nWe identified 37 knees diagnosed with osteoarthritis with a preoperative knee flexion ‚â•120¬∞ but a 12-month postoperative range of motion (ROM) ‚â§110¬∞. A random sample of 111 patients (1:3) from the same database, whose knees had a preoperative and 12-month postoperative ROM ‚â•120¬∞, based on a diagnosis of primary osteoarthritis and no previous open knee surgery, were selected as the controls.\nThis study reports a series of patients operated on by anterior cruciate ligament (ACL) reconstruction combined with valgus high tibial osteotomy (HTO) for chronic anterior knee instability associated with medial tibiofemoral osteoarthritis. It was hypothesized that the combined surgery would enable return to sport, stabilize the knee and relieve medial pain.\nTo determine the prevalence and factors associated with knee osteoarthritis (OA) defined by magnetic resonance imaging (MRI) and specific OA features on MRI 1 year after anterior cruciate ligament reconstruction (ACLR).\nIn anterior ankle arthroscopy, the anterior working area (AWA) is restricted by the presence of the dorsalis pedis artery (DPA) and tendons. Pseudoaneurysms caused by iatrogenic damage to the DPA are difficult to identify intraoperatively. In knee arthroscopy, risk of popliteal artery damage is reduced in the flexed position [1]. This study investigates how DPA movement is affected by dorsiflexion and plantarflexion with the aim of identifying the positions providing the greatest AWA.\nTo investigate whether sex affects the trajectory of functional recovery after total knee arthroplasty (TKA).\nTo determine whether magnetic resonance imaging (MRI) evidence of tendinopathy in early rheumatoid arthritis (RA) could be used to predict the course of tendon involvement in later disease and specifically the risk of tendon rupture.\nWith the advent of MRI (Magnetic Resonance Imaging), Synovial lesions around knee are being more and more easily detected. Synovial lesions of knee present with boggy swelling, effusion, pain, and restriction of motion. Differential diagnoses of such lesions include pigmented villonodular synovitis, synovial lipoma, synovial chondromatosis, rheumatoid arthritis, synovial hemangioma, amyloid arthropathy, xanthomata and lipoma arborescens. CT and MRI often help in diagnosis of such lesions. MRI of Lipoma Arborescens has been regarded to have characteristic diagnostic appearance - it includes a synovial mass with frond-like architecture and fat signal intensity on all pulse sequences. Sometimes Lipoma Arborescens can present in conjunction with inflammatory arthritis. Synovectomy is often curative for such conditions.\nJoint trauma can lead to a spectrum of acute lesions, including cartilage degradation, ligament or meniscus tears, and synovitis, all potentially associated with osteoarthritis (OA). This study was undertaken to generate and validate a murine model of knee joint trauma following noninvasive controlled injurious compression in vivo.\nTo determine which subregions of the knee joint have a high prevalence of pre-radiographic osteoarthritic changes, i.e., cartilage damage and osteophytes that can only be detected by magnetic resonance imaging (MRI), in radiographically normal knees.\n78 of initially 84 patients (80 of 86 knees) were clinically and radiographically reassessed 5 (5.1-5.9) years after conventional, image-based, and image-free total knee arthroplasty. The methodology was identical to that used preoperatively and at 2 years, including the Knee Society score (KSS) and the functional score (FS), and AP and true lateral standard radiographs.\nIn adult patients with trauma, an increase in the thickness of the retropharyngeal soft tissues is commonly used as a potential indicator of occult injury, but no studies have examined this parameter using computed tomography (CT) as a screening modality.\nTo evaluate the thickness of cartilage at the posterior aspect of the medial and lateral condyle in Osteoarthritis (OA) knees compared to non-OA knees using computed tomography arthrography (CTA).\nTotal knee arthroplasty (TKA) successfully alleviates pain from knee osteoarthritis; but deficits in function can persist long term. Despite these well-known deficits, there is little evidence supporting the use of rehabilitation interventions following TKA.\nThe anterior intermeniscal ligament of the knee is at risk during knee arthroscopy, anterior cruciate ligament reconstruction, and tibial nail insertion.\nIt has been previously demonstrated that radiographic severity of arthritis predicts outcome following knee replacement. In certain circumstances, patients may undergo arthroplasty without severe radiographic disease. An example may be the patient with significant chondral damage unsuccessfully treated with arthroscopy. This patient may proceed to joint replacement when their radiographs would not normally merit such intervention. We investigated whether these findings were also applicable to total ankle replacements (TARs).\nThere is an increasing body of evidence that magnetic resonance imaging-occult tissue damage is an important component of primary progressive multiple sclerosis (PPMS) pathology. Proton magnetic resonance spectroscopy (1H-MRS) can be used to measure in vivo whole-brain N-acetylaspartate (WBNAA) concentrations, the decrease of whose levels is considered a marker of neuronal-axonal injury.\nMeniscectomy and articular cartilage damage have been found to increase the prevalence of osteoarthritis after anterior cruciate ligament reconstruction, but the effect of knee range of motion has not been extensively studied.\nPatients 50 years and older with knee osteoarthritis who underwent arthroscopy between 1998 and 2010 were retrospectively identified and an annual arthroscopy rate was calculated from 1998 through 2002 and from 2006 through 2010. Patients who underwent knee arthroplasty within 2 years of arthroscopy during each period were identified, and a 2-year conversion to arthroplasty rate was calculated.\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nmiracl\nDataset: miracl at 07e2b62\nSize: 789,900 training samples\nColumns: anchor, positive, and negative\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 11 charactersmean: 39.23 charactersmax: 129 characters\nmin: 72 charactersmean: 745.86 charactersmax: 4292 characters\nmin: 18 charactersmean: 649.52 charactersmax: 3570 characters\nSamples:\nanchor\npositive\nnegative\nWho created The Walking Dead comic books?\nDays Gone Bye (The Walking Dead)Robert Kirkman, the creator of the eponymous series of comic books, considered the idea of creating a television show based on the comic series, but did not move forward. Frank Darabont expressed interest in developing the series for television. In January 2010, AMC formally announced that it had ordered a pilot for a possible series adapted from \"The Walking Dead\" comic book. In the announcement, the executives stated that Darabont would serve as writer, director, and an executive producer alongside Gale Anne Hurd.\nLiving DeadThe Walking Dead\nWhen was the first car invented?\nCarIn 1879, Benz was granted a patent for his first engine, which had been designed in 1878. Many of his other inventions made the use of the internal combustion engine feasible for powering a vehicle. His first \"Motorwagen\" was built in 1885 in Mannheim, Germany. He was awarded the patent for its invention as of his application on 29 January 1886 (under the auspices of his major company, Benz & Cie., which was founded in 1883). Benz began promotion of the vehicle on 3 July 1886, and about 25 Benz vehicles were sold between 1888 and 1893, when his first four-wheeler was introduced along with a model intended for affordability. They also were powered with four-stroke engines of his own design. Emile Roger of France, already producing Benz engines under license, now added the Benz car to his line of products. Because France was more open to the early cars, initially more were built and sold in France through Roger than Benz sold in Germany. In August 1888 Bertha Benz, the wife of Karl B...\nElwood HaynesIn 1905, three years after the Apperson brothers split from Haynes, Haynes-Apperson was renamed the Haynes Automobile Company and Haynes launched a series of publicity campaigns. A parade of 2,000 cars was organized in New York City during 1908 and Haynes, whom many recognized as the inventor of the American automobile, led the parade down Broadway riding in the \"Pioneer\". He was followed by ten Haynes cars, a model from each year to display the advancement in technology. On his way to the parade, Haynes was unaware of the city's newly established speeding laws and was arrested for driving too fast‚Äîin a car with a top speed of 15¬†mph (17¬†km/h)‚Äîand taken to jail. He was soon able to see a magistrate who released him after learning that he was Elwood Haynes and had come to lead the parade. The celebration was intended to be a ten-year commemoration of the invention of the automobile, although earlier self-vehicles dated back nearly twenty years in Europe. Haynes donated the...\nHow many doctors are in Doctor Who?\nThe Doctor (Doctor Who)The Doctor is the title character in the long-running BBC science fiction television programme \"Doctor Who\". Since the show's inception in 1963, the character has been portrayed by thirteen lead actors. In the programme, \"the Doctor\" is the alias assumed by a centuries-old alien‚Äîa Time Lord from the planet Gallifrey‚Äîwho travels through space and time in the TARDIS, frequently with companions. The transition to each succeeding actor is explained within the show's narrative through the plot device of \"regeneration\", a biological function of the Time Lord race that allows a change of cellular structure and appearance with recovery following a potentially fatal injury.\nSixth DoctorThe Sixth Doctor is an incarnation of the Doctor, the protagonist of the BBC science fiction television series \"Doctor Who\". He is portrayed by Colin Baker. Although his televisual time on the series was comparatively brief and turbulent, Baker has continued as the Sixth Doctor in Big Finish's range of original \"Doctor Who\" audio adventures. Within the series' narrative, the Doctor is a centuries-old Time Lord alien from the planet Gallifrey who travels in time and space in his TARDIS, frequently with companions. At the end of life, the Doctor can regenerate his body; in doing so, his physical appearance and personality change. Baker portrays the sixth such incarnation, an arrogant, flamboyant character in brightly coloured, mismatched clothes whose brash, often patronising personality set him apart from all his previous incarnations.\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nmldr\nDataset: mldr at 40ad767\nSize: 200,000 training samples\nColumns: anchor, positive, and negative\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 17 charactersmean: 65.31 charactersmax: 210 characters\nmin: 2432 charactersmean: 20354.29 charactersmax: 123500 characters\nmin: 3035 charactersmean: 16236.77 charactersmax: 166364 characters\nSamples:\nanchor\npositive\nnegative\nWhen was the art museum in Santa Barbara first opened to the public?\nThe Santa Barbara Museum of Art (SBMA) is an art museum located in downtown Santa Barbara, California.Founded in 1941, it is home to both permanent and special collections, the former of which includes Asian, American, and European art that spans 4,000 years from ancient to modern.HistoryThe Santa Barbara Museum of Art opened to the public on June 5, 1941, in a building that was at one time the Santa Barbara Post Office (1914‚Äì1932). The idea for an art museum first came from the local artist Colin Campbell Cooper when he learned that the post office was going to be sold. In a letter to the editor published in the Santa Barbara News-Press in July 1937, Cooper proposed that the impressive Italianate structure should be transformed into a museum. After gaining momentum in town and with the support of local businesses, politicians and art collectors the Santa Barbara Museum of Art was officially established just four years after Cooper's letter was published. The renowned Chicago arc...\nThe Knott's Berry Farm amusement park in Orange County, California, originated from a berry farm owned by Walter Knott (1889‚Äì1981). In the 1920s, Knott and his wife, Cordelia, sold berries, berry preserves and pies from a roadside stand beside State Route 39, near the small town of Buena Park.In 1932, on a visit to Rudolph Boysen's farm in nearby Anaheim, Walter Knott was introduced to a new hybrid berry of a blackberry, a red raspberry, and a loganberry cross-bred by Boysen, who gave Walter his last six wilted berry-hybrid plants. Walter planted and cultivated them, then the family sold the berries at their roadside stand. When people asked what kind they were, he called them \"boysenberries\".In 1934, to make ends meet, Knott's wife Cordelia (1890‚Äì1974) reluctantly began serving fried chicken dinners on their wedding china. For dessert, Knott's signature Boysenberry Pie was also served to guests dining in the small tea room. As Southern California developed, Highway 39 became the ma...\nWhat is the objective of Op√©ration Chammal?\nOp√©ration Chammal is a French military operation in Iraq and Syria in an attempt to contain the expansion of the Islamic State of Iraq and the Levant and to support the Iraqi Army. Its name comes from the Shamal (Chammal in French), a northwesterly wind that blows over Iraq and the Persian Gulf states.Airstrikes over Iraq started 19 September 2014 and airstrikes over Syria started by the end of September 2015. The French operation is limited to airstrikes; French president Fran√ßois Hollande has reiterated that no ground troops would be used in the conflict. Additionally, the French frigate  has joined the United States Navy's Commander Task Force 50 (CTF 50) as an escort.On 14 November 2015, ISIL claimed that the attacks that took place in Paris the previous day were retaliation for Op√©ration Chammal. In response, French forces increased their attacks against ISIL in Syria.Background On 10 June 2014, the terrorist group of the Islamic State of Iraq and the Levant and several ot...\nCMA CGM S.A. is a French container transportation and shipping company. It is the world‚Äôs 3rd largest container shipping company, using 257 shipping routes between 420 ports in 160 different countries. Its headquarters are in Marseille, France and its North American headquarters are in Norfolk, Virginia, United States.The name is an acronym of two predecessor companies, Compagnie Maritime d'Affr√®tement (CMA) and Compagnie G√©n√©rale Maritime (CGM), which translate as \"Maritime Freighting Company\" and \"General Maritime Company\".HistoryThe history of CMA CGM can be traced back to the middle of the 19th century, when two major French shipping lines were created, respectively Messageries Maritimes (MM) in 1851 and Compagnie G√©n√©rale Maritime (CGM) in 1855, soon renamed Compagnie G√©n√©rale Transatlantique in 1861. Both companies were created partly with the backing of the French State, through the award of mail contracts to various destinations, French colonies and overseas territories a...\nWhat was the reason for Williams-Franklin's decision to become a vegan during her time as a player?\nTaj McWilliams-Franklin (born October 20, 1970) is a former American professional women's basketball player.A two-time WNBA champion with the Detroit Shock and Minnesota Lynx and six-time all-star, McWilliams-Franklin's professional career has spanned three decades, and began before the WNBA was founded. She retired from the WNBA after the 2012 season.College yearsAfter attending T. W. Josey High School in Augusta, Georgia, McWilliams-Franklin attended Georgia State University in 1989 and played on the school's basketball team for one season. However, she had become pregnant during her senior year in high school, and after the coach who recruited her to Georgia State was let go, the incoming staff told her \"school was no place for kids.\" McWilliams-Franklin moved to Austin, Texas, where a friend connected her with St. Edward's University coach Dave McKey. She enrolled at St. Edwards as a Rhetoric major.While at St. Edward's, she set school records and individual achievements, in...\nSpider-Woman (Gwendolyne Maxine Stacy; colloquial: \"Spider-Gwen\" or \"Ghost-Spider\") is a superhero appearing in American comic books published by Marvel Comics. She was created by Jason Latour and Robbi Rodriguez. The character debuted in Edge of Spider-Verse issue #2 as part of the 2014‚Äì15 \"Spider-Verse\" comic book storyline, leading to the ongoing series Spider-Gwen that began in 2015.Spider-Woman is a variant of Spider-Man and an alternate-universe version of Gwen Stacy. She lives on Earth-65, where Gwen Stacy is bitten by a radioactive spider and becomes a superhero instead of Peter Parker becoming Spider-Man. The character's various enemies include Earth-65 versions of Matt Murdock and Frank Castle. Gwen Stacy's Spider-Woman harbors much of Peter's personality and conflicts along with his powers and abilities.Spider-Woman was met with positive reviews from critics, with them applauding her design‚Äîcited as a popular choice for cosplay‚Äîand a feminist perspective. For promotion, ...\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nmr_tydi\nDataset: mr_tydi at abbdf55\nSize: 354,700 training samples\nColumns: anchor, positive, and negative\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 12 charactersmean: 38.85 charactersmax: 95 characters\nmin: 64 charactersmean: 645.85 charactersmax: 4067 characters\nmin: 21 charactersmean: 626.85 charactersmax: 2870 characters\nSamples:\nanchor\npositive\nnegative\nIs amnesia real?\nAmnesiaAmnesia is a deficit in memory caused by brain damage, disease, or psychological trauma.[1] Amnesia can also be caused temporarily by the use of various sedatives and hypnotic drugs. The memory can be either wholly or partially lost due to the extent of damage that was caused.[2] There are two main types of amnesia: retrograde amnesia and anterograde amnesia. Retrograde amnesia is the inability to retrieve information that was acquired before a particular date, usually the date of an accident or operation.[3] In some cases the memory loss can extend back decades, while in others the person may lose only a few months of memory. Anterograde amnesia is the inability to transfer new information from the short-term store into the long-term store. People with this type of amnesia cannot remember things for long periods of time. These two types are not mutually exclusive; both can occur simultaneously.\nAmnesiaHead trauma is a very broad range as it deals with any kind of injury or active action toward the brain which might cause amnesia. Retrograde and anterograde amnesia is more often seen from events like this, an exact example of a cause of the two would be electroconvulsive therapy, which would cause both briefly for the receiving patient. Traumatic events are more subjective. What is traumatic is dependent on what the person finds to be traumatic. Regardless, a traumatic event is an event where something so distressing occurs that the mind chooses to forget rather than deal with the stress. A common example of amnesia that is caused by traumatic events is dissociative amnesia, which occurs when the person forgets an event that has deeply disturbed them.[8] An example would be a person forgetting a fatal and graphic car accident involving their loved ones. Physical deficiencies are different from head trauma because physical deficiencies lean more toward passive physical issues.\nWhat is the largest naval base in the world?\nNaval Station NorfolkNaval Station Norfolk, is a United States Navy base in Norfolk, Virginia. It supports naval forces in the United States Fleet Forces Command,[1] those operating in the Atlantic Ocean, Mediterranean Sea, and the Indian Ocean. The installation occupies about 4 miles (6.4km) of waterfront space and 11 miles (18km) of pier and wharf space of the Hampton Roads peninsula known as Sewell's Point. It is the world's largest naval station, with the largest concentration of U.S. Navy forces through 75 ships alongside 14 piers and with 134 aircraft and 11 aircraft hangars at the adjacently operated Chambers Field and [2] Port Services controls more than 3,100 ships' movements annually as they arrive and depart their berths.\nClark Air BaseClark Air Base was arguably the most urbanized military facility in history and was the largest American base overseas. At its peak around 1990, it had a permanent population of 15,000. It had a base exchange, a large commissary, a small shopping arcade, a branch department store, cafeterias, teen centers, a hotel, miniature golf, riding stables, zoo, and other concessions.\nWhat is the power of the Red Lantern?\nRed Lantern CorpsIn Final Crisis: Rage of the Red Lanterns, Atrocitus is shown in a flashback as having apparently formed a central power battery by using the blood of the other Inversions in blood magic rituals. The battery stands before a great lake of blood from which he forms his red power ring (crystallized by his anger), as well as other rings and batteries used to form the Red Lantern Corps. Harnessing the red light of rage, he sends his rings out into the universe; however, upon accepting the rings, his recruits' hearts are rendered useless. Their blood spoils from within, forcing them to expel the violently flammable and corrosive material from their mouths. Additionally, the Red Lanterns are reduced to an almost animalistic state, with only Atrocitus appearing to be in full control of himself. Once Atrocitus assembles a sufficient force, he leads them on a mission to capture Sinestro (who is being transferred to Korugar for his execution). Coincidentally, the Sinestro Corps ...\nGreen Lantern in other mediaJohn Stewart is a member of the Justice League in the \"Justice League\" animated series. In this series, Stewart's ring was initially constrained to permitting him to fly, generating a protective force field, creating walls, and firing energy blasts; this limitation was established as being due to Stewart's mindset, not an inherent limitation of the ring itself (the series' version of Stewart is a former U.S. Marine, not an architect). After being berated by Katma Tui for his unimaginative use of the ring, Stewart has learned to generate complex tools (to defuse a bomb in one instance) and weapons. (He was also shown to be more creative when transformed into a child in the episode \"Kids Stuff\".) In a development not seen in any other version of the Green Lantern mythos, Stewart's eyes glow green when wearing his charged power ring. The glow fades when the ring runs out of power. The series has been inconsistent about the ring's effectiveness against yellow; ...\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nEvaluation Datasets\ngooaq\nDataset: gooaq at b089f72\nSize: 3,012,496 evaluation samples\nColumns: question and answer\nApproximate statistics based on the first 1000 samples:\nquestion\nanswer\ntype\nstring\nstring\ndetails\nmin: 18 charactersmean: 43.17 charactersmax: 98 characters\nmin: 51 charactersmean: 254.12 charactersmax: 360 characters\nSamples:\nquestion\nanswer\nhow do i program my directv remote with my tv?\n['Press MENU on your remote.', 'Select Settings & Help > Settings > Remote Control > Program Remote.', 'Choose the device (TV, audio, DVD) you wish to program. ... ', 'Follow the on-screen prompts to complete programming.']\nare rodrigues fruit bats nocturnal?\nBefore its numbers were threatened by habitat destruction, storms, and hunting, some of those groups could number 500 or more members. Sunrise, sunset. Rodrigues fruit bats are most active at dawn, at dusk, and at night.\nwhy does your heart rate increase during exercise bbc bitesize?\nDuring exercise there is an increase in physical activity and muscle cells respire more than they do when the body is at rest. The heart rate increases during exercise. The rate and depth of breathing increases - this makes sure that more oxygen is absorbed into the blood, and more carbon dioxide is removed from it.\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nmsmarco\nDataset: msmarco at 84ed2d3\nSize: 502,939 evaluation samples\nColumns: query, positive, and negative\nApproximate statistics based on the first 1000 samples:\nquery\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 10 charactersmean: 33.36 charactersmax: 137 characters\nmin: 67 charactersmean: 347.87 charactersmax: 906 characters\nmin: 57 charactersmean: 318.18 charactersmax: 906 characters\nSamples:\nquery\npositive\nnegative\nis cabinet refacing worth the cost?\nFans of refacing say this mini-makeover can give a kitchen a whole new look at a much lower cost than installing all-new cabinets. Cabinet refacing can save up to 50 percent compared to the cost of replacing, says Cheryl Catalano, owner of Kitchen Solvers, a cabinet refacing franchise in Napierville, Illinois. From.\nMost cabinet refacing projects cost about $4,000 to $10,000. The price varies based on the materials you select and the size and configuration of your kitchen. Wood veneer doors, for example, will cost less than solid wood doors.\nis the fovea ethmoidalis a bone\nEthmoid bone/fovea ethmoidalis. The medial portion of the ethmoid bone is a cruciate membranous bone composed of the crista galli, cribriform plate, and perpendicular ethmoidal plate. The crista is a thick piece of bone, shaped like a √¢¬Ä¬úcock's comb,√¢¬Ä¬ù that projects intracranially and attaches to the falx cerebri.\nEthmoid bone/fovea ethmoidalis. The medial portion of the ethmoid bone is a cruciate membranous bone composed of the crista galli, cribriform plate, and perpendicular ethmoidal plate. The crista is a thick piece of bone, shaped like a √¢¬Ä¬úcock's comb,√¢¬Ä¬ù that projects intracranially and attaches to the falx cerebri.\naverage pitches per inning\nThe likelihood of a pitcher completing nine innings if he throws an average of 14 pitches or less per inning is reinforced by the totals of the 89 games in which pitchers did actually complete nine innings of work.\nThe likelihood of a pitcher completing nine innings if he throws an average of 14 pitches or less per inning is reinforced by the totals of the 89 games in which pitchers did actually complete nine innings of work.\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nsquad\nDataset: squad at d84c8c2\nSize: 87,599 evaluation samples\nColumns: question and answer\nApproximate statistics based on the first 1000 samples:\nquestion\nanswer\ntype\nstring\nstring\ndetails\nmin: 1 charactersmean: 60.25 charactersmax: 161 characters\nmin: 152 charactersmean: 761.88 charactersmax: 2525 characters\nSamples:\nquestion\nanswer\nWhen did the Russian Empire begin to question the existence of the Ottoman Empire?\nIn 1853 the Russian Empire on behalf of the Slavic Balkan states began to question the very existence of the Ottoman Empire. The result was the Crimean War, 1853‚Äì1856, in which the British Empire and the French Empire supported the Ottoman Empire in its struggle against the incursions of the Russian Empire. Eventually, the Ottoman Empire lost control of the Balkan region.\nHow would one describe the control of universities before nation-states in the 17th century?\nThe propagation of universities was not necessarily a steady progression, as the 17th century was rife with events that adversely affected university expansion. Many wars, and especially the Thirty Years' War, disrupted the university landscape throughout Europe at different times. War, plague, famine, regicide, and changes in religious power and structure often adversely affected the societies that provided support for universities. Internal strife within the universities themselves, such as student brawling and absentee professors, acted to destabilize these institutions as well. Universities were also reluctant to give up older curricula, and the continued reliance on the works of Aristotle defied contemporary advancements in science and the arts. This era was also affected by the rise of the nation-state. As universities increasingly came under state control, or formed under the auspices of the state, the faculty governance model (begun by the University of Paris) became more and m...\nWhen did Jewish law recognize copyright?\nThe concept's origins can potentially be traced back further. Jewish law includes several considerations whose effects are similar to those of modern intellectual property laws, though the notion of intellectual creations as property does not seem to exist ‚Äì notably the principle of Hasagat Ge'vul (unfair encroachment) was used to justify limited-term publisher (but not author) copyright in the 16th century. In 500 BCE, the government of the Greek state of Sybaris offered one year's patent \"to all who should discover any new refinement in luxury\".\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\ns2orc\nDataset: s2orc at 8cfc394\nSize: 10,000 evaluation samples\nColumns: title and abstract\nApproximate statistics based on the first 1000 samples:\ntitle\nabstract\ntype\nstring\nstring\ndetails\nmin: 31 charactersmean: 80.04 charactersmax: 198 characters\nmin: 96 charactersmean: 653.93 charactersmax: 1023 characters\nSamples:\ntitle\nabstract\nScreen Printing Ink Film Thickness Analysis of the Passive RFID Tag Antenna\nThe relationship between the screen mesh and the theoretical and practical ink film thickness was analyzed based on the main influencing factors of the ink film thickness by screen printing.A calculation model for the ink thickness was established based on the screen under static and compressive deformation.The relation curve between the screen mesh and the ink film thickness was fitted and the suitable printing craft parameter was chosen to print two kinds of RFID tag antennas.The fluctuation of the antenna resistance was analyzed to demonstrate the reliability of the passive RFID tag antenna manufactured by screen printing technology.\nSubclinical organ damage and cardiovascular risk prediction\nAbstractTraditional cardiovascular risk factors have poor prognostic value for individuals and screening for subclinical organ damage has been recommended in hypertension in recent guidelines. The aim of this review was to investigate the clinical impact of the additive prognostic information provided by measuring subclinical organ damage. We have (i) reviewed recent studies linking markers of subclinical organ damage in the heart, blood vessels and kidney to cardiovascular risk; (ii) discussed the evidence for improvement in cardiovascular risk prediction using markers of subclinical organ damage; (iii) investigated which and how many markers to measure and (iv) finally discussed whether measuring subclinical organ damage provided benefits beyond risk prediction. In conclusion, more studies and if possible randomized studies are needed to investigate (i) the importance of markers of subclinical organ damage for risk discrimination, calibration and reclassification; and (ii) the econom...\nA Novel Approach to Simulate Climate Change Impacts on Vascular Epiphytes: Case Study in Taiwan\nIn the wet tropics, epiphytes form a conspicuous layer in the forest canopy, support abundant coexisting biota, and are known to have a critical influence on forest hydrology and nutrient cycling. Since canopy-dwelling plants have no vascular connection to the ground or their host plants, they are likely more sensitive to environmental changes than their soil-rooted counterparts, subsequently regarded as one of the groups most vulnerable to global climate change. Epiphytes have adapted to life in highly dynamic forest canopies by producing many, mostly wind-dispersed, seeds or spores. Consequently, epiphytes should colonize trees rapidly, which, in addition to atmospheric sensitivity and short life cycles, make epiphytes suitable climate change indicators. In this study, we assess the impact of climate change on Taiwanese epiphytes using a modeling approach.\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nallnli\nDataset: allnli at d482672\nSize: 6,584 evaluation samples\nColumns: anchor, positive, and negative\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 15 charactersmean: 72.82 charactersmax: 300 characters\nmin: 12 charactersmean: 34.11 charactersmax: 126 characters\nmin: 11 charactersmean: 36.38 charactersmax: 121 characters\nSamples:\nanchor\npositive\nnegative\nTwo women are embracing while holding to go packages.\nTwo woman are holding packages.\nThe men are fighting outside a deli.\nTwo young children in blue jerseys, one with the number 9 and one with the number 2 are standing on wooden steps in a bathroom and washing their hands in a sink.\nTwo kids in numbered jerseys wash their hands.\nTwo kids in jackets walk to school.\nA man selling donuts to a customer during a world exhibition event held in the city of Angeles\nA man selling donuts to a customer.\nA woman drinks her coffee in a small cafe.\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\npaq\nDataset: paq at 74601d8\nSize: 64,371,441 evaluation samples\nColumns: query and answer\nApproximate statistics based on the first 1000 samples:\nquery\nanswer\ntype\nstring\nstring\ndetails\nmin: 25 charactersmean: 51.3 charactersmax: 108 characters\nmin: 504 charactersmean: 623.09 charactersmax: 835 characters\nSamples:\nquery\nanswer\nwhen did season 3 of the voice brasil start\nThe Voice Brasil (season 3) The third season of \"The Voice Brasil\", premiered on Rede Globo on September 18, 2014 in the 10:30 p.m. (BRT/AMT) slot immediately following the primetime telenovela \"Imp√©rio\". The 22- and 24-year-old sertanejo duo Danilo Reis e Rafael won the competition on December 25, 2014 with 43% of the votes cast. This marked Lulu Santos' first win as a coach, the first stolen artist to win a Brazilian season of \"The Voice\", and the first time in any \"The Voice\" franchise that a duo won the competition. Online applications for \"The Voice Brasil\" were open on\nwhen did the little ranger first come out\nGang\" theme song was an instrumental medley of \"London Bridge\", \"Here We Go Round the Mulberry Bush\" and \"The Farmer in the Dell\". It remained in use until the series ended in 1944. The Little Ranger The Little Ranger is a 1938 \"Our Gang\" short comedy film directed by Gordon Douglas. It was the 169th short in the \"Our Gang\" series, and the first produced by Metro-Goldwyn-Mayer, who purchased the rights to the series from creator Hal Roach. Snubbed by his girlfriend Darla, Alfalfa accepts the invitation of tomboyish Muggsy to attend the local picture show. While watching the adventures\nwhat is the name of rachel's sister in ninjaaiden\nher among ten female characters who have never been featured on their games' cover arts, Samir Torres of VentureBeat wrote that while \"Team Ninja sexualy exploits all of their female characters, yet Rachel somehow got axed from every modern \"Ninja Gaiden\" box art.\" Rachel (Ninja Gaiden) In 2004's \"Ninja Gaiden\", Rachel is a fiend hunter whom the game's protagonist Ryu Hayabusa meets in the Holy Vigoor Empire, where she is on a mission to destroy the fiends, as well as find her missing sister, Alma, who has become a Greater Fiend. Soon after they first meet, she is captured but\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\ntrivia_qa\nDataset: trivia_qa at a7c36e3\nSize: 73,346 evaluation samples\nColumns: query and answer\nApproximate statistics based on the first 1000 samples:\nquery\nanswer\ntype\nstring\nstring\ndetails\nmin: 26 charactersmean: 77.62 charactersmax: 258 characters\nmin: 135 charactersmean: 3169.71 charactersmax: 4096 characters\nSamples:\nquery\nanswer\nIn which country is 'Ninety Mile Beach'?\nNinety (90) Mile Beach, Gippsland, Victoria - Tourism Australia Gippsland Find travel information on Ninety Mile Beach, one of the longest uninterrupted beaches in the world, located outside of Melbourne at Gippsland Lakes. Ninety Mile Beach, located in the Gippsland region on Victoria's south-eastern coastline, is one of the longest uninterrupted beaches in the world. Stand on the beach and watch the beach disappear into the salty sea spray in the distance. You might find that your footprints are the only ones in the sand that day. This is one of the most natural and unspoilt beaches in the world and is ideal for activities from beach fishing and swimming to walking, whale and dolphin-spotting or just lazing in the sun. Sun, sand and lush national parks all create the perfect holiday environment. Victoria's Ninety Mile Beach is a 90-mile long stretch of pristine golden sand that separates the Gippsland Lakes from Bass Strait. Stretching as far as the eye can see it is one of the most ...\nWhat country gets nearly 75% of its electricity from nuclear power?\nNuclear Power in France\nWhich Spaniard led an expedition which reached Tenochtitlan, the Aztec capital in 1519?\nThe Spanish Conquest (1519-1521) : Mexico History History\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nmsmarco_10m\nDataset: msmarco_10m at 8c5139a\nSize: 10,000,000 evaluation samples\nColumns: query, positive, and negative\nApproximate statistics based on the first 1000 samples:\nquery\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 10 charactersmean: 33.98 charactersmax: 131 characters\nmin: 56 charactersmean: 353.39 charactersmax: 1029 characters\nmin: 85 charactersmean: 339.79 charactersmax: 983 characters\nSamples:\nquery\npositive\nnegative\nwhat does a dental hygienist do\nDuring a dental appointment, a hygienist typically removes soft and hard deposits from a patient's teeth; examines the gums and teeth to discern the presence of disease or oral abnormality; and strips the teeth of calculus (tartar), stains and plaque. dental hygienist takes on a somewhat academic role as well; he or she educates dental patients on how to establish and maintain suitable oral hygiene, often with the aid of teeth models to give the patient a visual sense.\nMichelly in Tennessee said: Can anyone tell me how much you get paid hourly working as a dental hygienist. I make $36 dollars an hour and I just graduated from school. Should I be making more. Try to visit this link www.payscale.com/research/US/Job=Dental_Hygienist/Hourly_Rate so you will know how much dental hygienist usually get based on experience.\naverage annual temperature by florida county\nLake County Weather. The average temperature of Lake County is 70.90√Ç¬∞F, which is about the same as the Florida average temperature of 71.80√Ç¬∞F and is much higher than the national average temperature of 54.45√Ç¬∞F. Historical Weather. Heating Cost Index, #29.\naverage rn salary in fl the average annual salary for a registered nurse in the state of florida in 2011 was $ 64020 the average does fluctuate throughout the state with median rn salaries at their highest in metro areas\nwhat does amortization mean\nWhat is 'Amortization'. Amortization is the paying off of debt with a fixed repayment schedule in regular installments over a period of time for example with a mortgage or a car loan.\nFirst off, your EBIT is the same as your operating profit, but you can also calculate it by subtracting interest and tax from net income: $100,000 / ($10,000 + $25,000) = $65,000 To get EBITDA, you need to add back in depreciation and amortization:\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nswim_ir\nDataset: swim_ir at 834c20f\nSize: 501,538 evaluation samples\nColumns: query and text\nApproximate statistics based on the first 1000 samples:\nquery\ntext\ntype\nstring\nstring\ndetails\nmin: 4 charactersmean: 59.74 charactersmax: 165 characters\nmin: 206 charactersmean: 522.53 charactersmax: 3079 characters\nSamples:\nquery\ntext\nWhere was he born?\nHe was born in Brownsville, Edmonson County, Kentucky, March 28, 1890; attended the public schools, Western Kentucky State Teachers College at Bowling Green, and the law department of the University of Kentucky at Lexington; was admitted to the bar in 1915 and commenced practice in Brownsville, Ky.; county judge of Edmonson County, Ky., 1916-1918.\nWhat was Channon's National Service?\nChannon completed his National Service in the Royal Horse Guards (the Blues) from 1955 to 1956, serving in Cyprus during the 1956 Cyprus emergency. In London, he was a member of the set around Princess Margaret, and then attended Christ Church, Oxford from 1956. He was president of the Oxford University Conservative Association.\nWhat is the role of Immunoglobulin A in the immune system?\nImmunoglobulin heavy constant alpha 1 is a immunoglobulin gene with symbol \"IGHA1\". It encodes a constant (C) segment of Immunoglobulin A heavy chain. Immunoglobulin A is an antibody that plays a critical role in immune function in the mucous membranes. IgA shows the same typical structure of other antibody classes, with two heavy chains and two light chains, and four distinct domains: one variable region, and three variable regions. As a major class of immunoglobulin in body secretions, IgA plays a role in defending against infection, as well as preventing the access of foreign antigens to the immunologic system.\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\npubmedqa\nDataset: pubmedqa at a1ef0b5\nSize: 1,660 evaluation samples\nColumns: anchor, positive, negative_1, negative_2, negative_3, negative_4, negative_5, negative_6, negative_7, negative_8, negative_9, negative_10, negative_11, negative_12, negative_13, negative_14, negative_15, negative_16, negative_17, negative_18, negative_19, and negative_20\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative_1\nnegative_2\nnegative_3\nnegative_4\nnegative_5\nnegative_6\nnegative_7\nnegative_8\nnegative_9\nnegative_10\nnegative_11\nnegative_12\nnegative_13\nnegative_14\nnegative_15\nnegative_16\nnegative_17\nnegative_18\nnegative_19\nnegative_20\ntype\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\nstring\ndetails\nmin: 38 charactersmean: 88.92 charactersmax: 140 characters\nmin: 12 charactersmean: 372.72 charactersmax: 1113 characters\nmin: 26 charactersmean: 371.1 charactersmax: 1185 characters\nmin: 20 charactersmean: 326.89 charactersmax: 1084 characters\nmin: 23 charactersmean: 334.62 charactersmax: 1477 characters\nmin: 13 charactersmean: 347.64 charactersmax: 1310 characters\nmin: 13 charactersmean: 337.96 charactersmax: 1221 characters\nmin: 13 charactersmean: 303.39 charactersmax: 953 characters\nmin: 14 charactersmean: 328.25 charactersmax: 1168 characters\nmin: 16 charactersmean: 318.59 charactersmax: 989 characters\nmin: 18 charactersmean: 284.81 charactersmax: 853 characters\nmin: 13 charactersmean: 333.89 charactersmax: 875 characters\nmin: 26 charactersmean: 327.33 charactersmax: 1041 characters\nmin: 19 charactersmean: 354.74 charactersmax: 1705 characters\nmin: 16 charactersmean: 334.33 charactersmax: 1593 characters\nmin: 18 charactersmean: 346.0 charactersmax: 1374 characters\nmin: 13 charactersmean: 367.52 charactersmax: 1625 characters\nmin: 17 charactersmean: 357.96 charactersmax: 1126 characters\nmin: 23 charactersmean: 322.84 charactersmax: 1060 characters\nmin: 25 charactersmean: 312.1 charactersmax: 805 characters\nmin: 22 charactersmean: 362.97 charactersmax: 1002 characters\nmin: 22 charactersmean: 328.62 charactersmax: 1310 characters\nSamples:\nanchor\npositive\nnegative_1\nnegative_2\nnegative_3\nnegative_4\nnegative_5\nnegative_6\nnegative_7\nnegative_8\nnegative_9\nnegative_10\nnegative_11\nnegative_12\nnegative_13\nnegative_14\nnegative_15\nnegative_16\nnegative_17\nnegative_18\nnegative_19\nnegative_20\nChronic functional somatic symptoms: a single syndrome?\nObservational study, with a comparison control group.\nNasal nitric oxide (NO) and olfactory function are decreased in patients with chronic inflammatory sinonasal disease, suggesting a link between these two parameters. The aim of the study was to investigate nasal NO levels in patients with olfactory dysfunction due to different causes.\nFibromyalgia (FM) is a worldwide diffuse musculoskeletal chronic pain condition that affects up to 5% of the general population. Many symptoms associated with mitochondrial diseases are reported in patients with FM such as exercise intolerance, fatigue, myopathy and mitochondrial dysfunction. In this study, we report a mutation in cytochrome b gene of mitochondrial DNA (mtDNA) in a family with FM with inflammasome complex activation.\nChronic fatigue syndrome (CFS) has an uncertain pathogenesis. Allergies have been suggested as one cause.\nPatient's self-reported symptoms on a structured case history questionnaire.\nChronic multisymptom illness (reporting at least one symptom in at least two of the following symptom constructs: general fatigue; mood and cognition problems; and musculoskeletal discomfort) was assessed, differentiating by potential burn pit exposure, among deployers who completed 2004 and 2007 Millennium Cohort questionnaires.\nfunctional decline and death.\nOlfactory loss is a debilitating symptom of chronic rhinosinusitis (CRS). The pathophysiology of inflammatory olfactory dysfunction likely involves both conductive and sensorineural components. To study the interaction of CRS-associated inflammatory cytokines with the olfactory epithelium (OE), a transgenic mouse model was developed that allows temporally-controlled local gene expression. Interferon-gamma (IFN-Œ≥) is a prototypical T helper 1 (Th1) cytokine linked to nonpolypoid CRS (CRSsNP), as well as sinonasal viral and bacterial infections. In this study, the effects of chronic IFN-Œ≥ expression on olfactory histology and function were investigated.\nMonosomy 1p36 syndrome is the most commonly observed subtelomeric deletion syndrome. Patients with this syndrome typically have common clinical features, such as intellectual disability, epilepsy, and characteristic craniofacial features.\nMigraine is frequently accompanied by symptoms consistent with functional gastrointestinal disorders (FGIDs). This study evaluated the prevalence of functional gastrointestinal symptoms and assessed the symptoms' relationship with the concomitant functional symptoms of anxiety, depression, and headache-related disability.\nThe 2003 Canadian Consensus Criteria for chronic fatigue syndrome (CFS) are often assumed to suggest low-grade systemic inflammation, but have never been formally validated. This study explored the content validity of the Criteria in a sample of adolescents with CFS selected according to a wide case definition.\nIrritable pouch syndrome (IPS) is a functional disorder in patients with ileal pouch-anal anastomosis (IPAA), which presents with symptoms in the absence of structural abnormalities of the pouch. Thus, it resembles other functional disorders, such as irritable bowel syndrome characterized by visceral hypersensitivity in the presence of normal rectal biomechanics. The aim was to assess pouch biomechanics and perception of balloon distension in different groups of subjects with IPAA and to correlate the findings with clinical features.\nFibromyalgia (FM) and chronic fatigue syndrome (CFS) frequently overlap clinically and have been considered variants of one common disorder. We have recently shown that CFS is associated with a short corrected electrocardiographic QT interval (QTc). In the present study, we evaluated whether FM and CFS can be distinguished by QTc.\nSingle case with clinical follow-up over 2 years.\nFibromyalgia syndrome (FMS) is a disease of unknown pathogenesis characterized by chronic musculoskeletal pain. FMS has been also associated with altered endocrinological responses, but findings are inconsistent.\nPrimary Sj√∂gren's syndrome (pSS) is a systemic rheumatic disease in which gastrointestinal (GI) symptoms are common. Faecal calprotectin (FC) is a non-invasive biomarker that has been suggested to discriminate organic intestinal disease from functional disorders. The purpose of this study was to explore the usefulness of FC testing in patients with pSS.\nAn inactive lifestyle has been associated with functional somatic symptoms (FSS), but findings are contradictory. Moreover, mediating factors in this relationship are unclear. We examined whether low physical activity was related to FSS in adolescents, and whether this association was mediated by low physical fitness.\nNephrotic syndrome is a common kidney disease in both children and adults that is characterized by dramatic structural changes in the actin-rich foot processes of glomerular podocytes. Although glucocorticoids are the primary treatment for nephrotic syndrome, neither the target cell nor mechanism of action of glucocorticoids in nephrotic syndrome is known. For the last 30 years glucocorticoids have been presumed to act by reducing the release of soluble mediators of disease by circulating lymphocytes. In contrast, we hypothesized that glucocorticoids exert their beneficial effects in nephrotic syndrome by direct action on podocytes.\nChronic sclerosing sialadenitis is a fibroinflammatory disease of the salivary glands, characteristically of the submandibular gland. One prior Asian study proposed that chronic sclerosing sialadenitis is a part of the spectrum of IgG4-associated disease. This association has not been confirmed in Western populations. We therefore, investigated the relationship between IgG4 and chronic sclerosing sialadenitis, and compared the histomorphologic features of this condition with those of chronic sialadenitis-not otherwise specified, Sj√∂gren syndrome, and lymphoepithelial sialadenitis.\nPresence of GI symptoms.\nDisrupted-in schizophrenia 1 (DISC1), identified in a pedigree with a familial psychosis with the chromosome translocation (1:11), is a putative susceptibility gene for psychoses such as schizophrenia and major depressive disorder (MDD). Patients with chronic fatigue syndrome (CFS) report having continuous severe fatigue and many overlapping symptoms with MDD; however, the mechanism and effective treatment of CFS are still unclear. We focused on the overlapping symptoms between CFS and MDD and performed an association study of the functional single-nucleotide polymorphism (SNP) in the DISC1 gene with CFS.\nDoes sonographic needle guidance affect the clinical outcome of intraarticular injections?\nIn total, 148 painful joints were randomized to IA triamcinolone acetonide injection by conventional palpation-guided anatomic injection or sonographic image-guided injection enhanced with a one-handed control syringe (the reciprocating device). A one-needle, 2-syringe technique was used, where the first syringe was used to introduce the needle, aspirate any effusion, and anesthetize and dilate the IA space with lidocaine. After IA placement and synovial space dilation were confirmed, a syringe exchange was performed, and corticosteroid was injected with the second syringe through the indwelling IA needle. Baseline pain, procedural pain, pain at outcome (2 weeks), and changes in pain scores were measured with a 0-10 cm visual analog pain scale (VAS).\nWe used a blinded, longitudinal observational design of effectiveness in an effort to determine the accuracy of intra-articular injections and the effect of that accuracy on pain and functional outcomes in patients with various shoulder pathologies.\nData from standardized procedure notes and postprocedure chest radiographs were extracted and individually reviewed to verify the presence of pneumothorax or misplacement, and any intervention performed for either complication. The overall success rate of ultrasound-guided right internal jugular vein central venous catheter placement was 96.9% with an average of 1.3 attempts. There was only one pneumothorax (0.1% [95% CI, 0-0.4%]), and the rate of catheter misplacement requiring repositioning or replacement was 1.0% (95% CI, 0.6-1.7%). There were no arterial placements found on chest radiographs. Multivariate regression analysis showed no correlation between high-risk patient characteristics and composite complication rate.\nReal-time ultrasound-guided techniques allow for improved cannulation of the internal jugular vein and femoral vein for hemodialysis; however, these techniques require extra sterilization procedures, specialized probes, or needle guides. A simpler ultrasound vessel localization method was performed to investigate whether this alternative approach would aid in the cannulation of the femoral vein for patients in whom temporary angioaccess was required for hemodialysis.\nTo determine whether duration or venue of intravenous antibiotic administration affect lung function.\nNeedle electromyography (NEE) would be more valuable if it could predict outcomes after lumbar epidural steroid injections (LESIs) in lumbosacral radiculopathy (LSR).\nAmong patients with Wilkes stage III and IV disease undergoing arthroscopic lysis and lavage, does the use of an intra-articular injection of sodium hyaluronate (SH), when compared with Ringer lavage, result in better postoperative pain control and temporomandibular joint (TMJ) function?\nIntraoperative MR imaging and sonography are used for navigation during neurosurgical procedures. The purpose of this experimental study was to evaluate the potential of high-resolution sonography using superparamagnetic iron oxide (SPIO) particles as a contrast medium to delineate brain tumors and to relate these findings with those of MR imaging.\nAcademic general internal medicine practice.\nTo evaluate how guidance on water-intake impacts the degree of nocturia.\nIntra-articular knee injections are commonly performed in clinical practice for treating various knee joint disorders such as osteoarthritis and rheumatoid arthritis. When selecting the portal for injection, not only intra-articular needle accuracy but also procedural pain should be taken into consideration. The purpose of this study was to determine whether injection through anterolateral portal provokes less pain and provides better pain relief compared to superolateral portal.\nUltrasound guidance reduces the required local anesthetic volume for successful peripheral nerve blockade, but it is unclear whether this impacts postoperative analgesia. This prospective, randomized, observer-blinded study tested the hypothesis that a low-volume ultrasound-guided ankle block would provide similar analgesia after foot surgery compared with a conventional-volume surface landmark technique.\nThe purpose of this study was to evaluate how intravascular ultrasound-determined thickness and reflectivity of the inner echogenic layer of coronary artery plaque is affected by changes in collagen, elastin, proteoglycan, calcium and lipid content in the intima and media.\nThe anesthetic requirement is decreased in animals with head injury, but there are no data regarding the effect of intracranial tumor on the potency for intravenous anesthetics. The authors compared the quantal dose-response curves for propofol in patients having large (> or = 30 mm, mass effect) brain tumor with those having smaller (< 30 mm) lesions and with control patients undergoing noncranial surgery.\nTo develop digital multimedia instruction on intraoral suturing.\nTo evaluate a long term efficiency of a deep sclerectomy with T-Flux implant on intraocular pressure\nIn bilateral cochlear implant users, electrodes mapped to the same frequency range in each ear may stimulate different places in each cochlea due to an insertion depth difference of electrode arrays. This interaural place of stimulation mismatch can lead to problems with auditory image fusion and sensitivity to binaural cues, which may explain the large localization errors seen in many patients. Previous work has shown that interaural place of stimulation mismatch can lead to off-centered auditory images being perceived even though interaural time and level differences (ITD and ILD, respectively) were zero. Large interaural mismatches reduced the ability to use ITDs for auditory image lateralization. In contrast, lateralization with ILDs was still possible but the mapping of ILDs to spatial locations was distorted. This study extends the previous work by systematically investigating the effect of interaural place of stimulation mismatch on ITD and ILD sensitivity directly and examining...\nMathematical modeling and use of real world clinical inputs.\nThe use of ultrasonography in regional anesthetic blocks has rapidly evolved over the past few years. It has been speculated that ultrasound guidance might increase success rates and reduce complications. The aim of our study is to compare the success rate and quality of interscalene brachial plexus blocks performed either with direct ultrasound visualization or with the aid of nerve stimulation to guide needle placement.\nIntra-operative nerve monitoring (IONM) of the recurrent laryngeal nerve (RLN) during thyroid and parathyroid surgery is thought to aid in identification and dissection of the RLN. While utilization of IONM is increasing, one area of variability in its application is the assessment of adequate endotracheal tube electrode placement for IONM during the case. The main objective of this study is to assess the overall success of utilizing respiratory variation to confirm proper endotracheal tube placement for RLN monitoring.\nTime-dependent development of intracochlear impedances was investigated in 4 different groups of adult patients up to 4 years after implantation. Additionally, during rehabilitation period just after first fitting, impedances before and after stimulation were measured as to investigate the influence of electrical stimulation on the impedances. Results from standard Nucleus 24 Contour (control), standard Nucleus 24 Contour with intraoperative application of steroids, iridium-coated Nucleus 24 Contour, and iridium-coated Nucleus 24 Contour with intraoperative application of steroids were compared.\nDoes type 1 diabetes mellitus affect Achilles tendon response to a 10¬†km run?\nParticipants were 7 individuals with T1DM and 10 controls. All regularly ran distances greater than 5¬†km and VISA-A scores indicated good tendon function (T1DM‚Äâ=‚Äâ94‚Äâ¬±‚Äâ11, control‚Äâ=‚Äâ94‚Äâ¬±‚Äâ10). There were no diabetic complications and HbA1c was 8.7‚Äâ¬±‚Äâ2.6¬†mmol/mol for T1DM and 5.3‚Äâ¬±‚Äâ0.4¬†mmol/mol for control groups. Baseline tendon structure was similar in T1DM and control groups - UTC echo-types (I-IV) and anterior-posterior thickness were all p‚Äâ>‚Äâ0.05. No response to load was seen in either T1DM or control group over the 4-days post exercise.\nThe inability to produce insulin endogenously precipitates the clinical symptoms of type 1 diabetes mellitus. However, the dynamic trajectory of beta cell destruction following onset remains unclear. Using model-based inference, the severity of beta cell destruction at onset decreases with age where, on average, a 40% reduction in beta cell mass was sufficient to precipitate clinical symptoms at 20 years of age. While plasma C-peptide provides a surrogate measure of endogenous insulin production post-onset, it is unclear as to whether plasma C-peptide represents changes in beta cell mass or beta cell function. The objective of this paper was to determine the relationship between beta cell mass and endogenous insulin production post-onset.\nSeven healthy individuals with type 1 diabetes were tested on two separate occasions, during which either a 30-min MOD or IHE protocol was performed. MOD consisted of continuous exercise at 40% Vo(2peak), while the IHE protocol involved a combination of continuous exercise at 40% Vo(2peak) interspersed with 4-s sprints performed every 2 min to simulate the activity patterns of team sports.\nIt is unclear how genetic type 1 diabetes mellitus (DM) influences infarct size when blood glucose is tightly controlled. The aim of this study was to determine the effect of genetic type 1 DM, as occurs in BB rats, on infarct size after transient unilateral middle cerebral artery occlusion (MCAO) in male and female rats. In addition, studies suggest that male type 1 DM rats have a higher incidence of end-organ complications than do females. A second aim of this study was to determine the effect of chronic 17beta-estradiol (E(2)) administration on infarct size in male BB rats.\nStudies on bone mineral characteristics in children with type 1 diabetes mellitus (T1DM) have generated conflicting results.\nIt has recently been reported that the risk of developing nephropathy in patients with insulin dependent (type 1) diabetes mellitus is strongly associated with synergism between poor glycaemic control and carriage of the hypertension associated angiotensin II (type 1) receptor C1166 allele. The same report also revealed an increase in risk of nephropathy in diabetic patients carrying a specific angiotensin II (type 1) receptor haplotype, i.e. C1166/140-bp microsatellite allele (major allele).\nThe angiotensin II (type 1) (AT1) receptor mediates many biological effects of the renin-angiotensin system (RAS), leading to remodelling of cardiac tissue. The present study was designed to analyse changes in the function and expression of the AT1 receptor as principal effector of the RAS in myocardium from type 2 diabetic patients compared with non-diabetic myocardium as control. In addition, we determined the effect of treatment with ACE inhibitors or AT1 receptor blockers on expression levels of the receptor in diabetic patients.\nType 2 diabetes mellitus increases the risk of atherosclerotic cardiovascular disease. Antioxidative properties of high density lipoprotein (HDL) are important for atheroprotection. This study investigated whether the antioxidative functionality of HDL is altered in type 2 diabetes mellitus and aimed to identify potential determinants of this parameter.\nLife-long insulin is the standard treatment for type 1 diabetes mellitus (T1DM). The role of traditional Chinese medicine (TCM) in T1DM is still not clear. The aim of this study is to explore the prescription pattern of TCM and its impact on the risk of diabetic ketoacidosis (DKA) in patients with T1DM.\nIn type 1 diabetes, lung diffusing capacity for carbon monoxide (DL(CO)) may be impaired, and insulin has been shown to be beneficial in cases in which near-normal metabolic control is achieved. An influence of insulin, per se, on the alveolar-capillary membrane conductance is unexplored. We aimed at testing this possibility.\naIMT, but not cIMT, was significantly greater in the children with type 1 diabetes mellitus than in control subjects (P < .001). In children with type 1 diabetes mellitus, aIMT correlated with glycosylated hemoglobin (r = 0.31, P = .01) and was independently associated with age (beta = 0.38, P = .001) and low-density lipoprotein cholesterol level (beta = 0.38, P = .001). Vascular function (GTN) was worse in children with type 1 diabetes mellitus who had an aIMT >95th percentile, as defined with the control subjects.\nType 1 diabetes mellitus (T1DM) is caused by specific destruction of the pancreatic beta cells in the islets of Langerhans. Increased sensitivity to cytokines, in particular to interleukin-1beta (IL-1beta) seems to be an acquired trait during beta-cell maturation. In response to cytokines both protective and deleterious mechanisms are induced in beta cells, and when the deleterious prevail, T1DM develops. The aims of this study were to identify perturbation in protein patterns (PiPP) associated with beta-cell maturation, and compare these changes to previous analyses of IL-1beta exposed rat islets. For this purpose, proteome analyses were carried out using a cell-line, which matures from a glucagon-producing pre-beta-cell phenotype (NHI-glu) to an insulin-producing beta-cell phenotype (NHI-ins). We have previously shown that this maturation is accompanied by acquired sensitivity to the toxic effects of IL-1beta.\nType 1 diabetes mellitus is a T-cell-mediated autoimmune disease that leads to a major loss of insulin-secreting beta cells. The further decline of beta-cell function after clinical onset might be prevented by treatment with CD3 monoclonal antibodies, as suggested by the results of a phase 1 study. To provide proof of this therapeutic principle at the metabolic level, we initiated a phase 2 placebo-controlled trial with a humanized antibody, an aglycosylated human IgG1 antibody directed against CD3 (ChAglyCD3).\nType 2 diabetes mellitus (T2DM) affects approximately 10% of Americans, while 79 million Americans are estimated to have glucose intolerance or prediabetes (pre-DM). The present study was designed to determine whether obese patients with pre-DM or T2DM would lose weight as effectively as obese normoglycemic patients, in a medically supervised high-protein, low-calorie-weight management program.\nWe have previously shown that long-term type 1 diabetes affects the structural organization, contractile apparatus and extracellular matrix (ECM) of the myometrium during early pregnancy in mice.\nIn Achilles injured participants, there was a significant difference between soleus and lateral gastrocnemius offset times during running compared to the asymptomatic controls (p<0.05). There were no significant differences in triceps surae muscle activity between the footwear only and footwear and orthoses condition in the Achilles injured participants.\nNine adolescents with type 1 diabetes mellitus (five males, four females, aged 16 +/- 1.8 yr, diabetes duration 8.2 +/- 4.1 yr, hemoglobin A1c 7.8 +/- 0.8%, mean +/- SD) were subjected on two different occasions to a rest or 45 min of exercise at 95% of their lactate threshold. Insulin was administered iv at a rate based on their usual insulin dose, with similar plasma insulin levels for both studies (82.1 +/- 19.0, exercise vs. 82.7 +/- 16.4 pmol/liter, rest). Glucose was infused to maintain euglycemia for 18 h.\nType 1 diabetes mellitus (T1DM) is influenced by genetic as well as environmental factors. Its incidence has risen considerably since the 1950s.\nInsulin resistance has been associated with hypertension and with renal complications in patients with type 1 diabetes mellitus. Causal relationships have not been fully explained.\nDiabetes mellitus (DM) is a common metabolic disease among the middle-aged and older population, which leads to an increase of stroke incidence and poor stroke recovery. The present study was designed to investigate the impact of DM on brain damage and on ischemic brain repair after stroke in aging animals.\nType 1 diabetes (T1D) is an autoimmune disease with multiple susceptibility genes. The aim of this study was to determine whether combining IDDM1/HLA and IDDM2/ insulin( INS) 5' variable number of tandem repeat locus (VNTR) genotypes improves T1D risk assessment.\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nmiracl\nDataset: miracl at 07e2b62\nSize: 789,900 evaluation samples\nColumns: anchor, positive, and negative\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 11 charactersmean: 38.08 charactersmax: 86 characters\nmin: 93 charactersmean: 746.58 charactersmax: 3851 characters\nmin: 18 charactersmean: 648.12 charactersmax: 2554 characters\nSamples:\nanchor\npositive\nnegative\nWhat was Paul Heyman's first production?\nPaul HeymanOn May 23, 2005, Heyman returned in a segment with Vince McMahon and Eric Bischoff announcing ECW One Night Stand, with Heyman in charge. On the May 22 episode of \"Raw\", Heyman appeared as ECW Representative promoting \"One Night Stand\". On May 25, 2006 it was announced that ECW would relaunch, as a third WWE brand. Heyman was in charge of the new brand on-camera but had minimal creative input off-camera as well. On the May 29 episode of \"Raw\", during a face-off with Mick Foley, Heyman announced that he was granted a draft pick from both Raw and SmackDown! by Vince McMahon. His Raw draft pick was former ECW wrestler (and Money in the Bank contract holder) Rob Van Dam, and his SmackDown! draft pick was Kurt Angle. Heyman predicted that Van Dam would defeat John Cena at \"One Night Stand\" for the WWE Championship and then declare himself the new ECW World Heavyweight Champion.\nJay WolpertWolpert left Goodson-Todman to form his own production company, and his first game show was the 1979 series \"Whew!\" for CBS. Wolpert produced the series with Burt Sugarman for most of its run. \"Whew!\" was canceled in 1980 and Wolpert did not return to television with a series until January 1983, despite shooting several pilots in the interim. On January 3, 1983, Wolpert's \"Hit Man\" debuted on NBC with Peter Tomarken as its host. \"Hit Man\" lasted thirteen weeks on the air.\nWhen was Locke born?\nAdria Locke LangleyLocke was born in Iowa, 1899, as the youngest of three children. When she was young her family moved to Stanton, Nebraska and that is where she grew up. Her father William Locke, was president of the Omaha livestock market and a Quaker. He had certain ideas of what a woman should be. Because of this Adria's grandfather, Thomas Glendenning, took over in teaching her what would later be a great social consciousness.\nKeith LockeLocke was born and grew up in Christchurch, to Jack and Elsie Locke, prominent lifelong political activists for a wide variety of causes. Their four children were brought up in this environment and followed their parents into a life of activism, (as well as Keith, his sister Maire Leadbeater is a well-known activist and former city councillor for Auckland City Council). His father Jack was under surveillance during the 1951 New Zealand waterfront dispute.Former Prime Minister Robert Muldoon is said to have described the Lockes as the most \"notorious Communist family in New Zealand\". The Lockes lived in the Avon Loop area of the Christchurch Central City and were very active in the community notably organising Avon River clean-ups and native tree planting and arguing against development of the area, and in favour of retaining the character of the area.\nHow do plants get blight?\nBacterial blight of soybeanBacterial blight of soybeans can enter leaves through wounds or natural openings such as stomata. After gaining entrance to the host leaves, \"Pseudomonas syringae pv. glycinea\" multiplies in the leaf intercellular fluid. The pathogen must then overcome the plants defenses. \"Pseudomonas syringae pv. glycinea\" accomplishes this by using the type three secretion system to inject a variety of pathogenicity effector proteins (Hrp proteins) into the plant cell cytoplasm. These proteins act by interfering with effector-triggered immunity and producing phytohormones/toxins that suppress plant defenses. The expression of these virulence factors depends on the environmental conditions at the time of infection (see \"environment section). Furthermore, expression of virulence factors will only take place when a sufficiently large population of bacteria is present, which is determined through quorum sensing. When successful, the common symptoms of bacterial blight will be...\nGummy stem blightGummy Stem Blight is a cucurbit-rot disease caused by the fungal plant pathogen \"Didymella bryoniae\" (anamorph \"Phoma cucurbitacearum\").Gummy Stem Blight can affect a host at any stage of growth in its development and affects all parts of the host including leaves, stems and fruits. Symptoms generally consist of circular dark tan lesions that blight the leaf, water soaked leaves, stem cankers, and gummy brown ooze that exudes from cankers, giving it the name Gummy Stem Blight. Gummy Stem Blight reduces yields of edible cucurbits by devastating the vines and leaves and rotting the fruits.\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nmldr\nDataset: mldr at 40ad767\nSize: 200,000 evaluation samples\nColumns: anchor, positive, and negative\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 16 charactersmean: 66.52 charactersmax: 527 characters\nmin: 3223 charactersmean: 20386.58 charactersmax: 131268 characters\nmin: 3611 charactersmean: 15772.93 charactersmax: 145135 characters\nSamples:\nanchor\npositive\nnegative\nWhat is an Aboriginal reserve and how was it created?\nAn Aboriginal reserve, also called simply reserve, was a government-sanctioned settlement for Aboriginal Australians, created under various state and federal legislation. Along with missions and other institutions, they were used from the 19th century to the 1960s to keep Aboriginal people separate from the white Australian population, for various reasons perceived by the government of the day. The Aboriginal reserve laws gave governments much power over all aspects of Aboriginal people‚Äôs lives.Protectors of Aborigines and (later) Aboriginal Protection Boards were appointed to look after the interests of the Aboriginal people.HistoryAboriginal reserves were used from the nineteenth century to keep Aboriginal people separate from the white Australian population, often ostensibly for their protection.Protectors of Aborigines had been appointed from as early as 1836 in South Australia (with Matthew Moorhouse as the first permanent appointment as Chief Protector in 1839), with the G...\nThe  Nature Reserve () is a nature reserve located in the municipalities of Sorsele and Storuman in V√§sterbotten County of Swedish Lapland. It is the largest natural reserve in Sweden and one of the largest protected areas in Europe, totaling 562,772 ha (approx. 5,628¬†km2).Most of the reserve is made up of several Scandinavian Mountains, the main ones being Artfj√§llet, Norra Storfj√§llet, Ammarfj√§llet and Bj√∂rkfj√§llet. Most of the landscapes of the Swedish mountains are represented. This ranges from the pronounced alpine character of Norra Storfj√§llet, which includes the highlight of the reserve, the Norra Sytertoppen (1,768 m), to the plateau and plains near the base of the mountains. The differences in elevation highlight the diversity of rocks in the mountains. Among the mountains are the valleys and waterways of the Ume River drainage basin. This includes a portion of the Vindel River, after which the reserve is named. Towards the east, the elevation decreases and the mountains gi...\nWhat is the English translation of the song \"Bosanac\"?\nThis is a list of Bosnian patriotic songs.{\nclass=\"wikitable sortable\"\nWho were the representatives that declined to run for reelection to the Vermont House of Representatives in 2016?\nSelene Colburn is an American politician currently serving in the Vermont House of Representatives from the Chittenden-6-4 district since 2017 as a member of the Vermont Progressive Party. Prior to her tenure in the State House she served on the city council in Burlington, Vermont. She is the first female chair of the House Progressive Caucus.Colburn was born in Burlington, and educated at Burlington High School, Bennington College, and Simmons University. She became active in politics in her youth when she joined anti-war demonstrations.Colburn was first elected to office with her election to the Burlington city council in the 2014 election and she won reelection in the 2015 and 2017 elections. She was elected to the state house alongside Brian Cina in the 2016 election with the nominations of the Progressive and Democratic parties and was reelected in the 2018 and 2020 elections. She was selected to serve as assistant chair of the Vermont Progressive Party's caucus in the state h...\nThe 2016 United States Senate election in Florida was held November 8, 2016 to elect a member of the United States Senate to represent the State of Florida, concurrently with the 2016 U.S. presidential election, as well as other elections to the United States Senate in other states and elections to the United States House of Representatives and various state and local elections.  The primary elections for both the Republicans and Democrats took place on August 30, 2016.Incumbent Republican Senator Marco Rubio ran for another term but faced well-funded Republican primary opposition after initially announcing he would not seek re-election to his Senate seat. He had openly considered whether to seek re-election or run for president in 2016. He stated in April 2014 that he would not run for both the Senate and president in 2016, as Florida law prohibits a candidate from simultaneously appearing twice on a ballot, but did not rule out running for either office.However, in April 2015, Ru...\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nmr_tydi\nDataset: mr_tydi at abbdf55\nSize: 354,700 evaluation samples\nColumns: anchor, positive, and negative\nApproximate statistics based on the first 1000 samples:\nanchor\npositive\nnegative\ntype\nstring\nstring\nstring\ndetails\nmin: 13 charactersmean: 38.6 charactersmax: 101 characters\nmin: 71 charactersmean: 667.4 charactersmax: 21297 characters\nmin: 10 charactersmean: 619.16 charactersmax: 3134 characters\nSamples:\nanchor\npositive\nnegative\nWhat is the largest city in Sardinia?\nSardiniaSardinia is politically a region of Italy, whose official name is Regione Autonoma della Sardegna / Regione Aut√≤noma de Sardigna (Autonomous Region of Sardinia),[3] and enjoys some degree of domestic autonomy granted by a specific Statute.[4] It is divided into four provinces and a metropolitan city, with Cagliari being the region's capital and its largest city as well. Sardinia's indigenous language and the other minority languages (Sassarese, Corsican Gallurese, Algherese Catalan and Ligurian Tabarchino) spoken on the island are recognized by the regional law and enjoy \"equal dignity\" with Italian.[5]\nHistory of CagliariCaralis (or Karales) was the capital of the Roman province of Sardinia and Corsica and was elevated to the rank of \"Municipium\", a result of the civil war between Julius Caesar and Pompey when Caesar himself granted this status in gratitude to the city for its fidelity during the bloody war. All Caralitani obtained Roman citizenship and were enrolled in the tribe Quirina. The territory of the city included the campidano plain, likely becoming Sanluri.With about 20,000 inhabitants Caralis was the largest and most populous city of the island and the most important of the western Mediterranean basin of the Republic, and later of the Roman Empire. The city was equipped with important road links to the main towns of the island such as Sulki, with the coastal road and with that running through the valley of the Cixerri, Olbia and Tibula along the east coast, and Turris and Tibula along the road modeled on the current \"Carlo Felice\", and finally a road through the centre ...\nWhat activism did Katherine Schmidt participate in?\nKatherine SchmidtKatherine Schmidt (February 6, 1899 ‚Äì April 18, 1978) was an American artist and art activist. Early in her career the figure studies, landscapes, and still lifes she painted drew praise for their \"purity and clarity of color,\" \"sound draftsmanship,\" and \"individual choice of subject and its handling.\"[3] During the 1930s she was known mainly for the quality of her still life paintings which showed, one critic said, \"impeccable artistry.\"[4] At the end of her career, in the 1960s and 1970s, she produced specialized and highly disciplined still lifes of objects such as dead leaves and pieces of crumpled paper, which, said a critic, approached a \"magical realism.\"[5] As an art activist she helped promote the rights of artists for fair remuneration.[6]\nCaroline KatzensteinCaroline Katzenstein (1888‚Äì1968) was an American suffragist, activist, advocate for equal rights, insurance agent, and author. She was active in the local Philadelphia suffragist movement through the Pennsylvania branch of the National American Woman Suffrage Association and the Equal Franchise Society of Philadelphia. She played a role in the formation of the Congressional Union for Women Suffrage, which later became the National Women's Party. Katzenstein was also active in the movement for equal rights, serving on the Women's Joint Legislative Committee with Alice Paul, and championing the cause for the Equal Rights Amendment. She was the author of \"Lifting the Curtain: the State and National Woman Suffrage Campaigns in Pennsylvania as I Saw Them\" (1955).\nWho made the first wristwatch?\nHistory of watchesFrom the beginning, wristwatches were almost exclusively worn by women, while men used pocketwatches up until the early 20th century. The concept of the wristwatch goes back to the production of the very earliest watches in the 16th century. Some people say the world's first wristwatch was created by Abraham-Louis Breguet for Caroline Murat, Queen of Naples, in 1810.[21][22][23][24][25] Elizabeth I of England received a wristwatch from Robert Dudley in 1571, described as an arm watch. By the mid nineteenth century, most watchmakers produced a range of wristwatches, often marketed as bracelets, for women.[26]\nRolexIn 1908, Wilsdorf registered the trademark \"Rolex\" and opened an office in La Chaux-de-Fonds, Switzerland.[14][15] The book The Best of Time: Rolex Wristwatches: An Unauthorized History by Jeffrey P. Hess and James Dowling says that the name was just made up.[16] One story, never confirmed by Wilsdorf, recounts that the name came from the French phrase horlogerie exquise, meaning \"exquisite clockwork\"[9] or as a contraction of \"horological excellence\". Wilsdorf was said to want his watch brand's name to be easily pronounceable in any language.[17] He also thought that the name \"Rolex\" was onomatopoeic, sounding like a watch being wound. It is easily pronounceable in many languages and, as all its upper-case letters have the same size, can be written symmetrically. It was also short enough to fit on the face of a watch.[17]\nLoss: MatryoshkaLoss with these parameters:{\n\"loss\": \"MultipleNegativesRankingLoss\",\n\"matryoshka_dims\": [\n1024,\n512,\n256,\n128,\n64,\n32\n],\n\"matryoshka_weights\": [\n1,\n1,\n1,\n1,\n1,\n1\n],\n\"n_dims_per_step\": -1\n}\nTraining Hyperparameters\nNon-Default Hyperparameters\neval_strategy: steps\nper_device_train_batch_size: 2048\nper_device_eval_batch_size: 2048\nlearning_rate: 0.2\nnum_train_epochs: 1\nwarmup_ratio: 0.1\nbf16: True\nbatch_sampler: no_duplicates\nAll Hyperparameters\nClick to expand\noverwrite_output_dir: False\ndo_predict: False\neval_strategy: steps\nprediction_loss_only: True\nper_device_train_batch_size: 2048\nper_device_eval_batch_size: 2048\nper_gpu_train_batch_size: None\nper_gpu_eval_batch_size: None\ngradient_accumulation_steps: 1\neval_accumulation_steps: None\ntorch_empty_cache_steps: None\nlearning_rate: 0.2\nweight_decay: 0.0\nadam_beta1: 0.9\nadam_beta2: 0.999\nadam_epsilon: 1e-08\nmax_grad_norm: 1.0\nnum_train_epochs: 1\nmax_steps: -1\nlr_scheduler_type: linear\nlr_scheduler_kwargs: {}\nwarmup_ratio: 0.1\nwarmup_steps: 0\nlog_level: passive\nlog_level_replica: warning\nlog_on_each_node: True\nlogging_nan_inf_filter: True\nsave_safetensors: True\nsave_on_each_node: False\nsave_only_model: False\nrestore_callback_states_from_checkpoint: False\nno_cuda: False\nuse_cpu: False\nuse_mps_device: False\nseed: 42\ndata_seed: None\njit_mode_eval: False\nuse_ipex: False\nbf16: True\nfp16: False\nfp16_opt_level: O1\nhalf_precision_backend: auto\nbf16_full_eval: False\nfp16_full_eval: False\ntf32: None\nlocal_rank: 0\nddp_backend: None\ntpu_num_cores: None\ntpu_metrics_debug: False\ndebug: []\ndataloader_drop_last: False\ndataloader_num_workers: 0\ndataloader_prefetch_factor: None\npast_index: -1\ndisable_tqdm: False\nremove_unused_columns: True\nlabel_names: None\nload_best_model_at_end: False\nignore_data_skip: False\nfsdp: []\nfsdp_min_num_params: 0\nfsdp_config: {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}\nfsdp_transformer_layer_cls_to_wrap: None\naccelerator_config: {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None}\ndeepspeed: None\nlabel_smoothing_factor: 0.0\noptim: adamw_torch\noptim_args: None\nadafactor: False\ngroup_by_length: False\nlength_column_name: length\nddp_find_unused_parameters: None\nddp_bucket_cap_mb: None\nddp_broadcast_buffers: False\ndataloader_pin_memory: True\ndataloader_persistent_workers: False\nskip_memory_metrics: True\nuse_legacy_prediction_loop: False\npush_to_hub: False\nresume_from_checkpoint: None\nhub_model_id: None\nhub_strategy: every_save\nhub_private_repo: False\nhub_always_push: False\ngradient_checkpointing: False\ngradient_checkpointing_kwargs: None\ninclude_inputs_for_metrics: False\neval_do_concat_batches: True\nfp16_backend: auto\npush_to_hub_model_id: None\npush_to_hub_organization: None\nmp_parameters:\nauto_find_batch_size: False\nfull_determinism: False\ntorchdynamo: None\nray_scope: last\nddp_timeout: 1800\ntorch_compile: False\ntorch_compile_backend: None\ntorch_compile_mode: None\ndispatch_batches: None\nsplit_batches: None\ninclude_tokens_per_second: False\ninclude_num_input_tokens_seen: False\nneftune_noise_alpha: None\noptim_target_modules: None\nbatch_eval_metrics: False\neval_on_start: False\nuse_liger_kernel: False\neval_use_gather_object: False\nbatch_sampler: no_duplicates\nmulti_dataset_batch_sampler: proportional\nTraining Logs\nClick to expand\nEpoch\nStep\nTraining Loss\ngooaq loss\nmsmarco loss\nsquad loss\ns2orc loss\nallnli loss\npaq loss\ntrivia qa loss\nmsmarco 10m loss\nswim ir loss\npubmedqa loss\nmiracl loss\nmldr loss\nmr tydi loss\nNanoClimateFEVER_cosine_ndcg@10\nNanoDBPedia_cosine_ndcg@10\nNanoFEVER_cosine_ndcg@10\nNanoFiQA2018_cosine_ndcg@10\nNanoHotpotQA_cosine_ndcg@10\nNanoMSMARCO_cosine_ndcg@10\nNanoNFCorpus_cosine_ndcg@10\nNanoNQ_cosine_ndcg@10\nNanoQuoraRetrieval_cosine_ndcg@10\nNanoSCIDOCS_cosine_ndcg@10\nNanoArguAna_cosine_ndcg@10\nNanoSciFact_cosine_ndcg@10\nNanoTouche2020_cosine_ndcg@10\nNanoBEIR_mean_cosine_ndcg@10\n0\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n0.0741\n0.3518\n0.2118\n0.0793\n0.3538\n0.3200\n0.1954\n0.1589\n0.6759\n0.1532\n0.0945\n0.4296\n0.1455\n0.2495\n0.0000\n1\n32.2074\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n0.0064\n250\n22.7851\n8.3992\n17.7191\n17.6791\n16.6296\n18.7913\n12.1404\n18.9992\n12.1891\n11.6795\n26.3440\n8.5795\n19.3571\n9.5985\n0.2366\n0.5129\n0.6004\n0.1960\n0.6334\n0.3941\n0.2713\n0.3392\n0.7977\n0.2416\n0.3819\n0.5448\n0.4679\n0.4321\n0.0127\n500\n9.6296\n4.6987\n13.6254\n11.7605\n12.5290\n17.2038\n6.9342\n12.0873\n7.3539\n9.1374\n23.1663\n4.2482\n14.5991\n3.3365\n0.2929\n0.5509\n0.6529\n0.2890\n0.6495\n0.4244\n0.2873\n0.3690\n0.8830\n0.2373\n0.3815\n0.5802\n0.5422\n0.4723\n0.0191\n750\n6.7008\n3.6302\n11.7061\n10.1299\n11.2366\n15.1612\n5.5833\n10.6967\n5.7074\n8.8117\n23.2404\n3.5115\n12.5734\n2.4346\n0.3101\n0.5565\n0.6684\n0.3406\n0.6354\n0.4111\n0.2972\n0.3894\n0.8611\n0.2513\n0.3613\n0.5840\n0.5578\n0.4788\n0.0255\n1000\n5.8282\n2.9789\n11.2050\n9.5095\n10.6029\n14.7717\n5.0173\n9.6170\n5.1146\n9.0596\n22.3746\n3.0912\n12.2982\n2.2626\n0.3066\n0.5514\n0.6654\n0.3252\n0.6390\n0.4139\n0.2917\n0.4168\n0.8678\n0.2590\n0.3884\n0.6214\n0.5614\n0.4852\n0.0318\n1250\n5.3975\n2.8335\n11.0393\n9.3407\n9.6014\n14.5350\n4.8262\n9.4577\n4.9009\n8.9271\n21.8053\n3.2513\n12.2634\n1.6880\n0.3090\n0.5449\n0.6607\n0.3432\n0.6243\n0.4145\n0.3026\n0.4392\n0.8801\n0.2608\n0.3760\n0.6102\n0.5768\n0.4879\n0.0382\n1500\n5.3077\n2.7030\n10.6366\n8.9914\n9.8588\n14.6669\n4.6253\n9.3728\n4.5863\n9.1788\n22.0617\n2.8378\n10.9618\n1.8702\n0.3240\n0.5421\n0.7026\n0.3507\n0.6227\n0.4134\n0.3150\n0.3996\n0.8776\n0.2493\n0.3625\n0.6120\n0.5642\n0.4874\n0.0445\n1750\n4.9354\n2.6691\n10.6339\n8.9606\n9.7095\n14.9174\n4.5880\n9.3114\n4.1786\n8.2898\n22.8332\n2.6850\n11.3781\n1.6352\n0.3092\n0.5591\n0.6615\n0.3253\n0.6363\n0.3926\n0.3165\n0.4057\n0.9019\n0.2600\n0.3685\n0.6030\n0.5563\n0.4843\n0.0509\n2000\n4.8017\n2.5867\n10.0547\n8.8155\n9.6765\n14.7973\n4.3931\n9.2721\n4.0193\n7.7955\n23.4468\n1.9884\n11.4315\n1.8009\n0.3274\n0.5615\n0.7024\n0.3531\n0.6481\n0.3959\n0.3134\n0.4183\n0.8849\n0.2505\n0.3694\n0.5991\n0.5664\n0.4916\n0.0573\n2250\n4.8193\n2.4974\n9.9855\n8.8389\n9.6763\n14.4220\n4.3112\n9.1019\n4.0176\n8.4064\n22.7034\n2.7534\n11.5256\n2.3585\n0.3149\n0.5392\n0.6689\n0.3344\n0.6495\n0.4080\n0.3058\n0.3953\n0.8857\n0.2588\n0.3426\n0.5986\n0.5756\n0.4829\n0.0636\n2500\n4.8773\n2.7116\n10.2180\n8.6935\n9.7664\n14.3161\n4.2722\n8.9829\n4.2454\n9.1911\n22.8367\n2.6666\n11.6110\n2.0147\n0.3377\n0.5455\n0.6547\n0.3130\n0.6396\n0.4259\n0.3256\n0.4226\n0.8825\n0.2491\n0.3908\n0.5852\n0.5656\n0.4875\n0.0700\n2750\n4.5856\n2.7758\n9.9754\n8.6197\n9.6282\n14.4828\n4.1534\n8.8766\n4.2312\n9.5281\n21.9368\n2.9119\n9.7259\n1.9405\n0.3384\n0.5763\n0.6646\n0.3068\n0.6740\n0.4195\n0.3155\n0.4301\n0.8809\n0.2366\n0.3965\n0.5963\n0.5714\n0.4928\n0.0764\n3000\n4.3725\n2.5993\n10.1291\n8.7361\n8.9502\n14.8227\n4.1163\n8.8371\n4.1014\n9.2259\n23.4047\n3.2715\n10.3051\n2.3604\n0.3040\n0.5703\n0.6836\n0.3006\n0.6355\n0.3993\n0.3318\n0.4236\n0.9001\n0.2579\n0.3966\n0.5902\n0.5770\n0.4900\n0.0827\n3250\n4.4409\n2.5753\n10.0879\n8.5131\n8.7106\n14.8015\n4.0560\n8.8296\n4.1868\n9.3069\n22.5793\n2.3810\n8.6639\n2.0435\n0.3147\n0.5806\n0.6766\n0.3342\n0.6293\n0.4134\n0.3208\n0.4089\n0.8834\n0.2656\n0.3784\n0.6119\n0.5731\n0.4916\n0.0891\n3500\n4.6192\n2.4352\n9.9932\n8.5716\n9.2016\n14.1559\n4.0585\n8.9413\n3.8278\n8.6089\n22.9941\n2.5541\n9.4271\n1.7271\n0.3052\n0.5531\n0.6921\n0.3284\n0.6391\n0.4027\n0.3288\n0.4235\n0.8938\n0.2565\n0.3928\n0.5848\n0.5741\n0.4904\n0.0955\n3750\n4.4805\n2.5370\n10.0723\n8.4652\n8.8024\n14.4678\n4.0045\n8.8487\n3.6855\n8.4129\n22.5177\n2.5961\n9.1362\n1.6572\n0.2996\n0.5669\n0.7051\n0.3007\n0.6433\n0.3822\n0.3127\n0.4419\n0.8853\n0.2741\n0.3696\n0.5911\n0.5796\n0.4886\n0.1018\n4000\n4.3246\n2.4130\n10.0235\n8.4203\n9.1794\n14.4445\n3.9667\n8.8021\n3.6692\n8.0637\n22.1590\n2.2690\n9.5487\n1.4625\n0.2987\n0.5659\n0.6905\n0.3103\n0.6280\n0.4004\n0.3025\n0.4273\n0.9026\n0.2728\n0.3710\n0.6004\n0.5746\n0.4881\n0.1082\n4250\n4.547\n2.2318\n10.0773\n8.4970\n8.6540\n13.9845\n3.9893\n8.7805\n3.4480\n8.1038\n21.2066\n2.4344\n9.2932\n1.4761\n0.3037\n0.5760\n0.7118\n0.3239\n0.6135\n0.4056\n0.3170\n0.4323\n0.8860\n0.2620\n0.3887\n0.6211\n0.5824\n0.4941\n0.1145\n4500\n4.3008\n2.3243\n9.8733\n8.5042\n9.0158\n14.0935\n3.9014\n8.8306\n3.5557\n8.4240\n21.2823\n2.6280\n9.4869\n1.8310\n0.3076\n0.5788\n0.6867\n0.3187\n0.6190\n0.3936\n0.3181\n0.4160\n0.8895\n0.2564\n0.3960\n0.6148\n0.5736\n0.4899\n0.1209\n4750\n4.2386\n2.4259\n9.8799\n8.3964\n9.1116\n13.9412\n3.8572\n8.7955\n3.6524\n9.6881\n21.3812\n2.2282\n8.9280\n1.5408\n0.3393\n0.5783\n0.7190\n0.3137\n0.6239\n0.3953\n0.3044\n0.4231\n0.8768\n0.2636\n0.3828\n0.6043\n0.5662\n0.4916\n0.1273\n5000\n4.141\n2.4005\n9.9973\n8.2741\n9.1627\n14.4273\n3.7931\n8.7825\n3.6856\n9.0001\n21.6595\n2.2374\n9.2771\n1.4845\n0.3243\n0.5705\n0.6858\n0.3304\n0.6328\n0.3888\n0.3145\n0.4096\n0.8775\n0.2492\n0.3769\n0.6001\n0.5600\n0.4862\n0.1336\n5250\n4.3221\n2.4200\n9.8792\n8.2559\n9.0431\n13.9564\n3.8055\n8.5773\n3.6137\n8.1900\n21.6272\n2.2271\n8.1229\n1.6308\n0.3207\n0.5876\n0.6945\n0.3449\n0.6232\n0.4072\n0.3011\n0.4084\n0.8894\n0.2557\n0.3668\n0.5905\n0.5582\n0.4883\n0.1400\n5500\n4.2121\n2.3857\n10.1277\n8.3257\n9.0878\n13.8545\n3.7696\n8.6034\n3.5613\n8.7845\n21.5562\n2.3611\n7.5145\n1.9243\n0.3160\n0.5683\n0.7024\n0.3382\n0.6244\n0.4038\n0.3075\n0.4316\n0.8784\n0.2603\n0.3876\n0.5866\n0.5650\n0.4900\n0.1464\n5750\n4.1071\n2.4265\n9.9555\n8.0947\n9.1289\n14.0017\n3.7337\n8.6306\n3.4562\n8.3132\n21.7894\n2.1157\n8.1967\n1.5567\n0.3317\n0.5604\n0.7101\n0.3645\n0.6460\n0.3857\n0.2987\n0.4236\n0.8830\n0.2535\n0.3867\n0.5767\n0.5612\n0.4909\n0.1527\n6000\n4.1189\n2.3586\n10.0799\n8.0905\n9.0291\n14.0232\n3.7064\n8.5220\n3.4742\n8.3858\n21.5903\n2.1663\n7.6242\n1.4405\n0.3264\n0.5614\n0.6825\n0.3668\n0.6296\n0.3972\n0.2863\n0.4296\n0.8869\n0.2482\n0.3809\n0.6004\n0.5556\n0.4886\n0.1591\n6250\n4.0873\n2.2906\n9.9813\n8.1351\n8.5907\n13.8665\n3.7028\n8.5648\n3.5042\n8.1623\n21.6688\n2.2940\n7.6652\n1.5228\n0.3444\n0.5666\n0.7035\n0.3415\n0.6188\n0.3992\n0.2989\n0.4318\n0.8816\n0.2504\n0.4014\n0.6042\n0.5637\n0.4928\n0.1654\n6500\n3.9586\n2.2378\n9.9318\n8.0887\n8.7977\n14.1260\n3.6614\n8.5028\n3.3178\n8.3118\n21.5718\n2.2074\n8.0905\n1.7266\n0.3299\n0.5828\n0.6994\n0.3505\n0.6375\n0.3988\n0.3225\n0.4173\n0.8891\n0.2448\n0.3930\n0.6034\n0.5614\n0.4946\n0.1718\n6750\n4.1981\n2.2863\n9.5475\n7.6881\n8.5462\n13.6929\n3.6704\n8.6660\n3.3401\n8.9262\n21.8933\n2.0578\n8.4832\n1.6796\n0.3456\n0.5600\n0.6983\n0.3549\n0.6445\n0.3844\n0.3139\n0.4263\n0.8952\n0.2626\n0.3877\n0.5820\n0.5597\n0.4935\n0.1782\n7000\n4.0528\n2.3292\n9.5129\n7.8273\n8.7743\n13.6930\n3.6284\n8.6346\n3.3430\n8.5204\n21.3129\n2.3350\n8.8695\n1.9034\n0.3457\n0.5673\n0.6850\n0.3274\n0.6321\n0.3981\n0.3171\n0.4252\n0.8830\n0.2643\n0.3901\n0.5888\n0.5590\n0.4910\n0.1845\n7250\n4.0547\n2.2386\n9.5373\n7.9214\n8.7896\n13.6151\n3.6172\n8.5316\n3.3128\n9.3566\n21.4568\n2.3743\n9.1696\n1.7235\n0.3528\n0.5597\n0.6931\n0.3369\n0.6327\n0.3951\n0.3111\n0.4368\n0.8787\n0.2552\n0.3758\n0.5911\n0.5515\n0.4900\n0.1909\n7500\n4.3005\n2.2273\n9.4397\n7.9013\n8.8606\n13.3847\n3.6401\n8.4134\n3.2583\n8.3415\n21.4206\n2.4573\n9.2348\n1.4832\n0.3557\n0.5642\n0.7145\n0.3380\n0.6412\n0.3772\n0.3085\n0.4278\n0.8792\n0.2522\n0.3738\n0.5843\n0.5587\n0.4904\n0.1973\n7750\n4.0054\n2.2277\n9.4653\n7.9297\n8.5999\n13.6106\n3.6049\n8.3861\n3.2335\n9.3198\n21.6595\n2.4730\n8.7335\n1.6145\n0.3396\n0.5535\n0.6901\n0.3556\n0.6311\n0.3867\n0.3182\n0.4308\n0.8692\n0.2590\n0.3654\n0.5925\n0.5558\n0.4883\n0.2036\n8000\n3.8426\n2.2970\n9.4352\n7.9532\n8.6501\n13.9004\n3.5835\n8.3664\n3.2109\n8.4302\n21.0340\n2.1047\n9.0103\n1.1751\n0.3420\n0.5695\n0.6868\n0.3746\n0.6434\n0.4042\n0.3193\n0.4259\n0.8847\n0.2623\n0.3785\n0.5945\n0.5702\n0.4966\n0.2100\n8250\n3.9404\n2.2417\n9.5349\n7.8978\n8.6899\n13.8131\n3.5697\n8.3664\n3.1548\n8.6003\n21.6214\n2.0881\n9.1829\n0.9559\n0.3306\n0.5651\n0.6959\n0.3448\n0.6455\n0.3857\n0.3123\n0.4431\n0.9009\n0.2580\n0.3981\n0.6073\n0.5748\n0.4971\n0.2164\n8500\n3.9522\n2.2103\n9.6575\n7.9030\n8.3617\n14.0083\n3.5433\n8.3198\n3.2148\n8.5004\n20.8166\n2.3194\n8.0428\n1.2475\n0.3343\n0.5688\n0.7054\n0.3411\n0.6625\n0.3919\n0.3148\n0.4213\n0.8965\n0.2665\n0.3454\n0.6133\n0.5782\n0.4954\n0.2227\n8750\n3.9665\n2.1659\n9.7216\n7.8772\n8.6394\n13.9406\n3.5289\n8.3360\n3.1863\n8.7835\n21.2033\n2.1874\n8.4683\n1.2399\n0.3231\n0.5715\n0.6794\n0.3378\n0.6684\n0.3966\n0.3171\n0.4048\n0.8887\n0.2660\n0.3461\n0.6094\n0.5559\n0.4896\n0.2291\n9000\n4.0217\n2.1042\n9.6765\n7.8951\n8.4255\n13.8092\n3.5314\n8.3896\n3.1097\n8.0204\n21.4246\n2.0600\n8.7244\n1.3343\n0.3218\n0.5696\n0.6931\n0.3569\n0.6654\n0.3978\n0.3159\n0.4193\n0.9000\n0.2827\n0.3750\n0.5890\n0.5796\n0.4974\n0.2354\n9250\n4.0008\n2.0865\n9.4154\n7.9689\n8.5298\n13.6352\n3.5371\n8.4191\n3.0414\n8.4828\n21.5173\n1.9966\n7.6465\n1.1097\n0.3282\n0.5675\n0.6934\n0.3476\n0.6559\n0.3907\n0.3272\n0.4132\n0.9038\n0.2712\n0.3891\n0.5951\n0.5716\n0.4965\n0.2418\n9500\n3.8041\n2.0969\n9.4478\n7.9720\n8.6298\n13.7493\n3.5003\n8.4702\n3.0939\n8.5108\n21.6929\n1.9457\n7.9947\n1.2784\n0.3291\n0.5655\n0.6915\n0.3533\n0.6495\n0.3949\n0.3291\n0.4313\n0.9007\n0.2641\n0.3910\n0.5838\n0.5710\n0.4965\n0.2482\n9750\n3.9483\n2.1627\n9.5085\n7.8994\n8.7048\n13.4591\n3.4941\n8.3342\n3.1202\n8.7011\n20.9101\n1.8594\n7.8214\n1.1181\n0.3312\n0.5665\n0.6870\n0.3608\n0.6530\n0.4038\n0.3267\n0.4511\n0.8973\n0.2613\n0.4063\n0.5832\n0.5660\n0.4996\n0.2545\n10000\n3.9843\n2.1229\n9.4348\n7.9006\n8.2377\n13.5086\n3.4905\n8.2574\n3.0114\n8.3314\n20.5157\n1.9033\n6.7485\n1.1358\n0.3365\n0.5704\n0.6858\n0.3616\n0.6561\n0.3953\n0.3263\n0.4322\n0.8833\n0.2690\n0.3995\n0.6004\n0.5691\n0.4989\n0.2609\n10250\n3.8779\n2.1442\n9.4343\n7.9411\n8.4677\n13.5196\n3.4558\n8.2314\n3.0592\n8.6126\n20.3879\n1.9378\n6.8287\n1.1270\n0.3192\n0.5688\n0.7051\n0.3669\n0.6577\n0.3913\n0.3167\n0.4479\n0.8978\n0.2619\n0.4030\n0.5966\n0.5699\n0.5002\n0.2673\n10500\n3.8708\n2.1487\n9.6030\n7.9352\n8.4657\n13.3417\n3.4574\n8.3005\n3.0167\n8.4265\n20.5354\n1.9044\n6.9217\n1.1079\n0.3255\n0.5730\n0.7020\n0.3445\n0.6527\n0.3861\n0.3178\n0.4322\n0.8859\n0.2644\n0.3997\n0.5969\n0.5739\n0.4965\n0.2736\n10750\n3.8153\n2.1486\n9.2440\n7.8432\n8.5271\n13.4366\n3.4702\n8.3222\n2.9680\n8.4780\n20.6855\n1.8790\n6.6261\n1.0838\n0.3240\n0.5727\n0.6970\n0.3487\n0.6546\n0.3897\n0.3294\n0.4421\n0.8908\n0.2548\n0.3802\n0.6028\n0.5744\n0.4970\n0.2800\n11000\n3.9693\n2.1542\n9.4072\n7.8122\n8.6226\n13.1083\n3.4426\n8.2961\n3.0086\n8.6255\n20.5001\n1.9112\n6.6305\n1.1554\n0.3273\n0.5631\n0.6912\n0.3502\n0.6431\n0.4012\n0.3173\n0.4422\n0.8834\n0.2740\n0.3829\n0.6117\n0.5745\n0.4971\n0.2864\n11250\n3.7596\n2.1288\n9.4115\n7.8479\n8.6295\n13.1814\n3.4282\n8.2609\n2.9531\n8.6024\n20.7827\n1.8339\n7.0503\n1.0273\n0.3301\n0.5714\n0.6814\n0.3457\n0.6447\n0.3962\n0.3142\n0.4545\n0.8751\n0.2687\n0.3827\n0.5921\n0.5492\n0.4928\n0.2927\n11500\n3.7377\n2.1764\n9.2284\n7.7482\n8.6753\n13.2556\n3.4186\n8.2092\n2.9631\n8.2251\n20.9522\n1.8887\n6.8783\n1.1493\n0.3184\n0.5710\n0.6896\n0.3620\n0.6411\n0.4016\n0.3141\n0.4598\n0.8871\n0.2660\n0.4035\n0.5946\n0.5680\n0.4982\n0.2991\n11750\n3.645\n2.1757\n9.2386\n7.7988\n8.4091\n13.3105\n3.3986\n8.1770\n3.0392\n8.3319\n20.8279\n1.8464\n7.2340\n1.2369\n0.3066\n0.5680\n0.6938\n0.3413\n0.6498\n0.4035\n0.3119\n0.4692\n0.8776\n0.2697\n0.4018\n0.5937\n0.5725\n0.4969\n0.3054\n12000\n3.8302\n2.1730\n9.2542\n7.8031\n8.3773\n13.3083\n3.4245\n8.2024\n2.9451\n8.4451\n20.3890\n1.8992\n7.1868\n1.3149\n0.3022\n0.5639\n0.7047\n0.3580\n0.6585\n0.3953\n0.3193\n0.4512\n0.8757\n0.2667\n0.4016\n0.5815\n0.5653\n0.4957\n0.3118\n12250\n3.7341\n2.1580\n9.1449\n7.7487\n8.2782\n13.4871\n3.4325\n8.1531\n2.8524\n8.0765\n20.4420\n1.8084\n7.4004\n1.1942\n0.3217\n0.5648\n0.7022\n0.3658\n0.6597\n0.4010\n0.3204\n0.4470\n0.8778\n0.2668\n0.4062\n0.5841\n0.5764\n0.4995\n0.3182\n12500\n3.6937\n2.2003\n9.0298\n7.7776\n8.3088\n13.3345\n3.4198\n8.0772\n2.8139\n8.7163\n20.4754\n1.8802\n7.2714\n1.2016\n0.3086\n0.5676\n0.6930\n0.3609\n0.6532\n0.4067\n0.3228\n0.4341\n0.8737\n0.2723\n0.4014\n0.5783\n0.5799\n0.4964\n0.3245\n12750\n3.6917\n2.1808\n9.0285\n7.7770\n8.3062\n13.5737\n3.3953\n8.1291\n2.8232\n8.3426\n20.9837\n1.9420\n7.1199\n1.2568\n0.3288\n0.5620\n0.6855\n0.3512\n0.6575\n0.4069\n0.3309\n0.4372\n0.8906\n0.2709\n0.4106\n0.5928\n0.5874\n0.5009\n0.3309\n13000\n3.6376\n2.1621\n9.0474\n7.7871\n8.1721\n13.4874\n3.3904\n8.1507\n2.8109\n8.5607\n21.0805\n1.9734\n7.0553\n1.3466\n0.3250\n0.5686\n0.6850\n0.3520\n0.6630\n0.4044\n0.3258\n0.4424\n0.8666\n0.2684\n0.4038\n0.5769\n0.5725\n0.4965\n0.3373\n13250\n3.7786\n2.1146\n9.1181\n7.7333\n8.2758\n13.4782\n3.3906\n8.2021\n2.8320\n8.3097\n21.1471\n1.8529\n7.3608\n1.2242\n0.3282\n0.5649\n0.6997\n0.3761\n0.6680\n0.4097\n0.3242\n0.4143\n0.8873\n0.2784\n0.3958\n0.5956\n0.5745\n0.5013\n0.3436\n13500\n3.8654\n2.2053\n9.0632\n7.6973\n8.4055\n13.2312\n3.3747\n8.1627\n2.8245\n8.4075\n20.2899\n1.7553\n7.1383\n1.1577\n0.3316\n0.5672\n0.6693\n0.3901\n0.6695\n0.4017\n0.3213\n0.4138\n0.8953\n0.2703\n0.4023\n0.5856\n0.5821\n0.5000\n0.3500\n13750\n3.7545\n2.1424\n9.0522\n7.6998\n8.3319\n13.5322\n3.3625\n8.1303\n2.8320\n8.1860\n20.4538\n1.7997\n7.0770\n1.2512\n0.3385\n0.5654\n0.6937\n0.3856\n0.6659\n0.4025\n0.3225\n0.4156\n0.8994\n0.2675\n0.4113\n0.5886\n0.5770\n0.5026\n0.3564\n14000\n3.715\n2.1826\n8.9396\n7.6755\n8.3111\n13.3344\n3.3331\n8.1973\n2.8760\n8.8218\n20.6306\n1.9014\n7.3386\n1.2366\n0.3256\n0.5675\n0.6903\n0.3846\n0.6706\n0.4032\n0.3300\n0.4426\n0.8876\n0.2647\n0.4082\n0.5874\n0.5770\n0.5030\n0.3627\n14250\n3.6348\n2.1393\n9.0032\n7.7843\n8.3942\n13.2654\n3.3368\n8.1124\n2.8874\n8.5999\n20.8261\n1.8395\n7.5311\n1.1380\n0.3339\n0.5642\n0.7175\n0.3780\n0.6600\n0.3965\n0.3229\n0.4305\n0.8915\n0.2714\n0.3914\n0.5816\n0.5732\n0.5009\n0.3691\n14500\n3.604\n2.1709\n8.9648\n7.7166\n8.4281\n13.5507\n3.3138\n8.1362\n2.8989\n8.3940\n20.6827\n1.9298\n7.3468\n1.2705\n0.3416\n0.5594\n0.7198\n0.3776\n0.6605\n0.3965\n0.3295\n0.4341\n0.8838\n0.2632\n0.3995\n0.5884\n0.5693\n0.5018\n0.3754\n14750\n3.5398\n2.1451\n9.0195\n7.7407\n8.4766\n13.6169\n3.2839\n8.1386\n2.9178\n8.2585\n21.1674\n1.9484\n7.5143\n1.2747\n0.3418\n0.5592\n0.6971\n0.3763\n0.6539\n0.4060\n0.3264\n0.4214\n0.8805\n0.2524\n0.3855\n0.5872\n0.5735\n0.4970\n0.3818\n15000\n3.7153\n2.1038\n8.9847\n7.7279\n8.2667\n13.4033\n3.2862\n8.1232\n2.8999\n8.3224\n20.9361\n1.9569\n7.3440\n1.3036\n0.3407\n0.5687\n0.7066\n0.3497\n0.6532\n0.4043\n0.3317\n0.4300\n0.8830\n0.2564\n0.3884\n0.6013\n0.5752\n0.4992\n0.3882\n15250\n3.752\n2.0765\n9.0293\n7.7275\n8.3168\n13.2782\n3.3105\n8.0286\n2.8404\n8.2745\n20.9582\n1.7465\n7.5436\n1.2770\n0.3313\n0.5710\n0.6945\n0.3742\n0.6699\n0.4022\n0.3327\n0.4277\n0.8903\n0.2640\n0.3872\n0.5821\n0.5662\n0.4995\n0.3945\n15500\n3.7794\n2.0409\n9.0264\n7.7501\n8.3903\n13.3169\n3.3058\n8.0128\n2.8463\n8.5307\n21.5663\n1.7179\n7.7448\n1.1827\n0.3286\n0.5684\n0.6961\n0.3692\n0.6743\n0.4031\n0.3267\n0.4267\n0.8927\n0.2549\n0.3777\n0.5810\n0.5686\n0.4975\n0.4009\n15750\n3.7444\n2.0122\n9.0629\n7.7541\n8.0134\n13.2091\n3.2956\n8.0839\n2.8528\n8.1722\n20.9378\n1.7628\n7.8655\n1.2965\n0.3282\n0.5744\n0.6839\n0.3807\n0.6644\n0.4032\n0.3277\n0.4451\n0.8896\n0.2706\n0.3916\n0.5851\n0.5685\n0.5010\n0.4073\n16000\n3.7817\n2.0448\n9.1787\n7.7705\n8.0529\n13.1694\n3.3128\n8.1419\n2.8104\n8.2099\n21.0454\n1.7436\n7.2934\n1.2463\n0.3137\n0.5640\n0.6884\n0.3692\n0.6600\n0.3978\n0.3215\n0.4314\n0.8937\n0.2719\n0.4094\n0.6031\n0.5697\n0.4995\n0.4136\n16250\n3.7293\n2.0586\n9.2379\n7.7514\n8.1877\n13.1981\n3.2983\n8.0763\n2.8564\n8.6500\n20.9279\n1.8403\n7.2051\n1.1732\n0.3405\n0.5648\n0.6944\n0.3620\n0.6614\n0.3953\n0.3286\n0.4245\n0.8921\n0.2698\n0.3946\n0.5915\n0.5770\n0.4997\n0.4200\n16500\n3.6243\n2.0477\n9.1718\n7.6943\n7.9493\n13.3019\n3.2908\n8.0963\n2.8306\n8.7436\n20.6790\n1.8745\n7.4356\n1.1781\n0.3272\n0.5657\n0.6944\n0.3726\n0.6848\n0.3974\n0.3318\n0.4274\n0.8939\n0.2636\n0.3948\n0.5950\n0.5746\n0.5018\n0.4263\n16750\n3.5071\n2.0483\n9.2054\n7.7004\n8.1887\n13.3662\n3.2727\n7.9229\n2.8256\n8.2771\n21.1584\n1.8469\n7.6476\n1.2131\n0.3318\n0.5660\n0.7000\n0.3801\n0.6902\n0.3938\n0.3224\n0.4191\n0.8969\n0.2643\n0.4052\n0.6106\n0.5684\n0.5038\n0.4327\n17000\n3.6337\n2.0383\n9.1228\n7.7337\n8.2262\n13.2250\n3.2714\n7.9983\n2.7662\n8.4949\n20.8407\n1.8184\n7.7876\n1.2807\n0.3251\n0.5595\n0.6957\n0.3897\n0.6697\n0.3933\n0.3123\n0.4334\n0.8934\n0.2663\n0.3935\n0.5931\n0.5700\n0.4996\n0.4391\n17250\n3.5075\n2.0327\n8.9777\n7.7194\n8.2392\n13.4002\n3.2678\n7.9239\n2.7551\n8.2470\n21.1674\n1.7744\n7.9402\n1.3115\n0.3418\n0.5595\n0.7123\n0.3790\n0.6625\n0.3978\n0.3174\n0.4232\n0.8866\n0.2683\n0.3926\n0.5876\n0.5764\n0.5004\n0.4454\n17500\n3.6595\n2.0419\n8.9509\n7.6536\n8.3099\n13.3537\n3.2766\n7.9939\n2.7604\n8.3880\n20.8993\n1.8358\n7.6156\n1.2238\n0.3311\n0.5643\n0.7049\n0.3564\n0.6686\n0.3932\n0.3099\n0.4350\n0.8872\n0.2687\n0.3826\n0.5854\n0.5665\n0.4964\n0.4518\n17750\n3.5743\n2.0049\n9.0019\n7.6693\n8.3489\n13.3261\n3.2758\n8.0051\n2.7881\n8.4247\n20.8115\n1.8714\n7.7491\n1.1884\n0.3341\n0.5614\n0.6972\n0.3571\n0.6625\n0.3893\n0.3036\n0.4368\n0.8911\n0.2625\n0.3861\n0.5734\n0.5681\n0.4941\n0.4582\n18000\n3.6038\n1.9810\n9.0106\n7.7162\n8.3584\n13.1195\n3.2674\n8.0266\n2.8184\n8.4383\n20.5199\n1.8854\n7.8663\n1.1762\n0.3318\n0.5583\n0.7086\n0.3591\n0.6509\n0.3878\n0.3137\n0.4247\n0.8831\n0.2601\n0.3921\n0.5978\n0.5648\n0.4948\n0.4645\n18250\n3.6903\n2.0005\n9.0187\n7.6743\n8.4280\n13.0108\n3.2733\n7.8845\n2.7810\n8.3511\n20.1457\n1.7802\n8.0015\n1.0885\n0.3407\n0.5657\n0.7033\n0.3626\n0.6644\n0.3841\n0.3299\n0.4358\n0.8844\n0.2642\n0.3904\n0.5871\n0.5632\n0.4981\n0.4709\n18500\n3.7208\n1.9972\n9.1020\n7.6472\n8.1589\n13.0717\n3.2601\n8.0039\n2.7673\n8.3361\n20.0231\n1.8054\n7.7381\n1.1832\n0.3328\n0.5652\n0.7043\n0.3473\n0.6693\n0.3810\n0.3313\n0.4293\n0.8770\n0.2633\n0.3946\n0.5914\n0.5634\n0.4962\n0.4773\n18750\n3.6357\n2.0069\n9.1473\n7.6843\n8.2110\n13.1578\n3.2540\n7.9856\n2.7390\n8.6913\n20.3263\n1.8252\n7.9545\n1.0354\n0.3285\n0.5631\n0.7093\n0.3648\n0.6685\n0.3842\n0.3285\n0.4361\n0.8918\n0.2744\n0.4065\n0.5814\n0.5610\n0.4998\n0.4836\n19000\n3.5737\n1.9755\n9.1397\n7.6784\n8.2604\n13.3462\n3.2391\n7.9876\n2.7643\n8.4540\n20.2047\n1.7528\n7.6572\n1.0906\n0.3284\n0.5631\n0.7083\n0.3657\n0.6660\n0.3795\n0.3186\n0.4393\n0.8876\n0.2613\n0.4056\n0.5835\n0.5637\n0.4977\n0.4900\n19250\n3.5325\n2.0232\n9.1987\n7.6517\n8.2727\n13.1941\n3.2276\n7.9841\n2.7238\n8.4698\n19.9076\n1.8015\n7.2144\n1.1134\n0.3337\n0.5635\n0.7076\n0.3570\n0.6572\n0.3891\n0.3297\n0.4360\n0.8864\n0.2689\n0.3997\n0.5813\n0.5608\n0.4978\n0.4963\n19500\n3.4782\n2.0033\n9.1235\n7.7026\n8.3383\n13.1817\n3.2308\n8.0122\n2.6934\n8.4448\n19.7121\n1.7192\n6.9417\n0.9934\n0.3222\n0.5656\n0.7009\n0.3562\n0.6654\n0.3792\n0.3292\n0.4544\n0.8803\n0.2703\n0.4027\n0.5892\n0.5611\n0.4982\n0.5027\n19750\n3.7141\n1.9830\n9.2166\n7.6724\n8.3188\n13.1514\n3.2465\n8.0979\n2.7005\n8.1956\n19.8289\n1.6910\n7.1995\n1.0668\n0.3323\n0.5632\n0.7209\n0.3442\n0.6745\n0.3864\n0.3287\n0.4255\n0.8828\n0.2751\n0.4097\n0.5785\n0.5597\n0.4986\n0.5091\n20000\n3.7058\n1.9625\n9.1729\n7.6554\n8.3323\n13.0269\n3.2368\n8.0075\n2.7159\n8.6974\n20.1650\n1.6862\n7.3387\n1.1706\n0.3414\n0.5637\n0.7214\n0.3498\n0.6745\n0.3817\n0.3309\n0.4290\n0.8861\n0.2711\n0.3861\n0.5854\n0.5757\n0.4998\n0.5154\n20250\n3.502\n1.9983\n9.1304\n7.6354\n8.3832\n13.2376\n3.2262\n7.9229\n2.7119\n8.7638\n20.0759\n1.6633\n6.9686\n1.1490\n0.3296\n0.5703\n0.7116\n0.3409\n0.6717\n0.3847\n0.3294\n0.4198\n0.8859\n0.2655\n0.3924\n0.5836\n0.5647\n0.4962\n0.5218\n20500\n3.6424\n1.9867\n9.0935\n7.6470\n8.4280\n13.0866\n3.2338\n7.9608\n2.7225\n8.4571\n20.1605\n1.6184\n6.8877\n1.1519\n0.3377\n0.5682\n0.7128\n0.3479\n0.6646\n0.3900\n0.3255\n0.4408\n0.8863\n0.2655\n0.4184\n0.5862\n0.5627\n0.5005\n0.5282\n20750\n3.6085\n1.9589\n9.1559\n7.6282\n8.3675\n13.1289\n3.2393\n8.0050\n2.7199\n8.3705\n20.4945\n1.6721\n6.8649\n1.2063\n0.3328\n0.5719\n0.7054\n0.3526\n0.6755\n0.3974\n0.3280\n0.4204\n0.8926\n0.2686\n0.4145\n0.5783\n0.5553\n0.4995\n0.5345\n21000\n3.5763\n1.9392\n9.1139\n7.6917\n8.2745\n13.3345\n3.2333\n7.9848\n2.7010\n8.4815\n20.5463\n1.7049\n7.0751\n1.1276\n0.3251\n0.5710\n0.7128\n0.3561\n0.6642\n0.4022\n0.3225\n0.4394\n0.8997\n0.2737\n0.4106\n0.5732\n0.5592\n0.5008\n0.5409\n21250\n3.5401\n1.9744\n9.0583\n7.5955\n8.2868\n13.2877\n3.2309\n7.9686\n2.6641\n8.2829\n20.3974\n1.6843\n6.9287\n1.0128\n0.3322\n0.5692\n0.7117\n0.3348\n0.6715\n0.3834\n0.3269\n0.4372\n0.8869\n0.2662\n0.4097\n0.5797\n0.5587\n0.4975\n0.5473\n21500\n3.489\n1.9417\n8.9543\n7.6468\n8.3612\n13.3847\n3.2314\n7.9631\n2.6635\n8.4673\n20.5367\n1.7459\n6.7037\n1.0989\n0.3328\n0.5741\n0.7097\n0.3508\n0.6660\n0.3916\n0.3277\n0.4391\n0.8860\n0.2632\n0.4100\n0.5910\n0.5578\n0.5000\n0.5536\n21750\n3.555\n1.9533\n8.9916\n7.6360\n8.3586\n13.3764\n3.2284\n7.9575\n2.7214\n8.3429\n20.5891\n1.7569\n6.6890\n1.1157\n0.3339\n0.5744\n0.7114\n0.3392\n0.6636\n0.3964\n0.3259\n0.4504\n0.8925\n0.2651\n0.4031\n0.5792\n0.5646\n0.5000\n0.5600\n22000\n3.586\n1.9301\n8.9806\n7.6738\n8.3535\n13.2453\n3.2316\n7.9946\n2.6907\n8.2869\n20.5606\n1.6573\n6.8912\n1.1068\n0.3239\n0.5788\n0.7121\n0.3537\n0.6589\n0.4023\n0.3243\n0.4499\n0.8891\n0.2644\n0.4083\n0.5802\n0.5616\n0.5006\n0.5663\n22250\n3.5084\n1.9406\n9.0040\n7.6810\n8.3815\n13.1792\n3.2167\n7.9700\n2.7316\n8.6409\n20.6425\n1.6145\n6.8099\n1.1404\n0.3174\n0.5772\n0.7177\n0.3519\n0.6613\n0.3950\n0.3324\n0.4362\n0.8894\n0.2648\n0.4173\n0.5857\n0.5626\n0.5007\n0.5727\n22500\n3.5095\n1.9202\n8.9619\n7.6717\n8.3825\n13.1359\n3.2147\n7.9776\n2.7114\n8.2541\n20.5107\n1.6855\n6.9227\n1.1935\n0.3335\n0.5726\n0.7134\n0.3574\n0.6578\n0.3959\n0.3262\n0.4401\n0.8968\n0.2604\n0.4105\n0.5826\n0.5620\n0.5007\n0.5791\n22750\n3.5059\n1.9225\n8.9956\n7.6775\n8.3968\n13.0329\n3.2114\n7.9836\n2.7107\n8.4843\n20.6793\n1.6821\n7.0987\n1.0579\n0.3273\n0.5703\n0.7118\n0.3689\n0.6490\n0.3850\n0.3257\n0.4320\n0.8824\n0.2622\n0.4119\n0.5898\n0.5605\n0.4982\n0.5854\n23000\n3.4047\n1.9716\n9.0017\n7.6435\n8.4379\n13.0467\n3.1957\n7.9843\n2.7017\n8.5995\n20.8783\n1.5818\n7.1997\n1.0067\n0.3272\n0.5718\n0.7132\n0.3728\n0.6544\n0.3913\n0.3255\n0.4377\n0.8869\n0.2583\n0.4107\n0.5916\n0.5539\n0.4996\n0.5918\n23250\n3.4732\n1.9518\n8.9827\n7.6143\n8.3925\n13.2800\n3.1877\n7.9616\n2.7122\n8.5013\n21.0376\n1.6291\n7.0831\n1.0816\n0.3316\n0.5689\n0.7129\n0.3705\n0.6536\n0.3847\n0.3211\n0.4495\n0.8844\n0.2622\n0.4141\n0.5916\n0.5551\n0.5000\n0.5982\n23500\n3.4271\n1.9688\n9.0092\n7.5830\n8.3763\n13.2765\n3.1857\n7.9625\n2.6794\n8.4899\n20.8080\n1.6519\n7.1604\n1.1423\n0.3346\n0.5716\n0.7054\n0.3637\n0.6562\n0.3844\n0.3249\n0.4346\n0.8912\n0.2577\n0.4009\n0.5872\n0.5697\n0.4986\n0.6045\n23750\n3.4701\n2.0238\n9.0036\n7.5173\n8.4058\n13.2881\n3.1791\n7.9275\n2.7149\n8.8465\n20.6630\n1.7025\n7.2286\n1.1973\n0.3308\n0.5670\n0.7040\n0.3754\n0.6609\n0.3936\n0.3252\n0.4475\n0.8948\n0.2542\n0.4062\n0.5877\n0.5717\n0.5015\n0.6109\n24000\n3.6199\n2.0084\n8.9869\n7.5146\n8.3790\n13.1350\n3.1771\n7.9137\n2.7032\n8.5424\n20.5441\n1.7652\n6.8017\n1.1617\n0.3224\n0.5656\n0.7045\n0.3643\n0.6643\n0.3864\n0.3174\n0.4577\n0.8873\n0.2571\n0.3735\n0.5939\n0.5611\n0.4966\n0.6173\n24250\n3.408\n2.0137\n9.0355\n7.5305\n8.3597\n13.1604\n3.1735\n7.9201\n2.7078\n9.0787\n20.5226\n1.7341\n6.7525\n1.0237\n0.3310\n0.5676\n0.7061\n0.3643\n0.6602\n0.3995\n0.3240\n0.4588\n0.8936\n0.2572\n0.3868\n0.5928\n0.5718\n0.5011\n0.6236\n24500\n3.4651\n1.9564\n8.9636\n7.5635\n8.3731\n13.3261\n3.1737\n7.9529\n2.6638\n8.5231\n20.4787\n1.7792\n6.7317\n1.0516\n0.3291\n0.5670\n0.7060\n0.3572\n0.6594\n0.3982\n0.3253\n0.4400\n0.8917\n0.2577\n0.3886\n0.5951\n0.5662\n0.4986\n0.6300\n24750\n3.6988\n1.9668\n8.9583\n7.5875\n8.4316\n12.9644\n3.1769\n7.9801\n2.6750\n8.5780\n20.3883\n1.7045\n6.9010\n1.0783\n0.3300\n0.5668\n0.7040\n0.3653\n0.6550\n0.3957\n0.3293\n0.4570\n0.8878\n0.2621\n0.3816\n0.5932\n0.5665\n0.4996\n0.6363\n25000\n3.4365\n1.9782\n8.9306\n7.5829\n8.4123\n13.0937\n3.1640\n7.9870\n2.6806\n8.6281\n20.2045\n1.7244\n6.9685\n1.0365\n0.3289\n0.5651\n0.6956\n0.3703\n0.6494\n0.4025\n0.3294\n0.4511\n0.8841\n0.2634\n0.3894\n0.5970\n0.5605\n0.4990\n0.6427\n25250\n3.6097\n1.9653\n8.9386\n7.5722\n8.4185\n13.0207\n3.1629\n7.9910\n2.6867\n8.7326\n20.2092\n1.7321\n6.8303\n1.0569\n0.3273\n0.5643\n0.7024\n0.3587\n0.6534\n0.4018\n0.3282\n0.4401\n0.8872\n0.2614\n0.3860\n0.5966\n0.5703\n0.4983\n0.6491\n25500\n3.5379\n1.9518\n8.9189\n7.5241\n8.4156\n13.0924\n3.1557\n7.9747\n2.6468\n8.6005\n20.3848\n1.7474\n6.9006\n1.0193\n0.3186\n0.5692\n0.7032\n0.3581\n0.6429\n0.4068\n0.3288\n0.4426\n0.8937\n0.2584\n0.4055\n0.5963\n0.5667\n0.4993\n0.6554\n25750\n3.6223\n1.9609\n8.9459\n7.5554\n8.4227\n12.8817\n3.1483\n7.9813\n2.6559\n8.8423\n20.3294\n1.6381\n6.8535\n1.0140\n0.3252\n0.5722\n0.7104\n0.3689\n0.6554\n0.4087\n0.3291\n0.4293\n0.8894\n0.2632\n0.3940\n0.5995\n0.5702\n0.5012\n0.6618\n26000\n3.402\n1.9602\n8.9366\n7.5236\n8.2409\n12.9294\n3.1424\n7.9619\n2.6323\n8.6640\n20.2959\n1.6627\n6.7331\n1.0115\n0.3276\n0.5676\n0.7031\n0.3652\n0.6493\n0.4030\n0.3263\n0.4298\n0.8830\n0.2655\n0.4030\n0.6017\n0.5724\n0.4998\n0.6682\n26250\n3.4876\n1.9929\n8.9621\n7.4947\n8.1977\n12.8814\n3.1350\n7.8663\n2.6554\n8.5629\n20.3156\n1.7019\n6.7602\n0.9582\n0.3253\n0.5639\n0.7127\n0.3621\n0.6544\n0.4146\n0.3278\n0.4350\n0.8895\n0.2613\n0.4056\n0.5997\n0.5635\n0.5012\n0.6745\n26500\n3.4301\n1.9752\n8.9474\n7.5246\n8.1379\n12.7941\n3.1369\n7.8999\n2.6256\n8.5223\n20.4417\n1.6627\n6.7999\n0.9626\n0.3232\n0.5661\n0.7094\n0.3719\n0.6474\n0.4077\n0.3299\n0.4533\n0.8849\n0.2620\n0.3993\n0.6008\n0.5665\n0.5017\n0.6809\n26750\n3.482\n1.9646\n8.9353\n7.5199\n8.2066\n12.6512\n3.1284\n7.9080\n2.6464\n8.6207\n20.4063\n1.6852\n6.7490\n1.0089\n0.3172\n0.5689\n0.7121\n0.3734\n0.6461\n0.4089\n0.3287\n0.4516\n0.8854\n0.2601\n0.3965\n0.5993\n0.5634\n0.5009\n0.6873\n27000\n3.5073\n1.9431\n8.9478\n7.5188\n8.0549\n12.6486\n3.1310\n7.9241\n2.6508\n8.5172\n20.4459\n1.6855\n6.8390\n1.0012\n0.3222\n0.5705\n0.6936\n0.3675\n0.6511\n0.3970\n0.3286\n0.4467\n0.8875\n0.2587\n0.3986\n0.6177\n0.5690\n0.5007\n0.6936\n27250\n3.5565\n1.9438\n8.9610\n7.5044\n8.0383\n12.4879\n3.1277\n7.9219\n2.6321\n8.4003\n20.6229\n1.7421\n6.7256\n1.0533\n0.3327\n0.5737\n0.6955\n0.3653\n0.6486\n0.4067\n0.3277\n0.4368\n0.8901\n0.2589\n0.3877\n0.6135\n0.5696\n0.5005\n0.7000\n27500\n3.4506\n1.9639\n8.9673\n7.4644\n8.1455\n12.4523\n3.1171\n7.9159\n2.6772\n8.5339\n20.7734\n1.7690\n6.6677\n1.0708\n0.3260\n0.5730\n0.6960\n0.3562\n0.6419\n0.4053\n0.3310\n0.4395\n0.8871\n0.2594\n0.4010\n0.6155\n0.5679\n0.5000\n0.7063\n27750\n3.4875\n1.9177\n8.9276\n7.4754\n8.1375\n12.5163\n3.1315\n7.8697\n2.6247\n8.4426\n20.4950\n1.7047\n6.6303\n1.0030\n0.3244\n0.5690\n0.6887\n0.3499\n0.6471\n0.4057\n0.3309\n0.4464\n0.8893\n0.2570\n0.4038\n0.6038\n0.5701\n0.4989\n0.7127\n28000\n3.5298\n1.8939\n8.9022\n7.4889\n8.1322\n12.5461\n3.1409\n7.8621\n2.5963\n8.4173\n20.4948\n1.7078\n6.4516\n0.9874\n0.3380\n0.5721\n0.6934\n0.3631\n0.6439\n0.4046\n0.3272\n0.4486\n0.8893\n0.2593\n0.4020\n0.6017\n0.5687\n0.5009\n0.7191\n28250\n3.3329\n1.8970\n8.9172\n7.5001\n8.1758\n12.5515\n3.1333\n7.9075\n2.5928\n8.5025\n20.3447\n1.6879\n6.5795\n0.9658\n0.3247\n0.5729\n0.7000\n0.3710\n0.6468\n0.4069\n0.3336\n0.4564\n0.8933\n0.2637\n0.4105\n0.6012\n0.5749\n0.5043\n0.7254\n28500\n3.3897\n1.8966\n8.9038\n7.5265\n8.2199\n12.6173\n3.1244\n7.8882\n2.5974\n8.3728\n20.3628\n1.6804\n6.6887\n0.9733\n0.3278\n0.5750\n0.7001\n0.3645\n0.6528\n0.4145\n0.3310\n0.4669\n0.8964\n0.2638\n0.4032\n0.6086\n0.5713\n0.5058\n0.7318\n28750\n3.4588\n1.8934\n8.8948\n7.5026\n8.2374\n12.4866\n3.1287\n7.8941\n2.5811\n8.3781\n20.4334\n1.7109\n6.6195\n0.9699\n0.3251\n0.5707\n0.7083\n0.3702\n0.6411\n0.4086\n0.3252\n0.4553\n0.8935\n0.2641\n0.4068\n0.6063\n0.5592\n0.5026\n0.7382\n29000\n3.3675\n1.8959\n8.8925\n7.5043\n8.2628\n12.6063\n3.1174\n7.9132\n2.5908\n8.2436\n20.3771\n1.6740\n6.7151\n0.9895\n0.3262\n0.5725\n0.7069\n0.3694\n0.6495\n0.4063\n0.3169\n0.4622\n0.8962\n0.2609\n0.4124\n0.6101\n0.5635\n0.5041\n0.7445\n29250\n3.3886\n1.8976\n8.9215\n7.4975\n8.2401\n12.4978\n3.1127\n7.8615\n2.5814\n8.2715\n20.4379\n1.6740\n6.7705\n0.9492\n0.3269\n0.5683\n0.7061\n0.3760\n0.6554\n0.4089\n0.3209\n0.4508\n0.8982\n0.2658\n0.3982\n0.6053\n0.5668\n0.5037\n0.7509\n29500\n3.4826\n1.8855\n8.9320\n7.5047\n8.2470\n12.5056\n3.1168\n7.8967\n2.5742\n8.3762\n20.4917\n1.7122\n6.3650\n0.9882\n0.3380\n0.5681\n0.7061\n0.3727\n0.6521\n0.4071\n0.3222\n0.4771\n0.8963\n0.2701\n0.3943\n0.6010\n0.5624\n0.5052\n0.7572\n29750\n3.4268\n1.8725\n8.9332\n7.5158\n8.2481\n12.4708\n3.1146\n7.9010\n2.5617\n8.2478\n20.3435\n1.6860\n6.4246\n0.9801\n0.3331\n0.5691\n0.7061\n0.3767\n0.6523\n0.4068\n0.3195\n0.4589\n0.9097\n0.2627\n0.4113\n0.6059\n0.5641\n0.5059\n0.7636\n30000\n3.2621\n1.8813\n8.9125\n7.5203\n8.2736\n12.5555\n3.1094\n7.9083\n2.5488\n8.3690\n20.3392\n1.7015\n6.5219\n0.9901\n0.3366\n0.5686\n0.6941\n0.3696\n0.6465\n0.4062\n0.3228\n0.4570\n0.8967\n0.2657\n0.3975\n0.6054\n0.5702\n0.5028\n0.7700\n30250\n3.3289\n1.8893\n8.9211\n7.5322\n8.1753\n12.4890\n3.1063\n7.9272\n2.5320\n8.4628\n20.3169\n1.6841\n6.5986\n0.9798\n0.3302\n0.5719\n0.6944\n0.3745\n0.6471\n0.4007\n0.3172\n0.4741\n0.9046\n0.2685\n0.4126\n0.6014\n0.5666\n0.5049\n0.7763\n30500\n3.5363\n1.8836\n8.8909\n7.5323\n8.2197\n12.4000\n3.1064\n7.9088\n2.5494\n8.3271\n20.3110\n1.7132\n6.4502\n0.9974\n0.3298\n0.5705\n0.6995\n0.3671\n0.6511\n0.4057\n0.3204\n0.4560\n0.8949\n0.2613\n0.4153\n0.6011\n0.5738\n0.5036\n0.7827\n30750\n3.3557\n1.8824\n8.9002\n7.5249\n8.2047\n12.4618\n3.1055\n7.9265\n2.5501\n8.2708\n20.2254\n1.7222\n6.4927\n0.9813\n0.3333\n0.5678\n0.6926\n0.3781\n0.6575\n0.4005\n0.3225\n0.4497\n0.8991\n0.2649\n0.4018\n0.6051\n0.5698\n0.5033\n0.7891\n31000\n3.4481\n1.8725\n8.9077\n7.5043\n8.2095\n12.5197\n3.1095\n7.9124\n2.5216\n8.1396\n20.0618\n1.6962\n6.4808\n0.9764\n0.3321\n0.5691\n0.6941\n0.3650\n0.6464\n0.4013\n0.3239\n0.4522\n0.9035\n0.2657\n0.4058\n0.6041\n0.5639\n0.5021\n0.7954\n31250\n3.3888\n1.8596\n8.9249\n7.5248\n8.2429\n12.4622\n3.1054\n7.9247\n2.5351\n8.3011\n19.9880\n1.7062\n6.5598\n0.9628\n0.3428\n0.5694\n0.7008\n0.3761\n0.6564\n0.4013\n0.3223\n0.4589\n0.9024\n0.2633\n0.4111\n0.6052\n0.5670\n0.5059\n0.8018\n31500\n3.3989\n1.8662\n8.9299\n7.5206\n8.1957\n12.4667\n3.1043\n7.9089\n2.5465\n8.2149\n19.8897\n1.6830\n6.6131\n0.9204\n0.3452\n0.5658\n0.7002\n0.3709\n0.6570\n0.4004\n0.3233\n0.4588\n0.9015\n0.2619\n0.4044\n0.6015\n0.5654\n0.5043\n0.8082\n31750\n3.3603\n1.8671\n8.9158\n7.5135\n8.1945\n12.5201\n3.1009\n7.9130\n2.5539\n8.3876\n19.9818\n1.6685\n6.6359\n0.9307\n0.3516\n0.5659\n0.6928\n0.3767\n0.6534\n0.4004\n0.3245\n0.4627\n0.9021\n0.2619\n0.4252\n0.6019\n0.5699\n0.5068\n0.8145\n32000\n3.3783\n1.8588\n8.8993\n7.4994\n8.2058\n12.5200\n3.0980\n7.8902\n2.5448\n8.3444\n19.9960\n1.6815\n6.7156\n0.9509\n0.3358\n0.5665\n0.7012\n0.3759\n0.6551\n0.4002\n0.3259\n0.4505\n0.8960\n0.2621\n0.4113\n0.6059\n0.5716\n0.5045\n0.8209\n32250\n3.4379\n1.8693\n8.8896\n7.4639\n8.0939\n12.5840\n3.0894\n7.8559\n2.5590\n8.4227\n19.9565\n1.6855\n6.6835\n0.9650\n0.3433\n0.5663\n0.7004\n0.3717\n0.6527\n0.4076\n0.3255\n0.4491\n0.8990\n0.2643\n0.4127\n0.5984\n0.5741\n0.5050\n0.8272\n32500\n3.4962\n1.8838\n8.8634\n7.4636\n8.1354\n12.5144\n3.0866\n7.8561\n2.5410\n8.4169\n20.0794\n1.6625\n6.6897\n0.9849\n0.3461\n0.5678\n0.7024\n0.3808\n0.6564\n0.4083\n0.3200\n0.4531\n0.9015\n0.2622\n0.4116\n0.6061\n0.5685\n0.5065\n0.8336\n32750\n3.4927\n1.8696\n8.8655\n7.4274\n8.0888\n12.5431\n3.0874\n7.8320\n2.5324\n8.3181\n20.0095\n1.6403\n6.7182\n0.9837\n0.3439\n0.5657\n0.7078\n0.3771\n0.6580\n0.4078\n0.3210\n0.4543\n0.9087\n0.2625\n0.4184\n0.6095\n0.5691\n0.5080\n0.8400\n33000\n3.33\n1.8743\n8.8776\n7.4438\n8.1127\n12.5571\n3.0793\n7.8474\n2.5423\n8.3195\n20.1064\n1.6546\n6.7761\n1.0118\n0.3400\n0.5684\n0.6938\n0.3760\n0.6560\n0.4077\n0.3248\n0.4586\n0.8965\n0.2613\n0.4085\n0.6094\n0.5704\n0.5055\n0.8463\n33250\n3.3431\n1.8687\n8.8074\n7.4577\n8.1313\n12.5392\n3.0816\n7.8629\n2.5306\n8.3183\n20.1216\n1.6653\n6.8390\n1.0199\n0.3233\n0.5685\n0.7031\n0.3770\n0.6493\n0.4075\n0.3243\n0.4586\n0.9015\n0.2600\n0.4068\n0.6050\n0.5659\n0.5039\n0.8527\n33500\n3.4455\n1.8618\n8.8108\n7.4587\n8.1421\n12.5672\n3.0800\n7.8533\n2.5364\n8.2589\n20.0766\n1.6537\n6.8825\n1.0161\n0.3346\n0.5647\n0.7003\n0.3761\n0.6523\n0.4075\n0.3239\n0.4598\n0.9088\n0.2641\n0.4134\n0.5976\n0.5702\n0.5056\n0.8591\n33750\n3.3189\n1.8578\n8.8046\n7.4684\n8.1488\n12.6076\n3.0817\n7.8637\n2.5249\n8.2008\n20.1733\n1.6486\n6.9228\n1.0004\n0.3428\n0.5684\n0.7019\n0.3782\n0.6483\n0.4049\n0.3229\n0.4525\n0.9039\n0.2641\n0.4088\n0.6114\n0.5669\n0.5058\n0.8654\n34000\n3.3815\n1.8587\n8.7921\n7.4675\n8.0968\n12.6359\n3.0800\n7.8680\n2.5266\n8.3492\n20.1037\n1.6352\n6.8485\n1.0061\n0.3412\n0.5655\n0.6945\n0.3806\n0.6510\n0.4061\n0.3178\n0.4583\n0.9085\n0.2640\n0.4078\n0.6105\n0.5689\n0.5058\n0.8718\n34250\n3.3381\n1.8586\n8.7828\n7.4672\n8.1115\n12.6341\n3.0783\n7.8743\n2.5275\n8.3363\n20.0616\n1.6445\n6.8898\n1.0146\n0.3255\n0.5645\n0.6941\n0.3784\n0.6488\n0.4027\n0.3190\n0.4518\n0.9039\n0.2637\n0.4079\n0.6115\n0.5730\n0.5034\n0.8782\n34500\n3.3992\n1.8597\n8.7906\n7.4658\n8.1316\n12.6647\n3.0781\n7.8643\n2.5249\n8.3280\n20.0237\n1.6348\n6.8277\n1.0203\n0.3283\n0.5645\n0.6925\n0.3744\n0.6525\n0.4027\n0.3250\n0.4593\n0.9040\n0.2625\n0.4097\n0.6121\n0.5676\n0.5042\n0.8845\n34750\n3.2951\n1.8608\n8.8033\n7.4729\n8.1063\n12.6896\n3.0719\n7.8671\n2.5243\n8.3368\n20.0587\n1.6455\n6.7907\n1.0106\n0.3274\n0.5651\n0.6925\n0.3738\n0.6527\n0.4045\n0.3219\n0.4592\n0.9041\n0.2619\n0.3985\n0.6120\n0.5679\n0.5032\n0.8909\n35000\n3.262\n1.8677\n8.8024\n7.4317\n8.1170\n12.7225\n3.0671\n7.8519\n2.5303\n8.4655\n20.1156\n1.6405\n6.7997\n1.0141\n0.3288\n0.5667\n0.6935\n0.3738\n0.6554\n0.4049\n0.3196\n0.4532\n0.9041\n0.2626\n0.4116\n0.6123\n0.5653\n0.5040\n0.8972\n35250\n3.3218\n1.8698\n8.7992\n7.4495\n8.1390\n12.7198\n3.0627\n7.8528\n2.5399\n8.4549\n20.1652\n1.6288\n6.7857\n1.0261\n0.3328\n0.5666\n0.6951\n0.3686\n0.6538\n0.4043\n0.3248\n0.4596\n0.9042\n0.2646\n0.4097\n0.6123\n0.5725\n0.5053\n0.9036\n35500\n3.3529\n1.8633\n8.7964\n7.4339\n8.1347\n12.7295\n3.0657\n7.8262\n2.5256\n8.4119\n20.1363\n1.6165\n6.7987\n1.0260\n0.3268\n0.5660\n0.6951\n0.3695\n0.6543\n0.4042\n0.3232\n0.4511\n0.9068\n0.2666\n0.4047\n0.6112\n0.5701\n0.5038\n0.9100\n35750\n3.2205\n1.8626\n8.7751\n7.4335\n8.1346\n12.7532\n3.0654\n7.8318\n2.5077\n8.3883\n20.0738\n1.5839\n6.7819\n1.0282\n0.3291\n0.5659\n0.6951\n0.3702\n0.6530\n0.4045\n0.3241\n0.4493\n0.9067\n0.2622\n0.4192\n0.6112\n0.5710\n0.5047\n0.9163\n36000\n3.3671\n1.8521\n8.7693\n7.4500\n8.1155\n12.8041\n3.0671\n7.8451\n2.4994\n8.3053\n20.0666\n1.5984\n6.7696\n1.0377\n0.3383\n0.5667\n0.6946\n0.3739\n0.6536\n0.4049\n0.3262\n0.4457\n0.9065\n0.2625\n0.4143\n0.6112\n0.5719\n0.5054\n0.9227\n36250\n3.4074\n1.8555\n8.7683\n7.4607\n8.1465\n12.7545\n3.0621\n7.8488\n2.5080\n8.3816\n20.0420\n1.6043\n6.7991\n1.0539\n0.3367\n0.5685\n0.6935\n0.3714\n0.6523\n0.4068\n0.3237\n0.4471\n0.9065\n0.2645\n0.4176\n0.6110\n0.5692\n0.5053\n0.9291\n36500\n3.4806\n1.8543\n8.7833\n7.4584\n8.1070\n12.7444\n3.0605\n7.8475\n2.5166\n8.3809\n19.3340\n1.6181\n6.7663\n1.0555\n0.3271\n0.5693\n0.6935\n0.3733\n0.6560\n0.3990\n0.3229\n0.4530\n0.9065\n0.2618\n0.4105\n0.6129\n0.5654\n0.5039\n0.9354\n36750\n3.4848\n1.8558\n8.7967\n7.4512\n8.1175\n12.6960\n3.0616\n7.8420\n2.5137\n8.4267\n19.4008\n1.6181\n6.7376\n1.0629\n0.3284\n0.5697\n0.6922\n0.3723\n0.6530\n0.4048\n0.3234\n0.4534\n0.9065\n0.2641\n0.4088\n0.6115\n0.5635\n0.5040\n0.9418\n37000\n3.1947\n1.8574\n8.8026\n7.4555\n8.1222\n12.7155\n3.0591\n7.8391\n2.5180\n8.4551\n19.4094\n1.6134\n6.7200\n1.0657\n0.3318\n0.5681\n0.6922\n0.3711\n0.6550\n0.4043\n0.3234\n0.4534\n0.9037\n0.2645\n0.4090\n0.6113\n0.5685\n0.5043\n0.9482\n37250\n3.3557\n1.8569\n8.7972\n7.4535\n8.1200\n12.7427\n3.0585\n7.8460\n2.5167\n8.5049\n19.4395\n1.6141\n6.7080\n1.0700\n0.3308\n0.5699\n0.6930\n0.3708\n0.6513\n0.4048\n0.3247\n0.4589\n0.9039\n0.2645\n0.4087\n0.6113\n0.5677\n0.5046\n0.9545\n37500\n3.369\n1.8580\n8.7958\n7.4558\n8.1193\n12.7407\n3.0579\n7.8509\n2.5139\n8.5231\n19.4691\n1.6139\n6.7228\n1.0696\n0.3388\n0.5683\n0.6922\n0.3696\n0.6556\n0.4050\n0.3236\n0.4460\n0.8965\n0.2624\n0.4145\n0.6113\n0.5682\n0.5040\n0.9609\n37750\n3.398\n1.8564\n8.7955\n7.4567\n8.1174\n12.7478\n3.0589\n7.8415\n2.5086\n8.5030\n19.4719\n1.5989\n6.7117\n1.0689\n0.3381\n0.5678\n0.6922\n0.3652\n0.6544\n0.4049\n0.3234\n0.4460\n0.9039\n0.2599\n0.3998\n0.6113\n0.5658\n0.5025\n0.9672\n38000\n3.3699\n1.8534\n8.8011\n7.4604\n8.1135\n12.7689\n3.0574\n7.8412\n2.5097\n8.4866\n19.4973\n1.5966\n6.7309\n1.0655\n0.3372\n0.5680\n0.6922\n0.3651\n0.6553\n0.4038\n0.3224\n0.4560\n0.9025\n0.2675\n0.4061\n0.6111\n0.5712\n0.5045\n0.9736\n38250\n3.4483\n1.8540\n8.8045\n7.4506\n8.0942\n12.7725\n3.0569\n7.8379\n2.5135\n8.4817\n19.5038\n1.5995\n6.7289\n1.0693\n0.3378\n0.5682\n0.6922\n0.3724\n0.6550\n0.4038\n0.3243\n0.4534\n0.9025\n0.2645\n0.4058\n0.6111\n0.5710\n0.5048\n0.9800\n38500\n3.254\n1.8546\n8.8026\n7.4530\n8.1004\n12.7796\n3.0559\n7.8378\n2.5086\n8.4538\n19.5061\n1.5997\n6.7390\n1.0724\n0.3381\n0.5681\n0.6922\n0.3647\n0.6547\n0.4041\n0.3242\n0.4534\n0.8951\n0.2642\n0.4064\n0.6111\n0.5677\n0.5034\n0.9863\n38750\n3.2759\n1.8549\n8.8024\n7.4545\n8.0996\n12.7875\n3.0562\n7.8400\n2.5054\n8.4401\n19.5192\n1.6018\n6.7433\n1.0737\n0.3382\n0.5681\n0.6922\n0.3647\n0.6547\n0.4041\n0.3241\n0.4534\n0.8951\n0.2642\n0.4078\n0.6111\n0.5689\n0.5036\n0.9927\n39000\n3.3273\n1.8548\n8.8017\n7.4536\n8.0908\n12.7885\n3.0557\n7.8396\n2.5052\n8.4335\n19.5169\n1.6002\n6.7439\n1.0716\n0.3382\n0.5681\n0.6922\n0.3647\n0.6547\n0.4041\n0.3242\n0.4534\n0.8951\n0.2641\n0.4078\n0.6111\n0.5703\n0.5037\n0.9991\n39250\n3.3902\n1.8540\n8.8010\n7.4543\n8.0872\n12.7890\n3.0561\n7.8412\n2.5040\n8.3945\n19.5165\n1.5997\n6.7462\n1.0718\n0.3309\n0.5681\n0.6922\n0.3651\n0.6547\n0.4041\n0.3242\n0.4534\n0.8951\n0.2643\n0.4078\n0.6111\n0.5703\n0.5032\n1.0\n39287\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n0.3309\n0.5681\n0.6922\n0.3651\n0.6547\n0.4041\n0.3242\n0.4534\n0.8951\n0.2643\n0.4078\n0.6111\n0.5703\n0.5032\nEnvironmental Impact\nCarbon emissions were measured using CodeCarbon.\nEnergy Consumed: 2.611 kWh\nCarbon Emitted: 1.015 kg of CO2\nHours Used: 17.883 hours\nTraining Hardware\nOn Cloud: No\nGPU Model: 1 x NVIDIA GeForce RTX 3090\nCPU Model: 13th Gen Intel(R) Core(TM) i7-13700K\nRAM Size: 31.78 GB\nFramework Versions\nPython: 3.11.6\nSentence Transformers: 3.3.0.dev0\nTransformers: 4.45.2\nPyTorch: 2.5.0.dev20240807+cu121\nAccelerate: 1.0.0\nDatasets: 2.20.0\nTokenizers: 0.20.1-dev.0\nCitation\nBibTeX\nSentence Transformers\n@inproceedings{reimers-2019-sentence-bert,\ntitle = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\nauthor = \"Reimers, Nils and Gurevych, Iryna\",\nbooktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\nmonth = \"11\",\nyear = \"2019\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://arxiv.org/abs/1908.10084\",\n}\nMatryoshkaLoss\n@misc{kusupati2024matryoshka,\ntitle={Matryoshka Representation Learning},\nauthor={Aditya Kusupati and Gantavya Bhatt and Aniket Rege and Matthew Wallingford and Aditya Sinha and Vivek Ramanujan and William Howard-Snyder and Kaifeng Chen and Sham Kakade and Prateek Jain and Ali Farhadi},\nyear={2024},\neprint={2205.13147},\narchivePrefix={arXiv},\nprimaryClass={cs.LG}\n}\nMultipleNegativesRankingLoss\n@misc{henderson2017efficient,\ntitle={Efficient Natural Language Response Suggestion for Smart Reply},\nauthor={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},\nyear={2017},\neprint={1705.00652},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "dat-lequoc/Fast-Apply-1.5B-v1.0_GGUF": "Uploaded  model\nUploaded  model\nDeveloped by: quocdat25\nLicense: apache-2.0\nFinetuned from model : unsloth/Qwen2.5-Coder-1.5B-Instruct-bnb-4bit\nThis qwen2 model was trained 2x faster with Unsloth and Huggingface's TRL library.",
    "marcelbinz/Llama-3.1-Centaur-70B-adapter": "Model Summary:\nUsage:\nLicensing Information\nCitation Information\nModel Summary:\nLlama-3.1-Centaur-70B is a foundation model of cognition model that can predict and simulate human behavior in any behavioral experiment expressed in natural language.\nPaper: A foundation model to predict and capture human cognition\nMore Information: Documentation\nPoint of Contact: Marcel Binz\nUsage:\nNote that Centaur is trained on a data set in which human choices are encapsulated by \"<<\" and \">>\" tokens. For optimal performance, it is recommended to adjust prompts accordingly.\nThe recommended usage is by loading the low-rank adapter using unsloth:\nfrom unsloth import FastLanguageModel\nmodel_name = \"marcelbinz/Llama-3.1-Centaur-70B-adapter\"\nmodel, tokenizer = FastLanguageModel.from_pretrained(\nmodel_name = model_name,\nmax_seq_length = 32768,\ndtype = None,\nload_in_4bit = True,\n)\nFastLanguageModel.for_inference(model)\nThis requires 80 GB GPU memory. More details are provided in this example script.\nYou can alternatively also directly use the less-tested merged model.\nLicensing Information\nLlama 3.1 Community License Agreement\nCitation Information\n@misc{binz2024centaurfoundationmodelhuman,\ntitle={Centaur: a foundation model of human cognition},\nauthor={Marcel Binz and Elif Akata and Matthias Bethge and Franziska Br√§ndle and Fred Callaway and Julian Coda-Forno and Peter Dayan and Can Demircan and Maria K. Eckstein and No√©mi √âltet≈ë and Thomas L. Griffiths and Susanne Haridi and Akshay K. Jagadish and Li Ji-An and Alexander Kipnis and Sreejan Kumar and Tobias Ludwig and Marvin Mathony and Marcelo Mattar and Alireza Modirshanechi and Surabhi S. Nath and Joshua C. Peterson and Milena Rmus and Evan M. Russek and Tankred Saanum and Natalia Scharfenberg and Johannes A. Schubert and Luca M. Schulze Buschoff and Nishad Singhi and Xin Sui and Mirko Thalmann and Fabian Theis and Vuong Truong and Vishaal Udandarao and Konstantinos Voudouris and Robert Wilson and Kristin Witte and Shuchen Wu and Dirk Wulff and Huadong Xiong and Eric Schulz},\nyear={2024},\neprint={2410.20268},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2410.20268},\n}",
    "zai-org/glm-4-voice-tokenizer": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nGLM-4-Voice-Tokenizer\nGLM-4-Voice-Tokenizer\nGLM-4-Voice ÊòØÊô∫Ë∞± AI Êé®Âá∫ÁöÑÁ´ØÂà∞Á´ØËØ≠Èü≥Ê®°Âûã„ÄÇGLM-4-Voice ËÉΩÂ§üÁõ¥Êé•ÁêÜËß£ÂíåÁîüÊàê‰∏≠Ëã±ÊñáËØ≠Èü≥ÔºåËøõË°åÂÆûÊó∂ËØ≠Èü≥ÂØπËØùÔºåÂπ∂‰∏îËÉΩÂ§üÊ†πÊçÆÁî®Êà∑ÁöÑÊåá‰ª§ÊîπÂèòËØ≠Èü≥ÁöÑÊÉÖÊÑü„ÄÅËØ≠Ë∞É„ÄÅËØ≠ÈÄü„ÄÅÊñπË®ÄÁ≠âÂ±ûÊÄß„ÄÇ\nGLM-4-Voice is an end-to-end voice model launched by Zhipu AI. GLM-4-Voice can directly understand and generate Chinese and English speech, engage in real-time voice conversations, and change attributes such as emotion, intonation, speech rate, and dialect based on user instructions.\nÊú¨‰ªìÂ∫ìÊòØ GLM-4-Voice ÁöÑ speech tokenizer ÈÉ®ÂàÜ„ÄÇÈÄöËøáÂú® Whisper ÁöÑ encoder ÈÉ®ÂàÜÂ¢ûÂä† vector quantization ËøõË°åËÆ≠ÁªÉÔºåÂ∞ÜËøûÁª≠ÁöÑËØ≠Èü≥ËæìÂÖ•ËΩ¨Âåñ‰∏∫Á¶ªÊï£ÁöÑ token„ÄÇÊØèÁßíÈü≥È¢ëËΩ¨Âåñ‰∏∫ 12.5 ‰∏™Á¶ªÊï£ token„ÄÇ\nThe repo provides the speech tokenzier of GLM-4-Voice, which is trained by adding vector quantization to the encoder part of Whisper and converts continuous speech input into discrete tokens. Each second of audio is converted into 12.5 discrete tokens.\nÊõ¥Â§ö‰ø°ÊÅØËØ∑ÂèÇËÄÉÊàë‰ª¨ÁöÑ‰ªìÂ∫ì GLM-4-Voice.\nFor more information please refer to our repo GLM-4-Voice.",
    "mradermacher/Llama-3.2-Nemotron-3B-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/itsnebulalol/Llama-3.2-Nemotron-3B\nFor a convenient overview and download list, visit our model page for this model.\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Llama-3.2-Nemotron-3B-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n1.5\nGGUF\nQ3_K_S\n1.6\nGGUF\nQ3_K_M\n1.8\nlower quality\nGGUF\nQ3_K_L\n1.9\nGGUF\nIQ4_XS\n1.9\nGGUF\nQ4_K_S\n2.0\nfast, recommended\nGGUF\nQ4_K_M\n2.1\nfast, recommended\nGGUF\nQ5_K_S\n2.4\nGGUF\nQ5_K_M\n2.4\nGGUF\nQ6_K\n2.7\nvery good quality\nGGUF\nQ8_0\n3.5\nfast, best quality\nGGUF\nf16\n6.5\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "jordand/whisper-d-v1a": "WhisperD\nExample Output:\nCaution:\nUsage:\nCitation:\nWhisperD\nWhisperD is a fine-tuned version of whisper-large-v2 that is able to transcribe multi-speaker, conversational speech. It was used to generate synthetic transcriptions for training Parakeet. Diarization is performed implicitly by the model, where \"[S1]\", \"[S2]\", etc. denote speaker identity. WhisperD is (often) able to transcribe non-speech events, e.g. \"(coughs)\", \"(laughs)\". Outputs include disfluencies.\nExample Output:\n[S1] What's sort of cool is that, uh, you can produce coughs if you have to. [S2] What do you mean? [S1] Well, (coughs) there, I just coughed.\nMore details can be found in the WhisperD blog post.\nCaution:\nThis model has only been tested on segments up to 30 seconds in length. It may be unable to handle conditioning on previous text, as this was not included during fine-tuning. Thus, if a pipeline / codebase uses this feature in order to transcribe audio with duration over 30 seconds, generation quality may be poor.\nUsage:\nimport torch\nimport torchaudio\nfrom transformers import WhisperForConditionalGeneration, WhisperProcessor, WhisperTokenizer\nmodel = WhisperForConditionalGeneration.from_pretrained('jordand/whisper-d-v1a', torch_dtype=torch.float16).cuda()\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-large-v2')\ntokenizer = WhisperTokenizer.from_pretrained('openai/whisper-large-v2')\nmodel.generation_config.suppress_tokens = None\nmodel.generation_config.forced_decoder_ids = None\naudio, sr = torchaudio.load('PATH_TO_AUDIO_FILE')\naudio = audio.mean(dim=0, keepdim=True)\naudio = torchaudio.transforms.Resample(sr, 16000)(audio)\naudio = audio[0, :16000*30] # whisper-d-v1 only can handle up to 30 seconds of audio\ninputs = processor(audio, return_tensors=\"pt\")\nmodel_out = model.generate(inputs['input_features'].cuda().half())\ntext = tokenizer.decode(model_out[0], skip_special_tokens=True)\nprint(text)\nCitation:\nFor now, please cite:\n@misc{darefsky2024parakeet,\nauthor = {Darefsky, Jordan and Zhu, Ge and Duan, Zhiyao},\ntitle = {Parakeet},\nyear = {2024},\nurl = {https://jordandarefsky.com/blog/2024/parakeet/}\n}",
    "Keltezaa/katy-perry-flux": "Katy Perry (Flux)\nModel description\nDownload model\nUse it with the üß® diffusers library\nKaty Perry (Flux)\nPrompt\nThis is a high resolution image of a women her facial features are sharp and defined. She has prominent cheekbones, including dark eyeshadow and a deep red lipstick., smoky eye makeup. bright blue eyes.  She has long, dark brown hair cascading over her shoulders and a light olive skin tone.  Wearing a long dress, standing in a cafe\nPrompt\nThis is a high resolution image of a women her facial features are sharp and defined. She has prominent cheekbones, including dark eyeshadow and a deep red lipstick., smoky eye makeup. bright blue eyes.  She has long, dark brown hair cascading over her shoulders and a light olive skin tone.  Wearing a long dress, standing in a cafe, smiling\nPrompt\nbeautiful detailed photograph, wearing a dress, long straight hair,  standing in a jungle, wearing an explorer's outfit\nPrompt\nbeautiful detailed photograph, wearing a dress, long straight hair,  standing in a spaceship, wearing a sci-fi space suit\nPrompt\nbeautiful detailed photograph, wearing a dress, long straight hair,  standing in a spaceship, wearing a sci-fi space suit\nPrompt\nbeautiful detailed photograph, wearing a dress, short hair curled at the bottom,  standing in a joyful forest clearing. dressed as snow white, closeup shot\nPrompt\nbeautiful detailed photograph, wearing a dress, short hair curled at the bottom,  standing in a joyful forest clearing. dressed as snow white, closeup shot\nPrompt\nbeautiful detailed photograph, wearing a dress, short hair curled at the bottom,  standing in a joyful forest clearing. dressed as snow white, closeup shot\n(CivitAI)\nModel description\nKaty Perry -Trained for Flux\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.\nUse it with the üß® diffusers library\nfrom diffusers import AutoPipelineForText2Image\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\npipeline = AutoPipelineForText2Image.from_pretrained('black-forest-labs/FLUX.1-dev', torch_dtype=torch.bfloat16).to(device)\npipeline.load_lora_weights('Keltezaa/katy-perry-flux', weight_name='Katy_Perry_Flux.safetensors')\nimage = pipeline(' beautiful detailed photograph, wearing a dress, short hair curled at the bottom,  standing in a joyful forest clearing. dressed as snow white, closeup shot').images[0]\nFor more details, including weighting, merging and fusing LoRAs, check the documentation on loading LoRAs in diffusers",
    "Keltezaa/jenna-ortega-flux": "Jenna Ortega FLUX\nModel description\nTrigger words\nDownload model\nUse it with the üß® diffusers library\nJenna Ortega FLUX\nPrompt\nA portrait of jenna_ortega in a dark, vintage setting. She holds a book titled 'Love is a LIE' with a black and white image of jenna_ortega's face on the cover.The woman wears a white collar and a black jacket. Her hair is styled in two braids. She looks directly at the camera with a serious expression. Behind her is a wooden wall with intricate carvings and a framed portrait depicting the frankenstein monster. The color palette consists of dark browns, blacks, and whites, creating a somber and reflective atmosphere.\nPrompt\nA portrait of jenna_ortega in a serene outdoor setting. She wears a floral crown of white and red flowers and holds a bouquet of yellow and orange flowers. Her long black hair flows down her back, and she wears a single red rose. The background is softly blurred, emphasizing the woman and the flowers. Sunlight filters through the leaves, casting a warm glow on the scene. The image has a dreamy, ethereal quality, with a color palette dominated by earthy tones.\nPrompt\nA portrait of jenna_ortega standing in a blossoming garden. She wears a wide-brimmed straw hat and a floral-patterned blouse. The blouse is white with pink and purple floral designs. The woman holds the hat with one hand and adjusts it with the other. The background is filled with delicate white blossoms, creating a serene and dreamy atmosphere. The image's color palette is soft and muted, with the white blouse contrasting against the pink blossoms.\nPrompt\nA black and white portrait of jenna_ortega at a festival. She wears a bohemian-style outfit with intricate patterns and layered jewelry, including multiple necklaces, earrings, and bracelets. Her hair is styled in loose waves, and she wears a headband. The woman stands confidently with her hands in her pockets, displaying a belt with a circular buckle. The background shows a crowd of people, tents, and a clear sky. The image is candid, capturing a moment of casual enjoyment at the festival.\nPrompt\nA portrait of jenna_ortega in a dark, vintage setting. She holds a book titled 'I hate everything' with a black and white image of jenna_ortega's face on the cover. There is text on the bottom of the book cover that reads 'Jenna Ortega'.The woman wears a white collar and a black jacket. Her hair is styled in two braids. She looks directly at the camera with a serious expression. Behind her is a wooden wall with intricate carvings and a framed portrait depicting the frankenstein monster. The color palette consists of dark browns, blacks, and whites, creating a somber and reflective atmosphere.\nPrompt\nA close-up portrait of jenna_ortega with dark hair styled in two braids. She wears a dark suit with a white shirt and a black tie. The woman points directly at the camera with a slight smile. The background is dimly lit with a warm, amber glow, creating a moody atmosphere. The image style is dramatic and evocative, emphasizing the subject's expression and the interplay of light and shadow.\nPrompt\nA portrait of jenna_ortega with long, wavy brown hair. She wears a white, off-the-shoulder sweater. The woman's gaze is direct and intense, and her lips are painted a soft pink. The background is a warm, fiery orange and yellow, creating a bokeh effect. The image style is candid and natural, emphasizing the subject's features and expressions.\nPrompt\nA portrait of jenna_ortega in a dark, vintage setting. She holds a book titled 'THE CRUEL DEATH OF A GUY WHO KEEPS BEGGING FOR BUZZ!' with a black and white image of jenna_ortega's face on the cover.The woman wears a white collar and a black jacket. Her hair is styled in two braids. She looks directly at the camera with a serious expression. Behind her is a wooden wall with intricate carvings and a framed portrait depicting the frankenstein monster. The color palette consists of dark browns, blacks, and whites, creating a somber and reflective atmosphere.\nPrompt\nA close-up portrait of jenna_ortega lying on a bed. She has long, dark hair and wears a white lace-up top. Her makeup is subtle, emphasizing her eyes and lips. The background is blurred, focusing attention on the subject. The color palette is soft and muted, with the woman's skin and hair contrasting against the neutral background.\nPrompt\nA portrait of jenna_ortega with striking red hair, captured in a side profile. She wears a black, dotted blouse with a high neckline and long sleeves. Her pose is relaxed; one hand rests on her neck, and the other touches her face. The background is a chain-link fence with a geometric pattern, illuminated by warm, ambient light. The color palette consists primarily of dark tones, with the red hair contrasting against the lighter background. The image conveys a mood of contemplation and introspection.\nPrompt\nA portrait of jenna_ortega seated on a burgundy leather couch in an indoor setting. She wears a ribbed turtleneck sweater and blue jeans. Her black hair is styled in loose waves, and she has a neutral expression. The background is blurred, but it appears to be a cafe or restaurant with warm, ambient lighting. The image style is candid and natural, capturing a serene moment.\nPrompt\nA portrait of jenna_ortega seated on a pilates reformer in a gym. She wears a black tank top and black leggings, with a black wristband on her left wrist. Her right hand rests on her head, and her gaze is directed to the side. The background shows gym equipment and a window with vertical blinds. Soft lighting illuminates the scene, creating a calm and focused atmosphere.\nPrompt\nA portrait of jenna_ortega in a traditional Japanese kimono, standing in a field of tall grasses. She holds a samurai sword in her right hand and her left hand rests on her hip. Her hair is blowing in the wind, and her gaze is directed away from the camera. The background shows a mountain range under a partly cloudy sky, with orange and yellow leaves floating in the air. The color palette is dominated by earthy tones, with the woman's dark kimona contrasting against the golden-brown grasses and the muted blue sky.\nPrompt\nA portrait of jenna_ortega in a dark, Victorian-era setting. She wears a black dress with a white collar and holds a black balloon with white text reading \"Every Day is Wednesday. \" The woman has braided hair and stares directly at the camera with a serious expression. The background features a wooden paneled wall with a framed photograph of a group of people. The color palette consists primarily of dark browns, blacks, and whites, creating a somber and reflective atmosphere.\nPrompt\nA portrait of jenna_ortega with long, wavy black hair , captured in a candid moment. She wears a white, long-sleeved blouse and smiles warmly at the camera. The background is softly blurred, highlighting the woman and the blossoms. The blossoms are in full bloom, with delicate pink and white petals contrasting against the green foliage. The image conveys a serene and natural mood, with the woman appearing relaxed and content.\nPrompt\nA portrait of jenna_ortega in a forest. She is captured in a side profile, her face partially hidden by her long, flowing hair. Her eyes are a striking blue, and she wears a red, ribbed sweater. The woman's pose is relaxed, with one hand gently touching her hair. The background is a dense forest with tall, slender trees, and the ground is covered in autumnal red leaves. The image has a warm, earthy color palette, with the red of the woman's sweater contrasting against the green of the trees and foliage.\nPrompt\nA vivid portrayal of jenna_ortega playing a double bass. She has dark hair styled in two braids and wears a dark, textured jacket. The woman's focused expression and intense gaze indicate concentration on her performance. The double bass is the main subject, with its sleek black body contrasting sharply with the vibrant colors of the stained glass behind her. The stained glass window behind her displays a rainbow of colors, including red, orange, yellow, green, blue, and purple. The background is a blend of architectural elements, including a large, ornate window with intricate designs. The image style is dramatic and evocative, emphasizing the interplay of light and shadow.\nPrompt\nA portrait of jenna_ortega with long, wavy black hair , captured in a side profile. She leans against a palm tree, her hand gently touching the leaves. The woman wears a black top and her nails are painted a vibrant red. The background is a serene beach scene with palm trees and a clear blue sky. The image has a soft, dreamy color palette of earthy tones, with the woman's black hair contrasting against the lighter background.\nPrompt\nA portrait of jenna_ortega leaning against a brick wall. She wears a white tank top and blue jeans. Her curly black hair is styled in loose waves, and she has a relaxed pose with one hand resting on her chin. The brick wall is weathered with red and brown bricks. The image is candid and natural, capturing a moment of stillness.\nPrompt\nA detailed portrayal of jenna_ortega warrior in ornate silver and gold armor, standing in a grand, gothic-style cathedral. She holds a long, ornate sword with a golden hilt and a red gem at the top. Her attire is intricately designed with gold and silver patterns and embellishments, including a prominent chest plate featuring a blue gem. Her hair is styled in loose waves, and she has a determined expression. The background is blurred, highlighting the warrior as the main subject, and the cathedral's architecture is visible behind her. The color palette is dominated by gold, silver, and white, creating a regal and majestic atmosphere.\n(CivitAI)\nModel description\nJenna Marie Ortega is an American actress who rose to prominence for her portrayal of Wednesday Addams in the Netflix horror comedy series 'Wednesday'. She has also starred in the slasher films 'Scream', 'X', and 'Scream VI', as well as in the fantasy film 'Beetlejuice Beetlejuice'..\nTrigger words\nYou should use jenna_ortega to trigger the image generation.\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.\nUse it with the üß® diffusers library\nfrom diffusers import AutoPipelineForText2Image\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\npipeline = AutoPipelineForText2Image.from_pretrained('black-forest-labs/FLUX.1-dev', torch_dtype=torch.bfloat16).to(device)\npipeline.load_lora_weights('Keltezaa/jenna-ortega-flux', weight_name='JennaOrtega_F1D.safetensors')\nimage = pipeline('A detailed portrayal of jenna_ortega warrior in ornate silver and gold armor, standing in a grand, gothic-style cathedral. She holds a long, ornate sword with a golden hilt and a red gem at the top. Her attire is intricately designed with gold and silver patterns and embellishments, including a prominent chest plate featuring a blue gem. Her hair is styled in loose waves, and she has a determined expression. The background is blurred, highlighting the warrior as the main subject, and the cathedral's architecture is visible behind her. The color palette is dominated by gold, silver, and white, creating a regal and majestic atmosphere.').images[0]\nFor more details, including weighting, merging and fusing LoRAs, check the documentation on loading LoRAs in diffusers",
    "Keltezaa/sarah-michelle-gellar-flux-model": "Sarah Michelle Gellar Flux Model\nModel description\nTrigger words\nDownload model\nUse it with the üß® diffusers library\nSarah Michelle Gellar Flux Model\nPrompt\nPrompt\nPrompt\nPrompt\nPrompt\n(CivitAI)\nModel description\nSarah Michelle Prinze (n√©e Gellar), born April 14, 1977, in New York City, NY, is an American actress, producer, and entrepreneur. As a child, Gellar modeled for Wilhelmina and appeared in commercials for Burger King, Avon, and Shake ‚Äôn Bake. She featured in the fast food industry‚Äôs very first ‚Äúattack ad,‚Äù in which she negatively compared McDonald's' food to that of Burger King. The former was outraged by the commercial and sued Burger King, naming the 5-year-old Gellar as a defendant. She reportedly received a lifetime ban from McDonald's that forbade her from eating at any of their restaurants. In 1983, Gellar made her screen acting debut in the television film \"An Invasion of Privacy,\" after impressing the casting director by reading both her own and co-star Valerie Bertolini's lines in her audition. After a stint on the short-lived teen drama series ‚ÄúSwans Crossing\" playing mean girl Sydney Rutledge, she was cast as a similar character, Kendall Hart, on the soap ‚ÄúAll My Children,‚Äù for which she received a Daytime Emmy Award for Outstanding Younger Actress. From 1997-2003, Gellar headlined the supernatural drama ‚ÄúBuffy the Vampire Slayer,‚Äù a hugely popular and influential series that launched her to stardom and brought her widespread acclaim and recognition, including a Golden Globe nomination for Best Performance by an Actress in a Television Series - Drama, a Saturn Award for Best Genre TV Actress, and a SFX Award for Best TV Actress, among many others. Gellar has also achieved significant success in film, having starred in box office hits like ‚ÄúI Know What You Did Last Summer‚Äù (1997), ‚ÄúScream 2‚Äù (1997), ‚ÄúCruel Intentions‚Äù (1999), ‚ÄúScooby-Doo‚Äù (2002) and ‚ÄúScooby-Doo 2: Monsters Unleashed‚Äù (2004), ‚ÄúThe Grudge‚Äù (2004) and ‚ÄúThe Grudge 2‚Äù (2006), and ‚ÄúTMNT‚Äù (2007), as well as independent films such as ‚ÄúSouthland Tales‚Äù (2006), ‚ÄúThe Air I Breathe‚Äù (2007), and ‚ÄúVeronika Decides to Die‚Äù (2009). Her small role in the 2022 film ‚ÄúDo Revenge‚Äù was written specifically for her, envisioned as an adult version of her character in ‚ÄúCruel Intentions.‚Äù Gellar's other notable credits include the television series ‚ÄúRinger, ‚ÄúThe Crazy Ones,‚Äù and ‚ÄúWolf Pack,‚Äù all of which were canceled after just one season, and the upcoming ‚ÄúDexter: Original Sin.\" Because of her involvement in so many horror-adjacent projects, Gellar has been dubbed the scream queen of her generation. In 2015, Gellar co-founded Foodstirs, an e-commerce startup selling organic baking kits, and later released her own cookbook, ‚ÄúStirring Up Fun with Food.‚Äù Gellar has been married to her ‚ÄúI Know What You Did Last Summer‚Äù and ‚ÄúScooby-Doo‚Äù co-star Freddie Prinze, Jr. since 2002.\nTrigger words\nYou should use smg to trigger the image generation.\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.\nUse it with the üß® diffusers library\nfrom diffusers import AutoPipelineForText2Image\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\npipeline = AutoPipelineForText2Image.from_pretrained('black-forest-labs/FLUX.1-dev', torch_dtype=torch.bfloat16).to(device)\npipeline.load_lora_weights('Keltezaa/sarah-michelle-gellar-flux-model', weight_name='SMGFluxModel.safetensors')\nimage = pipeline('`smg`').images[0]\nFor more details, including weighting, merging and fusing LoRAs, check the documentation on loading LoRAs in diffusers",
    "bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF": "Llamacpp imatrix Quantizations of Llama-3.2-3B-Instruct-uncensored\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nQ4_0_X_X\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of Llama-3.2-3B-Instruct-uncensored\nUsing llama.cpp release b3972 for quantization.\nOriginal model: https://huggingface.co/chuanli11/Llama-3.2-3B-Instruct-uncensored\nAll quants made using imatrix option with dataset from here\nRun them in LM Studio\nPrompt format\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\nToday Date: 25 Oct 2024\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nLlama-3.2-3B-Instruct-uncensored-f16.gguf\nf16\n7.22GB\nfalse\nFull F16 weights.\nLlama-3.2-3B-Instruct-uncensored-Q8_0.gguf\nQ8_0\n3.84GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nLlama-3.2-3B-Instruct-uncensored-Q6_K_L.gguf\nQ6_K_L\n3.16GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nLlama-3.2-3B-Instruct-uncensored-Q6_K.gguf\nQ6_K\n2.97GB\nfalse\nVery high quality, near perfect, recommended.\nLlama-3.2-3B-Instruct-uncensored-Q5_K_L.gguf\nQ5_K_L\n2.84GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nLlama-3.2-3B-Instruct-uncensored-Q5_K_M.gguf\nQ5_K_M\n2.59GB\nfalse\nHigh quality, recommended.\nLlama-3.2-3B-Instruct-uncensored-Q5_K_S.gguf\nQ5_K_S\n2.54GB\nfalse\nHigh quality, recommended.\nLlama-3.2-3B-Instruct-uncensored-Q4_K_L.gguf\nQ4_K_L\n2.53GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nLlama-3.2-3B-Instruct-uncensored-Q3_K_XL.gguf\nQ3_K_XL\n2.33GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nLlama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf\nQ4_K_M\n2.24GB\nfalse\nGood quality, default size for must use cases, recommended.\nLlama-3.2-3B-Instruct-uncensored-Q4_K_S.gguf\nQ4_K_S\n2.15GB\nfalse\nSlightly lower quality with more space savings, recommended.\nLlama-3.2-3B-Instruct-uncensored-Q4_0_8_8.gguf\nQ4_0_8_8\n2.14GB\nfalse\nOptimized for ARM inference. Requires 'sve' support (see link below). Don't use on Mac or Windows.\nLlama-3.2-3B-Instruct-uncensored-Q4_0_4_8.gguf\nQ4_0_4_8\n2.14GB\nfalse\nOptimized for ARM inference. Requires 'i8mm' support (see link below). Don't use on Mac or Windows.\nLlama-3.2-3B-Instruct-uncensored-Q4_0_4_4.gguf\nQ4_0_4_4\n2.14GB\nfalse\nOptimized for ARM inference. Should work well on all ARM chips, pick this if you're unsure. Don't use on Mac or Windows.\nLlama-3.2-3B-Instruct-uncensored-Q4_0.gguf\nQ4_0\n2.14GB\nfalse\nLegacy format, generally not worth using over similarly sized formats\nLlama-3.2-3B-Instruct-uncensored-IQ4_XS.gguf\nIQ4_XS\n2.04GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nLlama-3.2-3B-Instruct-uncensored-Q3_K_L.gguf\nQ3_K_L\n1.98GB\nfalse\nLower quality but usable, good for low RAM availability.\nLlama-3.2-3B-Instruct-uncensored-Q2_K_L.gguf\nQ2_K_L\n1.88GB\nfalse\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\nLlama-3.2-3B-Instruct-uncensored-Q3_K_M.gguf\nQ3_K_M\n1.86GB\nfalse\nLow quality.\nLlama-3.2-3B-Instruct-uncensored-IQ3_M.gguf\nIQ3_M\n1.77GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nLlama-3.2-3B-Instruct-uncensored-Q3_K_S.gguf\nQ3_K_S\n1.71GB\nfalse\nLow quality, not recommended.\nLlama-3.2-3B-Instruct-uncensored-IQ3_XS.gguf\nIQ3_XS\n1.65GB\nfalse\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nLlama-3.2-3B-Instruct-uncensored-Q2_K.gguf\nQ2_K\n1.49GB\nfalse\nVery low quality but surprisingly usable.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nSome say that this improves the quality, others don't notice any difference. If you use these models PLEASE COMMENT with your findings. I would like feedback that these are actually used and useful so I don't keep uploading quants no one is using.\nThanks!\nDownloading using huggingface-cli\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF --include \"Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF --include \"Llama-3.2-3B-Instruct-uncensored-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (Llama-3.2-3B-Instruct-uncensored-Q8_0) or download them all in place (./)\nQ4_0_X_X\nThese are NOT for Metal (Apple) offloading, only ARM chips.\nIf you're using an ARM chip, the Q4_0_X_X quants will have a substantial speedup. Check out Q4_0_4_4 speed comparisons on the original pull request\nTo check which one would work best for your ARM chip, you can check AArch64 SoC features (thanks EloyOn!).\nWhich file should I choose?\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset\nThank you ZeroWw for the inspiration to experiment with embed/output\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "ai4bharat/indic-parler-tts": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nIndic Parler-TTS\nüìñ Quick Index\nüë®‚Äçüíª Installation\nüõ†Ô∏è Key capabilities\nKey Features\nüé≤ Random voice\nüåç Switching languages\nüéØ Using a specific speaker\nSome Description Examples\nüìê Evaluation\nMotivation\nTraining dataset\nCitation\nLicense\nIndic Parler-TTS\nIndic Parler-TTS is a multilingual Indic extension of Parler-TTS Mini.\nIt is a fine-tuned version of Indic Parler-TTS Pretrained, trained on a 1,806 hours multilingual Indic and English dataset.\nIndic Parler-TTS Mini can officially speak in 20 Indic languages, making it comprehensive for regional language technologies, and in English. The 21 languages supported are: Assamese, Bengali, Bodo, Dogri, English, Gujarati, Hindi, Kannada, Konkani, Maithili, Malayalam, Manipuri, Marathi, Nepali, Odia, Sanskrit, Santali, Sindhi, Tamil, Telugu, and Urdu.\nThanks to its better prompt tokenizer, it can easily be extended to other languages. This tokenizer has a larger vocabulary and handles byte fallback, which simplifies multilingual training.\nüö® This work is the result of a collaboration between the HuggingFace audio team and the AI4Bharat team. üö®\nüìñ Quick Index\nüë®‚Äçüíª Installation\nüõ†Ô∏è Key capabilities\nüé≤ Using a random voice\nüåç Switching languages\nüéØ Using a specific speaker\nSome Description Examples\nüìêEvaluation\nMotivation\nOptimizing inference\nüë®‚Äçüíª Installation\nUsing Parler-TTS is as simple as \"bonjour\". Simply install the library once:\npip install git+https://github.com/huggingface/parler-tts.git\nüõ†Ô∏è Key capabilities\nThe model accepts two primary inputs:\nTranscript - The text to be converted to speech.\nCaption - A detailed description of how the speech should sound, e.g., \"Leela speaks in a high-pitched, fast-paced, and cheerful tone, full of energy and happiness. The recording is very high quality with no background noise.\"\nKey Features\nLanguage Support\nOfficially supported languages: Assamese, Bengali, Bodo, Dogri, Kannada, Malayalam, Marathi, Sanskrit, Nepali, English, Telugu, Hindi, Gujarati, Konkani, Maithili, Manipuri, Odia, Santali, Sindhi, Tamil, and Urdu.\nUnofficial support: Chhattisgarhi, Kashmiri, Punjabi.\nSpeaker Diversity\n69 unique voices across the supported languages.\nSupported languages have a set of recommended voices optimized for naturalness and intelligibility.\nEmotion Rendering\n10 languages officially support emotion-specific prompts: Assamese, Bengali, Bodo, Dogri, Kannada, Malayalam, Marathi, Sanskrit, Nepali, and Tamil.\nEmotion support for other languages exists but has not been extensively tested.\nAvailable emotions include: Command, Anger, Narration, Conversation, Disgust, Fear, Happy, Neutral, Proper Noun, News, Sad, and Surprise.\nAccent Flexibility\nThe model officially supports Indian English accents through its English voices, providing clear and natural speech.\nFor other accents, the model allows customization by specifying accent details, such as \"A male British speaker\" or \"A female American speaker,\" using style transfer for more dynamic and personalized outputs.\nCustomizable OutputIndic Parler-TTS offers precise control over various speech characteristics using the caption input:\nBackground Noise: Adjust the noise level in the audio, from clear to slightly noisy environments.\nReverberation: Control the perceived distance of the voice, from close-sounding to distant-sounding speech.\nExpressivity: Specify how dynamic or monotone the speech should be, ranging from expressive to slightly expressive or monotone.\nPitch: Modify the pitch of the speech, including high, low, or balanced tones.\nSpeaking Rate: Change the speaking rate, from slow to fast.\nVoice Quality: Control the overall clarity and naturalness of the speech, adjusting from basic to refined voice quality.\nüé≤ Random voice\nüö® Unlike previous versions of Parler-TTS, here we use two tokenizers - one for the prompt and one for the description. üö®\nIndic Parler-TTS has been trained to generate speech with features that can be controlled with a simple text prompt, for example:\nimport torch\nfrom parler_tts import ParlerTTSForConditionalGeneration\nfrom transformers import AutoTokenizer\nimport soundfile as sf\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel = ParlerTTSForConditionalGeneration.from_pretrained(\"ai4bharat/indic-parler-tts\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-parler-tts\")\ndescription_tokenizer = AutoTokenizer.from_pretrained(model.config.text_encoder._name_or_path)\nprompt = \"Hey, how are you doing today?\"\ndescription = \"A female speaker with a British accent delivers a slightly expressive and animated speech with a moderate speed and pitch. The recording is of very high quality, with the speaker's voice sounding clear and very close up.\"\ndescription_input_ids = description_tokenizer(description, return_tensors=\"pt\").to(device)\nprompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\ngeneration = model.generate(input_ids=description_input_ids.input_ids, attention_mask=description_input_ids.attention_mask, prompt_input_ids=prompt_input_ids.input_ids, prompt_attention_mask=prompt_input_ids.attention_mask)\naudio_arr = generation.cpu().numpy().squeeze()\nsf.write(\"indic_tts_out.wav\", audio_arr, model.config.sampling_rate)\nIndic Parler-TTS provides highly effective control over key aspects of speech synthesis using descriptive captions. Below is a summary of what each control parameter can achieve:\nControl Type\nCapabilities\nBackground Noise\nAdjusts the level of background noise, supporting clear and slightly noisy environments.\nReverberation\nControls the perceived distance of the speaker‚Äôs voice, allowing close or distant sounds.\nExpressivity\nModulates the emotional intensity of speech, from monotone to highly expressive.\nPitch\nVaries the pitch to achieve high, low, or moderate tonal output.\nSpeaking Rate\nChanges the speed of speech delivery, ranging from slow to fast-paced.\nSpeech Quality\nImproves or degrades the overall audio clarity, supporting basic to refined outputs.\nüåç Switching languages\nThe model automatically adapts to the language it detects in the prompt. You don't need to specify the language you want to use. For example, to switch to Hindi, simply use an Hindi prompt:\nimport torch\nfrom parler_tts import ParlerTTSForConditionalGeneration\nfrom transformers import AutoTokenizer\nimport soundfile as sf\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel = ParlerTTSForConditionalGeneration.from_pretrained(\"ai4bharat/indic-parler-tts\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-parler-tts\")\ndescription_tokenizer = AutoTokenizer.from_pretrained(model.config.text_encoder._name_or_path)\nprompt = \"‡§Ö‡§∞‡•á, ‡§§‡•Å‡§Æ ‡§Ü‡§ú ‡§ï‡•à‡§∏‡•á ‡§π‡•ã?\"\ndescription = \"A female speaker delivers a slightly expressive and animated speech with a moderate speed and pitch. The recording is of very high quality, with the speaker's voice sounding clear and very close up.\"\ndescription_input_ids = description_tokenizer(description, return_tensors=\"pt\").to(device)\nprompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\ngeneration = model.generate(input_ids=description_input_ids.input_ids, attention_mask=description_input_ids.attention_mask, prompt_input_ids=prompt_input_ids.input_ids, prompt_attention_mask=prompt_input_ids.attention_mask)\naudio_arr = generation.cpu().numpy().squeeze()\nsf.write(\"indic_tts_out.wav\", audio_arr, model.config.sampling_rate)\nüéØ Using a specific speaker\nTo ensure speaker consistency across generations, this checkpoint was also trained on pre-determined speakers, characterized by name (e.g. Rohit, Karan, Leela, Maya, Sita, ...).\nTo take advantage of this, simply adapt your text description to specify which speaker to use: Divya's voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise.\nimport torch\nfrom parler_tts import ParlerTTSForConditionalGeneration\nfrom transformers import AutoTokenizer\nimport soundfile as sf\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel = ParlerTTSForConditionalGeneration.from_pretrained(\"ai4bharat/indic-parler-tts\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-parler-tts\")\ndescription_tokenizer = AutoTokenizer.from_pretrained(model.config.text_encoder._name_or_path)\nprompt = \"‡§Ö‡§∞‡•á, ‡§§‡•Å‡§Æ ‡§Ü‡§ú ‡§ï‡•à‡§∏‡•á ‡§π‡•ã?\"\ndescription = \"Divya's voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise.\"\ndescription_input_ids = description_tokenizer(description, return_tensors=\"pt\").to(device)\nprompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\ngeneration = model.generate(input_ids=description_input_ids.input_ids, attention_mask=description_input_ids.attention_mask, prompt_input_ids=prompt_input_ids.input_ids, prompt_attention_mask=prompt_input_ids.attention_mask)\naudio_arr = generation.cpu().numpy().squeeze()\nsf.write(\"indic_tts_out.wav\", audio_arr, model.config.sampling_rate)\nThe model includes 69 speakers across 18 officially supported languages, with each language having a set of recommended voices for optimal performance. Below is a table summarizing the available speakers for each language, along with the recommended ones.\nHere is the table based on the provided data:\nLanguage\nAvailable Speakers\nRecommended Speakers\nAssamese\nAmit, Sita, Poonam, Rakesh\nAmit, Sita\nBengali\nArjun, Aditi, Tapan, Rashmi, Arnav, Riya\nArjun, Aditi\nBodo\nBikram, Maya, Kalpana\nBikram, Maya\nChhattisgarhi\nBhanu, Champa\nBhanu, Champa\nDogri\nKaran\nKaran\nEnglish\nThoma, Mary, Swapna, Dinesh, Meera, Jatin, Aakash, Sneha, Kabir, Tisha, Chingkhei, Thoiba, Priya, Tarun, Gauri, Nisha, Raghav, Kavya, Ravi, Vikas, Riya\nThoma, Mary\nGujarati\nYash, Neha\nYash, Neha\nHindi\nRohit, Divya, Aman, Rani\nRohit, Divya\nKannada\nSuresh, Anu, Chetan, Vidya\nSuresh, Anu\nMalayalam\nAnjali, Anju, Harish\nAnjali, Harish\nManipuri\nLaishram, Ranjit\nLaishram, Ranjit\nMarathi\nSanjay, Sunita, Nikhil, Radha, Varun, Isha\nSanjay, Sunita\nNepali\nAmrita\nAmrita\nOdia\nManas, Debjani\nManas, Debjani\nPunjabi\nDivjot, Gurpreet\nDivjot, Gurpreet\nSanskrit\nAryan\nAryan\nTamil\nKavitha, Jaya\nJaya\nTelugu\nPrakash, Lalitha, Kiran\nPrakash, Lalitha\nTips:\nWe've set up an inference guide to make generation faster. Think SDPA, torch.compile, batching and streaming!\nInclude the term \"very clear audio\" to generate the highest quality audio, and \"very noisy audio\" for high levels of background noise\nPunctuation can be used to control the prosody of the generations, e.g. use commas to add small breaks in speech\nThe remaining speech features (gender, speaking rate, pitch and reverberation) can be controlled directly through the prompt\nSome Description Examples\nAditi - Slightly High-Pitched, Expressive Tone:\"Aditi speaks with a slightly higher pitch in a close-sounding environment. Her voice is clear, with subtle emotional depth and a normal pace, all captured in high-quality recording.\"\nSita - Rapid, Slightly Monotone:\"Sita speaks at a fast pace with a slightly low-pitched voice, captured clearly in a close-sounding environment with excellent recording quality.\"\nTapan - Male, Moderate Pace, Slightly Monotone:\"Tapan speaks at a moderate pace with a slightly monotone tone. The recording is clear, with a close sound and only minimal ambient noise.\"\nSunita - High-Pitched, Happy Tone:\"Sunita speaks with a high pitch in a close environment. Her voice is clear, with slight dynamic changes, and the recording is of excellent quality.\"\nKaran - High-Pitched, Positive Tone:\"Karan‚Äôs high-pitched, engaging voice is captured in a clear, close-sounding recording. His slightly slower delivery conveys a positive tone.\"\nAmrita - High-Pitched, Flat Tone:\"Amrita speaks with a high pitch at a slow pace. Her voice is clear, with excellent recording quality and only moderate background noise.\"\nAditi - Slow, Slightly Expressive:\"Aditi speaks slowly with a high pitch and expressive tone. The recording is clear, showcasing her energetic and emotive voice.\"\nYoung Male Speaker, American Accent:\"A young male speaker with a high-pitched American accent delivers speech at a slightly fast pace in a clear, close-sounding recording.\"\nBikram - High-Pitched, Urgent Tone:\"Bikram speaks with a higher pitch and fast pace, conveying urgency. The recording is clear and intimate, with great emotional depth.\"\nAnjali - High-Pitched, Neutral Tone: \"Anjali speaks with a high pitch at a normal pace in a clear, close-sounding environment. Her neutral tone is captured with excellent audio quality.\"\nüìê Evaluation\nIndic Parler-TTS has been evaluated using a MOS-like framework by native and non-native speakers. The results highlight its exceptional performance in generating natural and intelligible speech, especially for native speakers of Indian languages.\nNSS stands for Native Speaker Score:\nLanguage\nNSS Pretrained (%)\nNSS Finetuned (%)\nHighlights\nAssamese\n82.56 ¬± 1.80\n87.36 ¬± 1.81\nClear, natural synthesis with excellent expressiveness.\nBengali\n77.41 ¬± 2.14\n86.16 ¬± 1.85\nHigh-quality outputs with smooth intonation.\nBodo\n90.83 ¬± 4.54\n94.47 ¬± 4.12\nNear-perfect accuracy for a lesser-resourced language.\nDogri\n82.61 ¬± 4.98\n88.80 ¬± 3.57\nRobust and consistent synthesis for Dogri.\nGujarati\n75.28 ¬± 1.94\n75.36 ¬± 1.78\nStrong clarity and naturalness even for smaller languages.\nHindi\n83.43 ¬± 1.53\n84.79 ¬± 2.09\nReliable and expressive outputs for India's most widely spoken language.\nKannada\n77.97 ¬± 3.43\n88.17 ¬± 2.81\nHighly natural and accurate voices for Kannada.\nKonkani\n87.20 ¬± 3.58\n76.60 ¬± 4.14\nProduces clear and natural outputs for diverse speakers.\nMaithili\n89.07 ¬± 4.47\n95.36 ¬± 2.52\nExceptionally accurate, showcasing fine-tuning success.\nMalayalam\n82.02 ¬± 2.06\n86.54 ¬± 1.67\nSmooth, high-quality synthesis with expressive outputs.\nManipuri\n89.58 ¬± 1.33\n85.63 ¬± 2.60\nNatural intonation with minimal errors.\nMarathi\n73.81 ¬± 1.93\n76.96 ¬± 1.45\nMaintains clarity and naturalness across speakers.\nNepali\n64.05 ¬± 8.33\n80.02 ¬± 5.75\nStrong synthesis for native and proximal Nepali speakers.\nOdia\n90.28 ¬± 2.52\n88.94 ¬± 3.26\nHigh expressiveness and quality for Odia speakers.\nSanskrit\n99.71 ¬± 0.58\n99.79 ¬± 0.34\nNear-perfect synthesis, ideal for classical use cases.\nSindhi\n76.44 ¬± 2.26\n76.46 ¬± 1.29\nClear and natural voices for underrepresented languages.\nTamil\n69.68 ¬± 2.73\n75.48 ¬± 2.18\nDelivers intelligible and expressive speech.\nTelugu\n89.77 ¬± 2.20\n88.54 ¬± 1.86\nSmooth and natural tonal quality for Telugu.\nUrdu\n77.15 ¬± 3.47\n77.75 ¬± 3.82\nProduces high-quality speech despite resource constraints.\nKey Strengths:\nExceptional performance for native speakers, with top scores for Maithili (95.36), Sanskrit (99.79), and Bodo (94.47).\nCompetitive results for lesser-resourced and unofficially supported languages like Kashmiri (55.30) and Sindhi (76.46).\nAdaptability to non-native and anonymous speaker scenarios with consistently high clarity.\nMotivation\nParler-TTS is a reproduction of work from the paper Natural language guidance of high-fidelity text-to-speech with synthetic annotations by Dan Lyth and Simon King, from Stability AI and Edinburgh University respectively.\nParler-TTS was released alongside:\nThe Parler-TTS repository - you can train and fine-tuned your own version of the model.\nThe Data-Speech repository - a suite of utility scripts designed to annotate speech datasets.\nThe Parler-TTS organization - where you can find the annotated datasets as well as the future checkpoints.\nTraining dataset\nDescription:The model was fine-tuned on a subset of the dataset used to train the pre-trained version: Indic-Parler Dataset, a large-scale multilingual speech corpus designed to train the Indic Parler-TTS model.\nKey Statistics:\nDataset\nDuration (hrs)\nLanguages Covered\nNo. of Utterances\nLicense\nGLOBE\n535.0\n1\n581,725\nCC V1\nIndicTTS\n382.0\n12\n220,606\nCC BY 4.0\nLIMMITS\n568.0\n7\n246,008\nCC BY 4.0\nRasa\n288.0\n9\n155,734\nCC BY 4.0\nLanguages Covered:The dataset supports 16 official languages of India, along with English and Chhattisgarhi, making it comprehensive for regional language technologies. These languages include Assamese, Bengali, Bodo, Chhattisgarhi, Dogri, English, Gujarati, Hindi, Kannada, Malayalam, Manipuri, Marathi, Nepali, Odia, Punjabi, Sanskrit, Tamil and Telugu.\nLanguage-Wise Data Breakdown:\nHere‚Äôs the table combining the duration (hours) and the number of utterances from the provided stats:\nLanguage\nDuration (hrs)\nNo. of Utterances\nAssamese\n69.78\n41,210\nBengali\n140.04\n70,305\nBodo\n49.14\n27,012\nChhattisgarhi\n80.11\n38,148\nDogri\n16.14\n7,823\nEnglish\n802.81\n735,482\nGujarati\n21.24\n5,679\nHindi\n107.00\n46,135\nKannada\n125.01\n54,575\nMalayalam\n25.21\n14,988\nManipuri\n20.77\n19,232\nMarathi\n122.47\n54,894\nNepali\n28.65\n16,016\nOdia\n19.18\n11,558\nPunjabi\n11.07\n6,892\nSanskrit\n19.91\n8,720\nTamil\n52.25\n29,204\nTelugu\n95.91\n37,405\nCitation\nIf you found this repository useful, please consider citing this work and also the original Stability AI paper:\n@inproceedings{sankar25_interspeech,\ntitle     = {{Rasmalai : Resources for Adaptive Speech Modeling in IndiAn Languages with Accents and Intonations}},\nauthor    = {Ashwin Sankar and Yoach Lacombe and Sherry Thomas and Praveen {Srinivasa Varadhan} and Sanchit Gandhi and Mitesh M. Khapra},\nyear      = {2025},\nbooktitle = {{Interspeech 2025}},\npages     = {4128--4132},\ndoi       = {10.21437/Interspeech.2025-2758},\nissn      = {2958-1796},\n}\n@misc{lacombe-etal-2024-parler-tts,\nauthor = {Yoach Lacombe and Vaibhav Srivastav and Sanchit Gandhi},\ntitle = {Parler-TTS},\nyear = {2024},\npublisher = {GitHub},\njournal = {GitHub repository},\nhowpublished = {\\url{https://github.com/huggingface/parler-tts}}\n}\n@misc{lyth2024natural,\ntitle={Natural language guidance of high-fidelity text-to-speech with synthetic annotations},\nauthor={Dan Lyth and Simon King},\nyear={2024},\neprint={2402.01912},\narchivePrefix={arXiv},\nprimaryClass={cs.SD}\n}\nLicense\nThis model is permissively licensed under the Apache 2.0 license."
}