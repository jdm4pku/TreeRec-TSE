{
    "microsoft/swinv2-tiny-patch4-window16-256": "Swin Transformer v2 (tiny-sized model)\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nSwin Transformer v2 (tiny-sized model)\nSwin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository.\nDisclaimer: The team releasing Swin Transformer v2 did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks. In contrast, previous vision Transformers produce feature maps of a single low resolution and have quadratic computation complexity to input image size due to computation of self-attention globally.\nSwin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.\nSource\nIntended uses & limitations\nYou can use the raw model for image classification. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\nfrom PIL import Image\nimport requests\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window16-256\")\nmodel = AutoModelForImageClassification.from_pretrained(\"microsoft/swinv2-tiny-patch4-window16-256\")\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\nFor more code examples, we refer to the documentation.\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-2111-09883,\nauthor    = {Ze Liu and\nHan Hu and\nYutong Lin and\nZhuliang Yao and\nZhenda Xie and\nYixuan Wei and\nJia Ning and\nYue Cao and\nZheng Zhang and\nLi Dong and\nFuru Wei and\nBaining Guo},\ntitle     = {Swin Transformer {V2:} Scaling Up Capacity and Resolution},\njournal   = {CoRR},\nvolume    = {abs/2111.09883},\nyear      = {2021},\nurl       = {https://arxiv.org/abs/2111.09883},\neprinttype = {arXiv},\neprint    = {2111.09883},\ntimestamp = {Thu, 02 Dec 2021 15:54:22 +0100},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-2111-09883.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "nvidia/speakerverification_en_titanet_large": "NVIDIA TitaNet-Large (en-US)\nNVIDIA NeMo: Training\nHow to Use this Model\nAutomatically instantiate the model\nEmbedding Extraction\nVerifying two utterances (Speaker Verification)\nExtracting Embeddings for more audio files\nInput\nOutput\nModel Architecture\nTraining\nDatasets\nPerformance\nLimitations\nNVIDIA Riva: Deployment\nReferences\nLicence\nNVIDIA TitaNet-Large (en-US)\n|\n|\n|\nThis model extracts speaker embeddings from given speech, which is the backbone for speaker verification and diarization tasks.\nIt is a \"large\" version of TitaNet (around 23M parameters) models.See the model architecture section and NeMo documentation for complete architecture details.\nNVIDIA NeMo: Training\nTo train, fine-tune or play with the model you will need to install NVIDIA NeMo. We recommend you install it after you've installed the latest Pytorch version.\npip install nemo_toolkit['all']\nHow to Use this Model\nThe model is available for use in the NeMo toolkit [3] and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.\nAutomatically instantiate the model\nimport nemo.collections.asr as nemo_asr\nspeaker_model = nemo_asr.models.EncDecSpeakerLabelModel.from_pretrained(\"nvidia/speakerverification_en_titanet_large\")\nEmbedding Extraction\nUsing\nemb = speaker_model.get_embedding(\"an255-fash-b.wav\")\nVerifying two utterances (Speaker Verification)\nNow to check if two audio files are from the same speaker or not, simply do:\nspeaker_model.verify_speakers(\"an255-fash-b.wav\",\"cen7-fash-b.wav\")\nExtracting Embeddings for more audio files\nTo extract embeddings from a bunch of audio files:\nWrite audio files to a manifest.json file with lines as in format:\n{\"audio_filepath\": \"<absolute path to dataset>/audio_file.wav\", \"duration\": \"duration of file in sec\", \"label\": \"speaker_id\"}\nThen running following script will extract embeddings and writes to current working directory:\npython <NeMo_root>/examples/speaker_tasks/recognition/extract_speaker_embeddings.py --manifest=manifest.json\nInput\nThis model accepts 16000 KHz Mono-channel Audio (wav files) as input.\nOutput\nThis model provides speaker embeddings for an audio file.\nModel Architecture\nTitaNet model is a depth-wise separable conv1D model [1] for Speaker Verification and diarization tasks. You may find more info on the detail of this model here: TitaNet-Model.\nTraining\nThe NeMo toolkit [3] was used for training the models for over several hundred epochs. These model are trained with this example script and this base config.\nDatasets\nAll the models in this collection are trained on a composite dataset comprising several thousand hours of English speech:\nVoxceleb-1\nVoxceleb-2\nFisher\nSwitchboard\nLibrispeech\nSRE (2004-2010)\nPerformance\nPerformances of the these models are reported in terms of Equal Error Rate (EER%) on speaker verification evaluation trial files and as Diarization Error Rate (DER%) on diarization test sessions.\nSpeaker Verification (EER%)\nVersion\nModel\nModel Size\nVoxCeleb1 (Cleaned trial file)\n1.10.0\nTitaNet-Large\n23M\n0.66\nSpeaker Diarization (DER%)\nVersion\nModel\nModel Size\nEvaluation Condition\nNIST SRE 2000\nAMI (Lapel)\nAMI (MixHeadset)\nCH109\n1.10.0\nTitaNet-Large\n23M\nOracle VAD KNOWN # of Speakers\n6.73\n2.03\n1.73\n1.19\n1.10.0\nTitaNet-Large\n23M\nOracle VAD UNKNOWN # of Speakers\n5.38\n2.03\n1.89\n1.63\nLimitations\nThis model is trained on both telephonic and non-telephonic speech from voxceleb datasets, Fisher and switch board. If your domain of data differs from trained data or doesnot show relatively good performance consider finetuning for that speech domain.\nNVIDIA Riva: Deployment\nNVIDIA Riva, is an accelerated speech AI SDK deployable on-prem, in all clouds, multi-cloud, hybrid, on edge, and embedded.\nAdditionally, Riva provides:\nWorld-class out-of-the-box accuracy for the most common languages with model checkpoints trained on proprietary data with hundreds of thousands of GPU-compute hours\nBest in class accuracy with run-time word boosting (e.g., brand and product names) and customization of acoustic model, language model, and inverse text normalization\nStreaming speech recognition, Kubernetes compatible scaling, and enterprise-grade support\nAlthough this model isn‚Äôt supported yet by Riva, the list of supported models is here.Check out Riva live demo.\nReferences\n[1] TitaNet: Neural Model for Speaker Representation with 1D Depth-wise Separable convolutions and global context\n[2] NVIDIA NeMo Toolkit\nLicence\nLicense to use this model is covered by the CC-BY-4.0. By downloading the public and release version of the model, you accept the terms and conditions of the CC-BY-4.0 license.",
    "impira/layoutlm-document-qa": "LayoutLM for Visual Question Answering\nGetting started with the model\nAbout us\nLayoutLM for Visual Question Answering\nThis is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents. It has been fine-tuned using both the SQuAD2.0 and DocVQA datasets.\nGetting started with the model\nTo run these examples, you must have PIL, pytesseract, and PyTorch installed in addition to transformers.\nfrom transformers import pipeline\nnlp = pipeline(\n\"document-question-answering\",\nmodel=\"impira/layoutlm-document-qa\",\n)\nnlp(\n\"https://templates.invoicehome.com/invoice-template-us-neat-750px.png\",\n\"What is the invoice number?\"\n)\n# {'score': 0.9943977, 'answer': 'us-001', 'start': 15, 'end': 15}\nnlp(\n\"https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg\",\n\"What is the purchase amount?\"\n)\n# {'score': 0.9912159, 'answer': '$1,000,000,000', 'start': 97, 'end': 97}\nnlp(\n\"https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png\",\n\"What are the 2020 net sales?\"\n)\n# {'score': 0.59147286, 'answer': '$ 3,750', 'start': 19, 'end': 20}\nNOTE: This model and pipeline was recently landed in transformers via PR #18407 and PR #18414, so you'll need to use a recent version of transformers, for example:\npip install git+https://github.com/huggingface/transformers.git@2ef774211733f0acf8d3415f9284c49ef219e991\nAbout us\nThis model was created by the team at Impira.",
    "laion/CLIP-ViT-H-14-laion2B-s32B-b79K": "Model Card for CLIP ViT-H/14 - LAION-2B\nTable of Contents\nModel Details\nModel Description\nUses\nDirect Use\nDownstream Use\nOut-of-Scope Use\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nTesting Data, Factors & Metrics\nTesting Data\nResults\nAcknowledgements\nCitation\nHow to Get Started with the Model\nModel Card for CLIP ViT-H/14 - LAION-2B\nTable of Contents\nModel Details\nUses\nTraining Details\nEvaluation\nAcknowledgements\nCitation\nHow To Get Started With the Model\nModel Details\nModel Description\nA CLIP ViT-H/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).\nModel training done by Romain Beaumont on the stability.ai cluster.\nUses\nAs per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model.\nThe OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset.\nDirect Use\nZero-shot image classification, image and text retrieval, among others.\nDownstream Use\nImage classification and other image task fine-tuning, linear probe image classification, image generation guiding and conditioning, among others.\nOut-of-Scope Use\nAs per the OpenAI models,\nAny deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP‚Äôs performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful.\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\nFurther the above notice, the LAION-5B dataset used in training of these models has additional considerations, see below.\nTraining Details\nTraining Data\nThis model was trained with the 2 Billion sample English subset of LAION-5B (https://laion.ai/blog/laion-5b/).\nIMPORTANT NOTE: The motivation behind dataset creation is to democratize research and experimentation around large-scale multi-modal model training and handling of uncurated, large-scale datasets crawled from publically available internet. Our recommendation is therefore to use the dataset for research purposes. Be aware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer. Therefore, please use the demo links with caution and at your own risk. It is possible to extract a ‚Äúsafe‚Äù subset by filtering out samples based on the safety tags (using a customized trained NSFW classifier that we built). While this strongly reduces the chance for encountering potentially harmful content when viewing, we cannot entirely exclude the possibility for harmful content being still present in safe mode, so that the warning holds also there. We think that providing the dataset openly to broad research and other interested communities will allow for transparent investigation of benefits that come along with training large-scale models as well as pitfalls and dangers that may stay unreported or unnoticed when working with closed large datasets that remain restricted to a small community. Providing our dataset openly, we however do not recommend using it for creating ready-to-go industrial products, as the basic research about general properties and safety of such large-scale models, which we would like to encourage with this release, is still in progress.\nTraining Procedure\nPlease see training notes and wandb logs.\nEvaluation\nEvaluation done with code in the LAION CLIP Benchmark suite.\nTesting Data, Factors & Metrics\nTesting Data\nThe testing is performed with VTAB+ (A combination of VTAB (https://arxiv.org/abs/1910.04867) w/ additional robustness datasets) for classification and COCO and Flickr for retrieval.\nTODO - more detail\nResults\nThe model achieves a 78.0 zero-shot top-1 accuracy on ImageNet-1k.\nAn initial round of benchmarks have been performed on a wider range of datasets, currently viewable at https://github.com/LAION-AI/CLIP_benchmark/blob/main/benchmark/results.ipynb\nTODO - create table for just this model's metrics.\nAcknowledgements\nAcknowledging stability.ai for the compute used to train this model.\nCitation\nBibTeX:\nLAION-5B\n@inproceedings{schuhmann2022laionb,\ntitle={{LAION}-5B: An open large-scale dataset for training next generation image-text models},\nauthor={Christoph Schuhmann and\nRomain Beaumont and\nRichard Vencu and\nCade W Gordon and\nRoss Wightman and\nMehdi Cherti and\nTheo Coombes and\nAarush Katta and\nClayton Mullis and\nMitchell Wortsman and\nPatrick Schramowski and\nSrivatsa R Kundurthy and\nKatherine Crowson and\nLudwig Schmidt and\nRobert Kaczmarczyk and\nJenia Jitsev},\nbooktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\nyear={2022},\nurl={https://openreview.net/forum?id=M3Y74vmsMcY}\n}\nOpenAI CLIP paper\n@inproceedings{Radford2021LearningTV,\ntitle={Learning Transferable Visual Models From Natural Language Supervision},\nauthor={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},\nbooktitle={ICML},\nyear={2021}\n}\nOpenCLIP software\n@software{ilharco_gabriel_2021_5143773,\nauthor       = {Ilharco, Gabriel and\nWortsman, Mitchell and\nWightman, Ross and\nGordon, Cade and\nCarlini, Nicholas and\nTaori, Rohan and\nDave, Achal and\nShankar, Vaishaal and\nNamkoong, Hongseok and\nMiller, John and\nHajishirzi, Hannaneh and\nFarhadi, Ali and\nSchmidt, Ludwig},\ntitle        = {OpenCLIP},\nmonth        = jul,\nyear         = 2021,\nnote         = {If you use this software, please cite it as below.},\npublisher    = {Zenodo},\nversion      = {0.1},\ndoi          = {10.5281/zenodo.5143773},\nurl          = {https://doi.org/10.5281/zenodo.5143773}\n}\nHow to Get Started with the Model\nUse the code below to get started with the model.\n** TODO ** - Hugging Face transformers, OpenCLIP, and timm getting started snippets",
    "umm-maybe/AI-image-detector": "Model Trained Using AutoTrain\nValidation Metrics\nLicense Notice\nNOTE: Unless you are trying to detect imagery generated using older models such as VQGAN+CLIP, please use the updated version of this detector instead.\nThis model is a proof-of-concept demonstration of using a ViT model to predict whether an artistic image was generated using AI.\nIt was created in October 2022, and as such, the training data did not include any samples generated by Midjourney 5, SDXL, or DALLE-3. It still may be able to correctly identify samples from these more recent models due to being trained on outputs of their predecessors.\nFurthermore the intended scope of this tool is artistic images; that is to say, it is not a deepfake photo detector, and general computer imagery (webcams, screenshots, etc.) may throw it off.\nIn general, this tool can only serve as one of many potential indicators that an image was AI-generated. Images scoring as very probably artificial (e.g. 90% or higher) could be referred to a human expert for further investigation, if needed.\nFor more information please see the blog post describing this project at:\nhttps://medium.com/@matthewmaybe/can-an-ai-learn-to-identify-ai-art-545d9d6af226\nModel Trained Using AutoTrain\nProblem type: Binary Classification\nModel ID: 1519658722\nCO2 Emissions (in grams): 7.9405\nValidation Metrics\nLoss: 0.163\nAccuracy: 0.942\nPrecision: 0.938\nRecall: 0.978\nAUC: 0.980\nF1: 0.958\nLicense Notice\nThis work is licensed under a Creative Commons Attribution-NoDerivatives 4.0 International License.\nYou may distribute and make this model available to others as part of your own web page, app, or service so long as you provide attribution. However, use of this model within text-to-image systems to evade AI image detection would be considered a \"derivative work\" and as such prohibited by the license terms.",
    "MingZhong/unieval-sum": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nIntroduction\nPre-trained Evaluator\nUsage\nPre-trained evaluator in EMNLP 2022 paper\nTowards a Unified Multi-Dimensional Evaluator for Text Generation\nIntroduction\nMulti-dimensional evaluation is the dominant paradigm for human evaluation in Natural Language Generation (NLG), i.e., evaluating the generated text from multiple explainable dimensions, such as coherence and fluency.\nHowever, automatic evaluation in NLG is still dominated by similarity-based metrics (e.g., ROUGE, BLEU), but they are not sufficient to portray the difference between the advanced generation models.\nTherefore, we propose UniEval to bridge this gap so that a more comprehensive and fine-grained evaluation of NLG systems can be achieved.\nPre-trained Evaluator\nunieval-sum is the pre-trained evaluator for the text summarization task. It can evaluate the model output from four dimensions:\ncoherence\nconsistency\nfluency\nrelevance\nIt can also be transferred to the new dimensions and generation tasks, such as naturalness and informativeness for data-to-text.\nUsage\nPlease refer to our GitHub repository.",
    "michellejieli/emotion_text_classifier": "Fine-tuned DistilRoBERTa-base for Emotion Classification ü§¨ü§¢üòÄüòêüò≠üò≤\nModel Description\nHow to Use\nContact\nReference\nFine-tuned DistilRoBERTa-base for Emotion Classification ü§¨ü§¢üòÄüòêüò≠üò≤\nModel Description\nDistilRoBERTa-base is a transformer model that performs sentiment analysis. I fine-tuned the model on transcripts from the Friends show with the goal of classifying emotions from text data, specifically dialogue from Netflix shows or movies. The model predicts 6 Ekman emotions and a neutral class. These emotions include anger, disgust, fear, joy, neutrality, sadness, and surprise.\nThe model is a fine-tuned version of Emotion English DistilRoBERTa-base and DistilRoBERTa-base. This model was initially trained on the following table from Emotion English DistilRoBERTa-base:\nName\nanger\ndisgust\nfear\njoy\nneutral\nsadness\nsurprise\nCrowdflower (2016)\nYes\n-\n-\nYes\nYes\nYes\nYes\nEmotion Dataset, Elvis et al. (2018)\nYes\n-\nYes\nYes\n-\nYes\nYes\nGoEmotions, Demszky et al. (2020)\nYes\nYes\nYes\nYes\nYes\nYes\nYes\nISEAR, Vikash (2018)\nYes\nYes\nYes\nYes\n-\nYes\n-\nMELD, Poria et al. (2019)\nYes\nYes\nYes\nYes\nYes\nYes\nYes\nSemEval-2018, EI-reg, Mohammad et al. (2018)\nYes\n-\nYes\nYes\n-\nYes\n-\nIt was fine-tuned on:\nName\nanger\ndisgust\nfear\njoy\nneutral\nsadness\nsurprise\nEmotion Lines (Friends)\nYes\nYes\nYes\nYes\nYes\nYes\nYes\nHow to Use\nfrom transformers import pipeline\nclassifier = pipeline(\"sentiment-analysis\", model=\"michellejieli/emotion_text_classifier\")\nclassifier(\"I love this!\")\nOutput:\n[{'label': 'joy', 'score': 0.9887555241584778}]\nContact\nPlease reach out to michelleli1999@gmail.com if you have any questions or feedback.\nReference\nJochen Hartmann, \"Emotion English DistilRoBERTa-base\". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022.\nAshritha R Murthy and K M Anil Kumar 2021 IOP Conf. Ser.: Mater. Sci. Eng. 1110 012009",
    "prompthero/openjourney": "Openjourney is an open source Stable Diffusion fine tuned model on Midjourney images, by PromptHero\nStable Diffusion v1.5 vs Openjourney\nüß® Diffusers\nOpenjourney Links\nStable Diffusion v1.5 vs Openjourney\nüß® Diffusers\nWant to learn AI art generation?:\nStable Diffusion v1.5 vs Openjourney\nüß® Diffusers\nUse it for free:\nStable Diffusion v1.5 vs Openjourney\nüß® Diffusers\nOpenjourney is an open source Stable Diffusion fine tuned model on Midjourney images, by PromptHero\nInclude 'mdjrny-v4 style' in prompt. Here you'll find hundreds of Openjourney prompts\nOpenjourney Links\nLora version\nOpenjourney v4\nWant to learn AI art generation?:\nCrash course in AI art generation\nLearn to fine-tune Stable Diffusion for photorealism\nUse it for free:\nStable Diffusion v1.5 vs Openjourney\n(Same parameters, just added \"mdjrny-v4 style\" at the beginning):\nüß® Diffusers\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the Stable Diffusion.\nYou can also export the model to ONNX, MPS and/or FLAX/JAX.\nfrom diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = \"prompthero/openjourney\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nprompt = \"retro serie of different cars with different colors and shapes, mdjrny-v4 style\"\nimage = pipe(prompt).images[0]\nimage.save(\"./retro_cars.png\")",
    "stabilityai/stable-diffusion-2": "Stable Diffusion v2 Model Card\nModel Details\nExamples\nUses\nDirect Use\nMisuse, Malicious Use, and Out-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nTraining\nEvaluation Results\nEnvironmental Impact\nCitation\nStable Diffusion v2 Model Card\nThis model card focuses on the model associated with the Stable Diffusion v2 model, available here.\nThis stable-diffusion-2 model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on 768x768 images.\nUse it with the stablediffusion repository: download the 768-v-ema.ckpt here.\nUse it with üß® diffusers\nModel Details\nDeveloped by: Robin Rombach, Patrick Esser\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: CreativeML Open RAIL++-M License\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H).\nResources for more information: GitHub Repository.\nCite as:\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nExamples\nUsing the ü§ó's Diffusers library to run Stable Diffusion 2 in a simple and efficient manner.\npip install diffusers transformers accelerate scipy safetensors\nRunning the pipeline (if you don't swap the scheduler it will run with the default DDIM, in this example we are swapping it to EulerDiscreteScheduler):\nfrom diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\nmodel_id = \"stabilityai/stable-diffusion-2\"\n# Use the Euler scheduler here instead\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\nimage.save(\"astronaut_rides_horse.png\")\nNotes:\nDespite not being a dependency, we highly recommend you to install xformers for memory efficient attention (better performance)\nIf you have low GPU RAM available, make sure to add a pipe.enable_attention_slicing() after sending it to cuda for less VRAM usage (to the cost of speed)\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and tasks include\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nExcluded uses are described below.\nMisuse, Malicious Use, and Out-of-Scope Use\nNote: This section is originally taken from the DALLE-MINI model card, was used for Stable Diffusion v1, but applies in the same way to Stable Diffusion v2.\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nMisuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\nImpersonating individuals without their consent.\nSexual content without consent of the people who might see it.\nMis- and disinformation\nRepresentations of egregious violence and gore\nSharing of copyrighted or licensed material in violation of its terms of use.\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\nFaces and people in general may not be generated properly.\nThe model was trained mainly with English captions and will not work as well in other languages.\nThe autoencoding part of the model is lossy\nThe model was trained on a subset of the large-scale dataset\nLAION-5B, which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nStable Diffusion was primarily trained on subsets of LAION-2B(en),\nwhich consists of images that are limited to English descriptions.\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for.\nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the\nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nStable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.\nTraining\nTraining Data\nThe model developers used the following dataset for training the model:\nLAION-5B and subsets (details below). The training data is further filtered using LAION's NSFW detector, with a \"p_unsafe\" score of 0.1 (conservative). For more details, please refer to LAION-5B's NeurIPS 2022 paper and reviewer discussions on the topic.\nTraining Procedure\nStable Diffusion v2 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training,\nImages are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\nText prompts are encoded through the OpenCLIP-ViT/H text-encoder.\nThe output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\nThe loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet. We also use the so-called v-objective, see https://arxiv.org/abs/2202.00512.\nWe currently provide the following checkpoints:\n512-base-ema.ckpt: 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score >= 4.5.\n850k steps at resolution 512x512 on the same dataset with resolution >= 512x512.\n768-v-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on a 768x768 subset of our dataset.\n512-depth-ema.ckpt: Resumed from 512-base-ema.ckpt and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized.\n512-inpainting-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized. The same strategy was used to train the 1.5-inpainting checkpoint.\nx4-upscaling-ema.ckpt: Trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model.\nIn addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule.\nHardware: 32 x 8 x A100 GPUs\nOptimizer: AdamW\nGradient Accumulations: 1\nBatch: 32 x 8 x 2 x 4 = 2048\nLearning rate: warmup to 0.0001 for 10,000 steps and then kept constant\nEvaluation Results\nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints:\nEvaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\nEnvironmental Impact\nStable Diffusion v1 Estimated Emissions\nBased on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\nHardware Type: A100 PCIe 40GB\nHours used: 200000\nCloud Provider: AWS\nCompute Region: US-east\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 15000 kg CO2 eq.\nCitation\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nThis model card was written by: Robin Rombach, Patrick Esser and David Ha and is based on the Stable Diffusion v1 and DALL-E Mini model card.",
    "mattmdjaga/segformer_b2_clothes": "Segformer B2 fine-tuned for clothes segmentation\nEvaluation\nLicense\nBibTeX entry and citation info\nSegformer B2 fine-tuned for clothes segmentation\nSegFormer model fine-tuned on ATR dataset for clothes segmentation but can also be used for human segmentation.\nThe dataset on hugging face is called \"mattmdjaga/human_parsing_dataset\".\nTraining code.\nfrom transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation\nfrom PIL import Image\nimport requests\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nprocessor = SegformerImageProcessor.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\nmodel = AutoModelForSemanticSegmentation.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\nurl = \"https://plus.unsplash.com/premium_photo-1673210886161-bfcc40f54d1f?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cGVyc29uJTIwc3RhbmRpbmd8ZW58MHx8MHx8&w=1000&q=80\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits.cpu()\nupsampled_logits = nn.functional.interpolate(\nlogits,\nsize=image.size[::-1],\nmode=\"bilinear\",\nalign_corners=False,\n)\npred_seg = upsampled_logits.argmax(dim=1)[0]\nplt.imshow(pred_seg)\nLabels: 0: \"Background\", 1: \"Hat\", 2: \"Hair\", 3: \"Sunglasses\", 4: \"Upper-clothes\", 5: \"Skirt\", 6: \"Pants\", 7: \"Dress\", 8: \"Belt\", 9: \"Left-shoe\", 10: \"Right-shoe\", 11: \"Face\", 12: \"Left-leg\", 13: \"Right-leg\", 14: \"Left-arm\", 15: \"Right-arm\", 16: \"Bag\", 17: \"Scarf\"\nEvaluation\nLabel Index\nLabel Name\nCategory Accuracy\nCategory IoU\n0\nBackground\n0.99\n0.99\n1\nHat\n0.73\n0.68\n2\nHair\n0.91\n0.82\n3\nSunglasses\n0.73\n0.63\n4\nUpper-clothes\n0.87\n0.78\n5\nSkirt\n0.76\n0.65\n6\nPants\n0.90\n0.84\n7\nDress\n0.74\n0.55\n8\nBelt\n0.35\n0.30\n9\nLeft-shoe\n0.74\n0.58\n10\nRight-shoe\n0.75\n0.60\n11\nFace\n0.92\n0.85\n12\nLeft-leg\n0.90\n0.82\n13\nRight-leg\n0.90\n0.81\n14\nLeft-arm\n0.86\n0.74\n15\nRight-arm\n0.82\n0.73\n16\nBag\n0.91\n0.84\n17\nScarf\n0.63\n0.29\nOverall Evaluation Metrics:\nEvaluation Loss: 0.15\nMean Accuracy: 0.80\nMean IoU: 0.69\nLicense\nThe license for this model can be found here.\nBibTeX entry and citation info\n@article{DBLP:journals/corr/abs-2105-15203,\nauthor    = {Enze Xie and\nWenhai Wang and\nZhiding Yu and\nAnima Anandkumar and\nJose M. Alvarez and\nPing Luo},\ntitle     = {SegFormer: Simple and Efficient Design for Semantic Segmentation with\nTransformers},\njournal   = {CoRR},\nvolume    = {abs/2105.15203},\nyear      = {2021},\nurl       = {https://arxiv.org/abs/2105.15203},\neprinttype = {arXiv},\neprint    = {2105.15203},\ntimestamp = {Wed, 02 Jun 2021 11:46:42 +0200},\nbiburl    = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}",
    "pszemraj/flan-t5-large-grammar-synthesis": "grammar-synthesis-large: FLAN-t5\nExample\nusage in Python\nModel description\nONNX Checkpoint\nOther checkpoints\nLimitations\nUse Cases\nCitation info\ngrammar-synthesis-large: FLAN-t5\nA fine-tuned version of google/flan-t5-large for grammar correction on an expanded version of the JFLEG dataset. Demo on HF spaces.\nExample\nCompare vs. the original grammar-synthesis-large.\nusage in Python\nThere's a colab notebook that already has this basic version implemented (click on the Open in Colab button)\nAfter pip install transformers run the following code:\nfrom transformers import pipeline\ncorrector = pipeline(\n'text2text-generation',\n'pszemraj/flan-t5-large-grammar-synthesis',\n)\nraw_text = 'i can has cheezburger'\nresults = corrector(raw_text)\nprint(results)\nFor Batch Inference: see this discussion thread for details, but essentially the dataset consists of several sentences at a time, and so I'd recommend running inference in the same fashion: batches of 64-96 tokens ish (or, 2-3 sentences split with regex)\nit is also helpful to first check whether or not a given sentence needs grammar correction before using the text2text model. You can do this with BERT-type models fine-tuned on CoLA like textattack/roberta-base-CoLA\nI made a notebook demonstrating batch inference here\nModel description\nThe intent is to create a text2text language model that successfully completes \"single-shot grammar correction\" on a potentially grammatically incorrect text that could have a lot of mistakes with the important qualifier of it does not semantically change text/information that IS grammatically correct.\nCompare some of the heavier-error examples on other grammar correction models to see the difference :)\nONNX Checkpoint\nThis model has been converted to ONNX and can be loaded/used with huggingface's optimum library.\nYou first need to install optimum\npip install optimum[onnxruntime]\n# ^ if you want to use a different runtime read their docs\nload with the optimum pipeline\nfrom optimum.pipelines import pipeline\ncorrector = pipeline(\n\"text2text-generation\", model=corrector_model_name, accelerator=\"ort\"\n)\n# use as normal\nOther checkpoints\nIf trading a slight decrease in grammatical correction quality for faster inference speed makes sense for your use case, check out the base and small checkpoints fine-tuned from the relevant t5 checkpoints.\nLimitations\ndataset: cc-by-nc-sa-4.0\nmodel: apache-2.0\nthis is still a work-in-progress and while probably useful for \"single-shot grammar correction\" in a lot of cases, give the outputs a glance for correctness ok?\nUse Cases\nObviously, this section is quite general as there are many things one can use \"general single-shot grammar correction\" for. Some ideas or use cases:\nCorrecting highly error-prone LM outputs. Some examples would be audio transcription (ASR) (this is literally some of the examples) or something like handwriting OCR.\nTo be investigated further, depending on what model/system is used it might be worth it to apply this after OCR on typed characters.\nCorrecting/infilling text generated by text generation models to be cohesive/remove obvious errors that break the conversation immersion. I use this on the outputs of this OPT 2.7B chatbot-esque model of myself.\nAn example of this model running on CPU with beam search:\nOriginal response:\nive heard it attributed to a bunch of different philosophical schools, including stoicism, pragmatism, existentialism and even some forms of post-structuralism. i think one of the most interesting (and most difficult) philosophical problems is trying to let dogs (or other animals) out of cages. the reason why this is a difficult problem is because it seems to go against our grain (so to\nsynthesizing took 306.12 seconds\nFinal response in 1294.857 s:\nI've heard it attributed to a bunch of different philosophical schools, including solipsism, pragmatism, existentialism and even some forms of post-structuralism. i think one of the most interesting (and most difficult) philosophical problems is trying to let dogs (or other animals) out of cages. the reason why this is a difficult problem is because it seems to go against our grain (so to speak)\nNote: that I have some other logic that removes any periods at the end of the final sentence in this chatbot setting to avoid coming off as passive aggressive\nSomewhat related to #2 above, fixing/correcting so-called tortured-phrases that are dead giveaways text was generated by a language model. Note that SOME of these are not fixed, especially as they venture into domain-specific terminology (i.e. irregular timberland instead of Random Forest).\nCitation info\nIf you find this fine-tuned model useful in your work, please consider citing it :)\n@misc {peter_szemraj_2022,\nauthor       = { {Peter Szemraj} },\ntitle        = { flan-t5-large-grammar-synthesis (Revision d0b5ae2) },\nyear         = 2022,\nurl          = { https://huggingface.co/pszemraj/flan-t5-large-grammar-synthesis },\ndoi          = { 10.57967/hf/0138 },\npublisher    = { Hugging Face }\n}",
    "stabilityai/stable-diffusion-2-1-base": "Stable Diffusion v2-1-base Model Card\nModel Details\nExamples\nUses\nDirect Use\nMisuse, Malicious Use, and Out-of-Scope Use\nLimitations and Bias\nLimitations\nBias\nTraining\nVersion 2.1\nVersion 2.0\nEvaluation Results\nEnvironmental Impact\nCitation\nStable Diffusion v2-1-base Model Card\nThis model card focuses on the model associated with the Stable Diffusion v2-1-base model.\nThis stable-diffusion-2-1-base model fine-tunes stable-diffusion-2-base (512-base-ema.ckpt) with 220k extra steps taken, with punsafe=0.98 on the same dataset.\nUse it with the stablediffusion repository: download the v2-1_512-ema-pruned.ckpt here.\nUse it with üß® diffusers\nModel Details\nDeveloped by: Robin Rombach, Patrick Esser\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: CreativeML Open RAIL++-M License\nModel Description: This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H).\nResources for more information: GitHub Repository.\nCite as:\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nExamples\nUsing the ü§ó's Diffusers library to run Stable Diffusion 2 in a simple and efficient manner.\npip install diffusers transformers accelerate scipy safetensors\nRunning the pipeline (if you don't swap the scheduler it will run with the default PNDM/PLMS scheduler, in this example we are swapping it to EulerDiscreteScheduler):\nfrom diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\nimport torch\nmodel_id = \"stabilityai/stable-diffusion-2-1-base\"\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]\nimage.save(\"astronaut_rides_horse.png\")\nNotes:\nDespite not being a dependency, we highly recommend you to install xformers for memory efficient attention (better performance)\nIf you have low GPU RAM available, make sure to add a pipe.enable_attention_slicing() after sending it to cuda for less VRAM usage (to the cost of speed)\nUses\nDirect Use\nThe model is intended for research purposes only. Possible research areas and tasks include\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models.\nExcluded uses are described below.\nMisuse, Malicious Use, and Out-of-Scope Use\nNote: This section is originally taken from the DALLE-MINI model card, was used for Stable Diffusion v1, but applies in the same way to Stable Diffusion v2.\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\nMisuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\nGenerating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\nIntentionally promoting or propagating discriminatory content or harmful stereotypes.\nImpersonating individuals without their consent.\nSexual content without consent of the people who might see it.\nMis- and disinformation\nRepresentations of egregious violence and gore\nSharing of copyrighted or licensed material in violation of its terms of use.\nSharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\nLimitations and Bias\nLimitations\nThe model does not achieve perfect photorealism\nThe model cannot render legible text\nThe model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\nFaces and people in general may not be generated properly.\nThe model was trained mainly with English captions and will not work as well in other languages.\nThe autoencoding part of the model is lossy\nThe model was trained on a subset of the large-scale dataset\nLAION-5B, which contains adult, violent and sexual content. To partially mitigate this, we have filtered the dataset using LAION's NFSW detector (see Training section).\nBias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\nStable Diffusion vw was primarily trained on subsets of LAION-2B(en),\nwhich consists of images that are limited to English descriptions.\nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for.\nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the\nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\nStable Diffusion v2 mirrors and exacerbates biases to such a degree that viewer discretion must be advised irrespective of the input or its intent.\nTraining\nTraining Data\nThe model developers used the following dataset for training the model:\nLAION-5B and subsets (details below). The training data is further filtered using LAION's NSFW detector, with a \"p_unsafe\" score of 0.1 (conservative). For more details, please refer to LAION-5B's NeurIPS 2022 paper and reviewer discussions on the topic.\nTraining Procedure\nStable Diffusion v2 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training,\nImages are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\nText prompts are encoded through the OpenCLIP-ViT/H text-encoder.\nThe output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\nThe loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet. We also use the so-called v-objective, see https://arxiv.org/abs/2202.00512.\nWe currently provide the following checkpoints, for various versions:\nVersion 2.1\n512-base-ema.ckpt: Fine-tuned on 512-base-ema.ckpt 2.0 with 220k extra steps taken, with punsafe=0.98 on the same dataset.\n768-v-ema.ckpt: Resumed from 768-v-ema.ckpt 2.0 with an additional 55k steps on the same dataset (punsafe=0.1), and then fine-tuned for another 155k extra steps with punsafe=0.98.\nVersion 2.0\n512-base-ema.ckpt: 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score >= 4.5.\n850k steps at resolution 512x512 on the same dataset with resolution >= 512x512.\n768-v-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on a 768x768 subset of our dataset.\n512-depth-ema.ckpt: Resumed from 512-base-ema.ckpt and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized.\n512-inpainting-ema.ckpt: Resumed from 512-base-ema.ckpt and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning.\nThe additional input channels of the U-Net which process this extra information were zero-initialized. The same strategy was used to train the 1.5-inpainting checkpoint.\nx4-upscaling-ema.ckpt: Trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model.\nIn addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule.\nHardware: 32 x 8 x A100 GPUs\nOptimizer: AdamW\nGradient Accumulations: 1\nBatch: 32 x 8 x 2 x 4 = 2048\nLearning rate: warmup to 0.0001 for 10,000 steps and then kept constant\nEvaluation Results\nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 steps DDIM sampling steps show the relative improvements of the checkpoints:\nEvaluated using 50 DDIM steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\nEnvironmental Impact\nStable Diffusion v1 Estimated Emissions\nBased on that information, we estimate the following CO2 emissions using the Machine Learning Impact calculator presented in Lacoste et al. (2019). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\nHardware Type: A100 PCIe 40GB\nHours used: 200000\nCloud Provider: AWS\nCompute Region: US-east\nCarbon Emitted (Power consumption x Time x Carbon produced based on location of power grid): 15000 kg CO2 eq.\nCitation\n@InProceedings{Rombach_2022_CVPR,\nauthor    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\\\"orn},\ntitle     = {High-Resolution Image Synthesis With Latent Diffusion Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth     = {June},\nyear      = {2022},\npages     = {10684-10695}\n}\nThis model card was written by: Robin Rombach, Patrick Esser and David Ha and is based on the Stable Diffusion v1 and DALL-E Mini model card.",
    "microsoft/Promptist": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nPromptist: reinforcement learning for automatic prompt optimization\nNews\nLoad Pretrained Model for Stable Diffusion v1.4\nPromptist: reinforcement learning for automatic prompt optimization\nNews\n[Demo Release] Dec, 2022: Demo at HuggingFace Space\n[Model Release] Dec, 2022: link\n[Paper Release] Dec, 2022: Optimizing Prompts for Text-to-Image Generation\nLanguage models serve as a prompt interface that optimizes user input into model-preferred prompts.\nLearn a language model for automatic prompt optimization via reinforcement learning.\nLoad Pretrained Model for Stable Diffusion v1.4\nYou can try the online demo at https://huggingface.co/spaces/microsoft/Promptist.\n[Note] the online demo at HuggingFace Space is using CPU, so slow generation speed would be expected. Please load the model locally with GPUs for faster generation.\nimport gradio as grad\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndef load_prompter():\nprompter_model = AutoModelForCausalLM.from_pretrained(\"microsoft/Promptist\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\nreturn prompter_model, tokenizer\nprompter_model, prompter_tokenizer = load_prompter()\ndef generate(plain_text):\ninput_ids = prompter_tokenizer(plain_text.strip()+\" Rephrase:\", return_tensors=\"pt\").input_ids\neos_id = prompter_tokenizer.eos_token_id\noutputs = prompter_model.generate(input_ids, do_sample=False, max_new_tokens=75, num_beams=8, num_return_sequences=8, eos_token_id=eos_id, pad_token_id=eos_id, length_penalty=-1.0)\noutput_texts = prompter_tokenizer.batch_decode(outputs, skip_special_tokens=True)\nres = output_texts[0].replace(plain_text+\" Rephrase:\", \"\").strip()\nreturn res\ntxt = grad.Textbox(lines=1, label=\"Initial Text\", placeholder=\"Input Prompt\")\nout = grad.Textbox(lines=1, label=\"Optimized Prompt\")\nexamples = [\"A rabbit is wearing a space suit\", \"Several railroad tracks with one train passing by\", \"The roof is wet from the rain\", \"Cats dancing in a space club\"]\ngrad.Interface(fn=generate,\ninputs=txt,\noutputs=out,\ntitle=\"Promptist Demo\",\ndescription=\"Promptist is a prompt interface for Stable Diffusion v1-4 (https://huggingface.co/CompVis/stable-diffusion-v1-4) that optimizes user input into model-preferred prompts.\",\nexamples=examples,\nallow_flagging='never',\ncache_examples=False,\ntheme=\"default\").launch(enable_queue=True, debug=True)",
    "FpOh/WuXiaSD": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nÁ¶ÅÊ≠¢Â∞Ü‰ªñ‰∫∫ÊàêÂìÅÂõæËøõË°åËΩ¨ÁªòÔºåÈô§Èùû‰Ω†ÊúâÂØπÊñπÁöÑÊéàÊùÉÔºÅ\nÁ®ãÂ∫èÊï¥ÂêàÂåÖÔºö\nWuXiaSD\nWuXiaSD-Studio\nAIÁªòÁîªÁ®ãÂ∫è‰ΩøÁî®ÊïôÁ®ã\nÁÆÄÊòìËΩ¨ÁªòËßÜÈ¢ë‰ªãÁªç\nStable Diffusion WebUIÁ≥ªÁªüËØæ\nSD-LORAÊ®°ÂûãËÆ≠ÁªÉÊïôÁ®ã\nÊïôÁ®ãÊñá‰ª∂Ôºö\n[PÁ´ô‰∏ì‰∫´]AIÁªòÁîªÊïôÂ≠¶\nÈ¢ùÂ§ñÂÜÖÂÆπÔºö\nÁ¶ÅÊ≠¢Â∞Ü‰ªñ‰∫∫ÊàêÂìÅÂõæËøõË°åËΩ¨ÁªòÔºåÈô§Èùû‰Ω†ÊúâÂØπÊñπÁöÑÊéàÊùÉÔºÅ\nhiÔºå‰Ω†Â•Ω ~\nËøô‰∏™È°πÁõÆÊúÄÂàùÊòØÁî±‰∫éÂ§©ÂàÄÊòéÊúàÂàÄÁîü‰∫ßÂêå‰∫∫ÂõæÊûÅ‰∏∫Âõ∞ÈöæËÄåÂª∫Á´ãÁöÑÔºåÁî®AIÊù•‰ª£ÊõøPÂõæ„ÄÅÁîªÂõæÁöÑÂ∑•‰ΩúÔºåÂ§ßÂπÖÂ∫¶ÂáèÂ∞ëÂêå‰∫∫Âàõ‰ΩúÁöÑÈöæÂ∫¶ÂèäÂ∑•‰ΩúÈáèÔºåAIÁªòÁîªÂÖ∑ÊúâÊ≥õÁî®ÊÄßÔºåÁêÜËÆ∫‰∏äÊú¨Â∏ñÊâÄÊúâËµÑÊ∫êÈÉΩÂèØ‰ª•Áî®‰∫éÂõΩÈ£éÊ∏∏ÊàèÔºå‰æãÂ¶ÇÂâëÁΩë‰∏â„ÄÅÈÄÜÊ∞¥ÂØí„ÄÅ‰ªôÂâëÁ≠â„ÄÇ\nÊâÄÊúâÊñá‰ª∂ÈÉΩÊòØÁΩëÁªúÊêúÈõÜËÄåÊù•ÔºåÊàëÂ∞ÜËµÑÊ∫êÊï¥ÂêàÂàÜ‰∫´Áªô‰Ω†ÔºåÁ®ãÂ∫è‰∏éÊïôÁ®ãÊñá‰ª∂‰Ω†ÈÉΩÂèØ‰ª•ÁÇπÂáª„ÄêÂêçÁß∞„Äë‰∏ãËΩΩÔºå‰∏ãËΩΩÊó∂Êé®ËçêÁî®Á¨¨‰∏âÊñπ‰∏ãËΩΩÂô®Ôºå‰æãÂ¶ÇIDM‰∏éXDownÔºåÂèØ‰ª•Êõ¥Âø´ÁöÑ‰∏ãËΩΩÔºåËß£ÂéãÊé®Ëçê‰ΩøÁî®7zipÔºå‰ª•ÂÖçÂá∫Áé∞Ëß£ÂéãÈîôËØØÔºÅ‰ΩÜÂú®Âì™‰πãÂâçÔºåËØ∑ÂÖàÊù•Áúã‰∏ãÊñá‰ª∂ÁöÑÁÆÄ‰ªãËØ¥Êòé„ÄÇ\nÊé®ËçêÊòæÂç°ÊòØNÂç°ÁöÑÁîµËÑë‰ΩøÁî®Ôºå‰∏çÊòØÁöÑËØù‰πüÂèØ‰ª•Áî®ÔºåÂ∞±ÊòØCPUÁîüÊàêÈÄüÂ∫¶ÈùûÂ∏∏ÊÖ¢ÔºåË¥¥Â≠êÂ∫ïÈÉ®ÊúâÊõ¥Â§öÊ®°ÂûãÁöÑ‰∏ãËΩΩÊñπÂºè„ÄÇ\nÂ¶ÇÊûú‰Ω†ËßâÂæóÊàëÂÅöÁöÑ‰∏çÈîôÔºåÊ¨¢Ëøé‰Ω†Êù•ËµûÂä©ÊàëÔºåËøôÂ∞ÜÊòØÊàëÊåÅÁª≠Êõ¥Êñ∞ÁöÑÊúÄÂ§ßÂä®ÂäõÔºÅ‚Äã„ÄêÁÇπÊàëÂéªËµûÂä©„Äë\n„ÄêÁÇπÊàëËøõÂÖ•QQÁæ§„Äë | „ÄêÁÇπÊàëËøõÂÖ•Discord„Äë | „ÄêÊàëÁöÑCivitai‰∏ªÈ°µ„Äë | „ÄêÊàëÁöÑPixiv‰∏ªÈ°µ„Äë | „ÄêÊàëÁöÑTwitter„Äë\nÁ®ãÂ∫èÊï¥ÂêàÂåÖÔºö\nWuXiaSD\nËß£ÂéãÂêéÂ§ßÂ∞è‰ªÖ‰∏∫11.6GBÔºåÊîØÊåÅ‰∏≠ÊñáËæìÂÖ•ËΩ¨tagÔºÅÂÜÖÁΩÆ3DÊ∏∏ÊàèÈÄöÁî®Ê®°ÂûãÔºåÈÄÇÂêàÂ∞ùÈ≤ú‰ΩøÁî®Ôºå‰ª•ËæÖÂä©PÂõæÁöÑÊÄùË∑ØÂá∫ÂèëÔºåÁ≤æÁÆÄÂÖ∂‰ªñÂäüËÉΩÔºåÊîØÊåÅSDXLÁ≥ªÂàóÊ®°ÂûãÔºåÊîØÊåÅÂú®ÊòæÂ≠ò‰∏çÂ§üÁî®ÁöÑÊÉÖÂÜµ‰∏ã‰ΩøÁî®ÂÜÖÂ≠òÊõø‰ª£ÊòæÂ≠òÁöÑÂç†Áî®Ôºå‰ΩøÁî®ÂâçËØ∑ÂÖàÁúãËØ¥ÊòéPDFÊñáÊ°£ÔºÅ\n„ÄêÁÇπÊàëË∑≥ËΩ¨WuXiaSD„Äë\nWuXiaSD-Studio\nÂ±û‰∫éÊàëÁî®Âï•Ôºå‰Ω†Áî®Âï•ÁöÑÁâàÊú¨ÔºÅ‰∏ÄÂàá‰ª•ÊèêÈ´òÁîü‰∫ßÂäõ‰∏∫ÁõÆÊ†áËÄåÈùûÊ∏∏ÊàèPÂõæÔºåÊâÄ‰ª•‰ΩìÁßØÊØîËæÉÊîæÈ£ûÔºå‰ΩÜÂäüËÉΩ‰∏ÄÂ∫î‰ø±ÂÖ®ÔºÅ\n„ÄêÁÇπÊàëË∑≥ËΩ¨WuXiaSD-Studio„Äë\nAIÁªòÁîªÁ®ãÂ∫è‰ΩøÁî®ÊïôÁ®ã\nÁÆÄÊòìËΩ¨ÁªòËßÜÈ¢ë‰ªãÁªç\nÁî®‰∫éÊà™ÂõæËΩ¨ÁªòÁöÑËßÜÈ¢ëÊìç‰ΩúÊºîÁ§∫ÔºåÂê´Á¥†ÊùêÂíåÊïàÊûúÈ¢ÑËßà„ÄêÁÇπÊàë‰∏ãËΩΩÁÆÄÊòìËΩ¨ÁªòËßÜÈ¢ë‰ªãÁªç„Äë\nStable Diffusion WebUIÁ≥ªÁªüËØæ\nÈùûÂ∏∏Á≥ªÁªüÁöÑSDËßÜÈ¢ëËØæÁ®ãÂêàÈõÜÔºåÈÄö‰øóÊòìÊáÇ‰∏îÂÜÖÂÆπËØ¶Â∞ΩÔºåÁî±BÁ´ôUP@NenlyÂêåÂ≠¶Âà∂‰Ωú„ÄêÁÇπÊàëÂâçÂæÄSDÁ≥ªÁªüËØæ„Äë\nSD-LORAÊ®°ÂûãËÆ≠ÁªÉÊïôÁ®ã\nÈùûÂ∏∏ËØ¶Â∞ΩÔºåÈíàÂØπLORAÊ®°ÂûãËÆ≠ÁªÉÁöÑÂèÇÊï∞ËÆ≤Ëß£ÔºåÊúâBÁ´ôUP@Êú±Â∞ºÈÖ±Âà∂‰Ωú„ÄêÁÇπÊàëÂâçÂæÄSDLORAËÆ≠ÁªÉÊïôÁ®ã„Äë\nÊïôÁ®ãÊñá‰ª∂Ôºö\n[PÁ´ô‰∏ì‰∫´]AIÁªòÁîªÊïôÂ≠¶\nÂà©Áî®AIÁªòÁîªËæÖÂä©Â§©ÂàÄËΩ¨ÁªòÁöÑÊïôÁ®ãÔºåÂåÖÂê´ÂêØÂä®ÊïôÁ®ã„ÄÅÂõæÁîüÂõæÊïôÁ®ã„ÄÅÂ±ÄÈÉ®ÈáçÁªòÊïôÁ®ã„ÄÅAIÂõæÁâáÊîæÂ§ßÊïôÁ®ã‰ª•ÂèäÊ∏∏ÊàèÊà™ÂõæÂÆûÊàòÊµÅÁ®ãÔºåÂùá‰∏∫PDFÊ†ºÂºèÔºåÂõæÁâáÂõ†ÊéíÁâàÈóÆÈ¢òÔºåÊâãÂä®ÊîæÂ§ßÂêéÂ∞±ËÉΩÁúãÊ∏Ö‰∫Ü„ÄêÁÇπÊàë‰∏ãËΩΩPÁ´ôÊïôÂ≠¶Êñá‰ª∂„Äë | „ÄêÁÇπÊàëËé∑ÂèñËß£Á†ÅÂØÜÁ†Å„Äë\nÈ¢ùÂ§ñÂÜÖÂÆπÔºö\n„ÄêÁÇπÊàëÂéªÊõ¥Â§öËµÑÊ∫êÂ∏ñ„Äë | „ÄêÁÇπÊàëÂéªËµûÂä©ËµÑÊ∫êÂ∏ñ„Äë",
    "Salesforce/blip2-opt-2.7b": "BLIP-2, OPT-2.7b, pre-trained only\nModel description\nDirect Use and Downstream Use\nBias, Risks, Limitations, and Ethical Considerations\nEthical Considerations\nHow to use\nMemory requirements\nBLIP-2, OPT-2.7b, pre-trained only\nBLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters).\nIt was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository.\nDisclaimer: The team releasing BLIP-2 did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nBLIP-2 consists of 3 models: a CLIP-like image encoder, a Querying Transformer (Q-Former) and a large language model.\nThe authors initialize the weights of the image encoder and large language model from pre-trained checkpoints and keep them frozen\nwhile training the Querying Transformer, which is a BERT-like Transformer encoder that maps a set of \"query tokens\" to query embeddings,\nwhich bridge the gap between the embedding space of the image encoder and the large language model.\nThe goal for the model is simply to predict the next text token, giving the query embeddings and the previous text.\nThis allows the model to be used for tasks like:\nimage captioning\nvisual question answering (VQA)\nchat-like conversations by feeding the image and the previous conversation as prompt to the model\nDirect Use and Downstream Use\nYou can use the raw model for conditional text generation given an image and optional text. See the model hub to look for\nfine-tuned versions on a task that interests you.\nBias, Risks, Limitations, and Ethical Considerations\nBLIP2-OPT uses off-the-shelf OPT as the language model. It inherits the same risks and limitations as mentioned in Meta's model card.\nLike other large language models for which the diversity (or lack thereof) of training\ndata induces downstream impact on the quality of our model, OPT-175B has limitations in terms\nof bias and safety. OPT-175B can also have quality issues in terms of generation diversity and\nhallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern\nlarge language models.\nBLIP2 is fine-tuned on image-text datasets (e.g. LAION ) collected from the internet.  As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\nBLIP2 has not been tested in real world applications. It should not be directly deployed in any applications. Researchers should first carefully assess the safety and fairness of the model in relation to the specific context they‚Äôre being deployed within.\nEthical Considerations\nThis release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people‚Äôs lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.\nHow to use\nFor code examples, we refer to the documentation.\nMemory requirements\nThe memory requirements differ based on the precision one uses. One can use 4-bit inference using Bitsandbytes, which greatly reduce the memory requirements.\ndtype\nLargest Layer or Residual Group\nTotal Size\nTraining using Adam\nfloat32\n490.94 MB\n14.43 GB\n57.72 GB\nfloat16/bfloat16\n245.47 MB\n7.21 GB\n28.86 GB\nint8\n122.73 MB\n3.61 GB\n14.43 GB\nint4\n61.37 MB\n1.8 GB\n7.21 GB\nRunning the model on CPU\nClick to expand\nimport requests\nfrom PIL import Image\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\")\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True).strip())\nRunning the model on GPU\nIn full precision\nClick to expand\n# pip install accelerate\nimport requests\nfrom PIL import Image\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", device_map=\"auto\")\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\")\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True).strip())\nIn half precision (float16)\nClick to expand\n# pip install accelerate\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16, device_map=\"auto\")\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True).strip())\nIn 8-bit precision (int8)\nClick to expand\n# pip install accelerate bitsandbytes\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map=\"auto\")\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True).strip())",
    "lllyasviel/Annotators": "README.md exists but content is empty.",
    "comfyanonymous/ControlNet-v1-1_fp16_safetensors": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nSafetensors/FP16 versions of the new ControlNet-v1-1 checkpoints.\nBest used with ComfyUI but should work fine with all other UIs that support controlnets.",
    "kalpeshk2011/dipper-paraphraser-xxl": "Paper and Github Repository\nWhat is DIPPER?\nUsing DIPPER\nThis is the HuggingFace model release of our paper \"Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense\".\nPaper and Github Repository\nPaper: https://arxiv.org/abs/2303.13408Code: https://github.com/martiansideofthemoon/ai-detection-paraphrasesUsage instructions: https://github.com/martiansideofthemoon/ai-detection-paraphrases#running-the-paraphraser-model-dipper\nWhat is DIPPER?\nDIPPER (\"Discourse Paraphraser\") is a 11B parameter paraphrase generation model built by fine-tuning T5-XXL. DIPPER possesses two unique features that help its outputs evade AI-generated text detectors:\nParaphrasing long-form text in context: Most modern paraphrasers are exclusively trained on sentence-level data, ignoring discourse-level information. However, many critical use cases of LLMs involve generating long-form text in responses to detailed userspecified prompts. Thus, we train DIPPER to paraphrase paragraph-length texts, re-order content, and optionally leverage context such as input prompts.\nControlling output diversity: Another weakness of existing paraphrasers is that they lack an easy way to control output diversity. An attacker may want to apply just the minimum amount of lexical and syntactic modifications necessary to evade a detection algorithm. DIPPER provides users with two intuitive scalar control knobs at inference time that are trained end-to-end: one controls the lexical diversity of the paraphrase, and the other controls the amount of content re-ordering.\nWe leverage the PAR3 dataset publicly released by Thai et al. (2022) to train DIPPER. This dataset contains multiple translations of non-English novels into English aligned at a paragraph level (e.g., it contains both the Henry Morley and Robert Adams translations of Voltaire‚Äôs Candide), which we treat as paragraphlevel paraphrases and use to train our paraphraser.\nUsing DIPPER\nFull instructions: https://github.com/martiansideofthemoon/ai-detection-paraphrases#running-the-paraphraser-model-dipper\nWe suggest using the code below to use the model correctly:\nclass DipperParaphraser(object):\ndef __init__(self, model=\"kalpeshk2011/dipper-paraphraser-xxl\", verbose=True):\ntime1 = time.time()\nself.tokenizer = T5Tokenizer.from_pretrained('google/t5-v1_1-xxl')\nself.model = T5ForConditionalGeneration.from_pretrained(model)\nif verbose:\nprint(f\"{model} model loaded in {time.time() - time1}\")\nself.model.cuda()\nself.model.eval()\ndef paraphrase(self, input_text, lex_diversity, order_diversity, prefix=\"\", sent_interval=3, **kwargs):\n\"\"\"Paraphrase a text using the DIPPER model.\nArgs:\ninput_text (str): The text to paraphrase. Make sure to mark the sentence to be paraphrased between <sent> and </sent> blocks, keeping space on either side.\nlex_diversity (int): The lexical diversity of the output, choose multiples of 20 from 0 to 100. 0 means no diversity, 100 means maximum diversity.\norder_diversity (int): The order diversity of the output, choose multiples of 20 from 0 to 100. 0 means no diversity, 100 means maximum diversity.\n**kwargs: Additional keyword arguments like top_p, top_k, max_length.\n\"\"\"\nassert lex_diversity in [0, 20, 40, 60, 80, 100], \"Lexical diversity must be one of 0, 20, 40, 60, 80, 100.\"\nassert order_diversity in [0, 20, 40, 60, 80, 100], \"Order diversity must be one of 0, 20, 40, 60, 80, 100.\"\nlex_code = int(100 - lex_diversity)\norder_code = int(100 - order_diversity)\ninput_text = \" \".join(input_text.split())\nsentences = sent_tokenize(input_text)\nprefix = \" \".join(prefix.replace(\"\\n\", \" \").split())\noutput_text = \"\"\nfor sent_idx in range(0, len(sentences), sent_interval):\ncurr_sent_window = \" \".join(sentences[sent_idx:sent_idx + sent_interval])\nfinal_input_text = f\"lexical = {lex_code}, order = {order_code}\"\nif prefix:\nfinal_input_text += f\" {prefix}\"\nfinal_input_text += f\" <sent> {curr_sent_window} </sent>\"\nfinal_input = self.tokenizer([final_input_text], return_tensors=\"pt\")\nfinal_input = {k: v.cuda() for k, v in final_input.items()}\nwith torch.inference_mode():\noutputs = self.model.generate(**final_input, **kwargs)\noutputs = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\nprefix += \" \" + outputs[0]\noutput_text += \" \" + outputs[0]\nreturn output_text\nif __name__ == \"__main__\":\ndp = DipperParaphraser()\nprompt = \"In a shocking finding, scientist discovered a herd of unicorns living in a remote valley.\"\ninput_text = \"They have never been known to mingle with humans. Today, it is believed these unicorns live in an unspoilt environment which is surrounded by mountains. Its edge is protected by a thick wattle of wattle trees, giving it a majestic appearance. Along with their so-called miracle of multicolored coat, their golden coloured feather makes them look like mirages. Some of them are rumored to be capable of speaking a large amount of different languages. They feed on elk and goats as they were selected from those animals that possess a fierceness to them, and can \\\"eat\\\" them with their long horns.\"\nprint(f\"Input = {prompt} <sent> {input_text} </sent>\\n\")\noutput_l60_sample = dp.paraphrase(input_text, lex_diversity=60, order_diversity=0, prefix=prompt, do_sample=True, top_p=0.75, top_k=None, max_length=512)\nprint(f\"Output (Lexical diversity = 60, Sample p = 0.75) = {output_l60_sample}\\n\")",
    "intfloat/multilingual-e5-base": "Multilingual-E5-base\nUsage\nSupported Languages\nTraining Details\nBenchmark Results on Mr. TyDi\nMTEB Benchmark Evaluation\nSupport for Sentence Transformers\nFAQ\nCitation\nLimitations\nMultilingual-E5-base\nMultilingual E5 Text Embeddings: A Technical Report.\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, arXiv 2024\nThis model has 12 layers and the embedding size is 768.\nUsage\nBelow is an example to encode queries and passages from the MS-MARCO passage ranking dataset.\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ndef average_pool(last_hidden_states: Tensor,\nattention_mask: Tensor) -> Tensor:\nlast_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\nreturn last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n# Each input text should start with \"query: \" or \"passage: \", even for non-English texts.\n# For tasks other than retrieval, you can simply use the \"query: \" prefix.\ninput_texts = ['query: how much protein should a female eat',\n'query: ÂçóÁìúÁöÑÂÆ∂Â∏∏ÂÅöÊ≥ï',\n\"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n\"passage: 1.Ê∏ÖÁÇíÂçóÁìú‰∏ù ÂéüÊñô:Â´©ÂçóÁìúÂçä‰∏™ Ë∞ÉÊñô:Ëë±„ÄÅÁõê„ÄÅÁôΩÁ≥ñ„ÄÅÈ∏°Á≤æ ÂÅöÊ≥ï: 1„ÄÅÂçóÁìúÁî®ÂàÄËñÑËñÑÁöÑÂâäÂéªË°®Èù¢‰∏ÄÂ±ÇÁöÆ,Áî®Âã∫Â≠êÂàÆÂéªÁì§ 2„ÄÅÊì¶ÊàêÁªÜ‰∏ù(Ê≤°ÊúâÊì¶ËèúÊùøÂ∞±Áî®ÂàÄÊÖ¢ÊÖ¢ÂàáÊàêÁªÜ‰∏ù) 3„ÄÅÈîÖÁÉßÁÉ≠ÊîæÊ≤π,ÂÖ•Ëë±Ëä±ÁÖ∏Âá∫È¶ôÂë≥ 4„ÄÅÂÖ•ÂçóÁìú‰∏ùÂø´ÈÄüÁøªÁÇí‰∏ÄÂàÜÈíüÂ∑¶Âè≥,ÊîæÁõê„ÄÅ‰∏ÄÁÇπÁôΩÁ≥ñÂíåÈ∏°Á≤æË∞ÉÂë≥Âá∫ÈîÖ 2.È¶ôËë±ÁÇíÂçóÁìú ÂéüÊñô:ÂçóÁìú1Âè™ Ë∞ÉÊñô:È¶ôËë±„ÄÅËíúÊú´„ÄÅÊ©ÑÊ¶ÑÊ≤π„ÄÅÁõê ÂÅöÊ≥ï: 1„ÄÅÂ∞ÜÂçóÁìúÂéªÁöÆ,ÂàáÊàêÁâá 2„ÄÅÊ≤πÈîÖ8ÊàêÁÉ≠Âêé,Â∞ÜËíúÊú´ÊîæÂÖ•ÁàÜÈ¶ô 3„ÄÅÁàÜÈ¶ôÂêé,Â∞ÜÂçóÁìúÁâáÊîæÂÖ•,ÁøªÁÇí 4„ÄÅÂú®ÁøªÁÇíÁöÑÂêåÊó∂,ÂèØ‰ª•‰∏çÊó∂Âú∞ÂæÄÈîÖÈáåÂä†Ê∞¥,‰ΩÜ‰∏çË¶ÅÂ§™Â§ö 5„ÄÅÊîæÂÖ•Áõê,ÁÇíÂåÄ 6„ÄÅÂçóÁìúÂ∑Æ‰∏çÂ§öËΩØÂíåÁªµ‰∫Ü‰πãÂêé,Â∞±ÂèØ‰ª•ÂÖ≥ÁÅ´ 7„ÄÅÊííÂÖ•È¶ôËë±,Âç≥ÂèØÂá∫ÈîÖ\"]\ntokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-base')\nmodel = AutoModel.from_pretrained('intfloat/multilingual-e5-base')\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\nSupported Languages\nThis model is initialized from xlm-roberta-base\nand continually trained on a mixture of multilingual datasets.\nIt supports 100 languages from xlm-roberta,\nbut low-resource languages may see performance degradation.\nTraining Details\nInitialization: xlm-roberta-base\nFirst stage: contrastive pre-training with weak supervision\nDataset\nWeak supervision\n# of text pairs\nFiltered mC4\n(title, page content)\n1B\nCC News\n(title, news content)\n400M\nNLLB\ntranslation pairs\n2.4B\nWikipedia\n(hierarchical section title, passage)\n150M\nFiltered Reddit\n(comment, response)\n800M\nS2ORC\n(title, abstract) and citation pairs\n100M\nStackexchange\n(question, answer)\n50M\nxP3\n(input prompt, response)\n80M\nMiscellaneous unsupervised SBERT data\n-\n10M\nSecond stage: supervised fine-tuning\nDataset\nLanguage\n# of text pairs\nMS MARCO\nEnglish\n500k\nNQ\nEnglish\n70k\nTrivia QA\nEnglish\n60k\nNLI from SimCSE\nEnglish\n<300k\nELI5\nEnglish\n500k\nDuReader Retrieval\nChinese\n86k\nKILT Fever\nEnglish\n70k\nKILT HotpotQA\nEnglish\n70k\nSQuAD\nEnglish\n87k\nQuora\nEnglish\n150k\nMr. TyDi\n11 languages\n50k\nMIRACL\n16 languages\n40k\nFor all labeled datasets, we only use its training set for fine-tuning.\nFor other training details, please refer to our paper at https://arxiv.org/pdf/2402.05672.\nBenchmark Results on Mr. TyDi\nModel\nAvg MRR@10\nar\nbn\nen\nfi\nid\nja\nko\nru\nsw\nte\nth\nBM25\n33.3\n36.7\n41.3\n15.1\n28.8\n38.2\n21.7\n28.1\n32.9\n39.6\n42.4\n41.7\nmDPR\n16.7\n26.0\n25.8\n16.2\n11.3\n14.6\n18.1\n21.9\n18.5\n7.3\n10.6\n13.5\nBM25 + mDPR\n41.7\n49.1\n53.5\n28.4\n36.5\n45.5\n35.5\n36.2\n42.7\n40.5\n42.0\n49.2\nmultilingual-e5-small\n64.4\n71.5\n66.3\n54.5\n57.7\n63.2\n55.4\n54.3\n60.8\n65.4\n89.1\n70.1\nmultilingual-e5-base\n65.9\n72.3\n65.0\n58.5\n60.8\n64.9\n56.6\n55.8\n62.7\n69.0\n86.6\n72.7\nmultilingual-e5-large\n70.5\n77.5\n73.2\n60.8\n66.8\n68.5\n62.5\n61.6\n65.8\n72.7\n90.2\n76.2\nMTEB Benchmark Evaluation\nCheck out unilm/e5 to reproduce evaluation results\non the BEIR and MTEB benchmark.\nSupport for Sentence Transformers\nBelow is an example for usage with sentence_transformers.\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('intfloat/multilingual-e5-base')\ninput_texts = [\n'query: how much protein should a female eat',\n'query: ÂçóÁìúÁöÑÂÆ∂Â∏∏ÂÅöÊ≥ï',\n\"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 i     s 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or traini     ng for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n\"passage: 1.Ê∏ÖÁÇíÂçóÁìú‰∏ù ÂéüÊñô:Â´©ÂçóÁìúÂçä‰∏™ Ë∞ÉÊñô:Ëë±„ÄÅÁõê„ÄÅÁôΩÁ≥ñ„ÄÅÈ∏°Á≤æ ÂÅöÊ≥ï: 1„ÄÅÂçóÁìúÁî®ÂàÄËñÑËñÑÁöÑÂâäÂéªË°®Èù¢‰∏ÄÂ±ÇÁöÆ     ,Áî®Âã∫Â≠êÂàÆÂéªÁì§ 2„ÄÅÊì¶ÊàêÁªÜ‰∏ù(Ê≤°ÊúâÊì¶ËèúÊùøÂ∞±Áî®ÂàÄÊÖ¢ÊÖ¢ÂàáÊàêÁªÜ‰∏ù) 3„ÄÅÈîÖÁÉßÁÉ≠ÊîæÊ≤π,ÂÖ•Ëë±Ëä±ÁÖ∏Âá∫È¶ôÂë≥ 4„ÄÅÂÖ•ÂçóÁìú‰∏ùÂø´ÈÄüÁøªÁÇí‰∏ÄÂàÜÈíüÂ∑¶Âè≥,     ÊîæÁõê„ÄÅ‰∏ÄÁÇπÁôΩÁ≥ñÂíåÈ∏°Á≤æË∞ÉÂë≥Âá∫ÈîÖ 2.È¶ôËë±ÁÇíÂçóÁìú ÂéüÊñô:ÂçóÁìú1Âè™ Ë∞ÉÊñô:È¶ôËë±„ÄÅËíúÊú´„ÄÅÊ©ÑÊ¶ÑÊ≤π„ÄÅÁõê ÂÅöÊ≥ï: 1„ÄÅÂ∞ÜÂçóÁìúÂéªÁöÆ,ÂàáÊàêÁâá 2„ÄÅÊ≤π     ÈîÖ8ÊàêÁÉ≠Âêé,Â∞ÜËíúÊú´ÊîæÂÖ•ÁàÜÈ¶ô 3„ÄÅÁàÜÈ¶ôÂêé,Â∞ÜÂçóÁìúÁâáÊîæÂÖ•,ÁøªÁÇí 4„ÄÅÂú®ÁøªÁÇíÁöÑÂêåÊó∂,ÂèØ‰ª•‰∏çÊó∂Âú∞ÂæÄÈîÖÈáåÂä†Ê∞¥,‰ΩÜ‰∏çË¶ÅÂ§™Â§ö 5„ÄÅÊîæÂÖ•Áõê,ÁÇíÂåÄ      6„ÄÅÂçóÁìúÂ∑Æ‰∏çÂ§öËΩØÂíåÁªµ‰∫Ü‰πãÂêé,Â∞±ÂèØ‰ª•ÂÖ≥ÁÅ´ 7„ÄÅÊííÂÖ•È¶ôËë±,Âç≥ÂèØÂá∫ÈîÖ\"\n]\nembeddings = model.encode(input_texts, normalize_embeddings=True)\nPackage requirements\npip install sentence_transformers~=2.2.2\nContributors: michaelfeil\nFAQ\n1. Do I need to add the prefix \"query: \" and \"passage: \" to input texts?\nYes, this is how the model is trained, otherwise you will see a performance degradation.\nHere are some rules of thumb:\nUse \"query: \" and \"passage: \" correspondingly for asymmetric tasks such as passage retrieval in open QA, ad-hoc information retrieval.\nUse \"query: \" prefix for symmetric tasks such as semantic similarity, bitext mining, paraphrase retrieval.\nUse \"query: \" prefix if you want to use embeddings as features, such as linear probing classification, clustering.\n2. Why are my reproduced results slightly different from reported in the model card?\nDifferent versions of transformers and pytorch could cause negligible but non-zero performance differences.\n3. Why does the cosine similarity scores distribute around 0.7 to 1.0?\nThis is a known and expected behavior as we use a low temperature 0.01 for InfoNCE contrastive loss.\nFor text embedding tasks like text retrieval or semantic similarity,\nwhat matters is the relative order of the scores instead of the absolute values,\nso this should not be an issue.\nCitation\nIf you find our paper or models helpful, please consider cite as follows:\n@article{wang2024multilingual,\ntitle={Multilingual E5 Text Embeddings: A Technical Report},\nauthor={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\njournal={arXiv preprint arXiv:2402.05672},\nyear={2024}\n}\nLimitations\nLong texts will be truncated to at most 512 tokens.",
    "moka-ai/m3e-base": "üÖú M3E Models\nüÜï Êõ¥Êñ∞ËØ¥Êòé\n‚öñÔ∏è Ê®°ÂûãÂØπÊØî\nüîß ‰ΩøÁî® M3E\nüé® ÂæÆË∞ÉÊ®°Âûã\n‚ûø ËÆ≠ÁªÉÊñπÊ°à\nüåü ÁâπÊÄß\nüíØ MTEB-zh ËØÑÊµã\nÊñáÊú¨ÂàÜÁ±ª\nÊ£ÄÁ¥¢ÊéíÂ∫è\nüìÇ M3EÊï∞ÊçÆÈõÜ\nüóìÔ∏è ËÆ°ÂàíË°®\nüôè Ëá¥Ë∞¢\nüìú License\nCitation\nüÖú M3E Models\nm3e-small | m3e-base\nM3E ÊòØ Moka Massive Mixed Embedding ÁöÑÁº©ÂÜô\nMokaÔºåÊ≠§Ê®°ÂûãÁî± MokaAI ËÆ≠ÁªÉÔºåÂºÄÊ∫êÂíåËØÑÊµãÔºåËÆ≠ÁªÉËÑöÊú¨‰ΩøÁî® uniem ÔºåËØÑÊµã BenchMark ‰ΩøÁî® MTEB-zh\nMassiveÔºåÊ≠§Ê®°ÂûãÈÄöËøáÂçÉ‰∏áÁ∫ß (2200w+) ÁöÑ‰∏≠ÊñáÂè•ÂØπÊï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉ\nMixedÔºåÊ≠§Ê®°ÂûãÊîØÊåÅ‰∏≠Ëã±ÂèåËØ≠ÁöÑÂêåË¥®ÊñáÊú¨Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÔºåÂºÇË¥®ÊñáÊú¨Ê£ÄÁ¥¢Á≠âÂäüËÉΩÔºåÊú™Êù•Ëøò‰ºöÊîØÊåÅ‰ª£Á†ÅÊ£ÄÁ¥¢\nEmbeddingÔºåÊ≠§Ê®°ÂûãÊòØÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÔºåÂèØ‰ª•Â∞ÜËá™ÁÑ∂ËØ≠Ë®ÄËΩ¨Êç¢ÊàêÁ®†ÂØÜÁöÑÂêëÈáè\nüÜï Êõ¥Êñ∞ËØ¥Êòé\n2023.06.24ÔºåÊ∑ªÂä†ÂæÆË∞É M3E ÁöÑÊïôÁ®ã notebookÔºåÂá†Ë°å‰ª£Á†ÅÔºåÊõ¥‰Ω≥ÈÄÇÈÖçÔºÅ\n2023.06.14ÔºåÊ∑ªÂä†‰∫Ü‰∏â‰∏™‰∏≠ÊñáÂºÄÊ∫êÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÂà∞ËØÑÊµã‰∏≠ÔºåÂåÖÊã¨ UER, ErLangShen, DMetaSoul\n2023.06.08ÔºåÊ∑ªÂä†Ê£ÄÁ¥¢‰ªªÂä°ÁöÑËØÑÊµãÁªìÊûúÔºåÂú® T2Ranking 1W ‰∏≠ÊñáÊï∞ÊçÆÈõÜ‰∏äÔºåm3e-base Âú® ndcg@10 ‰∏äËææÂà∞‰∫Ü 0.8004ÔºåË∂ÖËøá‰∫Ü openai-ada-002 ÁöÑ 0.7786\n2023.06.07ÔºåÊ∑ªÂä†ÊñáÊú¨ÂàÜÁ±ª‰ªªÂä°ÁöÑËØÑÊµãÁªìÊûúÔºåÂú® 6 ÁßçÊñáÊú¨ÂàÜÁ±ªÊï∞ÊçÆÈõÜ‰∏äÔºåm3e-base Âú® accuracy ‰∏äËææÂà∞‰∫Ü 0.6157ÔºåË∂ÖËøá‰∫Ü openai-ada-002 ÁöÑ 0.5956\n‚öñÔ∏è Ê®°ÂûãÂØπÊØî\nÂèÇÊï∞Êï∞Èáè\nÁª¥Â∫¶\n‰∏≠Êñá\nËã±Êñá\ns2s\ns2p\ns2c\nÂºÄÊ∫ê\nÂÖºÂÆπÊÄß\ns2s Acc\ns2p ndcg@10\nm3e-small\n24M\n512\nÊòØ\nÂê¶\nÊòØ\nÂê¶\nÂê¶\nÊòØ\n‰ºò\n0.5834\n0.7262\nm3e-base\n110M\n768\nÊòØ\nÊòØ\nÊòØ\nÊòØ\nÂê¶\nÊòØ\n‰ºò\n0.6157\n0.8004\ntext2vec\n110M\n768\nÊòØ\nÂê¶\nÊòØ\nÂê¶\nÂê¶\nÊòØ\n‰ºò\n0.5755\n0.6346\nopenai-ada-002\nÊú™Áü•\n1536\nÊòØ\nÊòØ\nÊòØ\nÊòØ\nÊòØ\nÂê¶\n‰ºò\n0.5956\n0.7786\nËØ¥ÊòéÔºö\ns2s, Âç≥ sentence to sentence Ôºå‰ª£Ë°®‰∫ÜÂêåË¥®ÊñáÊú¨‰πãÈó¥ÁöÑÂµåÂÖ•ËÉΩÂäõÔºåÈÄÇÁî®‰ªªÂä°ÔºöÊñáÊú¨Áõ∏‰ººÂ∫¶ÔºåÈáçÂ§çÈóÆÈ¢òÊ£ÄÊµãÔºåÊñáÊú¨ÂàÜÁ±ªÁ≠â\ns2p, Âç≥ sentence to passage Ôºå‰ª£Ë°®‰∫ÜÂºÇË¥®ÊñáÊú¨‰πãÈó¥ÁöÑÂµåÂÖ•ËÉΩÂäõÔºåÈÄÇÁî®‰ªªÂä°ÔºöÊñáÊú¨Ê£ÄÁ¥¢ÔºåGPT ËÆ∞ÂøÜÊ®°ÂùóÁ≠â\ns2c, Âç≥ sentence to code Ôºå‰ª£Ë°®‰∫ÜËá™ÁÑ∂ËØ≠Ë®ÄÂíåÁ®ãÂ∫èËØ≠Ë®Ä‰πãÈó¥ÁöÑÂµåÂÖ•ËÉΩÂäõÔºåÈÄÇÁî®‰ªªÂä°Ôºö‰ª£Á†ÅÊ£ÄÁ¥¢\nÂÖºÂÆπÊÄßÔºå‰ª£Ë°®‰∫ÜÊ®°ÂûãÂú®ÂºÄÊ∫êÁ§æÂå∫‰∏≠ÂêÑÁßçÈ°πÁõÆË¢´ÊîØÊåÅÁöÑÁ®ãÂ∫¶ÔºåÁî±‰∫é m3e Âíå text2vec ÈÉΩÂèØ‰ª•Áõ¥Êé•ÈÄöËøá sentence-transformers Áõ¥Êé•‰ΩøÁî®ÔºåÊâÄ‰ª•Âíå openai Âú®Á§æÂå∫ÁöÑÊîØÊåÅÂ∫¶‰∏äÁõ∏ÂΩì\nACC & ndcg@10ÔºåËØ¶ÊÉÖËßÅ‰∏ãÊñπÁöÑËØÑÊµã\nTips:\n‰ΩøÁî®Âú∫ÊôØ‰∏ªË¶ÅÊòØ‰∏≠ÊñáÔºåÂ∞ëÈáèËã±ÊñáÁöÑÊÉÖÂÜµÔºåÂª∫ËÆÆ‰ΩøÁî® m3e Á≥ªÂàóÁöÑÊ®°Âûã\nÂ§öËØ≠Ë®Ä‰ΩøÁî®Âú∫ÊôØÔºåÂπ∂‰∏î‰∏ç‰ªãÊÑèÊï∞ÊçÆÈöêÁßÅÁöÑËØùÔºåÊàëÂª∫ËÆÆ‰ΩøÁî® openai text-embedding-ada-002\n‰ª£Á†ÅÊ£ÄÁ¥¢Âú∫ÊôØÔºåÊé®Ëçê‰ΩøÁî® openai text-embedding-ada-002\nÊñáÊú¨Ê£ÄÁ¥¢Âú∫ÊôØÔºåËØ∑‰ΩøÁî®ÂÖ∑Â§áÊñáÊú¨Ê£ÄÁ¥¢ËÉΩÂäõÁöÑÊ®°ÂûãÔºåÂè™Âú® S2S ‰∏äËÆ≠ÁªÉÁöÑÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÔºåÊ≤°ÊúâÂäûÊ≥ïÂÆåÊàêÊñáÊú¨Ê£ÄÁ¥¢‰ªªÂä°\nüîß ‰ΩøÁî® M3E\nÊÇ®ÈúÄË¶ÅÂÖàÂÆâË£Ö sentence-transformers\npip install -U sentence-transformers\nÂÆâË£ÖÂÆåÊàêÂêéÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ã‰ª£Á†ÅÊù•‰ΩøÁî® M3E Models\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('moka-ai/m3e-base')\n#Our sentences we like to encode\nsentences = [\n'* Moka Ê≠§ÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÁî± MokaAI ËÆ≠ÁªÉÂπ∂ÂºÄÊ∫êÔºåËÆ≠ÁªÉËÑöÊú¨‰ΩøÁî® uniem',\n'* Massive Ê≠§ÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÈÄöËøá**ÂçÉ‰∏áÁ∫ß**ÁöÑ‰∏≠ÊñáÂè•ÂØπÊï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉ',\n'* Mixed Ê≠§ÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÊîØÊåÅ‰∏≠Ëã±ÂèåËØ≠ÁöÑÂêåË¥®ÊñáÊú¨Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÔºåÂºÇË¥®ÊñáÊú¨Ê£ÄÁ¥¢Á≠âÂäüËÉΩÔºåÊú™Êù•Ëøò‰ºöÊîØÊåÅ‰ª£Á†ÅÊ£ÄÁ¥¢ÔºåALL in one'\n]\n#Sentences are encoded by calling model.encode()\nembeddings = model.encode(sentences)\n#Print the embeddings\nfor sentence, embedding in zip(sentences, embeddings):\nprint(\"Sentence:\", sentence)\nprint(\"Embedding:\", embedding)\nprint(\"\")\nM3E Á≥ªÂàóÁöÑÊâÄÊúâÊ®°ÂûãÂú®ËÆæËÆ°ÁöÑÊó∂ÂÄôÂ∞±ËÄÉËôëÂà∞ÂÆåÂÖ®ÂÖºÂÆπ sentence-transformers ÔºåÊâÄ‰ª•‰Ω†ÂèØ‰ª•ÈÄöËøáÊõøÊç¢ÂêçÁß∞Â≠óÁ¨¶‰∏≤ÁöÑÊñπÂºèÂú®ÊâÄÊúâÊîØÊåÅ sentence-transformers ÁöÑÈ°πÁõÆ‰∏≠Êó†Áºù‰ΩøÁî® M3E ModelsÔºåÊØîÂ¶Ç chroma, guidance, semantic-kernel „ÄÇ\nüé® ÂæÆË∞ÉÊ®°Âûã\nuniem Êèê‰æõ‰∫ÜÈùûÂ∏∏ÊòìÁî®ÁöÑ finetune Êé•Âè£ÔºåÂá†Ë°å‰ª£Á†ÅÔºåÂç≥ÂàªÈÄÇÈÖçÔºÅ\nfrom datasets import load_dataset\nfrom uniem.finetuner import FineTuner\ndataset = load_dataset('shibing624/nli_zh', 'STS-B')\n# ÊåáÂÆöËÆ≠ÁªÉÁöÑÊ®°Âûã‰∏∫ m3e-small\nfinetuner = FineTuner.from_pretrained('moka-ai/m3e-small', dataset=dataset)\nfinetuner.run(epochs=1)\nËØ¶ËßÅ uniem ÂæÆË∞ÉÊïôÁ®ã\n‚ûø ËÆ≠ÁªÉÊñπÊ°à\nM3E ‰ΩøÁî® in-batch Ë¥üÈááÊ†∑ÁöÑÂØπÊØîÂ≠¶‰π†ÁöÑÊñπÂºèÂú®Âè•ÂØπÊï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉÔºå‰∏∫‰∫Ü‰øùËØÅ in-batch Ë¥üÈááÊ†∑ÁöÑÊïàÊûúÔºåÊàë‰ª¨‰ΩøÁî® A100 80G Êù•ÊúÄÂ§ßÂåñ batch-sizeÔºåÂπ∂Âú®ÂÖ±ËÆ° 2200W+ ÁöÑÂè•ÂØπÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉ‰∫Ü 1 epoch„ÄÇËÆ≠ÁªÉËÑöÊú¨‰ΩøÁî® uniemÔºåÊÇ®ÂèØ‰ª•Âú®ËøôÈáåÊü•ÁúãÂÖ∑‰ΩìÁªÜËäÇ„ÄÇ\nüåü ÁâπÊÄß\n‰∏≠ÊñáËÆ≠ÁªÉÈõÜÔºåM3E Âú®Â§ßËßÑÊ®°Âè•ÂØπÊï∞ÊçÆÈõÜ‰∏äÁöÑËÆ≠ÁªÉÔºåÂåÖÂê´‰∏≠ÊñáÁôæÁßëÔºåÈáëËûçÔºåÂåªÁñóÔºåÊ≥ïÂæãÔºåÊñ∞ÈóªÔºåÂ≠¶ÊúØÁ≠âÂ§ö‰∏™È¢ÜÂüüÂÖ±ËÆ° 2200W Âè•ÂØπÊ†∑Êú¨ÔºåÊï∞ÊçÆÈõÜËØ¶ËßÅ M3E Êï∞ÊçÆÈõÜ\nËã±ÊñáËÆ≠ÁªÉÈõÜÔºåM3E ‰ΩøÁî® MEDI 145W Ëã±Êñá‰∏âÂÖÉÁªÑÊï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉÔºåÊï∞ÊçÆÈõÜËØ¶ËßÅ MEDI Êï∞ÊçÆÈõÜÔºåÊ≠§Êï∞ÊçÆÈõÜÁî± instructor team Êèê‰æõ\nÊåá‰ª§Êï∞ÊçÆÈõÜÔºåM3E ‰ΩøÁî®‰∫Ü 300W + ÁöÑÊåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆÈõÜÔºåËøô‰ΩøÂæó M3E ÂØπÊñáÊú¨ÁºñÁ†ÅÁöÑÊó∂ÂÄôÂèØ‰ª•ÈÅµ‰ªéÊåá‰ª§ÔºåËøôÈÉ®ÂàÜÁöÑÂ∑•‰Ωú‰∏ªË¶ÅË¢´ÂêØÂèë‰∫é instructor-embedding\nÂü∫Á°ÄÊ®°ÂûãÔºåM3E ‰ΩøÁî® hfl ÂÆûÈ™åÂÆ§ÁöÑ Roberta Á≥ªÂàóÊ®°ÂûãËøõË°åËÆ≠ÁªÉÔºåÁõÆÂâçÊèê‰æõ  small Âíå  base ‰∏§‰∏™ÁâàÊú¨ÔºåÂ§ßÂÆ∂ÂàôÈúÄÈÄâÁî®\nALL IN ONEÔºåM3E Êó®Âú®Êèê‰æõ‰∏Ä‰∏™ ALL IN ONE ÁöÑÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÔºå‰∏ç‰ªÖÊîØÊåÅÂêåË¥®Âè•Â≠êÁõ∏‰ººÂ∫¶Âà§Êñ≠ÔºåËøòÊîØÊåÅÂºÇË¥®ÊñáÊú¨Ê£ÄÁ¥¢Ôºå‰Ω†Âè™ÈúÄË¶Å‰∏Ä‰∏™Ê®°ÂûãÂ∞±ÂèØ‰ª•Ë¶ÜÁõñÂÖ®ÈÉ®ÁöÑÂ∫îÁî®Âú∫ÊôØÔºåÊú™Êù•Ëøò‰ºöÊîØÊåÅ‰ª£Á†ÅÊ£ÄÁ¥¢\nüíØ MTEB-zh ËØÑÊµã\nËØÑÊµãÊ®°ÂûãÔºåtext2vec, m3e-base, m3e-small, openai text-embedding-ada-002, DMetaSoul, UER, ErLangShen\nËØÑÊµãËÑöÊú¨ÔºåÂÖ∑‰ΩìÂèÇËÄÉ [MTEB-zh] (https://github.com/wangyuxinwhy/uniem/blob/main/mteb-zh)\nÊñáÊú¨ÂàÜÁ±ª\nÊï∞ÊçÆÈõÜÈÄâÊã©ÔºåÈÄâÊã©ÂºÄÊ∫êÂú® HuggingFace ‰∏äÁöÑ 6 ÁßçÊñáÊú¨ÂàÜÁ±ªÊï∞ÊçÆÈõÜÔºåÂåÖÊã¨Êñ∞Èóª„ÄÅÁîµÂïÜËØÑËÆ∫„ÄÅËÇ°Á•®ËØÑËÆ∫„ÄÅÈïøÊñáÊú¨Á≠â\nËØÑÊµãÊñπÂºèÔºå‰ΩøÁî® MTEB ÁöÑÊñπÂºèËøõË°åËØÑÊµãÔºåÊä•Âëä Accuracy„ÄÇ\ntext2vec\nm3e-small\nm3e-base\nopenai\nDMetaSoul\nuer\nerlangshen\nTNews\n0.43\n0.4443\n0.4827\n0.4594\n0.3084\n0.3539\n0.4361\nJDIphone\n0.8214\n0.8293\n0.8533\n0.746\n0.7972\n0.8283\n0.8356\nGubaEastmony\n0.7472\n0.712\n0.7621\n0.7574\n0.735\n0.7534\n0.7787\nTYQSentiment\n0.6099\n0.6596\n0.7188\n0.68\n0.6437\n0.6662\n0.6444\nStockComSentiment\n0.4307\n0.4291\n0.4363\n0.4819\n0.4309\n0.4555\n0.4482\nIFlyTek\n0.414\n0.4263\n0.4409\n0.4486\n0.3969\n0.3762\n0.4241\nAverage\n0.5755\n0.5834\n0.6157\n0.5956\n0.552016667\n0.57225\n0.594516667\nÊ£ÄÁ¥¢ÊéíÂ∫è\nT2Ranking 1W\nÊï∞ÊçÆÈõÜÈÄâÊã©Ôºå‰ΩøÁî® T2Ranking Êï∞ÊçÆÈõÜÔºåÁî±‰∫é T2Ranking ÁöÑÊï∞ÊçÆÈõÜÂ§™Â§ßÔºåopenai ËØÑÊµãËµ∑Êù•ÁöÑÊó∂Èó¥ÊàêÊú¨Âíå api Ë¥πÁî®Êúâ‰∫õÈ´òÔºåÊâÄ‰ª•Êàë‰ª¨Âè™ÈÄâÊã©‰∫Ü T2Ranking ‰∏≠ÁöÑÂâç 10000 ÁØáÊñáÁ´†\nËØÑÊµãÊñπÂºèÔºå‰ΩøÁî® MTEB ÁöÑÊñπÂºèËøõË°åËØÑÊµãÔºåÊä•Âëä map@1, map@10, mrr@1, mrr@10, ndcg@1, ndcg@10\nÊ≥®ÊÑèÔºÅ‰ªéÂÆûÈ™åÁªìÊûúÂíåËÆ≠ÁªÉÊñπÂºèÊù•ÁúãÔºåÈô§‰∫Ü M3E Ê®°ÂûãÂíå openai Ê®°ÂûãÂ§ñÔºåÂÖ∂‰ΩôÊ®°ÂûãÈÉΩÊ≤°ÊúâÂÅöÊ£ÄÁ¥¢‰ªªÂä°ÁöÑËÆ≠ÁªÉÔºåÊâÄ‰ª•ÁªìÊûú‰ªÖ‰æõÂèÇËÄÉ„ÄÇ\ntext2vec\nopenai-ada-002\nm3e-small\nm3e-base\nDMetaSoul\nuer\nerlangshen\nmap@1\n0.4684\n0.6133\n0.5574\n0.626\n0.25203\n0.08647\n0.25394\nmap@10\n0.5877\n0.7423\n0.6878\n0.7656\n0.33312\n0.13008\n0.34714\nmrr@1\n0.5345\n0.6931\n0.6324\n0.7047\n0.29258\n0.10067\n0.29447\nmrr@10\n0.6217\n0.7668\n0.712\n0.7841\n0.36287\n0.14516\n0.3751\nndcg@1\n0.5207\n0.6764\n0.6159\n0.6881\n0.28358\n0.09748\n0.28578\nndcg@10\n0.6346\n0.7786\n0.7262\n0.8004\n0.37468\n0.15783\n0.39329\nT2Ranking\nÊï∞ÊçÆÈõÜÈÄâÊã©Ôºå‰ΩøÁî® T2RankingÔºåÂà®Èô§ openai-ada-002 Ê®°ÂûãÂêéÔºåÊàë‰ª¨ÂØπÂâ©‰ΩôÁöÑ‰∏â‰∏™Ê®°ÂûãÔºåËøõË°å T2Ranking 10W Âíå T2Ranking 50W ÁöÑËØÑÊµã„ÄÇÔºàT2Ranking ËØÑÊµãÂ§™ËÄóÂÜÖÂ≠ò‰∫Ü... 128G ÈÉΩ‰∏çË°åÔºâ\nËØÑÊµãÊñπÂºèÔºå‰ΩøÁî® MTEB ÁöÑÊñπÂºèËøõË°åËØÑÊµãÔºåÊä•Âëä ndcg@10\ntext2vec\nm3e-small\nm3e-base\nt2r-1w\n0.6346\n0.72621\n0.8004\nt2r-10w\n0.44644\n0.5251\n0.6263\nt2r-50w\n0.33482\n0.38626\n0.47364\nËØ¥ÊòéÔºö\nÊ£ÄÁ¥¢ÊéíÂ∫èÂØπ‰∫é text2vec Âπ∂‰∏çÂÖ¨Âπ≥ÔºåÂõ†‰∏∫ text2vec Âú®ËÆ≠ÁªÉÁöÑÊó∂ÂÄôÊ≤°Êúâ‰ΩøÁî®ËøáÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÁöÑÊï∞ÊçÆÈõÜÔºåÊâÄ‰ª•Ê≤°ÊúâÂäûÊ≥ïÂæàÂ•ΩÁöÑÂÆåÊàêÊ£ÄÁ¥¢‰ªªÂä°‰πüÊòØÊ≠£Â∏∏ÁöÑ„ÄÇ\nüìÇ M3EÊï∞ÊçÆÈõÜ\nÂ¶ÇÊûúÊÇ®ÊÉ≥Ë¶Å‰ΩøÁî®Ëøô‰∫õÊï∞ÊçÆÈõÜÔºå‰Ω†ÂèØ‰ª•Âú® uniem process_zh_datasets ‰∏≠ÊâæÂà∞Âä†ËΩΩ huggingface Êï∞ÊçÆÈõÜÁöÑËÑöÊú¨ÔºåÈùû huggingface Êï∞ÊçÆÈõÜÈúÄË¶ÅÊÇ®Ê†πÊçÆ‰∏ãÊñπÊèê‰æõÁöÑÈìæÊé•Ëá™Ë°å‰∏ãËΩΩÂíåÂ§ÑÁêÜ„ÄÇ\nÊï∞ÊçÆÈõÜÂêçÁß∞\nÈ¢ÜÂüü\nÊï∞Èáè\n‰ªªÂä°Á±ªÂûã\nPrompt\nË¥®Èáè\nÊï∞ÊçÆÊèê‰æõËÄÖ\nËØ¥Êòé\nÊòØÂê¶ÂºÄÊ∫ê/Á†îÁ©∂‰ΩøÁî®\nÊòØÂê¶ÂïÜÁî®\nËÑöÊú¨\nDone\nURL\nÊòØÂê¶ÂêåË¥®\ncmrc2018\nÁôæÁßë\n14,363\nÈóÆÁ≠î\nÈóÆÁ≠î\n‰ºò\nYiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, Guoping Hu\nhttps://github.com/ymcui/cmrc2018/blob/master/README_CN.md ‰∏ìÂÆ∂Ê†áÊ≥®ÁöÑÂü∫‰∫éÁª¥Âü∫ÁôæÁßëÁöÑ‰∏≠ÊñáÈòÖËØªÁêÜËß£Êï∞ÊçÆÈõÜÔºåÂ∞ÜÈóÆÈ¢òÂíå‰∏ä‰∏ãÊñáËßÜ‰∏∫Ê≠£‰æã\nÊòØ\nÂê¶\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/cmrc2018\nÂê¶\nbelle_2m\nÁôæÁßë\n2,000,000\nÊåá‰ª§ÂæÆË∞É\nÊó†\n‰ºò\nLianjiaTech/BELLE\nbelle ÁöÑÊåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆÈõÜÔºå‰ΩøÁî® self instruct ÊñπÊ≥ïÂü∫‰∫é gpt3.5 ÁîüÊàê\nÊòØ\nÂê¶\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/BelleGroup/train_2M_CN\nÂê¶\nfirefily\nÁôæÁßë\n1,649,399\nÊåá‰ª§ÂæÆË∞É\nÊó†\n‰ºò\nYeungNLP\nFireflyÔºàÊµÅËê§Ôºâ ÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑ‰∏≠ÊñáÂØπËØùÂºèÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºå‰ΩøÁî®Êåá‰ª§ÂæÆË∞ÉÔºàInstruction TuningÔºâÂú®‰∏≠ÊñáÊï∞ÊçÆÈõÜ‰∏äËøõË°åË∞É‰ºò„ÄÇ‰ΩøÁî®‰∫ÜËØçË°®Ë£ÅÂâ™„ÄÅZeROÁ≠âÊäÄÊúØÔºåÊúâÊïàÈôç‰ΩéÊòæÂ≠òÊ∂àËÄóÂíåÊèêÈ´òËÆ≠ÁªÉÊïàÁéá„ÄÇ Âú®ËÆ≠ÁªÉ‰∏≠ÔºåÊàë‰ª¨‰ΩøÁî®‰∫ÜÊõ¥Â∞èÁöÑÊ®°ÂûãÂèÇÊï∞ÈáèÔºå‰ª•ÂèäÊõ¥Â∞ëÁöÑËÆ°ÁÆóËµÑÊ∫ê„ÄÇ\nÊú™ËØ¥Êòé\nÊú™ËØ¥Êòé\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/YeungNLP/firefly-train-1.1M\nÂê¶\nalpaca_gpt4\nÁôæÁßë\n48,818\nÊåá‰ª§ÂæÆË∞É\nÊó†\n‰ºò\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao\nÊú¨Êï∞ÊçÆÈõÜÊòØÂèÇËÄÉAlpacaÊñπÊ≥ïÂü∫‰∫éGPT4ÂæóÂà∞ÁöÑself-instructÊï∞ÊçÆÔºåÁ∫¶5‰∏áÊù°„ÄÇ\nÊòØ\nÂê¶\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/shibing624/alpaca-zh\nÂê¶\nzhihu_kol\nÁôæÁßë\n1,006,218\nÈóÆÁ≠î\nÈóÆÁ≠î\n‰ºò\nwangrui6\nÁü•‰πéÈóÆÁ≠î\nÊú™ËØ¥Êòé\nÊú™ËØ¥Êòé\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/wangrui6/Zhihu-KOL\nÂê¶\nhc3_chinese\nÁôæÁßë\n39,781\nÈóÆÁ≠î\nÈóÆÁ≠î\nËâØ\nHello-SimpleAI\nÈóÆÁ≠îÊï∞ÊçÆÔºåÂåÖÊã¨‰∫∫Â∑•ÂõûÁ≠îÂíå GPT ÂõûÁ≠î\nÊòØ\nÊú™ËØ¥Êòé\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese\nÂê¶\namazon_reviews_multi\nÁîµÂïÜ\n210,000\nÈóÆÁ≠î ÊñáÊú¨ÂàÜÁ±ª\nÊëòË¶Å\n‰ºò\n‰∫öÈ©¨ÈÄä\n‰∫öÈ©¨ÈÄä‰∫ßÂìÅËØÑËÆ∫Êï∞ÊçÆÈõÜ\nÊòØ\nÂê¶\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/amazon_reviews_multi/viewer/zh/train?row=8\nÂê¶\nmlqa\nÁôæÁßë\n85,853\nÈóÆÁ≠î\nÈóÆÁ≠î\nËâØ\npatrickvonplaten\n‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞Ë∑®ËØ≠Ë®ÄÈóÆÁ≠îÊÄßËÉΩÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜ\nÊòØ\nÊú™ËØ¥Êòé\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/mlqa/viewer/mlqa-translate-train.zh/train?p=2\nÂê¶\nxlsum\nÊñ∞Èóª\n93,404\nÊëòË¶Å\nÊëòË¶Å\nËâØ\nBUET CSE NLP Group\nBBCÁöÑ‰∏ì‰∏öÊ≥®ÈáäÊñáÁ´†ÊëòË¶ÅÂØπ\nÊòØ\nÂê¶\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/csebuetnlp/xlsum/viewer/chinese_simplified/train?row=259\nÂê¶\nocnli\nÂè£ËØ≠\n17,726\nËá™ÁÑ∂ËØ≠Ë®ÄÊé®ÁêÜ\nÊé®ÁêÜ\nËâØ\nThomas Wolf\nËá™ÁÑ∂ËØ≠Ë®ÄÊé®ÁêÜÊï∞ÊçÆÈõÜ\nÊòØ\nÂê¶\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/clue/viewer/ocnli\nÊòØ\nBQ\nÈáëËûç\n60,000\nÊñáÊú¨ÂàÜÁ±ª\nÁõ∏‰ºº\nËâØ\nIntelligent Computing Research Center, Harbin Institute of Technology(Shenzhen)\nhttp://icrc.hitsz.edu.cn/info/1037/1162.htm BQ ËØ≠ÊñôÂ∫ìÂåÖÂê´Êù•Ëá™ÁΩë‰∏äÈì∂Ë°åËá™ÂÆö‰πâÊúçÂä°Êó•ÂøóÁöÑ 120Ôºå000 ‰∏™ÈóÆÈ¢òÂØπ„ÄÇÂÆÉÂàÜ‰∏∫‰∏âÈÉ®ÂàÜÔºö100Ôºå000 ÂØπÁî®‰∫éËÆ≠ÁªÉÔºå10Ôºå000 ÂØπÁî®‰∫éÈ™åËØÅÔºå10Ôºå000 ÂØπÁî®‰∫éÊµãËØï„ÄÇ Êï∞ÊçÆÊèê‰æõËÄÖÔºö ÂìàÂ∞îÊª®Â∑•‰∏öÂ§ßÂ≠¶ÔºàÊ∑±Âú≥ÔºâÊô∫ËÉΩËÆ°ÁÆóÁ†îÁ©∂‰∏≠ÂøÉ\nÊòØ\nÂê¶\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/shibing624/nli_zh/viewer/BQ\nÊòØ\nlcqmc\nÂè£ËØ≠\n149,226\nÊñáÊú¨ÂàÜÁ±ª\nÁõ∏‰ºº\nËâØ\nMing Xu\nÂìàÂ∑•Â§ßÊñáÊú¨ÂåπÈÖçÊï∞ÊçÆÈõÜÔºåLCQMC ÊòØÂìàÂ∞îÊª®Â∑•‰∏öÂ§ßÂ≠¶Âú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂõΩÈôÖÈ°∂‰ºö COLING2018 ÊûÑÂª∫ÁöÑÈóÆÈ¢òËØ≠‰πâÂåπÈÖçÊï∞ÊçÆÈõÜÔºåÂÖ∂ÁõÆÊ†áÊòØÂà§Êñ≠‰∏§‰∏™ÈóÆÈ¢òÁöÑËØ≠‰πâÊòØÂê¶Áõ∏Âêå\nÊòØ\nÂê¶\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/shibing624/nli_zh/viewer/LCQMC/train\nÊòØ\npaws-x\nÁôæÁßë\n23,576\nÊñáÊú¨ÂàÜÁ±ª\nÁõ∏‰ºº\n‰ºò\nBhavitvya Malik\nPAWS Wiki‰∏≠ÁöÑÁ§∫‰æã\nÊòØ\nÊòØ\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/paws-x/viewer/zh/train\nÊòØ\nwiki_atomic_edit\nÁôæÁßë\n1,213,780\nÂπ≥Ë°åËØ≠‰πâ\nÁõ∏‰ºº\n‰ºò\nabhishek thakur\nÂü∫‰∫é‰∏≠ÊñáÁª¥Âü∫ÁôæÁßëÁöÑÁºñËæëËÆ∞ÂΩïÊî∂ÈõÜÁöÑÊï∞ÊçÆÈõÜ\nÊú™ËØ¥Êòé\nÊú™ËØ¥Êòé\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/wiki_atomic_edits\nÊòØ\nchatmed_consult\nÂåªËçØ\n549,326\nÈóÆÁ≠î\nÈóÆÁ≠î\n‰ºò\nWei Zhu\nÁúüÂÆû‰∏ñÁïåÁöÑÂåªÂ≠¶Áõ∏ÂÖ≥ÁöÑÈóÆÈ¢òÔºå‰ΩøÁî® gpt3.5 ËøõË°åÂõûÁ≠î\nÊòØ\nÂê¶\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset\nÂê¶\nwebqa\nÁôæÁßë\n42,216\nÈóÆÁ≠î\nÈóÆÁ≠î\n‰ºò\nsuolyer\nÁôæÂ∫¶‰∫é2016Âπ¥ÂºÄÊ∫êÁöÑÊï∞ÊçÆÈõÜÔºåÊï∞ÊçÆÊù•Ëá™‰∫éÁôæÂ∫¶Áü•ÈÅìÔºõÊ†ºÂºè‰∏∫‰∏Ä‰∏™ÈóÆÈ¢òÂ§öÁØáÊÑèÊÄùÂü∫Êú¨‰∏ÄËá¥ÁöÑÊñáÁ´†ÔºåÂàÜ‰∏∫‰∫∫‰∏∫Ê†áÊ≥®‰ª•ÂèäÊµèËßàÂô®Ê£ÄÁ¥¢ÔºõÊï∞ÊçÆÊï¥‰ΩìË¥®Èáè‰∏≠ÔºåÂõ†‰∏∫Ê∑∑Âêà‰∫ÜÂæàÂ§öÊ£ÄÁ¥¢ËÄåÊù•ÁöÑÊñáÁ´†\nÊòØ\nÊú™ËØ¥Êòé\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/suolyer/webqa/viewer/suolyer--webqa/train?p=3\nÂê¶\ndureader_robust\nÁôæÁßë\n65,937\nÊú∫Âô®ÈòÖËØªÁêÜËß£ ÈóÆÁ≠î\nÈóÆÁ≠î\n‰ºò\nÁôæÂ∫¶\nDuReader robustÊó®Âú®Âà©Áî®ÁúüÂÆûÂ∫îÁî®‰∏≠ÁöÑÊï∞ÊçÆÊ†∑Êú¨Êù•Ë°°ÈáèÈòÖËØªÁêÜËß£Ê®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÔºåËØÑÊµãÊ®°ÂûãÁöÑËøáÊïèÊÑüÊÄß„ÄÅËøáÁ®≥ÂÆöÊÄß‰ª•ÂèäÊ≥õÂåñËÉΩÂäõÔºåÊòØÈ¶ñ‰∏™‰∏≠ÊñáÈòÖËØªÁêÜËß£È≤ÅÊ£íÊÄßÊï∞ÊçÆÈõÜ„ÄÇ\nÊòØ\nÊòØ\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/PaddlePaddle/dureader_robust/viewer/plain_text/train?row=96\nÂê¶\ncsl\nÂ≠¶ÊúØ\n395,927\nËØ≠Êñô\nÊëòË¶Å\n‰ºò\nYudong Li, Yuqing Zhang, Zhe Zhao, Linlin Shen, Weijie Liu, Weiquan Mao and Hui Zhang\nÊèê‰æõÈ¶ñ‰∏™‰∏≠ÊñáÁßëÂ≠¶ÊñáÁåÆÊï∞ÊçÆÈõÜÔºàCSLÔºâÔºåÂåÖÂê´ 396,209 ÁØá‰∏≠ÊñáÊ†∏ÂøÉÊúüÂàäËÆ∫ÊñáÂÖÉ‰ø°ÊÅØ ÔºàÊ†áÈ¢ò„ÄÅÊëòË¶Å„ÄÅÂÖ≥ÈîÆËØç„ÄÅÂ≠¶Áßë„ÄÅÈó®Á±ªÔºâ„ÄÇCSL Êï∞ÊçÆÈõÜÂèØ‰ª•‰Ωú‰∏∫È¢ÑËÆ≠ÁªÉËØ≠ÊñôÔºå‰πüÂèØ‰ª•ÊûÑÂª∫ËÆ∏Â§öNLP‰ªªÂä°Ôºå‰æãÂ¶ÇÊñáÊú¨ÊëòË¶ÅÔºàÊ†áÈ¢òÈ¢ÑÊµãÔºâ„ÄÅ ÂÖ≥ÈîÆËØçÁîüÊàêÂíåÊñáÊú¨ÂàÜÁ±ªÁ≠â„ÄÇ\nÊòØ\nÊòØ\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/neuclir/csl\nÂê¶\nmiracl-corpus\nÁôæÁßë\n4,934,368\nËØ≠Êñô\nÊëòË¶Å\n‰ºò\nMIRACL\nThe corpus for each language is prepared from a Wikipedia dump, where we keep only the plain text and discard images, tables, etc. Each article is segmented into multiple passages using WikiExtractor based on natural discourse units (e.g., \\n\\n in the wiki markup). Each of these passages comprises a \"document\" or unit of retrieval. We preserve the Wikipedia article title of each passage.\nÊòØ\nÊòØ\nÊòØ\nÊòØ\nhttps://huggingface.co/datasets/miracl/miracl-corpus\nÂê¶\nlawzhidao\nÊ≥ïÂæã\n36,368\nÈóÆÁ≠î\nÈóÆÁ≠î\n‰ºò\nÂíåÈ≤∏Á§æÂå∫-Ustinian\nÁôæÂ∫¶Áü•ÈÅìÊ∏ÖÊ¥óÂêéÁöÑÊ≥ïÂæãÈóÆÁ≠î\nÊòØ\nÊòØ\nÂê¶\nÊòØ\nhttps://www.heywhale.com/mw/dataset/5e953ca8e7ec38002d02fca7/content\nÂê¶\nCINLID\nÊàêËØ≠\n34,746\nÂπ≥Ë°åËØ≠‰πâ\nÁõ∏‰ºº\n‰ºò\nÈ´òÈïøÂÆΩ\n‰∏≠ÊñáÊàêËØ≠ËØ≠‰πâÊé®ÁêÜÊï∞ÊçÆÈõÜÔºàChinese Idioms Natural Language Inference DatasetÔºâÊî∂ÈõÜ‰∫Ü106832Êù°Áî±‰∫∫Â∑•Êí∞ÂÜôÁöÑÊàêËØ≠ÂØπÔºàÂê´Â∞ëÈáèÊ≠áÂêéËØ≠„ÄÅ‰øóËØ≠Á≠âÁü≠ÊñáÊú¨ÔºâÔºåÈÄöËøá‰∫∫Â∑•Ê†áÊ≥®ÁöÑÊñπÂºèËøõË°åÂπ≥Ë°°ÂàÜÁ±ªÔºåÊ†áÁ≠æ‰∏∫entailment„ÄÅcontradictionÂíåneutralÔºåÊîØÊåÅËá™ÁÑ∂ËØ≠Ë®ÄÊé®ÁêÜÔºàNLIÔºâÁöÑ‰ªªÂä°„ÄÇ\nÊòØ\nÂê¶\nÂê¶\nÊòØ\nhttps://www.luge.ai/#/luge/dataDetail?id=39\nÊòØ\nDuSQL\nSQL\n25,003\nNL2SQL\nSQL\n‰ºò\nÁôæÂ∫¶\nDuSQLÊòØ‰∏Ä‰∏™Èù¢ÂêëÂÆûÈôÖÂ∫îÁî®ÁöÑÊï∞ÊçÆÈõÜÔºåÂåÖÂê´200‰∏™Êï∞ÊçÆÂ∫ìÔºåË¶ÜÁõñ‰∫Ü164‰∏™È¢ÜÂüüÔºåÈóÆÈ¢òË¶ÜÁõñ‰∫ÜÂåπÈÖç„ÄÅËÆ°ÁÆó„ÄÅÊé®ÁêÜÁ≠âÂÆûÈôÖÂ∫îÁî®‰∏≠Â∏∏ËßÅÂΩ¢Âºè„ÄÇËØ•Êï∞ÊçÆÈõÜÊõ¥Ë¥¥ËøëÁúüÂÆûÂ∫îÁî®Âú∫ÊôØÔºåË¶ÅÊ±ÇÊ®°ÂûãÈ¢ÜÂüüÊó†ÂÖ≥„ÄÅÈóÆÈ¢òÊó†ÂÖ≥Ôºå‰∏îÂÖ∑Â§áËÆ°ÁÆóÊé®ÁêÜÁ≠âËÉΩÂäõ„ÄÇ\nÊòØ\nÂê¶\nÂê¶\nÊòØ\nhttps://www.luge.ai/#/luge/dataDetail?id=13\nÂê¶\nZhuiyi-NL2SQL\nSQL\n45,918\nNL2SQL\nSQL\n‰ºò\nËøΩ‰∏ÄÁßëÊäÄ Âàò‰∫ëÂ≥∞\nNL2SQLÊòØ‰∏Ä‰∏™Â§öÈ¢ÜÂüüÁöÑÁÆÄÂçïÊï∞ÊçÆÈõÜÔºåÂÖ∂‰∏ªË¶ÅÂåÖÂê´ÂåπÈÖçÁ±ªÂûãÈóÆÈ¢ò„ÄÇËØ•Êï∞ÊçÆÈõÜ‰∏ªË¶ÅÈ™åËØÅÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂÖ∂Ë¶ÅÊ±ÇÊ®°ÂûãÂÖ∑ÊúâËæÉÂº∫ÁöÑÈ¢ÜÂüüÊ≥õÂåñËÉΩÂäõ„ÄÅÈóÆÈ¢òÊ≥õÂåñËÉΩÂäõ„ÄÇ\nÊòØ\nÂê¶\nÂê¶\nÊòØ\nhttps://www.luge.ai/#/luge/dataDetail?id=12\nÂê¶\nCspider\nSQL\n7,785\nNL2SQL\nSQL\n‰ºò\nË•øÊπñÂ§ßÂ≠¶ Âº†Â≤≥\nCSpiderÊòØ‰∏Ä‰∏™Â§öËØ≠Ë®ÄÊï∞ÊçÆÈõÜÔºåÂÖ∂ÈóÆÈ¢ò‰ª•‰∏≠ÊñáË°®ËææÔºåÊï∞ÊçÆÂ∫ì‰ª•Ëã±ÊñáÂ≠òÂÇ®ÔºåËøôÁßçÂèåËØ≠Ê®°ÂºèÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠‰πüÈùûÂ∏∏Â∏∏ËßÅÔºåÂ∞§ÂÖ∂ÊòØÊï∞ÊçÆÂ∫ìÂºïÊìéÂØπ‰∏≠ÊñáÊîØÊåÅ‰∏çÂ•ΩÁöÑÊÉÖÂÜµ‰∏ã„ÄÇËØ•Êï∞ÊçÆÈõÜË¶ÅÊ±ÇÊ®°ÂûãÈ¢ÜÂüüÊó†ÂÖ≥„ÄÅÈóÆÈ¢òÊó†ÂÖ≥Ôºå‰∏îËÉΩÂ§üÂÆûÁé∞Â§öËØ≠Ë®ÄÂåπÈÖç„ÄÇ\nÊòØ\nÂê¶\nÂê¶\nÊòØ\nhttps://www.luge.ai/#/luge/dataDetail?id=11\nÂê¶\nnews2016zh\nÊñ∞Èóª\n2,507,549\nËØ≠Êñô\nÊëòË¶Å\nËâØ\nBright Xu\nÂåÖÂê´‰∫Ü250‰∏áÁØáÊñ∞Èóª„ÄÇÊñ∞ÈóªÊù•Ê∫êÊ∂µÁõñ‰∫Ü6.3‰∏á‰∏™Â™í‰ΩìÔºåÂê´Ê†áÈ¢ò„ÄÅÂÖ≥ÈîÆËØç„ÄÅÊèèËø∞„ÄÅÊ≠£Êñá„ÄÇ\nÊòØ\nÊòØ\nÂê¶\nÊòØ\nhttps://github.com/brightmart/nlp_chinese_corpus\nÂê¶\nbaike2018qa\nÁôæÁßë\n1,470,142\nÈóÆÁ≠î\nÈóÆÁ≠î\nËâØ\nBright Xu\nÂê´Êúâ150‰∏á‰∏™È¢ÑÂÖàËøáÊª§ËøáÁöÑ„ÄÅÈ´òË¥®ÈáèÈóÆÈ¢òÂíåÁ≠îÊ°àÔºåÊØè‰∏™ÈóÆÈ¢òÂ±û‰∫é‰∏Ä‰∏™Á±ªÂà´„ÄÇÊÄªÂÖ±Êúâ492‰∏™Á±ªÂà´ÔºåÂÖ∂‰∏≠È¢ëÁéáËææÂà∞ÊàñË∂ÖËøá10Ê¨°ÁöÑÁ±ªÂà´Êúâ434‰∏™„ÄÇ\nÊòØ\nÊòØ\nÂê¶\nÊòØ\nhttps://github.com/brightmart/nlp_chinese_corpus\nÂê¶\nwebtext2019zh\nÁôæÁßë\n4,258,310\nÈóÆÁ≠î\nÈóÆÁ≠î\n‰ºò\nBright Xu\nÂê´Êúâ410‰∏á‰∏™È¢ÑÂÖàËøáÊª§ËøáÁöÑ„ÄÅÈ´òË¥®ÈáèÈóÆÈ¢òÂíåÂõûÂ§ç„ÄÇÊØè‰∏™ÈóÆÈ¢òÂ±û‰∫é‰∏Ä‰∏™„ÄêËØùÈ¢ò„ÄëÔºåÊÄªÂÖ±Êúâ2.8‰∏á‰∏™ÂêÑÂºèËØùÈ¢òÔºåËØùÈ¢òÂåÖÁΩó‰∏áË±°„ÄÇ\nÊòØ\nÊòØ\nÂê¶\nÊòØ\nhttps://github.com/brightmart/nlp_chinese_corpus\nÂê¶\nSimCLUE\nÁôæÁßë\n775,593\nÂπ≥Ë°åËØ≠‰πâ\nÁõ∏‰ºº\nËâØ\nÊï∞ÊçÆÈõÜÂêàÔºåËØ∑Âú® simCLUE ‰∏≠Êü•Áúã\nÊï¥Âêà‰∫Ü‰∏≠ÊñáÈ¢ÜÂüüÁªùÂ§ßÂ§öÊï∞ÂèØÁî®ÁöÑÂºÄÊ∫êÁöÑËØ≠‰πâÁõ∏‰ººÂ∫¶ÂíåËá™ÁÑ∂ËØ≠Ë®ÄÊé®ÁêÜÁöÑÊï∞ÊçÆÈõÜÔºåÂπ∂ÈáçÊñ∞ÂÅö‰∫ÜÊï∞ÊçÆÊãÜÂàÜÂíåÊï¥ÁêÜ„ÄÇ\nÊòØ\nÂê¶\nÂê¶\nÊòØ\nhttps://github.com/CLUEbenchmark/SimCLUE\nÊòØ\nChinese-SQuAD\nÊñ∞Èóª\n76,449\nÊú∫Âô®ÈòÖËØªÁêÜËß£\nÈóÆÁ≠î\n‰ºò\njunzeng-pluto\n‰∏≠ÊñáÊú∫Âô®ÈòÖËØªÁêÜËß£Êï∞ÊçÆÈõÜÔºåÈÄöËøáÊú∫Âô®ÁøªËØëÂä†‰∫∫Â∑•Ê†°Ê≠£ÁöÑÊñπÂºè‰ªéÂéüÂßãSquadËΩ¨Êç¢ËÄåÊù•\nÊòØ\nÂê¶\nÂê¶\nÊòØ\nhttps://github.com/pluto-junzeng/ChineseSquad\nÂê¶\nüóìÔ∏è ËÆ°ÂàíË°®\nÂÆåÊàê MTEB ‰∏≠ÊñáËØÑÊµã BenchMark, MTEB-zh\nÂÆåÊàê Large Ê®°ÂûãÁöÑËÆ≠ÁªÉÂíåÂºÄÊ∫ê\nÂÆåÊàê Finetuner ÔºåÂÖÅËÆ∏Êõ¥‰ºòÈõÖÁöÑÂæÆË∞É\nÂÆåÊàêÊîØÊåÅ‰ª£Á†ÅÊ£ÄÁ¥¢ÁöÑÊ®°Âûã\nÂØπ M3E Êï∞ÊçÆÈõÜËøõË°åÊ∏ÖÊ¥óÔºå‰øùÁïôÈ´òË¥®ÈáèÁöÑÈÉ®ÂàÜÔºåÁªÑÊàê m3e-hqÔºåÂπ∂Âú® huggingface ‰∏äÂºÄÊ∫ê\nÂú® m3e-hq ÁöÑÊï∞ÊçÆÈõÜ‰∏äË°•ÂÖÖ hard negative ÁöÑÊ†∑Êú¨ÂèäÁõ∏‰ººÂ∫¶ÂàÜÊï∞ÔºåÁªÑÊàê m3e-hq-with-scoreÔºåÂπ∂Âú® huggingface ‰∏äÂºÄÊ∫ê\nÂú® m3e-hq-with-score ‰∏äÈÄöËøá cosent loss loss ËøõË°åËÆ≠ÁªÉÂπ∂ÂºÄÊ∫êÊ®°ÂûãÔºåCoSent ÂéüÁêÜÂèÇËÄÉËøôÁØáÂçöÂÆ¢\nÂºÄÊ∫êÂïÜÁî®ÁâàÊú¨ÁöÑ M3E models\nüôè Ëá¥Ë∞¢\nÊÑüË∞¢ÂºÄÊ∫êÁ§æÂå∫Êèê‰æõÁöÑ‰∏≠ÊñáËØ≠ÊñôÔºåÊÑüË∞¢ÊâÄÊúâÂú®Ê≠§Â∑•‰Ωú‰∏≠Êèê‰æõÂ∏ÆÂä©ÁöÑ‰∫∫‰ª¨ÔºåÂ∏åÊúõ‰∏≠ÊñáÁ§æÂå∫Ë∂äÊù•Ë∂äÂ•ΩÔºåÂÖ±ÂãâÔºÅ\nüìú License\nM3E models ‰ΩøÁî®ÁöÑÊï∞ÊçÆÈõÜ‰∏≠ÂåÖÊã¨Â§ßÈáèÈùûÂïÜÁî®ÁöÑÊï∞ÊçÆÈõÜÔºåÊâÄ‰ª• M3E models ‰πüÊòØÈùûÂïÜÁî®ÁöÑÔºå‰ªÖ‰æõÁ†îÁ©∂‰ΩøÁî®„ÄÇ‰∏çËøáÊàë‰ª¨Â∑≤ÁªèÂú® M3E Êï∞ÊçÆÈõÜ‰∏äÊ†áËØÜ‰∫ÜÂïÜÁî®ÂíåÈùûÂïÜÁî®ÁöÑÊï∞ÊçÆÈõÜÔºåÊÇ®ÂèØ‰ª•Ê†πÊçÆËá™Â∑±ÁöÑÈúÄÊ±ÇËá™Ë°åËÆ≠ÁªÉ„ÄÇ\nCitation\nPlease cite this model using the following format:\n@software {Moka Massive Mixed Embedding,\nauthor = {Wang Yuxin,Sun Qingxuan,He sicheng},\ntitle = {M3E: Moka Massive Mixed Embedding Model},\nyear = {2023}\n}",
    "facebook/musicgen-large": "MusicGen - Large - 3.3B\nExample\nü§ó Transformers Usage\nAudiocraft Usage\nModel details\nIntended use\nMetrics\nEvaluation datasets\nTraining datasets\nEvaluation results\nLimitations and biases\nMusicGen - Large - 3.3B\nMusicGen is a text-to-music model capable of genreating high-quality music samples conditioned on text descriptions or audio prompts.\nIt is a single stage auto-regressive Transformer model trained over a 32kHz EnCodec tokenizer with 4 codebooks sampled at 50 Hz.\nUnlike existing methods, like MusicLM, MusicGen doesn't require a self-supervised semantic representation, and it generates all 4 codebooks in one pass.\nBy introducing a small delay between the codebooks, we show we can predict them in parallel, thus having only 50 auto-regressive steps per second of audio.\nMusicGen was published in Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre D√©fossez.\nFour checkpoints are released:\nsmall\nmedium\nlarge (this checkpoint)\nmelody\nExample\nTry out MusicGen yourself!\nAudiocraft Colab:\nHugging Face Colab:\nHugging Face Demo:\nü§ó Transformers Usage\nYou can run MusicGen locally with the ü§ó Transformers library from version 4.31.0 onwards.\nFirst install the ü§ó Transformers library and scipy:\npip install --upgrade pip\npip install --upgrade transformers scipy\nRun inference via the Text-to-Audio (TTA) pipeline. You can infer the MusicGen model via the TTA pipeline in just a few lines of code!\nfrom transformers import pipeline\nimport scipy\nsynthesiser = pipeline(\"text-to-audio\", \"facebook/musicgen-large\")\nmusic = synthesiser(\"lo-fi music with a soothing melody\", forward_params={\"do_sample\": True})\nscipy.io.wavfile.write(\"musicgen_out.wav\", rate=music[\"sampling_rate\"], data=music[\"audio\"])\nRun inference via the Transformers modelling code. You can use the processor + generate code to convert text into a mono 32 kHz audio waveform for more fine-grained control.\nfrom transformers import AutoProcessor, MusicgenForConditionalGeneration\nprocessor = AutoProcessor.from_pretrained(\"facebook/musicgen-large\")\nmodel = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-large\")\ninputs = processor(\ntext=[\"80s pop track with bassy drums and synth\", \"90s rock song with loud guitars and heavy drums\"],\npadding=True,\nreturn_tensors=\"pt\",\n)\naudio_values = model.generate(**inputs, max_new_tokens=256)\nListen to the audio samples either in an ipynb notebook:\nfrom IPython.display import Audio\nsampling_rate = model.config.audio_encoder.sampling_rate\nAudio(audio_values[0].numpy(), rate=sampling_rate)\nOr save them as a .wav file using a third-party library, e.g. scipy:\nimport scipy\nsampling_rate = model.config.audio_encoder.sampling_rate\nscipy.io.wavfile.write(\"musicgen_out.wav\", rate=sampling_rate, data=audio_values[0, 0].numpy())\nFor more details on using the MusicGen model for inference using the ü§ó Transformers library, refer to the MusicGen docs.\nAudiocraft Usage\nYou can also run MusicGen locally through the original Audiocraft library:\nFirst install the audiocraft library\npip install git+https://github.com/facebookresearch/audiocraft.git\nMake sure to have ffmpeg installed:\napt get install ffmpeg\nRun the following Python code:\nfrom audiocraft.models import MusicGen\nfrom audiocraft.data.audio import audio_write\nmodel = MusicGen.get_pretrained(\"large\")\nmodel.set_generation_params(duration=8)  # generate 8 seconds.\ndescriptions = [\"happy rock\", \"energetic EDM\"]\nwav = model.generate(descriptions)  # generates 2 samples.\nfor idx, one_wav in enumerate(wav):\n# Will save under {idx}.wav, with loudness normalization at -14 db LUFS.\naudio_write(f'{idx}', one_wav.cpu(), model.sample_rate, strategy=\"loudness\")\nModel details\nOrganization developing the model: The FAIR team of Meta AI.\nModel date: MusicGen was trained between April 2023 and May 2023.\nModel version: This is the version 1 of the model.\nModel type: MusicGen consists of an EnCodec model for audio tokenization, an auto-regressive language model based on the transformer architecture for music modeling. The model comes in different sizes: 300M, 1.5B and 3.3B parameters ; and two variants: a model trained for text-to-music generation task and a model trained for melody-guided music generation.\nPaper or resources for more information: More information can be found in the paper Simple and Controllable Music Generation.\nCitation details:\n@misc{copet2023simple,\ntitle={Simple and Controllable Music Generation},\nauthor={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre D√©fossez},\nyear={2023},\neprint={2306.05284},\narchivePrefix={arXiv},\nprimaryClass={cs.SD}\n}\nLicense: Code is released under MIT, model weights are released under CC-BY-NC 4.0.\nWhere to send questions or comments about the model: Questions and comments about MusicGen can be sent via the Github repository of the project, or by opening an issue.\nIntended use\nPrimary intended use: The primary use of MusicGen is research on AI-based music generation, including:\nResearch efforts, such as probing and better understanding the limitations of generative models to further improve the state of science\nGeneration of music guided by text or melody to understand current abilities of generative AI models by machine learning amateurs\nPrimary intended users: The primary intended users of the model are researchers in audio, machine learning and artificial intelligence, as well as amateur seeking to better understand those models.\nOut-of-scope use cases: The model should not be used on downstream applications without further risk evaluation and mitigation. The model should not be used to intentionally create or disseminate music pieces that create hostile or alienating environments for people. This includes generating music that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nMetrics\nModels performance measures: We used the following objective measure to evaluate the model on a standard music benchmark:\nFrechet Audio Distance computed on features extracted from a pre-trained audio classifier (VGGish)\nKullback-Leibler Divergence on label distributions extracted from a pre-trained audio classifier (PaSST)\nCLAP Score between audio embedding and text embedding extracted from a pre-trained CLAP model\nAdditionally, we run qualitative studies with human participants, evaluating the performance of the model with the following axes:\nOverall quality of the music samples;\nText relevance to the provided text input;\nAdherence to the melody for melody-guided music generation.\nMore details on performance measures and human studies can be found in the paper.\nDecision thresholds: Not applicable.\nEvaluation datasets\nThe model was evaluated on the MusicCaps benchmark and on an in-domain held-out evaluation set, with no artist overlap with the training set.\nTraining datasets\nThe model was trained on licensed data using the following sources: the Meta Music Initiative Sound Collection, Shutterstock music collection and the Pond5 music collection. See the paper for more details about the training set and corresponding preprocessing.\nEvaluation results\nBelow are the objective metrics obtained on MusicCaps with the released model. Note that for the publicly released models, we had all the datasets go through a state-of-the-art music source separation method, namely using the open source Hybrid Transformer for Music Source Separation (HT-Demucs), in order to keep only the instrumental part. This explains the difference in objective metrics with the models used in the paper.\nModel\nFrechet Audio Distance\nKLD\nText Consistency\nChroma Cosine Similarity\nfacebook/musicgen-small\n4.88\n1.42\n0.27\n-\nfacebook/musicgen-medium\n5.14\n1.38\n0.28\n-\nfacebook/musicgen-large\n5.48\n1.37\n0.28\n-\nfacebook/musicgen-melody\n4.93\n1.41\n0.27\n0.44\nMore information can be found in the paper Simple and Controllable Music Generation, in the Results section.\nLimitations and biases\nData: The data sources used to train the model are created by music professionals and covered by legal agreements with the right holders. The model is trained on 20K hours of data, we believe that scaling the model on larger datasets can further improve the performance of the model.\nMitigations: Vocals have been removed from the data source using corresponding tags, and then using a state-of-the-art music source separation method, namely using the open source Hybrid Transformer for Music Source Separation (HT-Demucs).\nLimitations:\nThe model is not able to generate realistic vocals.\nThe model has been trained with English descriptions and will not perform as well in other languages.\nThe model does not perform equally well for all music styles and cultures.\nThe model sometimes generates end of songs, collapsing to silence.\nIt is sometimes difficult to assess what types of text descriptions provide the best generations. Prompt engineering may be required to obtain satisfying results.\nBiases: The source of data is potentially lacking diversity and all music cultures are not equally represented in the dataset. The model may not perform equally well on the wide variety of music genres that exists. The generated samples from the model will reflect the biases from the training data. Further work on this model should include methods for balanced and just representations of cultures, for example, by scaling the training data to be both diverse and inclusive.\nRisks and harms: Biases and limitations of the model may lead to generation of samples that may be considered as biased, inappropriate or offensive. We believe that providing the code to reproduce the research and train new models will allow to broaden the application to new and more representative data.\nUse cases: Users must be aware of the biases, limitations and risks of the model. MusicGen is a model developed for artificial intelligence research on controllable music generation. As such, it should not be used for downstream applications without further investigation and mitigation of risks.",
    "rhasspy/piper-voices": "Voices for Piper text to speech system.\nFor checkpoints that you can use to train your own voices, see piper-checkpoints",
    "google/umt5-xxl": "Abstract\nGoogle's UMT5\nUMT5 is pretrained on the an updated version of mC4 corpus, covering 107 languages:\nAfrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.\nNote: UMT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.\nPretraining Dataset: mC4\nOther Community Checkpoints: here\nPaper: UniMax, Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining\nAuthors: by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant\nAbstract\nPretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with UniMax sampling.",
    "openai/shap-e": "Shap-E\nIntroduction\nReleased checkpoints\nUsage examples in üß® diffusers\nResults\nTraining details\nKnown limitations and potential biases\nCitation\nShap-E\nShap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in Shap-E: Generating Conditional 3D Implicit Functions by Heewoo Jun and Alex Nichol from OpenAI.\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e.\nThe authors of Shap-E didn't author this model card. They provide a separate model card here.\nIntroduction\nThe abstract of the Shap-E paper:\nWe present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. We release model weights, inference code, and samples at this https URL.\nReleased checkpoints\nThe authors released the following checkpoints:\nopenai/shap-e: produces a 3D image from a text input prompt\nopenai/shap-e-img2img: samples a 3D image from synthetic 2D image\nUsage examples in üß® diffusers\nFirst make sure you have installed all the dependencies:\npip install transformers accelerate -q\npip install git+https://github.com/huggingface/diffusers@@shap-ee\nOnce the dependencies are installed, use the code below:\nimport torch\nfrom diffusers import ShapEPipeline\nfrom diffusers.utils import export_to_gif\nckpt_id = \"openai/shap-e\"\npipe = ShapEPipeline.from_pretrained(repo).to(\"cuda\")\nguidance_scale = 15.0\nprompt = \"a shark\"\nimages = pipe(\nprompt,\nguidance_scale=guidance_scale,\nnum_inference_steps=64,\nsize=256,\n).images\ngif_path = export_to_gif(images, \"shark_3d.gif\")\nResults\nA bird\nA shark\nA bowl of vegetables\nTraining details\nRefer to the original paper.\nKnown limitations and potential biases\nRefer to the original model card.\nCitation\n@misc{jun2023shape,\ntitle={Shap-E: Generating Conditional 3D Implicit Functions},\nauthor={Heewoo Jun and Alex Nichol},\nyear={2023},\neprint={2305.02463},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "vinesmsuic/magicbrush-jul7": "License\ndiffuser port of https://huggingface.co/osunlp/InstructPix2Pix-MagicBrush.\ndiffuser version of MagicBrush-epoch-52-step-4999.ckpt\nfrom PIL import Image, ImageOps\nimport requests\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\nfrom PIL import Image\nurl = \"https://huggingface.co/datasets/diffusers/diffusers-images-docs/resolve/main/mountain.png\"\ndef download_image(url):\nimage = Image.open(requests.get(url, stream=True).raw)\nimage = ImageOps.exif_transpose(image)\nimage = image.convert(\"RGB\")\nreturn image\nimage = download_image(url)\nprompt = \"make the mountains snowy\"\nclass MagicBrush():\ndef __init__(self, weight=\"vinesmsuic/magicbrush-jul7\"):\nself.pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(\nweight,\ntorch_dtype=torch.float16\n).to(\"cuda\")\nself.pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(self.pipe.scheduler.config)\ndef infer_one_image(self, src_image, instruct_prompt, seed):\ngenerator = torch.manual_seed(seed)\nimage = self.pipe(instruct_prompt, image=src_image, num_inference_steps=20, image_guidance_scale=1.5, guidance_scale=7, generator=generator).images[0]\nreturn image\nmodel = MagicBrush()\nimage_output = model.infer_one_image(image, prompt, 42)\nimage_output\nLicense\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies:\nYou can't use the model to deliberately produce nor share illegal or harmful outputs or content\nThe authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\nYou may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\nPlease read the full license here",
    "BlinkDL/temp-latest-training-models": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nhttps://rwkv.com\nhttps://x.com/BlinkDL_AI\nHere are some cutting-edge preview RWKV models: https://huggingface.co/BlinkDL/temp-latest-training-models/tree/main\nincluding RWKV-7 checkpts (already supported by https://pypi.org/project/rwkv/ and https://github.com/Ai00-X/ai00_server)\nRef: https://huggingface.co/BlinkDL/rwkv-7-world\n\"single_round_qa\" are \"states\" (check https://x.com/BlinkDL_AI/status/1788354345807057035) and these are single round Q&A states\ndata_sample is random subsample of world dataset. note: due to technical reasons (very complicated due to my horrible messy code), some distill instruct data are not included, and only subsamples of these instruct data are included:\nflan, Buzz-V12, WebInstructSub, SKGInstruct, PIPPA, COIG-PC-core",
    "TheBloke/MythoMax-L2-13B-GPTQ": "MythoMax L2 13B - GPTQ\nDescription\nRepositories available\nPrompt template: Custom\nLicensing\nProvided files and GPTQ parameters\nHow to download from branches\nHow to easily download and use this model in text-generation-webui.\nHow to use this GPTQ model from Python code\nInstall the necessary packages\nFor CodeLlama models only: you must use Transformers 4.33.0 or later.\nYou can then use the following code\nCompatibility\nDiscord\nThanks, and how to contribute\nOriginal model card: Gryphe's MythoMax L2 13B\nModel details\nPrompt Format\nlicense: other\nChat & support: TheBloke's Discord server\nWant to contribute? TheBloke's Patreon page\nTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)\nMythoMax L2 13B - GPTQ\nModel creator: Gryphe\nOriginal model: MythoMax L2 13B\nDescription\nThis repo contains GPTQ model files for Gryphe's MythoMax L2 13B.\nMultiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.\nRepositories available\nAWQ model(s) for GPU inference.\nGPTQ models for GPU inference, with multiple quantisation parameter options.\n2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference\nGryphe's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions\nPrompt template: Custom\n{system_message}\n### Instruction:\n{prompt}\n(For roleplay purposes, I suggest the following - Write <CHAR NAME>'s next reply in a chat between <YOUR NAME> and <CHAR NAME>. Write a single reply only.)\n### Response:\nLicensing\nThe creator of the source model has listed its license as other, and this quantization has therefore used that same license.\nAs this model is based on Llama 2, it is also subject to the Meta Llama 2 license terms, and the license files for that are additionally included. It should therefore be considered as being claimed to be licensed under both licenses. I contacted Hugging Face for clarification on dual licensing but they do not yet have an official position. Should this change, or should Meta provide any feedback on this situation, I will update this section accordingly.\nIn the meantime, any questions regarding licensing, and in particular how these two licenses might interact, should be directed to the original model repository: Gryphe's MythoMax L2 13B.\nProvided files and GPTQ parameters\nMultiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.\nEach separate quant is in a different branch.  See below for instructions on fetching from different branches.\nAll recent GPTQ files are made with AutoGPTQ, and all files in non-main branches are made with AutoGPTQ. Files in the main branch which were uploaded before August 2023 were made with GPTQ-for-LLaMa.\nExplanation of GPTQ parameters\nBits: The bit size of the quantised model.\nGS: GPTQ group size. Higher numbers use less VRAM, but have lower quantisation accuracy. \"None\" is the lowest possible value.\nAct Order: True or False. Also known as desc_act. True results in better quantisation accuracy. Some GPTQ clients have had issues with models that use Act Order plus Group Size, but this is generally resolved now.\nDamp %: A GPTQ parameter that affects how samples are processed for quantisation. 0.01 is default, but 0.1 results in slightly better accuracy.\nGPTQ dataset: The dataset used for quantisation. Using a dataset more appropriate to the model's training can improve quantisation accuracy. Note that the GPTQ dataset is not the same as the dataset used to train the model - please refer to the original model repo for details of the training dataset(s).\nSequence Length: The length of the dataset sequences used for quantisation. Ideally this is the same as the model sequence length. For some very long sequence models (16+K), a lower sequence length may have to be used.  Note that a lower sequence length does not limit the sequence length of the quantised model. It only impacts the quantisation accuracy on longer inference sequences.\nExLlama Compatibility: Whether this file can be loaded with ExLlama, which currently only supports Llama models in 4-bit.\nBranch\nBits\nGS\nAct Order\nDamp %\nGPTQ Dataset\nSeq Len\nSize\nExLlama\nDesc\nmain\n4\n128\nNo\n0.1\nwikitext\n4096\n7.26 GB\nYes\n4-bit, without Act Order and group size 128g.\ngptq-4bit-32g-actorder_True\n4\n32\nYes\n0.1\nwikitext\n4096\n8.00 GB\nYes\n4-bit, with Act Order and group size 32g. Gives highest possible inference quality, with maximum VRAM usage.\ngptq-4bit-64g-actorder_True\n4\n64\nYes\n0.1\nwikitext\n4096\n7.51 GB\nYes\n4-bit, with Act Order and group size 64g. Uses less VRAM than 32g, but with slightly lower accuracy.\ngptq-4bit-128g-actorder_True\n4\n128\nYes\n0.1\nwikitext\n4096\n7.26 GB\nYes\n4-bit, with Act Order and group size 128g. Uses even less VRAM than 64g, but with slightly lower accuracy.\ngptq-8bit--1g-actorder_True\n8\nNone\nYes\n0.1\nwikitext\n4096\n13.36 GB\nNo\n8-bit, with Act Order. No group size, to lower VRAM requirements.\ngptq-8bit-128g-actorder_True\n8\n128\nYes\n0.1\nwikitext\n4096\n13.65 GB\nNo\n8-bit, with group size 128g for higher inference quality and with Act Order for even higher accuracy.\nHow to download from branches\nIn text-generation-webui, you can add :branch to the end of the download name, eg TheBloke/MythoMax-L2-13B-GPTQ:main\nWith Git, you can clone a branch with:\ngit clone --single-branch --branch main https://huggingface.co/TheBloke/MythoMax-L2-13B-GPTQ\nIn Python Transformers code, the branch is the revision parameter; see below.\nHow to easily download and use this model in text-generation-webui.\nPlease make sure you're using the latest version of text-generation-webui.\nIt is strongly recommended to use the text-generation-webui one-click-installers unless you're sure you know how to make a manual install.\nClick the Model tab.\nUnder Download custom model or LoRA, enter TheBloke/MythoMax-L2-13B-GPTQ.\nTo download from a specific branch, enter for example TheBloke/MythoMax-L2-13B-GPTQ:main\nsee Provided Files above for the list of branches for each option.\nClick Download.\nThe model will start downloading. Once it's finished it will say \"Done\".\nIn the top left, click the refresh icon next to Model.\nIn the Model dropdown, choose the model you just downloaded: MythoMax-L2-13B-GPTQ\nThe model will automatically load, and is now ready for use!\nIf you want any custom settings, set them and then click Save settings for this model followed by Reload the Model in the top right.\nNote that you do not need to and should not set manual GPTQ parameters any more. These are set automatically from the file quantize_config.json.\nOnce you're ready, click the Text Generation tab and enter a prompt to get started!\nHow to use this GPTQ model from Python code\nInstall the necessary packages\nRequires: Transformers 4.32.0 or later, Optimum 1.12.0 or later, and AutoGPTQ 0.4.2 or later.\npip3 install transformers>=4.32.0 optimum>=1.12.0\npip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7\nIf you have problems installing AutoGPTQ using the pre-built wheels, install it from source instead:\npip3 uninstall -y auto-gptq\ngit clone https://github.com/PanQiWei/AutoGPTQ\ncd AutoGPTQ\npip3 install .\nFor CodeLlama models only: you must use Transformers 4.33.0 or later.\nIf 4.33.0 is not yet released when you read this, you will need to install Transformers from source:\npip3 uninstall -y transformers\npip3 install git+https://github.com/huggingface/transformers.git\nYou can then use the following code\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nmodel_name_or_path = \"TheBloke/MythoMax-L2-13B-GPTQ\"\n# To use a different branch, change revision\n# For example: revision=\"main\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\ndevice_map=\"auto\",\ntrust_remote_code=False,\nrevision=\"main\")\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\nprompt = \"Tell me about AI\"\nprompt_template=f'''Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{prompt}\n### Response:\n'''\nprint(\"\\n\\n*** Generate:\")\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n# Inference can also be done using transformers' pipeline\nprint(\"*** Pipeline:\")\npipe = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\nmax_new_tokens=512,\ndo_sample=True,\ntemperature=0.7,\ntop_p=0.95,\ntop_k=40,\nrepetition_penalty=1.1\n)\nprint(pipe(prompt_template)[0]['generated_text'])\nCompatibility\nThe files provided are tested to work with AutoGPTQ, both via Transformers and using AutoGPTQ directly. They should also work with Occ4m's GPTQ-for-LLaMa fork.\nExLlama is compatible with Llama models in 4-bit. Please see the Provided Files table above for per-file compatibility.\nHuggingface Text Generation Inference (TGI) is compatible with all GPTQ models.\nDiscord\nFor further support, and discussions on these models and AI in general, join us at:\nTheBloke AI's Discord server\nThanks, and how to contribute\nThanks to the chirper.ai team!\nThanks to Clay from gpus.llm-utils.org!\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\nPatreon: https://patreon.com/TheBlokeAI\nKo-Fi: https://ko-fi.com/TheBlokeAI\nSpecial thanks to: Aemon Algiz.\nPatreon special mentions: Alicia Loh, Stephen Murray, K, Ajan Kanaga, RoA, Magnesian, Deo Leter, Olakabola, Eugene Pentland, zynix, Deep Realms, Raymond Fosdick, Elijah Stavena, Iucharbius, Erik Bj√§reholt, Luis Javier Navarrete Lozano, Nicholas, theTransient, John Detwiler, alfie_i, knownsqashed, Mano Prime, Willem Michiel, Enrico Ros, LangChain4j, OG, Michael Dempsey, Pierre Kircher, Pedro Madruga, James Bentley, Thomas Belote, Luke @flexchar, Leonard Tan, Johann-Peter Hartmann, Illia Dulskyi, Fen Risland, Chadd, S_X, Jeff Scroggin, Ken Nordquist, Sean Connelly, Artur Olbinski, Swaroop Kallakuri, Jack West, Ai Maven, David Ziegler, Russ Johnson, transmissions 11, John Villwock, Alps Aficionado, Clay Pascal, Viktor Bowallius, Subspace Studios, Rainer Wilmers, Trenton Dambrowitz, vamX, Michael Levine, Ï§ÄÍµê ÍπÄ, Brandon Frisco, Kalila, Trailburnt, Randy H, Talal Aujan, Nathan Dryer, Vadim, ÈòøÊòé, ReadyPlayerEmma, Tiffany J. Kim, George Stoitzev, Spencer Kim, Jerry Meng, Gabriel Tamborski, Cory Kujawski, Jeffrey Morgan, Spiking Neurons AB, Edmond Seymore, Alexandros Triantafyllidis, Lone Striker, Cap'n Zoog, Nikolai Manek, danny, ya boyyy, Derek Yates, usrbinkat, Mandus, TL, Nathan LeClaire, subjectnull, Imad Khwaja, webtim, Raven Klaugh, Asp the Wyvern, Gabriel Puliatti, Caitlyn Gatomon, Joseph William Delisle, Jonathan Leane, Luke Pendergrass, SuperWojo, Sebastain Graf, Will Dee, Fred von Graf, Andrey, Dan Guido, Daniel P. Andersen, Nitin Borwankar, Elle, Vitor Caleffi, biorpg, jjj, NimbleBox.ai, Pieter, Matthew Berman, terasurfer, Michael Davis, Alex, Stanislav Ovsiannikov\nThank you to all my generous patrons and donaters!\nAnd thank you again to a16z for their generous grant.\nOriginal model card: Gryphe's MythoMax L2 13B\nAn improved, potentially even perfected variant of MythoMix, my MythoLogic-L2 and Huginn merge using a highly experimental tensor type merge technique. The main difference with MythoMix is that I allowed more of Huginn to intermingle with the single tensors located at the front and end of a model, resulting in increased coherency across the entire structure.\nThe script and the acccompanying templates I used to produce both can be found here.\nThis model is proficient at both roleplaying and storywriting due to its unique nature.\nQuantized models are available from TheBloke: GGML - GPTQ (You're the best!)\nModel details\nThe idea behind this merge is that each layer is composed of several tensors, which are in turn responsible for specific functions. Using MythoLogic-L2's robust understanding as its input and Huginn's extensive writing capability as its output seems to have resulted in a model that exceeds at both, confirming my theory. (More details to be released at a later time)\nThis type of merge is incapable of being illustrated, as each of its 363 tensors had an unique ratio applied to it. As with my prior merges, gradients were part of these ratios to further finetune its behaviour.\nPrompt Format\nThis model primarily uses Alpaca formatting, so for optimal model performance, use:\n<System prompt/Character Card>\n### Instruction:\nYour instruction or question here.\nFor roleplay purposes, I suggest the following - Write <CHAR NAME>'s next reply in a chat between <YOUR NAME> and <CHAR NAME>. Write a single reply only.\n### Response:\nlicense: other",
    "Xenova/m2m100_418M": "https://huggingface.co/facebook/m2m100_418M with ONNX weights to be compatible with Transformers.js.\nNote: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using ü§ó Optimum and structuring your repo like this one (with ONNX weights located in a subfolder named onnx).",
    "Unbabel/XCOMET-XL": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nPaper\nUsage (unbabel-comet)\nLicense\nUsage Permissions:\nLimitations:\nContact Information:\nLanguages Covered:\nxCOMET stands for eXplainable COMET. This is an evaluation model that is trained to identify errors in sentences along with a final quality score and thus leading to an explainable neural metric. This is the XL version with ~3.5B parameters.\nPaper\nxCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection\nUsage (unbabel-comet)\nThis model requires unbabel-comet (>=2.2.0) to be installed:\npip install --upgrade pip  # ensures that pip is current\npip install \"unbabel-comet>=2.2.0\"\nThen you can use it through comet CLI:\ncomet-score -s {source-inputs}.txt -t {translation-outputs}.txt -r {references}.txt --model Unbabel/XCOMET-XL\nand if used with the --to_json flag you can also export the error spans detected by the model:\ncomet-score -s {source-inputs}.txt -t {translation-outputs}.txt -r {references}.txt --model Unbabel/XCOMET-XL --to_json {output}.json\nOr using Python:\nfrom comet import download_model, load_from_checkpoint\nmodel_path = download_model(\"Unbabel/XCOMET-XL\")\nmodel = load_from_checkpoint(model_path)\ndata = [\n{\n\"src\": \"Boris Johnson teeters on edge of favour with Tory MPs\",\n\"mt\": \"Boris Johnson ist bei Tory-Abgeordneten v√∂llig in der Gunst\",\n\"ref\": \"Boris Johnsons Beliebtheit bei Tory-MPs steht auf der Kippe\"\n}\n]\nmodel_output = model.predict(data, batch_size=8, gpus=1)\n# Segment-level scores\nprint (model_output.scores)\n# System-level score\nprint (model_output.system_score)\n# Score explanation (error spans)\nprint (model_output.metadata.error_spans)\nLicense\ncc-by-nc-sa-4.0\nUsage Permissions:\nEvaluation: You are encouraged to use this model for non-commercial evaluation purposes. Feel free to test and assess its performance in machine translation and various generative tasks.\nLimitations:\nCommercial Services: If you intend to utilize this model to build a commercial service, such as for profit, you are required to contact Unbabel to obtain proper authorization. This requirement is in place to ensure that any commercial use of the model for evaluation services is done in collaboration with Unbabel. This helps maintain the quality and consistency of the model's use in commercial contexts.\nContact Information:\nFor inquiries regarding commercial use authorization or any other questions, please contact us at ai-research@unbabel.com.\nWe believe in the power of open-source and collaborative efforts, and we're excited to contribute to the community's advancements in the field of natural language processing. Please respect the terms of the CC-BY-NC-SA-4.0 license when using XCOMET-XL.\nLanguages Covered:\nThis model builds on top of XLM-R XL which cover the following languages:\nAfrikaans, Albanian, Amharic, Arabic, Armenian, Assamese, Azerbaijani, Basque, Belarusian, Bengali, Bengali Romanized, Bosnian, Breton, Bulgarian, Burmese, Burmese, Catalan, Chinese (Simplified), Chinese (Traditional), Croatian, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Hausa, Hebrew, Hindi, Hindi Romanized, Hungarian, Icelandic, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish (Kurmanji), Kyrgyz, Lao, Latin, Latvian, Lithuanian, Macedonian, Malagasy, Malay, Malayalam, Marathi, Mongolian, Nepali, Norwegian, Oriya, Oromo, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Sanskri, Scottish, Gaelic, Serbian, Sindhi, Sinhala, Slovak, Slovenian, Somali, Spanish, Sundanese, Swahili, Swedish, Tamil, Tamil Romanized, Telugu, Telugu Romanized, Thai, Turkish, Ukrainian, Urdu, Urdu Romanized, Uyghur, Uzbek, Vietnamese, Welsh, Western, Frisian, Xhosa, Yiddish.\nThus, results for language pairs containing uncovered languages are unreliable!",
    "codellama/CodeLlama-7b-Instruct-hf": "Code Llama\nModel Use\nModel Details\nIntended Use\nHardware and Software\nTraining Data\nEvaluation Results\nEthical Considerations and Limitations\nCode Llama\nCode Llama is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 34 billion parameters. This is the repository for the 7B instruct-tuned version in the Hugging Face Transformers format. This model is designed for general code synthesis and understanding. Links to other models can be found in the index at the bottom.\nThis is a non-official Code Llama repo. You can find the official Meta repository in the Meta Llama organization.\nBase Model\nPython\nInstruct\n7B\ncodellama/CodeLlama-7b-hf\ncodellama/CodeLlama-7b-Python-hf\ncodellama/CodeLlama-7b-Instruct-hf\n13B\ncodellama/CodeLlama-13b-hf\ncodellama/CodeLlama-13b-Python-hf\ncodellama/CodeLlama-13b-Instruct-hf\n34B\ncodellama/CodeLlama-34b-hf\ncodellama/CodeLlama-34b-Python-hf\ncodellama/CodeLlama-34b-Instruct-hf\n70B\ncodellama/CodeLlama-70b-hf\ncodellama/CodeLlama-70b-Python-hf\ncodellama/CodeLlama-70b-Instruct-hf\nModel Use\nTo use this model, please make sure to install transformers:\npip install transformers accelerate\nModel capabilities:\nCode completion.\nInfilling.\nInstructions / chat.\nPython specialist.\nModel Details\n*Note: Use of this model is governed by the Meta license. Meta developed and publicly released the Code Llama family of large language models (LLMs).\nModel Developers Meta\nVariations Code Llama comes in three model sizes, and three variants:\nCode Llama: base models designed for general code synthesis and understanding\nCode Llama - Python: designed specifically for Python\nCode Llama - Instruct: for instruction following and safer deployment\nAll variants are available in sizes of 7B, 13B and 34B parameters.\nThis repository contains the Instruct version of the 7B parameters model.\nInput Models input text only.\nOutput Models generate text only.\nModel Architecture Code Llama is an auto-regressive language model that uses an optimized transformer architecture.\nModel Dates Code Llama and its variants have been trained between January 2023 and July 2023.\nStatus This is a static model trained on an offline dataset. Future versions of Code Llama - Instruct will be released as we improve model safety with community feedback.\nLicense A custom commercial license is available at: https://ai.meta.com/resources/models-and-libraries/llama-downloads/\nResearch Paper More information can be found in the paper \"Code Llama: Open Foundation Models for Code\" or its arXiv page.\nIntended Use\nIntended Use Cases Code Llama and its variants is intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistant and generation applications.\nOut-of-Scope Uses Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Code Llama and its variants.\nHardware and Software\nTraining Factors We used custom training libraries. The training and fine-tuning of the released models have been performed Meta‚Äôs Research Super Cluster.\nCarbon Footprint In aggregate, training all 9 Code Llama models required 400K GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 65.3 tCO2eq, 100% of which were offset by Meta‚Äôs sustainability program.\nTraining Data\nAll experiments reported here and the released models have been trained and fine-tuned using the same data as Llama 2 with different weights (see Section 2 and Table 1 in the research paper for details).\nEvaluation Results\nSee evaluations for the main models and detailed ablations in Section 3 and safety evaluations in Section 4 of the research paper.\nEthical Considerations and Limitations\nCode Llama and its variants are a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Code Llama‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any applications of Code Llama, developers should perform safety testing and tuning tailored to their specific applications of the model.\nPlease see the Responsible Use Guide available available at https://ai.meta.com/llama/responsible-use-guide."
}