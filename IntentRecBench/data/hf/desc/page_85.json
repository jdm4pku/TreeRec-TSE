{
    "facebook/sapiens-pose-1b-torchscript": "Pose-Sapiens-1B-Torchscript\nUses\nPose-Sapiens-1B-Torchscript\nModel Details\nSapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. The pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions.\nSapiens-1B natively support 1K high-resolution inference. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic.\nDeveloped by: Meta\nModel type: Vision Transformer\nLicense: Creative Commons Attribution-NonCommercial 4.0\nTask: pose\nFormat: torchscript\nFile: sapiens_1b_goliath_best_goliath_AP_639_torchscript.pt2\nModel Card\nImage Size: 1024 x 768 (H x W)\nNum Parameters: 1.169 B\nFLOPs: 4.647 TFLOPs\nPatch Size: 16 x 16\nEmbedding Dimensions: 1536\nNum Layers: 40\nNum Heads: 24\nFeedforward Channels: 6144\nMore Resources\nRepository: https://github.com/facebookresearch/sapiens\nPaper: https://arxiv.org/abs/2408.12569\nDemo: https://huggingface.co/spaces/facebook/sapiens-pose\nProject Page: https://about.meta.com/realitylabs/codecavatars/sapiens\nAdditional Results: https://rawalkhirodkar.github.io/sapiens\nHuggingFace Collection: https://huggingface.co/collections/facebook/sapiens-66d22047daa6402d565cb2fc\nUses\nPose 1B model can be used for estimate 308 keypoints (body + face + hands + feet) on a single image.",
    "TheDrummer/UnslopNemo-12B-v2-GGUF": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nUnslopNemo v2 (Experiment)\nFeedback\nUsage\n(Previously BeaverAI/Rocinante-12B-v2d-GGUF)\nUnslopNemo v2 (Experiment)\nI unslopped roughly 90% of my RP dataset in an attempt to make the model more expressive.\nFeedback\nFeedback is very important for this one. If you have any thoughts, positive or negative, please share it in the discussion: https://huggingface.co/TheDrummer/UnslopNemo-v1-GGUF/discussions/1\nUsage\nMistral, ChatML, or Text Completion\nPlay around with your samplers. You might have better results if you disable the usual enabled stuff.",
    "facebook/sapiens-pose-1b": "Pose-Sapiens-1B\nUses\nPose-Sapiens-1B\nModel Details\nSapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. The pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions.\nSapiens-1B natively support 1K high-resolution inference. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic.\nDeveloped by: Meta\nModel type: Vision Transformer\nLicense: Creative Commons Attribution-NonCommercial 4.0\nTask: pose\nFormat: original\nFile: sapiens_1b_goliath_best_goliath_AP_639.pth\nModel Card\nImage Size: 1024 x 768 (H x W)\nNum Parameters: 1.169 B\nFLOPs: 4.647 TFLOPs\nPatch Size: 16 x 16\nEmbedding Dimensions: 1536\nNum Layers: 40\nNum Heads: 24\nFeedforward Channels: 6144\nMore Resources\nRepository: https://github.com/facebookresearch/sapiens\nPaper: https://arxiv.org/abs/2408.12569\nDemo: https://huggingface.co/spaces/facebook/sapiens-pose\nProject Page: https://about.meta.com/realitylabs/codecavatars/sapiens\nAdditional Results: https://rawalkhirodkar.github.io/sapiens\nHuggingFace Collection: https://huggingface.co/collections/facebook/sapiens-66d22047daa6402d565cb2fc\nUses\nPose 1B model can be used for estimate 308 keypoints (body + face + hands + feet) on a single image.",
    "fishaudio/fish-speech-1.4": "Fish Speech V1.4\nCitation\nLicense\nFish Speech V1.4\nFish Speech V1.4 is a leading text-to-speech (TTS) model trained on 700k hours of audio data in multiple languages.\nSupported languages:\nEnglish (en) ~300k hours\nChinese (zh) ~300k hours\nGerman (de) ~20k hours\nJapanese (ja) ~20k hours\nFrench (fr) ~20k hours\nSpanish (es) ~20k hours\nKorean (ko) ~20k hours\nArabic (ar) ~20k hours\nPlease refer to Fish Speech Github for more info.Demo available at Fish Audio.\nCitation\nIf you found this repository useful, please consider citing this work:\n@article{fish-speech-v1.4,\nauthor = {Shijia Liao, Tianyu Li and others},\ntitle = {Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis},\nyear = {2024},\njournal = {arXiv preprint arXiv:2411.01156},\neprint = {2411.01156},\narchivePrefix = {arXiv},\nprimaryClass = {cs.SD},\nurl = {https://arxiv.org/abs/2411.01156}\n}\nLicense\nThis model is permissively licensed under the BY-CC-NC-SA-4.0 license.\nThe source code is released under BSD-3-Clause license.",
    "922CA/gemma-2-9b-etude-2": "Test model fine-tuned to work with COLANA program format and tags, fine-tuned on few samples of its logs.",
    "SemanticAlignment/Llama-3.1-8B-Italian-SAVA": "Llama-3.1-8B-Italian-SAVA\nData used for the adaptation\nUse with Transformers\nCitation\nLlama-3.1-8B-Italian-SAVA\nThe Llama-3.1-8B-Adapted collection of large language models (LLMs), is a collection of adapted generative models in 8B (text in/text out), adapted models from Llama-3.1-8B.\nLlama-3.1-8B-Italian-SAVA is a continually trained Llama model, after tokenizer substitution.\nThe tokenizer of this model after adaptation is the same as Minverva-3B.\nModel developer: SapienzaNLP, ISTI-CNR, ILC-CNR\nModel Architecture: Llama-3.1-8B-Adapted is an auto-regressive language model that uses an optimized transformer architecture.\nData used for the adaptation\nThe Llama-3.1-8B-Adapted models are trained on a collection of Italian and English data extracted from CulturaX.\nThe data is extracted to be skewed toward the Italian language with a ratio of one over four. Extracting the first 9B tokens from the Italian part of CulturaX and the first 3B tokens from the English part of CulturaX.\nUse with Transformers\nYou can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\nMake sure to update your transformers installation via pip install --upgrade transformers.\nimport transformers\nimport torch\nmodel_id = \"SemanticAlignment/Llama-3.1-8B-Italian-SAVA\"\npipeline = transformers.pipeline(\n\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n)\npipeline(\"Cosa si pu√≤ fare in una bella giornata di sole?\")\nCode: https://github.com/SapienzaNLP/sava\nCitation\nIf you use any part of this work, please consider citing the paper as follows:\n@misc{moroni2025optimizingllmsitalianreducing,\ntitle={Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation},\nauthor={Luca Moroni and Giovanni Puccetti and Pere-Lluis Huguet Cabot and Andrei Stefan Bejgu and Edoardo Barba and Alessio Miaschi and Felice Dell'Orletta and Andrea Esuli and Roberto Navigli},\nyear={2025},\neprint={2504.17025},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2504.17025},\n}",
    "SemanticAlignment/Llama-3.1-8B-Italian-LAPT": "Llama-3.1-8B-Italian-LAPT\nData used for the adaptation\nUse with Transformers\nCitation\nLlama-3.1-8B-Italian-LAPT\nThe Llama-3.1-8B-Adapted collection of large language models (LLMs), is a collection of adapted generative models in 8B (text in/text out), adapted models from Llama-3.1-8B.\nLlama-3.1-8B-Italian-LAPT is a continually trained Llama model.\nModel developer: SapienzaNLP, ISTI-CNR, ILC-CNR\nModel Architecture: Llama-3.1-8B-Adapted is an auto-regressive language model that uses an optimized transformer architecture.\nData used for the adaptation\nThe Llama-3.1-8B-Adapted models are trained on a collection of Italian and English data extracted from CulturaX.\nThe data are extracted to be skewed toward Italian language with a ratio of one over four. Extracting the first 9B tokens from the Italian part of CulturaX and the first 3B tokens from the English part of CulturaX.\nUse with Transformers\nYou can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\nMake sure to update your transformers installation via pip install --upgrade transformers.\nimport transformers\nimport torch\nmodel_id = \"SemanticAlignment/Llama-3.1-8B-Italian-LAPT\"\npipeline = transformers.pipeline(\n\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n)\npipeline(\"Cosa si pu√≤ fare in una bella giornata di sole?\")\nCode: https://github.com/SapienzaNLP/sava\nCitation\nIf you use any part of this work, please consider citing the paper as follows:\n@misc{moroni2025optimizingllmsitalianreducing,\ntitle={Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation},\nauthor={Luca Moroni and Giovanni Puccetti and Pere-Lluis Huguet Cabot and Andrei Stefan Bejgu and Edoardo Barba and Alessio Miaschi and Felice Dell'Orletta and Andrea Esuli and Roberto Navigli},\nyear={2025},\neprint={2504.17025},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2504.17025},\n}",
    "nvidia/Nemotron-Mini-4B-Instruct": "Nemotron-Mini-4B-Instruct\nModel Overview\nLicense\nModel Architecture\nPrompt Format:\nUsage\nAI Safety Efforts\nLimitations\nEthical Considerations\nNemotron-Mini-4B-Instruct\nModel Overview\nNemotron-Mini-4B-Instruct is a model for generating responses for roleplaying, retrieval augmented generation, and function calling.  It is a small language model (SLM) optimized through distillation, pruning and quantization for speed and on-device deployment. It is a fine-tuned version of nvidia/Minitron-4B-Base, which was pruned and distilled from Nemotron-4 15B using our LLM compression technique. This instruct model is optimized for roleplay, RAG QA, and function calling in English. It supports a context length of 4,096 tokens. This model is ready for commercial use.\nTry this model on build.nvidia.com.\nFor more details about how this model is used for NVIDIA ACE, please refer to this blog post and this demo video, which showcases how the model can be integrated into a video game. You can download the model checkpoint for NVIDIA AI Inference Manager (AIM) SDK from here.\nModel Developer: NVIDIA\nModel Dates: Nemotron-Mini-4B-Instruct was trained between February 2024 and Aug 2024.\nLicense\nNVIDIA Community Model License\nModel Architecture\nNemotron-Mini-4B-Instruct uses a model embedding size of 3072, 32 attention heads, and an MLP intermediate dimension of 9216. It also uses Grouped-Query Attention (GQA) and Rotary Position Embeddings (RoPE).\nArchitecture Type: Transformer Decoder (auto-regressive language model)\nNetwork Architecture: Nemotron-4\nPrompt Format:\nWe recommend using the following prompt template, which was used to fine-tune the model. The model may not perform optimally without it.\nSingle Turn\n<extra_id_0>System\n{system prompt}\n<extra_id_1>User\n{prompt}\n<extra_id_1>Assistant\\n\nTool use\n<extra_id_0>System\n{system prompt}\n<tool> ... </tool>\n<context> ... </context>\n<extra_id_1>User\n{prompt}\n<extra_id_1>Assistant\n<toolcall> ... </toolcall>\n<extra_id_1>Tool\n{tool response}\n<extra_id_1>Assistant\\n\nUsage\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n# Load the tokenizer and model\ntokenizer  = AutoTokenizer.from_pretrained(\"nvidia/Nemotron-Mini-4B-Instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\"nvidia/Nemotron-Mini-4B-Instruct\")\n# Use the prompt template\nmessages = [\n{\n\"role\": \"system\",\n\"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n},\n{\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\noutputs = model.generate(tokenized_chat, max_new_tokens=128)\nprint(tokenizer.decode(outputs[0]))\nYou can also use pipeline but you need to create a tokenizer object and assign it to the pipeline manually.\nfrom transformers import AutoTokenizer\nfrom transformers import pipeline\ntokenizer  = AutoTokenizer.from_pretrained(\"nvidia/Nemotron-Mini-4B-Instruct\")\nmessages = [\n{\"role\": \"user\", \"content\": \"Who are you?\"},\n]\npipe = pipeline(\"text-generation\", model=\"nvidia/Nemotron-Mini-4B-Instruct\")\npipe.tokenizer = tokenizer  # You need to assign tokenizer manually\npipe(messages)\nAI Safety Efforts\nThe Nemotron-Mini-4B-Instruct model underwent AI safety evaluation including adversarial testing via three distinct methods:\nGarak, is an automated LLM vulnerability scanner that probes for common weaknesses, including prompt injection and data leakage.\nAEGIS, is a content safety evaluation dataset and LLM based content safety classifier model, that adheres to a broad taxonomy of 13 categories of critical risks in human-LLM interactions.\nHuman Content Red Teaming leveraging human interaction and evaluation of the models' responses.\nLimitations\nThe model was trained on data that contains toxic language and societal biases originally crawled from the internet. Therefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. The model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive. This issue could be exacerbated without the use of the recommended prompt template. This issue could be exacerbated without the use of the recommended prompt template.\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++. Please report security vulnerabilities or NVIDIA AI Concerns here.",
    "kyutai/moshiko-pytorch-bf16": "Model Card for Moshi\nModel Details\nModel Description\nModel Sources\nUses\nDirect Use\nDownstream Use\nOut-of-Scope Use\nBias, Risks, and Limitations\nHow to Get Started with the Model\nTraining Details\nTraining Data\nTraining procedure and hyper-parameters\nCompute Infrastructure\nCitation\nModel Card Authors\nModel Card for Moshi\nMoshi is a speech-text foundation model and full-duplex spoken dialogue framework\nModel Details\nPytorch version quantized in bf16 precision.\nModel Description\nMoshi is a speech-text foundation model that casts spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics.\nMoshi also predicts time-aligned text tokens as a prefix to audio tokens. This ‚ÄúInner\nMonologue‚Äù method significantly improves the linguistic quality of generated speech and provides streaming speech recognition and text-to-speech. As a result, Moshi is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice.\nDeveloped by:  Kyutai\nModel type: Multimodal speech-text foundation model\nLanguage(s) (NLP): English\nLicense: CC-BY\nModel Sources\nRepository: repo\nPaper: paper\nDemo: demo\nUses\nDirect Use\nThe model can be used as a conversational agent for casual conversations, basic facts and advice (e.g. recipes, trivia), roleplay, etc. However, the model has limited abilities for complex tasks and cannot access tools, but rather focues on natural, low-latency interactions.\nDownstream Use\nSome components of the model can be used independently or repurposed relatively easily.\nFor instance the Mimi codec is a state-of-the-art audio neural codec that combines semantic and acoustic information into audio tokens running at 12Hz and a bitrate of 1.1kbps, which make it particularly adapted to train speech language models or text-to-speech systems.. Regarding the main Moshi architecture, other downstream usecases would require some finetuning / domain adaptation.\nOut-of-Scope Use\nThe model is not intended to be used to impersonate other people or any malicious use of any kind.\nThis model is for research only and we do not recommend it for providing advices or to perform any professionnal duty.\nBias, Risks, and Limitations\nThe model has been trained with a few safeguards to try to limit potential toxic usages, however our toxicity analysis shows that it behaves in the middle of existing models with respect to textual generation. It has some bias towards certain domains and topics that are over-represented in the training data. Its capabilities are relatively limited so far and it is trained to produce only one voice to avoid impersonation. Yet, we need the perspective in time to establish the sociotechnical limitations.\nHow to Get Started with the Model\nSee the main README file.\nTraining Details\nTraining Data\nTextual data: The underlying Helium model is trained on a mix of data, more precisely:\n12.5% is high-quality data sources from the following curated sources: Wikipedia Wikibooks, Wikisource, Wikinews,\nStackExchange and the collection of scientific articles pes2o. For Wikipedia, we use five different dumps from 2017, 2018, 2019, 2021 and 2022.\n87.5% is filtered web data from CommonCrawl, using the following crawls: 2018-30, 2019-04, 2019-30, 2020-05, 2020-34, 2021-04, 2021-31, 2022-05, 2022-33, 2023-40.\nAudio data\nUnsupervised audio dataset: used for pre-training, this is a collection of 7 million hours of readily available audio content, which consists mostly of English speech. This training set is transcribed with Whisper (large v3 model)\nThe Fisher dataset:: used to enable multi-stream. It consists of 2000 hours of phone conversations at 8kHz from Fisher, which we upsample to 24kHz using AudioSR.\nSupervised multi-stream dataset: A dataset of 170 hours of natural and scripted conversation between multiple pairs of participants, collected by Kyutai. This dataset is used to train the TTS system used to create synthetic data.\nSynthetic data: 20,000 hours of synthetic data generated by our TTS system, and simulating a dialogue between Moshi and a user.\nTraining procedure and hyper-parameters\nThe different stages of the training procedure are detailled in the paper along with the hyper-parameters.\nCompute Infrastructure\nThe training was performed on 127 DGX nodes provided by Scaleway, accounting for 1016 H100 Nvidia GPUs.\nCitation\n@techreport{kyutai2024moshi,\nauthor = {Alexandre D\\'efossez and Laurent Mazar\\'e and Manu Orsini and Am\\'elie Royer and Patrick P\\'erez and Herv\\'e J\\'egou and Edouard Grave and Neil Zeghidour},\ntitle = {Moshi: a speech-text foundation model for real-time dialogue},\ninstitution = {Kyutai},\nyear={2024},\nmonth={September},\nurl={http://kyutai.org/Moshi.pdf},\n}\nModel Card Authors\nAlexandre D√©fossez, Laurent Mazar√©, Manu Orsini, Am√©lie Royer, Patrick P√©rez, Herv√© J√©gou, Edouard Grave, Neil Zeghidour",
    "bartowski/reader-lm-1.5b-GGUF": "Llamacpp imatrix Quantizations of reader-lm-1.5b\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nQ4_0_X_X\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of reader-lm-1.5b\nUsing llama.cpp release b3715 for quantization.\nOriginal model: https://huggingface.co/jinaai/reader-lm-1.5b\nAll quants made using imatrix option with dataset from here\nRun them in LM Studio\nPrompt format\n<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nreader-lm-1.5b-f16.gguf\nf16\n3.09GB\nfalse\nFull F16 weights.\nreader-lm-1.5b-Q8_0.gguf\nQ8_0\n1.65GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nreader-lm-1.5b-Q6_K_L.gguf\nQ6_K_L\n1.33GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nreader-lm-1.5b-Q6_K.gguf\nQ6_K\n1.27GB\nfalse\nVery high quality, near perfect, recommended.\nreader-lm-1.5b-Q5_K_L.gguf\nQ5_K_L\n1.18GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nreader-lm-1.5b-Q5_K_M.gguf\nQ5_K_M\n1.13GB\nfalse\nHigh quality, recommended.\nreader-lm-1.5b-Q5_K_S.gguf\nQ5_K_S\n1.10GB\nfalse\nHigh quality, recommended.\nreader-lm-1.5b-Q4_K_L.gguf\nQ4_K_L\n1.04GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nreader-lm-1.5b-Q4_K_M.gguf\nQ4_K_M\n0.99GB\nfalse\nGood quality, default size for must use cases, recommended.\nreader-lm-1.5b-Q4_K_S.gguf\nQ4_K_S\n0.94GB\nfalse\nSlightly lower quality with more space savings, recommended.\nreader-lm-1.5b-Q4_0.gguf\nQ4_0\n0.94GB\nfalse\nLegacy format, generally not worth using over similarly sized formats\nreader-lm-1.5b-Q3_K_XL.gguf\nQ3_K_XL\n0.94GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nreader-lm-1.5b-Q4_0_8_8.gguf\nQ4_0_8_8\n0.93GB\nfalse\nOptimized for ARM inference. Requires 'sve' support (see link below).\nreader-lm-1.5b-Q4_0_4_8.gguf\nQ4_0_4_8\n0.93GB\nfalse\nOptimized for ARM inference. Requires 'i8mm' support (see link below).\nreader-lm-1.5b-Q4_0_4_4.gguf\nQ4_0_4_4\n0.93GB\nfalse\nOptimized for ARM inference. Should work well on all ARM chips, pick this if you're unsure.\nreader-lm-1.5b-IQ4_XS.gguf\nIQ4_XS\n0.90GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nreader-lm-1.5b-Q3_K_L.gguf\nQ3_K_L\n0.88GB\nfalse\nLower quality but usable, good for low RAM availability.\nreader-lm-1.5b-IQ3_M.gguf\nIQ3_M\n0.78GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nSome say that this improves the quality, others don't notice any difference. If you use these models PLEASE COMMENT with your findings. I would like feedback that these are actually used and useful so I don't keep uploading quants no one is using.\nThanks!\nDownloading using huggingface-cli\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/reader-lm-1.5b-GGUF --include \"reader-lm-1.5b-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/reader-lm-1.5b-GGUF --include \"reader-lm-1.5b-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (reader-lm-1.5b-Q8_0) or download them all in place (./)\nQ4_0_X_X\nThese are NOT for Metal (Apple) offloading, only ARM chips.\nIf you're using an ARM chip, the Q4_0_X_X quants will have a substantial speedup. Check out Q4_0_4_4 speed comparisons on the original pull request\nTo check which one would work best for your ARM chip, you can check AArch64 SoC features (thanks EloyOn!).\nWhich file should I choose?\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset\nThank you ZeroWw for the inspiration to experiment with embed/output\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "DaydreamerF/TibetaMind": "TibetaMind: Advanced Tibetan Language Model\nHow to use\nUse with transformers\nTransformers AutoModelForCausalLM\nTibetaMind: Advanced Tibetan Language Model\nTibetaMind is an advanced language model based on the Llama 3-8B-Instruct architecture, further fine-tuned using extensive Tibetan language corpora. Through this specialized fine-tuning, TibetaMind has significantly enhanced its ability to comprehend, process, and generate Tibetan language content, while also providing seamless cross-language understanding between Tibetan and Chinese. This allows for accurate translation and communication across these languages. TibetaMind can be applied to a variety of tasks, including Tibetan text generation, summarization, and translation between Tibetan and Chinese, playing a pivotal role in preserving and advancing Tibetan linguistics in the digital age.\nHow to use\nUse with transformers\nTransformers AutoModelForCausalLM\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nmodel_id = \"DaydreamerF/TibetaMind\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\ndevice_map=\"auto\",\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"Â¶Ç‰ΩïÁî®ËóèËØ≠Ë°®Ëææ‰∏ãÈù¢Ê±âËØ≠ÁöÑÊÑèÊÄùÔºöÊ±âËØ≠Âè•Â≠êÔºöÂ§ßÁãóÂú®Ê•ºÈáå‰∏çÂ•ΩÂÖª„ÄÇ\"},\n]\ninput_ids = tokenizer.apply_chat_template(\nmessages,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\"\n).to(model.device)\nterminators = [\ntokenizer.eos_token_id,\ntokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\noutputs = model.generate(\ninput_ids,\nmax_new_tokens=256,\neos_token_id=terminators,\ndo_sample=True,\ntemperature=0.6,\ntop_p=0.9,\n)\nresponse = outputs[0][input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))",
    "Epiculous/Violet_Twilight-v0.2": "Quants!\nPrompting\nContext and Instruct\nCurrent Top Sampler Settings\nMerging\nOpen LLM Leaderboard Evaluation Results\nOpen LLM Leaderboard Evaluation Results\nNow for something a bit different, Violet_Twilight-v0.2! This model is a SLERP merge of Azure_Dusk-v0.2 and Crimson_Dawn-v0.2!\nQuants!\nfull / exl2 / gguf\nPrompting\nThe v0.2 models are trained on ChatML, the prompting structure goes a little something like this:\n<|im_start|>user\nHi there!<|im_end|>\n<|im_start|>assistant\nNice to meet you!<|im_end|>\n<|im_start|>user\nCan I ask a question?<|im_end|>\n<|im_start|>assistant\nContext and Instruct\nThe v0.2 models are trained on ChatML, please use that Context and Instruct template.\nCurrent Top Sampler Settings\nSmooth Creativity: Credit to Juelsman for researching this one!\nVariant Chimera: Credit to Numbra!\nSpicy_Temp\nViolet_Twilight-Nitral-Special\nMerging\nThe following config was used to merge Azure Dusk and Crimson Dawn\nslices:\n- sources:\n- model: Epiculous/Azure_Dusk-v0.2\nlayer_range: [0, 40]\n- model: Epiculous/Crimson_Dawn-V0.2\nlayer_range: [0, 40]\nmerge_method: slerp\nbase_model: Epiculous/Azure_Dusk-v0.2\nparameters:\nt:\n- filter: self_attn\nvalue: [0, 0.5, 0.3, 0.7, 1]\n- filter: mlp\nvalue: [1, 0.5, 0.7, 0.3, 0]\n- value: 0.5 # fallback for rest of tensors\ndtype: bfloat16\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n18.53\nIFEval (0-Shot)\n45.32\nBBH (3-Shot)\n23.94\nMATH Lvl 5 (4-Shot)\n2.72\nGPQA (0-shot)\n2.13\nMuSR (0-shot)\n13.61\nMMLU-PRO (5-shot)\n23.45\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n18.53\nIFEval (0-Shot)\n45.32\nBBH (3-Shot)\n23.94\nMATH Lvl 5 (4-Shot)\n2.72\nGPQA (0-shot)\n2.13\nMuSR (0-shot)\n13.61\nMMLU-PRO (5-shot)\n23.45",
    "v000000/L3.1-Storniitova-8B": "Llama-3.1-Storniitova-8B\n(GGUF) mradermacher quants:\n(GGUF) QuantFactory quants:\nmerge\nMerge Details\nMerge Method\nModels Merged\nRecipe\nOpen LLM Leaderboard Evaluation Results\nLlama-3.1-Storniitova-8B\nStorniitova-8B is a RP/Instruct model built on the foundation of Llama-3.1-SuperNova-Lite, which is distilled from the 405B parameter variant of Llama-3.1\nBy only changing the vector tasks, I attempt to retain the full 405B distillation while learning roleplaying capabilties.\n(GGUF) mradermacher quants:\nGGUFs\nGGUFs imatrix\n(GGUF) QuantFactory quants:\nGGUFs\nmerge\nThis is a merge of pre-trained language models created using mergekit and other proprietary tools.\nMerge Details\nMerge Method\nThis model was merged using the SLERP, Task_Arithmetic and NEARSWAP merge method.\nModels Merged\nThe following models were included in the merge:\nv000000/L3.1-Niitorm-8B-t0.0001\nakjindal53244/Llama-3.1-Storm-8B\narcee-ai/Llama-Spark\narcee-ai/Llama-3.1-SuperNova-Lite\nv000000/L3.1-8B-RP-Test-003-Task_Arithmetic\nSao10K/L3.1-8B-Niitama-v1.1 + grimjim/Llama-3-Instruct-abliteration-LoRA-8B\nv000000/L3.1-8B-RP-Test-002-Task_Arithmetic + grimjim/Llama-3-Instruct-abliteration-LoRA-8B\nRecipe\nThe following YAML configuration was used to produce this model:\n#Step1 - Add smarts to Niitama with alchemonaut's algorithm.\nslices:\n- sources:\n- model: Sao10K/L3.1-8B-Niitama-v1.1+grimjim/Llama-3-Instruct-abliteration-LoRA-8B\nlayer_range: [0, 32]\n- model: akjindal53244/Llama-3.1-Storm-8B\nlayer_range: [0, 32]\nmerge_method: nearswap\nbase_model: Sao10K/L3.1-8B-Niitama-v1.1+grimjim/Llama-3-Instruct-abliteration-LoRA-8B\nparameters:\nt:\n- value: 0.0001\ndtype: bfloat16\nout_type: float16\n#Step 2 - Learn vectors onto Supernova 0.4(Niitorm)\nmodels:\n- model: arcee-ai/Llama-3.1-SuperNova-Lite\nparameters:\nweight: 1.0\n- model: v000000/L3.1-Niitorm-8B-t0.0001\nparameters:\nweight: 0.4\nmerge_method: task_arithmetic\nbase_model: arcee-ai/Llama-3.1-SuperNova-Lite\nparameters:\nnormalize: false\ndtype: float16\n#Step 3 - Fully learn vectors onto Supernova 1.25(Niitorm)\nmodels:\n- model: arcee-ai/Llama-3.1-SuperNova-Lite\nparameters:\nweight: 0.0\n- model: v000000/L3.1-Niitorm-8B-t0.0001\nparameters:\nweight: 1.25\nmerge_method: task_arithmetic\nbase_model: arcee-ai/Llama-3.1-SuperNova-Lite\nparameters:\nnormalize: false\ndtype: float16\n#Step 4 - Merge checkpoints and keep output/input Supernova heavy\n#Merge with a triangular slerp from sophosympatheia.\nmodels:\n- model: v000000/L3.1-8B-RP-Test-003-Task_Arithmetic\nmerge_method: slerp\nbase_model: v000000/L3.1-8B-RP-Test-002-Task_Arithmetic+grimjim/Llama-3-Instruct-abliteration-LoRA-8B\n# This model needed some abliteration^\nparameters:\nt:\n- value: [0, 0, 0.3, 0.4, 0.5, 0.6, 0.5, 0.4, 0.3, 0, 0]\ndtype: float16\nSLERP distribution used to smoothly blend the mostly Supernova base with the roleplay vectors:\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n28.06\nIFEval (0-Shot)\n78.17\nBBH (3-Shot)\n30.81\nMATH Lvl 5 (4-Shot)\n13.29\nGPQA (0-shot)\n5.26\nMuSR (0-shot)\n9.96\nMMLU-PRO (5-shot)\n30.84",
    "behbudiy/Mistral-Nemo-Instruct-Uz": "Usage\nMistral Inference\nInstall\nDownload\nChat\nInstruction Following\nInformation on Evaluation Method\nMMLU\nMore\nModel Description\nThe Mistral-Nemo-Instruct-Uz model has been continually pre-trained and instruction-tuned using a mix of publicly available and syntheticly constructed Uzbek and English data to preserve its original knowledge while enhancing its capabilities. This model is designed to support various natural language processing tasks in Uzbek, such as machine translation, summarization, and dialogue systems, ensuring robust performance across these applications.\nFor details regarding the performance metrics compared to the base model, see this post.\nDeveloped by:\nEldor Fozilov\nAzimjon Urinov\nKhurshid Juraev\nüìä Performance Comparison:\nModel Name\nBLEU Uz-En (One-shot)\nBLEU En-Uz (One-shot)\nCOMET (Uz-En)\nCOMET (Ez-Un)\nUzbek Sentiment Analysis\nUzbek News Classification\nMMLU (English) (5-shot)\nLlama-3.1 8B Instruct\n23.74\n6.72\n84.30\n82.70\n68.96\n55.41\n65.77\nLlama-3.1 8B Instruct Uz\n27.42\n11.58\n85.63\n86.53\n82.42\n60.84\n62.78\nMistral 7B Instruct\n7.47\n0.67\n68.14\n45.58\n62.02\n47.52\n61.07\nMistral 7B Instruct Uz\n29.39\n16.77\n86.91\n88.75\n79.13\n59.38\n55.72\nMistral Nemo Instruct\n25.68\n9.79\n85.56\n85.04\n72.47\n49.24\n67.62\nMistral Nemo Instruct Uz\n30.49\n15.52\n87.04\n88.01\n82.05\n58.2\n67.36\nGoogle Translate\n41.18\n22.98\n89.16\n90.67\n‚Äî\n‚Äî\n‚Äî\nThe results show that Uzbek-optimized models consistently outperform their base counterparts in translation benchmarks (BLEU and COMET) on the FLORES+ Uz-En / En-Uz evaluation datasets, sentiment analysis and news classification in Uzbek language.\nAlso, on the MMLU benchmark, which measures general language understanding across multiple tasks in English, the finetuned models did not show significant decline. (The base Llama model‚Äôs MMLU score differs from the official score due to our evaluation method. Refer to the links below to see evaluation details.)\nLooking ahead, these models are just early versions. We are actively working on further improving our data curation and fine-tuning method to provide even better results in the near future. In addition, we will scale up the dataset size both for continual-pretraining and instruction-tuning, and also customize other strong open-source LLMs for Uzbek language.\nWe‚Äôre eager to see how these models will be used by our Uzbek üá∫üáø community and look forward to continuing this work. üöÄ\nUsage\nThe model can be used with frameworks:\nmistral_inference: See here\nMistral Inference\nInstall\nIt is recommended to use behbudiy/Mistral-Nemo-Instruct-Uz with mistral-inference. For HF transformers code snippets, please keep scrolling.\npip install mistral_inference\nDownload\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\nmistral_models_path = Path.home().joinpath('mistral_models', 'Nemo-Instruct-Uz')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\nsnapshot_download(repo_id=\"behbudiy/Mistral-Nemo-Instruct-Uz\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tekken.json\"], local_dir=mistral_models_path)\nChat\nAfter installing mistral_inference, a mistral-chat CLI command should be available in your environment. You can chat with the model using\nmistral-chat $HOME/mistral_models/Nemo-Instruct-Uz --instruct --max_tokens 256 --temperature 0.35\nE.g. Try out something like:\nO'zbek tilida kichik bir she'r yozib bera olasanmi?\nInstruction Following\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tekken.json\")\nmodel = Transformer.from_folder(mistral_models_path)\nprompt = \"O'zbek tilida kichik bir she'r yozib bera olasanmi?\"\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=prompt)])\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.35, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.decode(out_tokens[0])\nprint(result)\nInformation on Evaluation Method\nTo evaluate on the translation task, we used FLORES+ Uz-En / En-Uz datasets, where we merged the dev and test sets to create a bigger evaluation data for each Uz-En and En-Uz subsets.\nWe used the following prompt to do one-shot Uz-En evaluation both for the base model and Uzbek-optimized model (for En-Uz eval, we changed the positions of the words \"English\" and \"Uzbek\").\nprompt = f'''You are a professional Uzbek-English translator. Your task is to accurately translate the given Uzbek text into English.\nInstructions:\n1. Translate the text from Uzbek to English.\n2. Maintain the original meaning and tone.\n3. Use appropriate English grammar and vocabulary.\n4. If you encounter an ambiguous or unfamiliar word, provide the most likely translation based on context.\n5. Output only the English translation, without any additional comments.\nExample:\nUzbek: \"Bugun ob-havo juda yaxshi, quyosh charaqlab turibdi.\"\nEnglish: \"The weather is very nice today, the sun is shining brightly.\"\nNow, please translate the following Uzbek text into English:\n\"{sentence}\"\n'''\nTo assess the model's ability in Uzbek sentiment analysis, we used the risqaliyevds/uzbek-sentiment-analysis dataset, for which we created binary labels (0: Negative, 1: Positive) using GPT-4o API (refer to behbudiy/uzbek-sentiment-analysis dataset).\nWe used the following prompt for the evaluation:\nprompt = f'''Given the following text, determine the sentiment as either 'Positive' or 'Negative.' Respond with only the word 'Positive' or 'Negative' without any additional text or explanation.\nText: {text}\"\n'''\nFor Uzbek News Classification, we used risqaliyevds/uzbek-zero-shot-classification dataset and asked the model to predict the category of the news using the following prompt:\nprompt = f'''Classify the given Uzbek news article into one of the following categories. Provide only the category number as the answer.\nCategories:\n0 - Politics (Siyosat)\n1 - Economy (Iqtisodiyot)\n2 - Technology (Texnologiya)\n3 - Sports (Sport)\n4 - Culture (Madaniyat)\n5 - Health (Salomatlik)\n6 - Family and Society (Oila va Jamiyat)\n7 - Education (Ta'lim)\n8 - Ecology (Ekologiya)\n9 - Foreign News (Xorijiy Yangiliklar)\nNow classify this article:\n\"{text}\"\nAnswer (number only):\"\n'''\nMMLU\nWe used this script.\nMore\nFor more details and examples, refer to the base model below:\nhttps://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407",
    "mistral-community/pixtral-12b": "Model Card for Model ID\nUsage example\nUsage with chat template\nModel Card for Model ID\nTransformers compatible pixtral checkpoints. Make sure to install from source or wait for v4.45!\nUsage example\nfrom PIL import Image\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\nmodel_id = \"mistral-community/pixtral-12b\"\nmodel = LlavaForConditionalGeneration.from_pretrained(model_id)\nprocessor = AutoProcessor.from_pretrained(model_id)\nIMG_URLS = [\n\"https://picsum.photos/id/237/400/300\",\n\"https://picsum.photos/id/231/200/300\",\n\"https://picsum.photos/id/27/500/500\",\n\"https://picsum.photos/id/17/150/600\",\n]\nPROMPT = \"<s>[INST]Describe the images.\\n[IMG][IMG][IMG][IMG][/INST]\"\ninputs = processor(text=PROMPT, images=IMG_URLS, return_tensors=\"pt\").to(\"cuda\")\ngenerate_ids = model.generate(**inputs, max_new_tokens=500)\noutput = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\nYou should get an output similar to the below:\n\"\"\"\nDescribe the images.\nSure, let's break down each image description:\n1. **Image 1:**\n- **Description:** A black dog with a glossy coat is sitting on a wooden floor. The dog has a focused expression and is looking directly at the camera.\n- **Details:** The wooden floor has a rustic appearance with visible wood grain patterns. The dog's eyes are a striking color, possibly brown or amber, which contrasts with its black fur.\n2. **Image 2:**\n- **Description:** A scenic view of a mountainous landscape with a winding road cutting through it. The road is surrounded by lush green vegetation and leads to a distant valley.\n- **Details:** The mountains are rugged with steep slopes, and the sky is clear, indicating good weather. The winding road adds a sense of depth and perspective to the image.\n3. **Image 3:**\n- **Description:** A beach scene with waves crashing against the shore. There are several people in the water and on the beach, enjoying the waves and the sunset.\n- **Details:** The waves are powerful, creating a dynamic and lively atmosphere. The sky is painted with hues of orange and pink from the setting sun, adding a warm glow to the scene.\n4. **Image 4:**\n- **Description:** A garden path leading to a large tree with a bench underneath it. The path is bordered by well-maintained grass and flowers.\n- **Details:** The path is made of small stones or gravel, and the tree provides a shaded area with the bench invitingly placed beneath it. The surrounding area is lush and green, suggesting a well-kept garden.\nEach image captures a different scene, from a close-up of a dog to expansive natural landscapes, showcasing various elements of nature and human interaction with it.\n\"\"\"\nUsage with chat template\nYou can also use a chat template to format your chat history for Pixtral. Make sure that the images argument to the processor contains the images in the order\nthat they appear in the chat, so that the model understands where each image is supposed to go.\nHere's an example with text and multiple images interleaved in the same message:\nfrom PIL import Image\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\nmodel_id = \"mistral-community/pixtral-12b\"\nmodel = LlavaForConditionalGeneration.from_pretrained(model_id)\nprocessor = AutoProcessor.from_pretrained(model_id)\nurl_dog = \"https://picsum.photos/id/237/200/300\"\nurl_mountain = \"https://picsum.photos/seed/picsum/200/300\"\nchat = [\n{\n\"role\": \"user\", \"content\": [\n{\"type\": \"text\", \"content\": \"Can this animal\"},\n{\"type\": \"image\"},\n{\"type\": \"text\", \"content\": \"live here?\"},\n{\"type\": \"image\"}\n]\n}\n]\nprompt = processor.apply_chat_template(chat)\ninputs = processor(text=prompt, images=[url_dog, url_mountain], return_tensors=\"pt\").to(model.device)\ngenerate_ids = model.generate(**inputs, max_new_tokens=500)\noutput = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\nFrom transformers>=v4.48, you can also pass image url or local path to the conversation history, and let the chat template handle the rest.\nChat template will load the image for you and return inputs in torch.Tensor which you can pass directly to model.generate().\nchat = [\n{\n\"role\": \"user\", \"content\": [\n{\"type\": \"text\", \"content\": \"Can this animal\"},\n{\"type\": \"image\", \"url\": url_dog},\n{\"type\": \"text\", \"content\": \"live here?\"},\n{\"type\": \"image\", \"url\" : url_mountain}\n]\n}\n]\ninputs = processor.apply_chat_template(chat, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors\"pt\").to(model.device)\ngenerate_ids = model.generate(**inputs, max_new_tokens=500)\noutput = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\nYou should get something like this:\nCan this animallive here?Certainly! Here are some details about the images you provided:\n### First Image\n- **Description**: The image shows a black dog lying on a wooden surface. The dog has a curious expression with its head tilted slightly to one side.\n- **Details**: The dog appears to be a young puppy with soft, shiny fur. Its eyes are wide and alert, and it has a playful demeanor.\n- **Context**: This image could be used to illustrate a pet-friendly environment or to showcase the dog's personality.\n### Second Image\n- **Description**: The image depicts a serene landscape with a snow-covered hill in the foreground. The sky is painted with soft hues of pink, orange, and purple, indicating a sunrise or sunset.\n- **Details**: The hill is covered in a blanket of pristine white snow, and the horizon meets the sky in a gentle curve. The scene is calm and peaceful.\n- **Context**: This image could be used to represent tranquility, natural beauty, or a winter wonderland.\n### Combined Context\nIf you're asking whether the dog can \"live here,\" referring to the snowy landscape, it would depend on the breed and its tolerance to cold weather. Some breeds, like Huskies or Saint Bernards, are well-adapted to cold environments, while others might struggle. The dog in the first image appears to be a breed that might prefer warmer climates.\nWould you like more information on any specific aspect?\nWhile it may appear that spacing in the input is disrupted, this is caused by us skipping special tokens for display, and actually \"Can this animal\" and \"live here\" are\ncorrectly separated by image tokens. Try decoding with special tokens included to see exactly what the model sees!",
    "wi-lab/lwm": "üì° LWM: Large Wireless Model\nJoin the growing community of researchers using LWM for their wireless communication and sensing research, and unlock a new level of performance and insight in your models!\nüõ† How to Use\n1. Install Conda\n2. Create a New Environment\n3. Install Required Packages\n4. Clone the Dataset Scenarios\n5. Clone the Model Repository\n6. Clone the Desired Dataset Scenarios\n7. Change the Working Directory to LWM\n8. Tokenize and Load the Model\n9. Perform Inference\n10. Generate Labels if Necessary\n11. Leverage the Dataset for Downstream Tasks\n12. Explore the Interactive Demo\nüì° LWM: Large Wireless Model\nüöÄ Click here to try the Interactive Demo!\nüöÄ Click here to try the Colab Notebook!\nLWM is a powerful pre-trained model developed as a universal feature extractor for wireless channels. As the world's first foundation model crafted for this domain, LWM leverages transformer architectures to extract refined representations from simulated datasets, such as DeepMIMO and Sionna, and real-world wireless data.\nHow is LWM built?\nThe LWM model‚Äôs structure is based on transformers, allowing it to capture both fine-grained and global dependencies within channel data. Unlike traditional models that are limited to specific tasks, LWM employs a self-supervised approach through our proposed technique, Masked Channel Modeling (MCM). This method trains the model on unlabeled data by predicting masked channel segments, enabling it to learn intricate relationships between antennas and subcarriers. Utilizing bidirectional attention, LWM interprets the full context by attending to both preceding and succeeding channel segments, resulting in embeddings that encode comprehensive spatial information, making them applicable to a variety of scenarios.\nWhat does LWM offer?\nLWM provides a universal feature extraction framework that can be applied across diverse wireless communication and sensing tasks. It is built to handle complex wireless environments, capturing channel characteristics in a way that facilitates robust performance across different scenarios and conditions.\nTrained on hundreds of thousands of wireless channel samples, LWM has been designed to generalize across varied environments‚Äîfrom dense urban areas to synthetic setups, ensuring its adaptability and consistency across a broad spectrum of wireless tasks.\nHow is LWM used?\nLWM is designed to be easily integrated into downstream applications as a source of high-quality embeddings that encapsulate complex channel features. By feeding raw wireless channel data into the pre-trained model, users obtain embeddings that capture essential spatial relationships and interactions within the channel environment.\nThese embeddings provide a versatile and contextualized representation of wireless data, which can be leveraged across different applications. By utilizing the pre-trained model in this way, users can reduce the need for extensive labeled data while benefiting from embeddings that retain the critical properties of the original channel.\nAdvantages of Using LWM\nVarious Tasks: Self-supervised and pre-trained without labels, LWM excels in a wide range of wireless tasks, offering flexibility and performance\nLimited Data: With LWM embeddings, downstream tasks achieve high accuracy with less data, cutting reliance on large datasets\nVarious Environments: Pre-trained on diverse data, LWM excels in various environments from urban to rural areas, ensuring reliable performance\nJoin the growing community of researchers using LWM for their wireless communication and sensing research, and unlock a new level of performance and insight in your models!\nPlease cite the following paper if you use the LWM model or any modified parts:\n@misc{alikhani2024largewirelessmodellwm,\ntitle={Large Wireless Model (LWM): A Foundation Model for Wireless Channels},\nauthor={Sadjad Alikhani and Gouranga Charan and Ahmed Alkhateeb},\nyear={2024},\neprint={2411.08872},\narchivePrefix={arXiv},\nprimaryClass={cs.IT},\nurl={https://arxiv.org/abs/2411.08872},\n}\nüõ† How to Use\n1. Install Conda\nFirst, ensure that you have a package manager like Conda installed to manage your Python environments and packages. You can install Conda via Anaconda or Miniconda.\nAnaconda includes a comprehensive scientific package suite. Download it here.\nMiniconda is a lightweight version that includes only Conda and Python. Download it here.\nOnce installed, you can use Conda to manage environments.\n2. Create a New Environment\nAfter installing Conda, follow these steps to create a new environment and install the required packages.\nStep 1: Create a new environment\nTo begin, open the Anaconda PowerShell Prompt and create a new Conda environment named lwm_env:\nconda create -n lwm_env\nStep 2: Activate the environment\nActivate the environment:\nconda activate lwm_env\n3. Install Required Packages\nOnce the environment is activated, install the necessary packages.\nInstall CUDA-enabled PyTorch\nAlthough inference can run efficiently on a CPU, you may need a GPU for training more resource-intensive downstream tasks. Visit this page and select the appropriate options based on your system's specifications. The website will generate a tailored installation command.\nFor instance, on an NVIDIA system, you can use a command like the following with the appropriate CUDA version for your system:\nconda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\nThis command installs PyTorch with CUDA support for GPU-accelerated training. Ensure that the specified CUDA version is compatible with your system, adjusting it if necessary.\nNote: If you encounter issues installing CUDA-enabled PyTorch, verify your CUDA version compatibility. It might also be due to conflicting installation attempts‚Äîtry a fresh environment.\nInstall Other Required Packages via Conda Forge\nconda install python numpy pandas matplotlib tqdm -c conda-forge\nInstall DeepMIMOv3 with pip\npip install DeepMIMOv3\n4. Clone the Dataset Scenarios\nThe following functions will help you clone specific dataset scenarios from a repository:\nimport subprocess\nimport os\nimport shutil\ndef clone_dataset_scenario(repo_url, model_repo_dir=\"./LWM\", scenarios_dir=\"scenarios\"):\n\"\"\"\nClones all scenarios from a repository, ensuring all files (small and large) are downloaded.\nArgs:\nrepo_url (str): URL of the Git repository\nmodel_repo_dir (str): Path to the model repository\nscenarios_dir (str): Directory name for storing scenarios\n\"\"\"\n# Ensure we're in the correct directory structure\ncurrent_dir = os.path.basename(os.getcwd())\nif current_dir == \"LWM\":\nmodel_repo_dir = \".\"\n# Create the scenarios directory if it doesn't exist\nscenarios_path = os.path.join(model_repo_dir, scenarios_dir)\nos.makedirs(scenarios_path, exist_ok=True)\n# Store the original working directory\noriginal_dir = os.getcwd()\ntry:\n# Clean up any existing temp directory\nif os.path.exists(scenarios_path):\nshutil.rmtree(scenarios_path)\n# Clone the entire repository (including all files)\nprint(f\"Cloning entire repository into temporary directory...\")\nsubprocess.run([\n\"git\", \"clone\",\nrepo_url,\nscenarios_path\n], check=True)\n# Navigate to the temporary clone directory\nos.chdir(scenarios_path)\n# Pull all files using Git LFS\nprint(f\"Pulling all files using Git LFS...\")\nsubprocess.run([\"git\", \"lfs\", \"install\"], check=True)  # Ensure LFS is installed\nsubprocess.run([\"git\", \"lfs\", \"pull\"], check=True)  # Pull all LFS files\nprint(f\"Successfully cloned all scenarios into {scenarios_path}\")\nexcept subprocess.CalledProcessError as e:\nprint(f\"Error cloning scenarios: {str(e)}\")\nfinally:\n# Clean up temporary directory\nif os.path.exists(scenarios_path):\nshutil.rmtree(scenarios_path)\n# Return to original directory\nos.chdir(original_dir)\n5. Clone the Model Repository\nNow, clone the LWM model repository to your local system.\n# Step 1: Clone the model repository (if not already cloned)\nmodel_repo_url = \"https://huggingface.co/wi-lab/lwm\"\nmodel_repo_dir = \"./LWM\"\nif not os.path.exists(model_repo_dir):\nprint(f\"Cloning model repository from {model_repo_url}...\")\nsubprocess.run([\"git\", \"clone\", model_repo_url, model_repo_dir], check=True)\n6. Clone the Desired Dataset Scenarios\nYou can now clone specific scenarios from the DeepMIMO dataset, as detailed in the table below:\nüìä Dataset Overview\nüìä Dataset\nüèôÔ∏è City\nüë• Number of Users\nüîó DeepMIMO Page\nDataset 0\nüåÜ Denver\n1354\nDeepMIMO City Scenario 18\nDataset 1\nüèôÔ∏è Indianapolis\n3248\nDeepMIMO City Scenario 15\nDataset 2\nüåá Oklahoma\n3455\nDeepMIMO City Scenario 19\nDataset 3\nüåÜ Fort Worth\n1902\nDeepMIMO City Scenario 12\nDataset 4\nüåâ Santa Clara\n2689\nDeepMIMO City Scenario 11\nDataset 5\nüåÖ San Diego\n2192\nDeepMIMO City Scenario 7\nIt is important to note that these six datasets were not used during the pre-training of the LWM model, and the high-quality embeddings produced are a testament to LWM‚Äôs robust generalization capabilities rather than overfitting.\nThe operational settings below were used in generating the datasets for both the pre-training of LWM and the downstream tasks. If you intend to use custom datasets, please ensure they adhere to these configurations:\nOperational Settings:\nAntennas at BS: 32\nAntennas at UEs: 1\nSubcarriers: 32\nPaths: 20\nFrequency: 3.5GHz (By the way, our results are consistent across different frequency ranges.)\nClone the Scenarios:\nimport numpy as np\ndataset_repo_url = \"https://huggingface.co/datasets/wi-lab/lwm\"  # Base URL for dataset repo\n# Clone the requested scenarios\nclone_dataset_scenario(dataset_repo_url, model_repo_dir)\n7. Change the Working Directory to LWM\nif os.path.exists(model_repo_dir):\nos.chdir(model_repo_dir)\nprint(f\"Changed working directory to {os.getcwd()}\")\nelse:\nprint(f\"Directory {model_repo_dir} does not exist. Please check if the repository is cloned properly.\")\n8. Tokenize and Load the Model\nBefore we dive into tokenizing the dataset and loading the model, let's understand how the tokenization process is adapted to the wireless communication context. In this case, tokenization refers to segmenting each wireless channel into patches, similar to how Vision Transformers (ViTs) work with images. Each wireless channel is structured as a 32x32 matrix, where rows represent antennas and columns represent subcarriers.\nThe tokenization process involves dividing the channel matrix into patches, with each patch containing information from 16 consecutive subcarriers. These patches are then embedded into a 64-dimensional space, providing the Transformer with a richer context for each patch. In this process, positional encodings are added to preserve the structural relationships within the channel, ensuring the Transformer captures both spatial and frequency dependencies.\nIf you choose to apply Masked Channel Modeling (MCM) during inference (by setting gen_raw=False), LWM will mask certain patches, as it did during pre-training. However, for standard inference, masking isn't necessary unless you want to test LWM's robustness to noisy inputs! The printed LWM loss after inference could show you how well it has predicted the masked patches.\nNow, let's move on to tokenize the dataset and load the pre-trained LWM model.\nfrom input_preprocess import tokenizer\nfrom lwm_model import lwm\nimport torch\nscenario_names = np.array([\n\"city_18_denver\", \"city_15_indianapolis\", \"city_19_oklahoma\",\n\"city_12_fortworth\", \"city_11_santaclara\", \"city_7_sandiego\"\n])\nscenario_idxs = np.array([0, 1, 2, 3, 4, 5])  # Select the scenario indexes\nselected_scenario_names = scenario_names[scenario_idxs]\npreprocessed_chs = tokenizer(\nselected_scenario_names=selected_scenario_names,  # Selects predefined DeepMIMOv3 scenarios. Set to None to load your own dataset.\nmanual_data=None,  # If using a custom dataset, ensure it is a wireless channel dataset of size (N,32,32) based on the settings provided above.\ngen_raw=True  # Set gen_raw=False to apply masked channel modeling (MCM), as used in LWM pre-training. For inference, masking is unnecessary unless you want to evaluate LWM's ability to handle noisy inputs.\n)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Loading the LWM model on {device}...\")\nmodel = lwm.from_pretrained(device=device)\nWith this setup, you're ready to pass your tokenized wireless channels through the pre-trained model, extracting rich, context-aware embeddings that are ready for use in downstream tasks.\n9. Perform Inference\nBefore running the inference, it's important to understand the benefits of the different embedding types. The CLS embeddings (cls_emb) provide a highly compressed, holistic view of the entire wireless channel, making them ideal for tasks requiring a general understanding, such as classification or high-level decision-making. On the other hand, channel embeddings (channel_emb) capture detailed spatial and frequency information from the wireless channel, making them more suitable for complex tasks like beamforming or channel prediction.\nYou can now perform inference on the preprocessed data using the LWM model.\nfrom inference import lwm_inference, create_raw_dataset\ninput_types = ['cls_emb', 'channel_emb', 'raw']\nselected_input_type = input_types[1]  # Change the index to select LWM CLS embeddings, LWM channel embeddings, or the original input channels.\nif selected_input_type in ['cls_emb', 'channel_emb']:\ndataset = lwm_inference(preprocessed_chs, selected_input_type, model, device)\nelse:\ndataset = create_raw_dataset(preprocessed_chs, device)\nBy selecting either cls_emb or channel_emb, you leverage the pre-trained model's rich feature extraction capabilities to transform raw channels into highly informative embeddings. If you prefer to work with the original raw data, you can choose the raw input type.\n10. Generate Labels if Necessary\nIf your dataset requires labels, you can easily generate them using DeepMIMO data. Here's an example to create labels for either LoS/NLoS classification or beam prediction, depending on the scenario selected:\nfrom input_preprocess import create_labels\ntasks = ['LoS/NLoS Classification', 'Beam Prediction']\ntask = tasks[1] # Choose 0 for LoS/NLoS labels or 1 for beam prediction labels.\nlabels = create_labels(task, selected_scenario_names, n_beams=64) # For beam prediction, n_beams specifies the number of beams in the codebook. If you're generating labels for LoS/NLoS classification, you can leave this value unchanged as it doesn't impact the label generation.\n11. Leverage the Dataset for Downstream Tasks\nLWM, pre-trained on a vast and diverse dataset using self-supervised learning, does not rely on labeled data. During inference, it transforms raw channels into rich embeddings in real time, capturing both general and intricate patterns within the wireless channels. These embeddings can be directly applied to various downstream tasks, offering a more powerful alternative to using the original channel data.\n12. Explore the Interactive Demo\nTo experience LWM interactively, visit our demo hosted on Hugging Face Spaces:\nTry the Interactive Demo!\nYou are now ready to explore the power of LWM in wireless communications! Start processing datasets and generate high-quality embeddings to advance your research or applications.\nIf you have questions or need assistance, feel free to:\nVisit the Hugging Face Discussions for community support.\nCheck out the LWM website FAQ.\nContact us directly via email at lwmwireless@gmail.com.",
    "MaLA-LM/emma-500-llama2-7b": "EMMA-500: Enhancing Massively Multilingual Adaptation of Large Language Models\nModel Description\nModel Details\nData Access\nUsage\nModel Performance\nCitation\nAcknowledgements\nEMMA-500: Enhancing Massively Multilingual Adaptation of Large Language Models\nModel Description\nEMMA-500 is a state-of-the-art multilingual language model designed to improve language representation, especially in low-resource languages, through continual pre-training on the Llama 2 7B architecture. Leveraging the MaLA Corpus, which spans over 500 languages and 74 billion tokens, EMMA-500 excels in multilingual tasks like commonsense reasoning, machine translation, open-ended generation, and text classification.\nEMMA-500 outperforms other Llama 2-based models in diverse multilingual settings while maintaining robustness in specialized tasks.\nModel Details\nArchitecture: Built on Llama 2 7B with enhanced language adaptation through continual pre-training.\nLanguages: Supports 546 languages with substantial training data (over 100k tokens each).\nData Mix: A diverse mix of text from domains like code, books, instruction data, and more.\nKey Tasks: Commonsense reasoning, machine translation, text classification, natural language inference, code generation, and open-ended generation.\nData Access\nMaLA Corpus\nPolyWrite Benchmark\nUsage\nYou can use EMMA-500 for multilingual text generation. Below is an example to generate text using the model:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"MaLA-LM/emma-500-llama2-7b\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ninput_text = \"Once upon a time\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\noutputs = model.generate(**inputs)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nModel Performance\nEMMA-500 was evaluated across multiple benchmarks and tasks, demonstrating:\nLowest negative log-likelihood in intrinsic evaluations.\nSignificant improvements in commonsense reasoning, machine translation, and open-ended generation.\nOutperformed all Llama 2-based models in text classification and natural language inference.\nEnhanced performance in code generation and machine reading comprehension (MRC).\nChallenges remain in low-resource languages, where the model tends to have higher Self-BLEU scores, indicating reduced output diversity.\nCitation\n@article{ji2024emma500enhancingmassivelymultilingual,\ntitle={{EMMA}-500: Enhancing Massively Multilingual Adaptation of Large Language Models},\nauthor={Shaoxiong Ji and Zihao Li and Indraneil Paul and Jaakko Paavola and Peiqin Lin and Pinzhen Chen and Dayy√°n O'Brien and Hengyu Luo and Hinrich Sch√ºtze and J√∂rg Tiedemann and Barry Haddow},\nyear={2024},\njournal={arXiv preprint 2409.17892},\nurl={https://arxiv.org/abs/2409.17892},\n}\nAcknowledgements\nWe extend our thanks to the language communities and contributors who helped source, clean, and validate the diverse data used in the MaLA Corpus. Their efforts are invaluable in supporting linguistic diversity in AI research.\nThis work is done by researchers at Helsinki-NLP in collaboration with partners from TU Darmstadt, the University of Edinburgh, and LMU Munich. It is funded by HPLT and UTTER.",
    "TroyDoesAI/DigitalSoul-BlackSheep": "README.md exists but content is empty.",
    "Qwen/Qwen2.5-72B": "Qwen2.5-72B\nIntroduction\nRequirements\nEvaluation & Performance\nCitation\nQwen2.5-72B\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the base 72B Qwen2.5 model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 72.7B\nNumber of Paramaters (Non-Embedding): 70.0B\nNumber of Layers: 80\nNumber of Attention Heads (GQA): 64 for Q and 8 for KV\nContext Length: 131,072 tokens\nWe do not recommend using base language models for conversations. Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., on this model.\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "Qwen/Qwen2.5-72B-Instruct": "Qwen2.5-72B-Instruct\nIntroduction\nRequirements\nQuickstart\nProcessing Long Texts\nEvaluation & Performance\nCitation\nQwen2.5-72B-Instruct\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the instruction-tuned 72B Qwen2.5 model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 72.7B\nNumber of Paramaters (Non-Embedding): 70.0B\nNumber of Layers: 80\nNumber of Attention Heads (GQA): 64 for Q and 8 for KV\nContext Length: Full 131,072 tokens and generation 8192 tokens\nPlease refer to this section for detailed instructions on how to deploy Qwen2.5 for handling long texts.\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-72B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nProcessing Long Texts\nThe current config.json is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nFor deployment, we recommend using vLLM.\nPlease refer to our Documentation for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "Qwen/Qwen2.5-Coder-7B": "Qwen2.5-Coder-7B\nIntroduction\nRequirements\nProcessing Long Texts\nEvaluation & Performance\nCitation\nQwen2.5-Coder-7B\nIntroduction\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\nSignificantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\nA more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\nLong-context Support up to 128K tokens.\nThis repo contains the 7B Qwen2.5-Coder model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 7.61B\nNumber of Paramaters (Non-Embedding): 6.53B\nNumber of Layers: 28\nNumber of Attention Heads (GQA): 28 for Q and 4 for KV\nContext Length: Full 131,072 tokens\nPlease refer to this section for detailed instructions on how to deploy Qwen2.5 for handling long texts.\nWe do not recommend using base language models for conversations. Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., or fill in the middle tasks on this model.\nFor more details, please refer to our blog, GitHub, Documentation, Arxiv.\nRequirements\nThe code of Qwen2.5-Coder has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nProcessing Long Texts\nThe current config.json is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nFor deployment, we recommend using vLLM.\nPlease refer to our Documentation for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{hui2024qwen2,\ntitle={Qwen2. 5-Coder Technical Report},\nauthor={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\njournal={arXiv preprint arXiv:2409.12186},\nyear={2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "Qwen/Qwen2.5-Math-7B": "Qwen2.5-Math-7B\nIntroduction\nModel Details\nRequirements\nQuick Start\nCitation\nQwen2.5-Math-7B\nüö® Qwen2.5-Math mainly supports solving English and Chinese math problems through CoT and TIR. We do not recommend using this series of models for other tasks.\nIntroduction\nIn August 2024, we released the first series of mathematical LLMs - Qwen2-Math - of our Qwen family. A month later, we have upgraded it and open-sourced Qwen2.5-Math series, including base models Qwen2.5-Math-1.5B/7B/72B, instruction-tuned models Qwen2.5-Math-1.5B/7B/72B-Instruct, and mathematical reward model Qwen2.5-Math-RM-72B.\nUnlike Qwen2-Math series which only supports using Chain-of-Thught (CoT) to solve English math problems, Qwen2.5-Math series is expanded to support using both CoT and Tool-integrated Reasoning (TIR) to solve math problems in both Chinese and English. The Qwen2.5-Math series models have achieved significant performance improvements compared to the Qwen2-Math series models on the Chinese and English mathematics benchmarks with CoT.\nWhile CoT plays a vital role in enhancing the reasoning capabilities of LLMs, it faces challenges in achieving computational accuracy and handling complex mathematical or algorithmic reasoning tasks, such as finding the roots of a quadratic equation or computing the eigenvalues of a matrix. TIR can further improve the model's proficiency in precise computation, symbolic manipulation, and algorithmic manipulation. Qwen2.5-Math-1.5B/7B/72B-Instruct achieve 79.7, 85.3, and 87.8 respectively on the MATH benchmark using TIR.\nModel Details\nFor more details, please refer to our blog post and GitHub repo.\nRequirements\ntransformers>=4.37.0 for Qwen2.5-Math models. The latest version is recommended.\nüö® This is a must because transformers integrated Qwen2 codes since 4.37.0.\nFor requirements on GPU memory and the respective throughput, see similar results of Qwen2 here.\nQuick Start\nQwen2.5-Math-7B-Instruct is an instruction model for chatting;\nQwen2.5-Math-7B is a base model typically used for completion and few-shot inference, serving as a better starting point for fine-tuning.\nCitation\nIf you find our work helpful, feel free to give us a citation.\n@article{yang2024qwen25mathtechnicalreportmathematical,\ntitle={Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement},\nauthor={An Yang and Beichen Zhang and Binyuan Hui and Bofei Gao and Bowen Yu and Chengpeng Li and Dayiheng Liu and Jianhong Tu and Jingren Zhou and Junyang Lin and Keming Lu and Mingfeng Xue and Runji Lin and Tianyu Liu and Xingzhang Ren and Zhenru Zhang},\njournal={arXiv preprint arXiv:2409.12122},\nyear={2024}\n}",
    "Qwen/Qwen2.5-32B-Instruct": "Qwen2.5-32B-Instruct\nIntroduction\nRequirements\nQuickstart\nProcessing Long Texts\nEvaluation & Performance\nCitation\nQwen2.5-32B-Instruct\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the instruction-tuned 32B Qwen2.5 model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 32.5B\nNumber of Paramaters (Non-Embedding): 31.0B\nNumber of Layers: 64\nNumber of Attention Heads (GQA): 40 for Q and 8 for KV\nContext Length: Full 131,072 tokens and generation 8192 tokens\nPlease refer to this section for detailed instructions on how to deploy Qwen2.5 for handling long texts.\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-32B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nProcessing Long Texts\nThe current config.json is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nFor deployment, we recommend using vLLM.\nPlease refer to our Documentation for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "Qwen/Qwen2.5-Math-RM-72B": "Qwen2.5-Math-RM-72B\nIntroduction\nModel Details\nRequirements\nQuick Start\nü§ó Hugging Face Transformers\nCitation\nQwen2.5-Math-RM-72B\nIntroduction\nQwen2.5-Math-RM-72B is specifically designed to guide the Qwen2.5-Math model throughout the training process by offering more granular feedback on the quality of reasoning and intermediate steps, ultimately facilitating more robust model improvements.\nKey Highlights:\nMultilingual and Multi-Modal Support: Offers preference signals across two languages (Chinese and English) and in dual modes (Chain-of-Thought and Tool-integrated Reasoning), enhancing versatility.\nModel Training Guide:\nTraining Data Enhancement: Employs a data selection process via reward model scoring combined with Rejection Sampling to incrementally enhance the quality of responses\nReinforcement Learning Training: Integrates seamlessly into the reinforcement learning training and provide effective reward signal, further improving model performance.\nInference Boosting:\nBest of N: By leveraging a combination of response sampling and Best-of-N strategies, we choose the response of top score judged by reward model, yielding better results with spending more inference time. For example, Qwen2.5-Math-1.5B-Instruct obtains 83.9 on MATH in RM@8 setting and even surpasses the performance of Qwen2.5-Math-7B-Instruct 83.6 with greedy decoding.\nComparasion with majority voting (Maj@N): RM@N scores are substantially better than Maj@N scores aross almost all benchmarks and models.\nModel Details\nFor more details, please refer to our blog post, Technical Report and GitHub repo.\nRequirements\ntransformers>=4.40.0 for Qwen2.5-Math models. The latest version is recommended.\nüö® This is a must because `transformers` integrated Qwen2.5 codes since `4.37.0`.\nFor requirements on GPU memory and the respective throughput, see similar results of Qwen2 here.\nQuick Start\nQwen2.5-Math-RM-72B is a reward model typically used for offering feedback on the quality of reasoning and intermediate steps, serving in Rejection Sampling, reinforcement learning training and RM@N.\nü§ó Hugging Face Transformers\nHere we show a code snippet to show you how to use the Qwen2.5-Math-RM-72B with transformers:\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-Math-RM-72B\"\ndevice = \"auto\" # the device to load the model onto\nmodel = AutoModel.from_pretrained(\nmodel_name,\ndevice_map=device,\ntorch_dtype=torch.bfloat16,\ntrust_remote_code=True,\n).eval()\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nchat = [\n{\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n{\"role\": \"user\", \"content\": \"Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"},\n{\"role\": \"assistant\", \"content\": \"To determine how much Janet makes from selling the duck eggs at the farmers' market, we need to follow these steps:\\n\\n1. Calculate the total number of eggs laid by the ducks each day.\\n2. Determine how many eggs Janet eats and bakes for herself each day.\\n3. Find out how many eggs are left to be sold.\\n4. Calculate the revenue from selling the remaining eggs at $2 per egg.\\n\\nLet's start with the first step:\\n\\n1. Janet's ducks lay 16 eggs per day.\\n\\nNext, we calculate how many eggs Janet eats and bakes for herself each day:\\n\\n2. Janet eats 3 eggs for breakfast every morning.\\n3. Janet bakes 4 eggs for her friends every day.\\n\\nSo, the total number of eggs Janet eats and bakes for herself each day is:\\n\\\\[ 3 + 4 = 7 \\\\text{ eggs} \\\\]\\n\\nNow, we find out how many eggs are left to be sold:\\n\\\\[ 16 - 7 = 9 \\\\text{ eggs} \\\\]\\n\\nFinally, we calculate the revenue from selling the remaining eggs at $2 per egg:\\n\\\\[ 9 \\\\times 2 = 18 \\\\text{ dollars} \\\\]\\n\\nTherefore, Janet makes boxed18\\\\boxed{18}boxed18 dollars every day at the farmers' market.\"}\n] # 3.75\nconversation_str = tokenizer.apply_chat_template(\nchat,\ntokenize=False,\nadd_generation_prompt=False\n)\ninput_ids = tokenizer.encode(\nconversation_str,\nreturn_tensors=\"pt\",\nadd_special_tokens=False\n).to(model.device)\noutputs = model(input_ids=input_ids)\nprint(outputs[0])\nCitation\nIf you find our work helpful, feel free to give us a citation.\n@article{yang2024qwen25mathtechnicalreportmathematical,\ntitle={Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement},\nauthor={An Yang and Beichen Zhang and Binyuan Hui and Bofei Gao and Bowen Yu and Chengpeng Li and Dayiheng Liu and Jianhong Tu and Jingren Zhou and Junyang Lin and Keming Lu and Mingfeng Xue and Runji Lin and Tianyu Liu and Xingzhang Ren and Zhenru Zhang},\njournal={arXiv preprint arXiv:2409.12122},\nyear={2024}\n}",
    "Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4": "Qwen2.5-14B-Instruct-GPTQ-Int4\nIntroduction\nRequirements\nQuickstart\nProcessing Long Texts\nEvaluation & Performance\nCitation\nQwen2.5-14B-Instruct-GPTQ-Int4\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the GPTQ-quantized 4-bit instruction-tuned 14B Qwen2.5 model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 14.7B\nNumber of Paramaters (Non-Embedding): 13.1B\nNumber of Layers: 48\nNumber of Attention Heads (GQA): 40 for Q and 8 for KV\nContext Length: Full 131,072 tokens and generation 8192 tokens\nPlease refer to this section for detailed instructions on how to deploy Qwen2.5 for handling long texts.\nQuantization: GPTQ 4-bit\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nAlso check out our GPTQ documentation for more usage guide.\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nProcessing Long Texts\nThe current config.json is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nFor deployment, we recommend using vLLM.\nPlease refer to our Documentation for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor quantized models, the benchmark results against the original bfloat16 models can be found here\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "mistralai/Mistral-Small-Instruct-2409": "Model Card for Mistral-Small-Instruct-2409\nUsage Examples\nvLLM (recommended)\nMistral-inference\nChat\nInstruct following\nFunction calling\nUsage in Hugging Face Transformers\nThe Mistral AI Team\nModel Card for Mistral-Small-Instruct-2409\nMistral-Small-Instruct-2409 is an instruct fine-tuned version with the following characteristics:\n22B parameters\nVocabulary to 32768\nSupports function calling\n32k sequence length\nUsage Examples\nvLLM (recommended)\nWe recommend using this model with the vLLM library\nto implement production-ready inference pipelines.\nInstallation\nMake sure you install vLLM >= v0.6.1.post1:\npip install --upgrade vllm\nAlso make sure you have mistral_common >= 1.4.1 installed:\npip install --upgrade mistral_common\nYou can also make use of a ready-to-go docker image.\nOffline\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\nmodel_name = \"mistralai/Mistral-Small-Instruct-2409\"\nsampling_params = SamplingParams(max_tokens=8192)\n# note that running Mistral-Small on a single GPU requires at least 44 GB of GPU RAM\n# If you want to divide the GPU requirement over multiple devices, please add *e.g.* `tensor_parallel=2`\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\", config_format=\"mistral\", load_format=\"mistral\")\nprompt = \"How often does the letter r occur in Mistral?\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": prompt\n},\n]\noutputs = llm.chat(messages, sampling_params=sampling_params)\nprint(outputs[0].outputs[0].text)\nServer\nYou can also use Mistral Small in a server/client setting.\nSpin up a server:\nvllm serve mistralai/Mistral-Small-Instruct-2409 --tokenizer_mode mistral --config_format mistral --load_format mistral\nNote: Running Mistral-Small on a single GPU requires at least 44 GB of GPU RAM.\nIf you want to divide the GPU requirement over multiple devices, please add e.g. --tensor_parallel=2\nAnd ping the client:\ncurl --location 'http://<your-node-url>:8000/v1/chat/completions' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer token' \\\n--data '{\n\"model\": \"mistralai/Mistral-Small-Instruct-2409\",\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": \"How often does the letter r occur in Mistral?\"\n}\n]\n}'\nMistral-inference\nWe recommend using mistral-inference to quickly try out / \"vibe-check\" the model.\nInstall\nMake sure to have mistral_inference >= 1.4.1 installed.\npip install mistral_inference --upgrade\nDownload\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\nmistral_models_path = Path.home().joinpath('mistral_models', '22B-Instruct-Small')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\nsnapshot_download(repo_id=\"mistralai/Mistral-Small-Instruct-2409\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\nChat\nAfter installing mistral_inference, a mistral-chat CLI command should be available in your environment. You can chat with the model using\nmistral-chat $HOME/mistral_models/22B-Instruct-Small --instruct --max_tokens 256\nInstruct following\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\nmodel = Transformer.from_folder(mistral_models_path)\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"How often does the letter r occur in Mistral?\")])\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\nprint(result)\nFunction calling\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\nmodel = Transformer.from_folder(mistral_models_path)\ncompletion_request = ChatCompletionRequest(\ntools=[\nTool(\nfunction=Function(\nname=\"get_current_weather\",\ndescription=\"Get the current weather\",\nparameters={\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city and state, e.g. San Francisco, CA\",\n},\n\"format\": {\n\"type\": \"string\",\n\"enum\": [\"celsius\", \"fahrenheit\"],\n\"description\": \"The temperature unit to use. Infer this from the users location.\",\n},\n},\n\"required\": [\"location\", \"format\"],\n},\n)\n)\n],\nmessages=[\nUserMessage(content=\"What's the weather like today in Paris?\"),\n],\n)\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\nprint(result)\nUsage in Hugging Face Transformers\nYou can also use Hugging Face transformers library to run inference using various chat templates, or fine-tune the model.\nExample for inference:\nfrom transformers import LlamaTokenizerFast, MistralForCausalLM\nimport torch\ndevice = \"cuda\"\ntokenizer = LlamaTokenizerFast.from_pretrained('mistralai/Mistral-Small-Instruct-2409')\ntokenizer.pad_token = tokenizer.eos_token\nmodel = MistralForCausalLM.from_pretrained('mistralai/Mistral-Small-Instruct-2409', torch_dtype=torch.bfloat16)\nmodel = model.to(device)\nprompt = \"How often does the letter r occur in Mistral?\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt},\n]\nmodel_input = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(device)\ngen = model.generate(model_input, max_new_tokens=150)\ndec = tokenizer.batch_decode(gen)\nprint(dec)\nAnd you should obtain\n<s>\n[INST]\nHow often does the letter r occur in Mistral?\n[/INST]\nTo determine how often the letter \"r\" occurs in the word \"Mistral,\"\nwe can simply count the instances of \"r\" in the word.\nThe word \"Mistral\" is broken down as follows:\n- M\n- i\n- s\n- t\n- r\n- a\n- l\nCounting the \"r\"s, we find that there is only one \"r\" in \"Mistral.\"\nTherefore, the letter \"r\" occurs once in the word \"Mistral.\"\n</s>\nThe Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Diogo Costa, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, L√©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Micka√´l Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Th√©ophile Gervet, Timoth√©e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",
    "bzikst/faster-whisper-large-v3-ru-podlodka-int8": "This is ctranslate2 converted version of bond005/whisper-large-v3-ru-podlodka\nCommand used to convert is\nct2-transformers-converter --model bond005/whisper-large-v3-ru-podlodka --output_dir faster-whisper-large-v3-ru-podlodka --copy_files tokenizer.json preprocessor_confi\ng.json --quantization int8_float32",
    "Qwen/Qwen2.5-7B-Instruct-AWQ": "Qwen2.5-7B-Instruct-AWQ\nIntroduction\nRequirements\nQuickstart\nProcessing Long Texts\nEvaluation & Performance\nCitation\nQwen2.5-7B-Instruct-AWQ\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the AWQ-quantized 4-bit instruction-tuned 72B Qwen2.5 model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 7.61B\nNumber of Paramaters (Non-Embedding): 6.53B\nNumber of Layers: 28\nNumber of Attention Heads (GQA): 28 for Q and 4 for KV\nContext Length: Full 131,072 tokens and generation 8192 tokens\nPlease refer to this section for detailed instructions on how to deploy Qwen2.5 for handling long texts.\nQuantization: AWQ 4-bit\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nAlso check out our AWQ documentation for more usage guide.\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct-AWQ\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nProcessing Long Texts\nThe current config.json is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nFor deployment, we recommend using vLLM.\nPlease refer to our Documentation for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor quantized models, the benchmark results against the original bfloat16 models can be found here\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "Qwen/Qwen2.5-14B-Instruct-AWQ": "Qwen2.5-14B-Instruct-AWQ\nIntroduction\nRequirements\nQuickstart\nProcessing Long Texts\nEvaluation & Performance\nCitation\nQwen2.5-14B-Instruct-AWQ\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the AWQ-quantized 4-bit instruction-tuned 72B Qwen2.5 model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 14.7B\nNumber of Paramaters (Non-Embedding): 13.1B\nNumber of Layers: 48\nNumber of Attention Heads (GQA): 40 for Q and 8 for KV\nContext Length: Full 131,072 tokens and generation 8192 tokens\nPlease refer to this section for detailed instructions on how to deploy Qwen2.5 for handling long texts.\nQuantization: AWQ 4-bit\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nAlso check out our AWQ documentation for more usage guide.\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-14B-Instruct-AWQ\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nProcessing Long Texts\nThe current config.json is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nFor deployment, we recommend using vLLM.\nPlease refer to our Documentation for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor quantized models, the benchmark results against the original bfloat16 models can be found here\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "dwililiya/food101-model-classification": "Food-101 Image Classification with EfficientNet\nModel Summary\nDataset\nModel Training\nTraining Process\nModel Performance\nHow to use the model\nRequirements\nModel Card\nFood-101 Image Classification with EfficientNet\nThis project focuses on fine-tuning the EfficientNet-B0 model for image classification using the Food-101 dataset. The model is trained to classify images into 101 different food categories.\nModel Summary\nModel Architecture: EfficientNet-B0\nDataset: Food-101\nNumber of Classes: 101\nInput Image Size: 224x224\nNormalization: Mean - [0.485, 0.456, 0.406], Std - [0.229, 0.224, 0.225]\nOptimizer: Adam\nLoss Function: CrossEntropyLoss\nLearning Rate: 0.001\nEpochs: 10\nDataset\nThe Food-101 dataset consists of 101 food categories, with 1,000 images for each category. The dataset is already split into training (75,750 images) and validation (25,250 images).\nDataset link: Food-101 on Hugging Face\nModel Training\nThe model is based on EfficientNet-B0, a state-of-the-art architecture for image classification. The pretrained model is fine-tuned on the Food-101 dataset using PyTorch.\nTraining Process\nImages are resized to 256x256 and center-cropped to 224x224.\nData augmentation techniques such as random horizontal flip and random rotation were applied to increase the variety of training images.\nThe Adam optimizer was used with a learning rate of 0.001, and cross-entropy loss was used for classification.\nModel Performance\nLoss: 1.0926\nValidation Accuracy: 0.6944\nTest Accuracy: 0.8088\nThese metrics were achieved after 10 epochs of training\nHow to use the model\nYou can use the model directly in your own projects by loading it from hugging face.\nRequirements\nPython 3.7+\nPyTorch 1.9+\ntorchvision 0.10+\nPIL (Pillow)\nHugging Face transformers library\nModel Card\nModelType: EfficientNet-B0\nDataset: Food-101\nFramework: Pytorch\nOptimized for: Image Classification"
}