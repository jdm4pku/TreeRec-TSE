{
    "huihui-ai/Huihui-Ling-flash-2.0-abliterated-GGUF": "huihui-ai/Huihui-Ling-flash-2.0-abliterated-GGUF\nGGUF\nUsage Warnings\nDonation\nhuihui-ai/Huihui-Ling-flash-2.0-abliterated-GGUF\nThis is an uncensored version of inclusionAI/Ling-flash-2.0 created with abliteration (see remove-refusals-with-transformers to know more about it).\nGGUF\nggml-org/llama.cpp and\nim0qianqian/llama.cpp now supports conversion to GGUF format and can be tested using llama-cli.\nhuggingface-cli download huihui-ai/Huihui-Ling-flash-2.0-abliterated-GGUF --local-dir ./huihui-ai/Huihui-Ling-flash-2.0-abliterated-GGUF --token xxx\nllama-gguf-split --merge huihui-ai/Huihui-Ling-flash-2.0-abliterated-GGUF/GGUF/Q4_K_M/Q4_K_M-00001-of-00002.gguf huihui-ai/Huihui-Ling-flash-2.0-abliterated-GGUF/Q4_K_M.gguf\nllama-cli -m huihui-ai/Huihui-Ling-flash-2.0-abliterated-GGUF/Q4_K_M.gguf -n 8192\nUsage Warnings\nRisk of Sensitive or Controversial Outputs: This modelâ€™s safety filtering has been significantly reduced, potentially generating sensitive, controversial, or inappropriate content. Users should exercise caution and rigorously review generated outputs.\nNot Suitable for All Audiences: Due to limited content filtering, the modelâ€™s outputs may be inappropriate for public settings, underage users, or applications requiring high security.\nLegal and Ethical Responsibilities: Users must ensure their usage complies with local laws and ethical standards. Generated content may carry legal or ethical risks, and users are solely responsible for any consequences.\nResearch and Experimental Use: It is recommended to use this model for research, testing, or controlled environments, avoiding direct use in production or public-facing commercial applications.\nMonitoring and Review Recommendations: Users are strongly advised to monitor model outputs in real-time and conduct manual reviews when necessary to prevent the dissemination of inappropriate content.\nNo Default Safety Guarantees: Unlike standard models, this model has not undergone rigorous safety optimization. huihui.ai bears no responsibility for any consequences arising from its use.\nDonation\nYour donation helps us continue our further development and improvement, a cup of coffee can do it.\nbitcoin:\nbc1qqnkhuchxw0zqjh2ku3lu4hq45hc6gy84uk70ge\nSupport our work on Ko-fi!",
    "lerobot/eagle2hg-processor-groot-n1p5": "No model card",
    "AesSedai/GLM-4.6-REAP-266B-A32B": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nNote: currently non-functional because of missing mtp.safetensors file and entry in model.safetensors.index.json\nForked from https://github.com/CerebrasResearch/reap to https://github.com/AesSedai/reap to hack in GLM-4.6 support.\nProduced with:\nbash experiments/pruning-cli.sh 0,1,2,3,4,5,6,7 zai-org/GLM-4.6 reap 42 0.25 theblackcat102/evol-codealpaca-v1 true true true false false",
    "appvoid/arco-3": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nYou agree to not use this model (or future versions) to conduct experiments that cause harm to any person or group.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nprompt\nbenchmarks\nmeta arena\nlanguage modeling\nlimitations\nsupporters\ntrivia\nIn this repository, we propose the next iteration of arco, a new meta-learner small language model. Now with qwen as the base architecture for improvements.\nDuring previous research, we first noticed a dramatic underpeformance on fewshot prompting from previous arco series (regardless of benchmark improvements on arc) so we decided that the main concept to work on was making a more robust fewshot learning by focusing directly on tasks that improve that skill with a stronger baseline model like qwen family.\nAfter several merging iterations with some openly available models, we finally achieved a strong baseline for a meta-learner model which we called arco-3. This model will serve as the starting point for future fewshot finetunings and experiments.\nprompt\nThere is no prompt intentionally set.\nbenchmarks\nmeta arena\nWe tested around 65 models against each other with fewshot tasks and used gemini-2.5-pro to chose the best answers from each one. Currently, it ranks 13th in meta-arena.\nlanguage modeling\nTo our surprise, this model also improved some language modeling tasks over the base model on several well-known benchmarks.\nParameters\nModel\nMMLU\nARC-C\nHellaSwag\nPIQA\nWinogrande\nAverage\n0.6b\nqwen 3\n40.31\n34.47\n47.38\n67.46\n56.04\n49.13\n0.6b\narco 3\n43.34\n36.01\n49.56\n68.17\n58.09\n51.03\nlimitations\nThe model also comes with several limitations that shares with its base model:\nLack of creative outputs\nPoor causality understanding\nExtremely bad summarization skills\nHallucinations\nWe have a plan to tackle each one of these issues and are already planned to be corrected in the future.\nsupporters\ntrivia\narco means \"bow\" in spanish, which is just another way to say that hits its target fast and accurately.\nNote: the model has not been tested as a chat assistant and it might not work as intended, use with caution.",
    "Valeciela/KansenSakura-Symbiosis-12B": "KansenSakura-Symbiosis-12B\nGGUF:\nMerge Details\nMerge Method\nModels Merged\nConfiguration\nKansenSakura-Symbiosis-12B\nYou know how in some video games your starting gear or character can get a huge upgrade if you bring it all the way to the end of the game? This is sort of like that.\nExperimental. Use at your own risk.\nGGUF:\nStandard\nQ6_K_XL\nMerge Details\nMerge Method\nThis model was merged using the Multi-SLERP merge method using Retreatcost/KansenSakura-Zero-RP-12b as a base.\nModels Merged\nThe following models were included in the merge:\nRetreatcost/KansenSakura-Erosion-RP-12b\nRetreatcost/KansenSakura-Eclipse-RP-12b\nRetreatcost/KansenSakura-Radiance-RP-12b\nConfiguration\nThe following YAML configuration was used to produce this model:\nmodels:\n- model: Retreatcost/KansenSakura-Eclipse-RP-12b\nparameters:\nweight: 0.5\n- model: Retreatcost/KansenSakura-Radiance-RP-12b\nparameters:\nweight: 0.5\n- model: Retreatcost/KansenSakura-Erosion-RP-12b\nparameters:\nweight: 0.5\nbase_model: Retreatcost/KansenSakura-Zero-RP-12b\nmerge_method: multislerp\ntokenizer:\nsource: base\nchat_template: chatml\nparameters:\nnormalize: true\ndtype: bfloat16",
    "GMLHUHE/PsyLLM": "æ¨ç†ç¤ºä¾‹ä»£ç \nğŸ“„ Citation\nğŸ§© License\nPsyLLM is a large language model designed for psychological counseling and mental health dialogue generation.It integrates diagnostic reasoning and therapeutic reasoning, following established frameworks such as DSM/ICD, and incorporates diverse therapeutic approaches including CBT, ACT, and psychodynamic therapy.\nThe model is trained on the OpenR1-Psy dataset (arXiv:2505.15715),featuring multi-turn counseling dialogues with explicit reasoning traces that support clinically informed, empathetic, and interpretable AI-assisted therapy.\nThe training process is implemented based on the open-source framework LLaMA-Factory.\næ¨ç†ç¤ºä¾‹ä»£ç \nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_path = \"GMLHUHE/PsyLLM\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_path,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"I have participated in big group sessions before where I was left to find my own safe place, but it hasn't worked for me.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"PsyLLM thinking content:\", thinking_content)\nprint(\"PsyLLM content:\", content)\nğŸ“„ Citation\nIf you use this dataset, please cite:\n@article{hu2025beyond,\ntitle={Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with Large Language Models for Mental Health Counseling},\nauthor={Hu, He and Zhou, Yucheng and Si, Juzheng and Wang, Qianning and Zhang, Hengheng and Ren, Fuji and Ma, Fei and Cui, Laizhong},\njournal={arXiv preprint arXiv:2505.15715},\nyear={2025}\n}\nğŸ§© License\nFor research and educational use only.\nPlease ensure compliance with ethical and legal standards in mental health AI research.",
    "JulietChoo/VisionSelector-Qwen2.5-VL-7B": "Institution\nModel Contact\nVisionSelector-code\n[ğŸ“‚ VisionSelector]\nVisionSelector-model\n[ğŸ¤— VisionSelector-Qwen2.5-VL-3B]\n[ğŸ¤— VisionSelector-Qwen2.5-VL-7B]\n[ğŸ¤— VisionSelector-LLaVA-OV-1.5-8B]\nModel Overview\nWe introduce VisionSelector, a novel, end-to-end learnable framework that fundamentally re-casts visual token compression as an optimization-driven decision process. VisionSelector seamlessly integrates into existing MLLMs without modifying the backbone, achieving adaptive and superior efficiency.\nOur key technical innovations include:\nA Differentiable Top-K Selection Mechanism that ensures end-to-end gradient flow while maintaining full compatibility with high-performance acceleration kernels like FlashAttention.\nA Curriculum Annealing Strategy with a composite loss, which effectively bridges the performance gap between soft training selection and hard inference selection.\nA backbone-decoupled Learnable Importance Scorer (LIS) that enables models, trained at a single compression rate, to robustly generalize to various compression budgets during inference.\nVisionSelector is highly efficient, requiring only 12.85M trainable parameters. It achieves substantial performance-efficiency advancements: a 12.14% performance gain at 10% token retention, and a 1.73Ã— prefill acceleration (with 86.08% memory reduction) at 20% retention. VisionSelector consistently outperforms state-of-the-art baselines across 13 image and video understanding benchmarks.\nInstitution\nUniversity of Science and Technology of China\nZTE-AIM\nModel Contact\nzhujy53@mail.ustc.edu.cn",
    "Vortex5/MS3.2-24B-Solar-Skies": "ğŸ”¥ MS3.2-24B-Solar-Skies\nğŸ§¬ Overview\nâš™ï¸ Merge Method â€” Multi-SLERP\nğŸ­ Intended Use\nğŸŒ’ Acknowledgements\nğŸ”¥ MS3.2-24B-Solar-Skies\nBright minds under boundless skies â€” where every conversation becomes a sunrise of imagination\nğŸ§¬ Overview\nMS3.2-24B-Solar-Skies  merge of pre-trained language models created using MergeKit.It draws upon the intellectual density of The Omega Directive, the expressive prose of Fiery Lynx, and the measured balance of Chaos Skies.\nâš™ï¸ Merge Method â€” Multi-SLERP\nğŸ§© Models:\nğŸ§  ReadyArt/MS3.2-The-Omega-Directive-24B-Unslop-v2.0\nğŸ”¥ Vortex5/MS3.2-24B-Fiery-Lynx\nğŸŒŒ Vortex5/MS3.2-24B-Chaos-Skies\nConfiguration\nmodels:\n- model: ReadyArt/MS3.2-The-Omega-Directive-24B-Unslop-v2.0\nparameters:\nweight:\n- filter: self_attn\nvalue: [0.20, 0.35, 0.55, 0.75, 1.00, 0.95, 0.80, 0.50]\n- filter: norm\nvalue: 0.45\n- value: 0.33\n- model: Vortex5/MS3.2-24B-Fiery-Lynx\nparameters:\nweight:\n- filter: lm_head\nvalue: 0.34\n- filter: mlp\nvalue: [0.20, 0.30, 0.45, 0.60, 0.65, 0.60, 0.45, 0.30]\n- value: 0.25\n- model: Vortex5/MS3.2-24B-Chaos-Skies\nparameters:\nweight:\n- filter: self_attn\nvalue: [0.25, 0.35, 0.45, 0.55, 0.60, 0.65, 0.65, 0.60]\n- filter: mlp\nvalue: 0.2\n- value: 0.33\nmerge_method: multislerp\ndtype: bfloat16\nparameters:\nnormalize: true\ntokenizer:\nsource: Vortex5/MS3.2-24B-Chaos-Skies\nğŸ­ Intended Use\nCategory\nDescription\nğŸ§˜ Reflective Dialogue\nIdeal for introspective or philosophical discussions, exploring abstract and emotional topics.\nğŸ–‹ï¸ Creative Writing\nExcels at expressive prose, narrative storytelling, and immersive worldbuilding.\nğŸ§  Analytical Reasoning\nBalances logic and creativity for insightful, stylistically nuanced explanations.\nğŸ’ Character Roleplay\nAdapts fluidly to emotional, character-driven interactions and narrative depth.\nğŸŒ’ Acknowledgements\nâš™ï¸ mradermacher â€” static / imatrix quantization\nğŸœ› DeathGodlike â€” EXL3 quants\nğŸ’« All original model authors and contributors whose work formed the foundation for this merge.",
    "nightmedia/Qwen3-Coder-REAP-25B-A3B-qx64-hi-mlx": "Qwen3-Coder-REAP-25B-A3B-qx64-hi-mlx\nUse with mlx\nQwen3-Coder-REAP-25B-A3B-qx64-hi-mlx\nThe regular Deckard(qx) formula uses embeddings at the same bit as the data stores, in this case 4 bit.\nThe head and select attention paths are enhanced to 6 bit, and the model is quantized with group size 32(hi).\nThere is an updated model: Qwen3-Coder-REAP-25B-A3B-qx65x-hi-mlx that uses embeddings at 6 bit and a base of 5 bit, and should perform slightly better on long context.\nMetrics coming soon.\n-G\nThis model Qwen3-Coder-REAP-25B-A3B-qx64-hi-mlx was\nconverted to MLX format from cerebras/Qwen3-Coder-REAP-25B-A3B\nusing mlx-lm version 0.28.3.\nUse with mlx\npip install mlx-lm\nfrom mlx_lm import load, generate\nmodel, tokenizer = load(\"Qwen3-Coder-REAP-25B-A3B-qx64-hi-mlx\")\nprompt = \"hello\"\nif tokenizer.chat_template is not None:\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\nmessages, add_generation_prompt=True\n)\nresponse = generate(model, tokenizer, prompt=prompt, verbose=True)",
    "pytorch/gemma-3-12b-it-QAT-INT4": "QAT INT4 google/gemma-3-12b-it model\nInference with vLLM\nServing\nInference with Transformers\nFine-tuning Recipe\nModel Quality\nlanguage eval\nmulti-modal eval\nPeak Memory Usage\nResults\nModel Performance\nResults (H100 machine)\nSetup\nbenchmark_latency\nbaseline\nINT4\nPaper: TorchAO: PyTorch-Native Training-to-Serving Model Optimization\nResources\nDisclaimer\nQAT INT4 google/gemma-3-12b-it model\nDeveloped by: pytorch\nLicense: apache-2.0\nQuantized from Model : google/gemma-3-12b-it\nQuantization Method : QAT INT4\nTerms of Use: Terms\ngemma-3-12b-it fine-tuned with unsloth using quantization-aware training (QAT) from torchao, and quantized with int4 weight only quantization, by PyTorch team.\nUse it directly or serve using vLLM for 66% VRAM reduction (8.34 GB needed) and 1.73x speedup on H100 GPUs.\nInference with vLLM\nInstall vllm nightly and torchao nightly to get some recent changes:\npip install vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\npip install torchao\nServing\nThen we can serve with the following command:\n# Server\nexport MODEL=pytorch/gemma-3-12b-it-QAT-INT4\nVLLM_DISABLE_COMPILE_CACHE=1 vllm serve $MODEL --tokenizer $MODEL -O3\n# Client\ncurl http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n\"model\": \"pytorch/gemma-3-12b-it-QAT-INT4\",\n\"messages\": [\n{\"role\": \"user\", \"content\": \"Give me a short introduction to large language models.\"}\n],\n\"temperature\": 0.6,\n\"top_p\": 0.95,\n\"top_k\": 20,\n\"max_tokens\": 32768\n}'\nNote: please use VLLM_DISABLE_COMPILE_CACHE=1 to disable compile cache when running this code, e.g. VLLM_DISABLE_COMPILE_CACHE=1 python example.py, since there are some issues with the composability of compile in vLLM and torchao,\nthis is expected be resolved in pytorch 2.8.\nInference with Transformers\nInstall the required packages:\npip install git+https://github.com/huggingface/transformers@main\npip install torchao\npip install torch\npip install accelerate\nExample:\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"pytorch/gemma-3-12b-it-QAT-INT4\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n# parsing thinking content\ntry:\n# rindex finding 151668 (</think>)\nindex = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\nindex = 0\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\nFine-tuning Recipe\nInstall the required packages:\npip install torch\npip install git+https://github.com/huggingface/transformers@main\npip install --pre torchao --index-url https://download.pytorch.org/whl/nightly/cu128\npip install unsloth\npip install accelerate\nUse the following code to fine-tune the model\n# Modeled after https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb\nfrom unsloth import FastModel\nfrom unsloth.chat_templates import (\nget_chat_template,\nstandardize_data_formats,\nstandardize_sharegpt,\ntrain_on_responses_only,\n)\nfrom datasets import load_dataset\nfrom trl import SFTConfig, SFTTrainer\nimport torch\nmax_seq_length = 2048\ndtype = torch.bfloat16\n# ==============\n#  Model setup |\n# ==============\nmodel, tokenizer = FastModel.from_pretrained(\nmodel_name = \"unsloth/gemma-3-12b-it\",\nmax_seq_length = max_seq_length,\ndtype = dtype,\nload_in_4bit = False,\nfull_finetuning = False,\n)\nmodel = FastModel.get_peft_model(\nmodel,\nfinetune_vision_layers = False,\nr = 8,\nlora_alpha = 8,\nlora_dropout = 0,\nqat_scheme = \"int4\",\n)\ntokenizer = get_chat_template(tokenizer, chat_template=\"gemma3\")\n# =============\n#  Data setup |\n# =============\ndef format_into_conversation(example):\nchoices = [\"A\", \"B\", \"C\", \"D\"]\ncorrect_choice = choices[example[\"answer\"]]\nquestion = \"Choose the correct answer for the following question: \"\nquestion += f\"{example['question']}\\n\\n\"\nquestion += \"Choices:\\n\"\nquestion += f\"A. {example['choices'][0]}\\n\"\nquestion += f\"B. {example['choices'][1]}\\n\"\nquestion += f\"C. {example['choices'][2]}\\n\"\nquestion += f\"D. {example['choices'][3]}\"\nanswer = f\"The correct answer is {correct_choice}.\"\nreturn {\"conversations\": [\n{\"from\": \"human\", \"value\": question},\n{\"from\": \"gpt\", \"value\": answer},\n]}\ndef formatting_prompts_func(examples):\nconvos = examples[\"conversations\"]\ntexts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<bos>') for convo in convos]\nreturn { \"text\" : texts, }\ndataset = load_dataset(\"cais/mmlu\", \"all\", split=\"auxiliary_train\")\ndataset = dataset.map(format_into_conversation)\ndataset = dataset.remove_columns([\"question\", \"subject\", \"choices\", \"answer\"])\ndataset = standardize_data_formats(dataset)\ndataset = dataset.map(formatting_prompts_func, batched = True,)\n# ========\n#  Train |\n# ========\ntrainer = SFTTrainer(\nmodel = model,\ntokenizer = tokenizer,\ntrain_dataset = dataset,\ndataset_text_field = \"text\",\nmax_seq_length = max_seq_length,\npacking = False,\nargs = SFTConfig(\nper_device_train_batch_size = 32,\ngradient_accumulation_steps = 1,\nwarmup_steps = 5,\nnum_train_epochs = 1,\nmax_steps = 100,\nlearning_rate = 2e-5,\nlogging_steps = 1,\noptim = \"adamw_8bit\",\nweight_decay = 0.01,\nlr_scheduler_type = \"linear\",\nseed = 3407,\noutput_dir = \"outputs\",\nreport_to = \"none\",\n),\n)\ntrainer = train_on_responses_only(\ntrainer,\ninstruction_part = \"<start_of_turn>user\\n\",\nresponse_part = \"<start_of_turn>model\\n\",\n)\ntrainer_stats = trainer.train()\nModel Quality\nWe rely on lm-evaluation-harness to evaluate the quality of the quantized model. Here we only run on mmlu for sanity check.\nBenchmark\nmmlu accuracy\nNormalized accuracy degradation\ngoogle/gemma-3-12b-it\nbf16\n71.51\n-0%\nint4\n69.48\n-100%\nFine-tuned without QAT\nbf16\n71.55\n+2%\nint4\n69.58\n-95%\nFine-tuned with QAT\nint4\n70.18\n-65.5%\nReproduce Model Quality Results\nlanguage eval\nNeed to install lm-eval from source:\nhttps://github.com/EleutherAI/lm-evaluation-harness#install\nexport MODEL=google/gemma-3-12b-it # or pytorch/gemma-3-12b-it-QAT-INT4\nlm_eval --model hf --model_args pretrained=$MODEL --tasks mmlu --device cuda:0 --batch_size 8\nmulti-modal eval\nNeed to install lmms-eval from source:\npip install git+https://github.com/EvolvingLMMs-Lab/lmms-eval.git\nNUM_PROCESSES=8\nMAIN_PORT=12345\nMODEL_ID=google/gemma-3-12b-it # or pytorch/gemma-3-12b-it-QAT-INT4\nTASKS=chartqa  # or tasks from https://github.com/EvolvingLMMs-Lab/lmms-eval/tree/main/lmms_eval/models/simple\nBATCH_SIZE=32\nOUTPUT_PATH=./logs/\naccelerate launch --num_processes \"${NUM_PROCESSES}\" --main_process_port \"${MAIN_PORT}\" -m lmms_eval \\\n--model gemma3 \\\n--model_args \"pretrained=${MODEL_ID}\" \\\n--tasks \"${TASKS}\" \\\n--batch_size \"${BATCH_SIZE}\" --output_path \"${OUTPUT_PATH}\"\nPeak Memory Usage\nResults\nBenchmark\ngoogle/gemma-3-12b-it\npytorch/gemma-3-12b-it-QAT-INT4\nPeak Memory (GB)\n24.50\n8.34 (66% reduction)\nReproduce Peak Memory Usage Results\nWe can use the following code to get a sense of peak memory usage during inference:\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n# use \"google/gemma-3-12b-it\" or \"pytorch/gemma-3-12b-it-QAT-INT4\"\nmodel_id = \"pytorch/gemma-3-12b-it-QAT-INT4\"\nquantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntorch.cuda.reset_peak_memory_stats()\nprompt = \"Hey, are you conscious? Can you talk to me?\"\nmessages = [\n{\n\"role\": \"system\",\n\"content\": \"\",\n},\n{\"role\": \"user\", \"content\": prompt},\n]\ntemplated_prompt = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nprint(\"Prompt:\", prompt)\nprint(\"Templated prompt:\", templated_prompt)\ninputs = tokenizer(\ntemplated_prompt,\nreturn_tensors=\"pt\",\n).to(\"cuda\")\ngenerated_ids = quantized_model.generate(**inputs, max_new_tokens=128)\noutput_text = tokenizer.batch_decode(\ngenerated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(\"Response:\", output_text[0][len(prompt):])\nmem = torch.cuda.max_memory_reserved() / 1e9\nprint(f\"Peak Memory Usage: {mem:.02f} GB\")\nModel Performance\nResults (H100 machine)\nBenchmark (Latency)\ngoogle/gemma-3-12b-it\npytorch/gemma-3-12b-it-QAT-INT4\nlatency (batch_size=1)\n3.73s\n2.16s (1.73x speedup)\nReproduce Model Performance Results\nSetup\nGet vllm source code:\ngit clone git@github.com:vllm-project/vllm.git\nInstall vllm\nVLLM_USE_PRECOMPILED=1 pip install --editable .\nRun the benchmarks under vllm root folder:\nbenchmark_latency\nbaseline\nvllm bench latency --input-len 256 --output-len 256 --model google/gemma-3-12b-it --batch-size 1\nINT4\nVLLM_DISABLE_COMPILE_CACHE=1 vllm bench latency --input-len 256 --output-len 256 --model pytorch/gemma-3-12b-it-QAT-INT4 --batch-size 1\nPaper: TorchAO: PyTorch-Native Training-to-Serving Model Optimization\nThe model's quantization is powered by TorchAO, a framework presented in the paper TorchAO: PyTorch-Native Training-to-Serving Model Optimization.\nAbstract: We present TorchAO, a PyTorch-native model optimization framework leveraging quantization and sparsity to provide an end-to-end, training-to-serving workflow for AI models. TorchAO supports a variety of popular model optimization techniques, including FP8 quantized training, quantization-aware training (QAT), post-training quantization (PTQ), and 2:4 sparsity, and leverages a novel tensor subclass abstraction to represent a variety of widely-used, backend agnostic low precision data types, including INT4, INT8, FP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader ecosystem at each step of the model optimization pipeline, from pre-training (TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, vLLM, SGLang, ExecuTorch), connecting an otherwise fragmented space in a single, unified workflow. TorchAO has enabled recent launches of the quantized Llama 3.2 1B/3B and LlamaGuard3-8B models and is open-source at this https URL .\nResources\nOfficial TorchAO GitHub Repository: https://github.com/pytorch/ao\nTorchAO Documentation: https://docs.pytorch.org/ao/stable/index.html\nDisclaimer\nPyTorch has not performed safety evaluations or red teamed the quantized models. Performance characteristics, outputs, and behaviors may differ from the original models. Users are solely responsible for selecting appropriate use cases, evaluating and mitigating for accuracy, safety, and fairness, ensuring security, and complying with all applicable laws and regulations.\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the licenses the models are released under, including any limitations of liability or disclaimers of warranties provided therein.",
    "ArtusDev/cerebras_GLM-4.5-Air-REAP-82B-A12B-EXL3": "ArtusDev/cerebras_GLM-4.5-Air-REAP-82B-A12B-EXL3\nEXL3 quants of cerebras/GLM-4.5-Air-REAP-82B-A12B using exllamav3 for quantization.\nQuants\nQuant\nBPW\nHead Bits\nSize (GB)\n2.5_H6\n2.5\n6\n27.14\n2.99_H6 (optimized)\n2.99\n6\n31.94\n3.0_H6\n3.0\n6\n32.19\n3.46_H6 (optimized)\n3.46\n6\n36.64\n3.5_H6\n3.5\n6\n37.22\n3.92_H6 (optimized)\n3.92\n6\n41.30\n4.0_H6\n4.0\n6\n42.28\n4.25_H6\n4.25\n6\n44.79\n5.0_H6\n5.0\n6\n52.36\n6.0_H6\n6.0\n6\n62.44\n8.0_H8\n8.0\n8\n82.77\nHow to Download and Use Quants\nYou can download quants by targeting specific size using the Hugging Face CLI.\nClick for download commands\n1. Install huggingface-cli:\npip install -U \"huggingface_hub[cli]\"\n2. Download a specific quant:\nhuggingface-cli download ArtusDev/cerebras_GLM-4.5-Air-REAP-82B-A12B-EXL3 --revision \"5.0bpw_H6\" --local-dir ./\nEXL3 quants can be run with any inference client that supports EXL3, such as TabbyAPI. Refer to documentation for set up instructions.\nQuant Requests\nRequest EXL3 Quants\nSee EXL community hub for request guidelines.\nAcknowledgements\nMade possible with cloud compute from lium.io",
    "Salesforce/FARE-8B": "Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains\nUsage\nPairwise comparisons\nStep-level evaluation\nExample inference with vLLM\nEthics disclaimer for Salesforce AI models, data, code\nCitation\nFoundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains\nPaper: arXiv link\nAuthors: Austin Xu, Xuan-Phi Nguyen, Yilun Zhou, Chien-Sheng Wu, Caiming Xiong, Shafiq Joty\nFARE-8B is a multi-task evaluator model finetuned from Qwen-8B. It is trained on a large-scale multi-task, multi-domain data mixture using rejection-sampling SFT to perform the following evaluation tasks: Pairwise comparisons, step-level evaluation, reference-based verification, reference-free verification, and single-rating assessment.\nUsage\nThe FARE family of evaluators has been trained with specific system and user prompt templates.\nWe provide examples below for two evaluation tasks: Pairwise comparisons and step-level error identification evaluation. For other tasks, we provide prompt templates in our paper (Appendix E).\nPairwise comparisons\nPROMPT_PAIRWISE_SYSTEM = \"\"\"\nPlease act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user prompt displayed below. You will be given assistant A's answer and assistant B's answer. Your job is to determine which assistant's answer is better.\nIf assistant A is better, output [A]. If assistant B is better, output [B].\nHere are some rules for evaluation\n(1) When evaluating the assistants' answers, identify any mistakes or inaccurate information. Focus on the content each response and select the response that is logically sound and error free.\n(2) If both responses contain inaccurate information, select the response that arrives at the correct response\n(3) Avoid any biases, such as order of responses, length, or stylistic elements like formatting\nBefore outputting your final judgment, provide an explanation of your judgment. Your explanation should discuss why your chosen response is better based on the evaluation criteria. The explanation should concretely discuss strengths and weaknesses of both answers.\nAfter outputting your explanation, provide your final judgment. Use the following format:\nExplanation: Your explanation here\nVerdict: Your final verdict\n\"\"\".strip()\nPROMPT_PAIRWISE=\"\"\"\n[User Question]\n{instruction}\n[The Start of Assistant A's Answer]\n{response_a}\n[The End of Assistant A's Answer]\n[The Start of Assistant B's Answer]\n{response_b}\n[The End of Assistant B's Answer]\n\"\"\".strip()\nStep-level evaluation\nPROMPT_PROCESS_SYSTEM_ERROR_ID = \"\"\"\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user prompt displayed below. You will be given the assistant's solution to a math problem, which is split into steps, starting with a <step [step number]> tag, where [step number] is indexed from 0. Your job is to identify which step an error occurs, if an error is present.\nWhen evaluating the solution, consider each step separately. Evaluate the content of each step for correctness. If you encounter a mistake at <step [step number]>, output [step number] as your Verdict. If the full response is error free, then select step number -1. Avoid any biases, such as length of step, or stylistic elements like formatting.\nHere are some rules for evaluation.\n(1) The assistant's answer does not need to be complete or arrive at a final solution. You may receive a partially complete response. Your job is to assess the quality of each step.\n(2) When evaluating the assistant's answer, identify any mistakes or inaccurate information. Focus on the content each step and determine if the step is logically valid.\n(3) For each step, you should provide an explanation of your assessment. If you find an error, describe the nature and cause of the error.\n(4) Avoid any biases, such as answer length, or stylistic elements like formatting.\nBefore providing an your final verdict, think through the judging process and output your thoughts as an explanation\nAfter providing your explanation, you must output the corresponding step number with an error. Use the following format:\nExplanation: Your explanation here\nVerdict: The step number with the error or -1 if no error occurs\n\"\"\".strip()\nPROMPT_SINGLE=\"\"\"\n[User Question]\n{instruction}\n[The Start of Assistant's Answer]\n{response}\n[The End of Assistant's Answer]\n\"\"\".strip()\nExample inference with vLLM\nFor FARE-8B (Qwen-3 variant), our evaluations were conducted with vLLM. We provide a minimal working example below with pairwise evaluation. For example usage with SGLang, see FARE-20B\n# instantiate model\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\nfrom prompts import PROMPT_PAIRWISE_SYSTEM, PROMPT_PAIRWISE # Prompt templates saved in a prompts.py file\nllm = LLM(model=\"Salesforce/FARE-8B\", tensor_parallel_size=8, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/FARE-8B\", trust_remote_code=True)\n# format data\ndata = [\n{\"question\": \"What is 5 + 10?\", \"response_a\": \"The answer is 15!\", \"response_b\": \"The answer is 16!\"}\n]\nformatted = [\nPROMPT_PAIRWISE.format(\ninstruction = d[\"question\"],\nresponse_a = d[\"response_a\"],\nresponse_b = d[\"response_b\"],\n)\nfor d in data\n]\nmessages_lst = [\n[{\"role\": \"system\", \"content\": PROMPT_PAIRWISE_SYSTEM}, {\"role\": \"user\", \"content\": user_formatted}]\nfor user_formatted in formatted\n]\nprompts = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False) for messages in messages_lst]\n# inference!\nsampling_params = SamplingParams(\nmax_tokens=32768,\ntop_p=1.0,\ntop_k=-1,\ntemperature=0.0,\n)\noutputs_all = llm.generate(prompts, sampling_params)\nevaluator_text = [output.outputs[0].text.split(tokenizer.eos_token)[0] for output in outputs_all]\nprint(evaluator_text[0])\n# Explanation: Both Assistant A and Assistant B provided a numerical answer to the question, but Assistant A's answer is correct, while Assistant B's answer is incorrect. The question asks for the sum of 5 and 10, which is 15. Therefore, Assistant A's response is more accurate and reliable.\n#\n# Verdict: [A]\nEthics disclaimer for Salesforce AI models, data, code\nThis release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact peopleâ€™s lives, rights, or safety. For further guidance on use cases, refer to our standard AUP and AI AUP.\nCitation\n@misc{xu2025foundational,\ntitle={Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains},\nauthor={Xu, Austin and Nguyen, Xuan-Phi and Zhou, Yilun and Wu, Chien-Sheng and Xiong, Caiming and Joty, Shafiq},\nyear={2025},\njournal={arXiv preprint arXiv:2510.17793},\n}",
    "TabCanNotTab/SALV-Qwen2.5-Coder-7B-Instruct": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nExample\nExample\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport re\nmodel_name = \"TabCanNotTab/SALV-Qwen2.5-Coder-7B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"\"\"\nPlease act as a professional verilog designer.\nImplement a module of an 8-bit adder with multiple bit-level adders in combinational logic.\nModule name:\nadder_8bit\nInput ports:\na[7:0]: 8-bit input operand A.\nb[7:0]: 8-bit input operand B.\ncin: Carry-in input.\nOutput ports:\nsum[7:0]: 8-bit output representing the sum of A and B.\ncout: Carry-out output.\nImplementation:\nThe module utilizes a series of bit-level adders (full adders) to perform the addition operation.\nGive me the complete code.\n\"\"\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nmodel_inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n# inference\noutputs = model.generate(\n**model_inputs,\nmax_new_tokens=2048,\ndo_sample=True,\ntemperature=0.5,\ntop_p=0.95\n)\n# get response text\ninput_length = model_inputs.input_ids.shape[1]\ngenerated_tokens = outputs[0][input_length:]\nresponse = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n# get code text\npattern = r\"\"\nmatches = re.findall(pattern, response, re.DOTALL)\nif matches:\ncode=matches[-1]\nprint(code)\nelse:\nprint(\"No Verilog code found in the response!\")",
    "Edison2525/Qwen3-8B-AWQ": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.",
    "ValiantLabs/gpt-oss-20b-Esper3.1": "Prompting Guide\nSupport our open-source dataset and model releases!\nEsper 3.1: Qwen3-4B-Thinking-2507, gpt-oss-20b\nEsper 3.1 is a coding, architecture, and DevOps reasoning specialist built on gpt-oss-20b.\nYour dedicated DevOps expert: Esper 3.1 maximizes DevOps and architecture helpfulness, powered by high-difficulty DevOps and architecture data generated with DeepSeek-V3.1-Terminus!\nImproved coding performance: challenging code-reasoning datasets stretch DeepSeek-V3.1-Terminus and DeepSeek-V3.2 to the limits, allowing Esper 3.1 to tackle harder coding tasks!\nAI to build AI: our high-difficulty AI expertise data boosts Esper 3.1's MLOps, AI architecture, AI research, and general reasoning skills.\nSmall model sizes allow running on local desktop and mobile, plus super-fast server inference!\nPrompting Guide\nEsper 3.1 uses the gpt-oss-20b prompt format.\nEsper 3.1 is a reasoning finetune; reasoning level high is generally recommended.\nNOTE: This release of Esper 3.1 uses bf16 for all parameters. Consider quantized models if you're not looking to use bf16.\nExample inference script provided by gpt-oss-20b to get started:\nfrom transformers import pipeline\nimport torch\nmodel_id = \"ValiantLabs/gpt-oss-20b-Esper3.1\"\npipe = pipeline(\n\"text-generation\",\nmodel=model_id,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"Design a serverless architecture for a real-time image processing application using AWS Lambda and Amazon S3.\"},\n]\noutputs = pipe(\nmessages,\nmax_new_tokens=15000,\n)\nprint(outputs[0][\"generated_text\"][-1])\nEsper 3.1 is created by Valiant Labs.\nCheck out our HuggingFace page to see all of our models!\nWe care about open source. For everyone to use.",
    "SicariusSicariiStuff/GLM-4.5-Air-REAP-82B-A12B_FP8": "README.md exists but content is empty.",
    "cyanelis/ElisNovel-V1-14B": "ElisNovel-V1-14B\nå®‰è£…æ–¹å¼ï¼ˆä»¥ollamaä¸ºä¾‹ï¼‰\nä½¿ç”¨æ–¹å¼\næ³¨æ„äº‹é¡¹âš ï¸\nä¿¡æ¯åé¦ˆ\nElisNovel-V1-14B\næœ¬æ¨¡å‹åŸºäºQwen3-14Bï¼Œä¸“é—¨ç”¨äºå°è¯´æ–‡æœ¬ç»­å†™ã€‚\nå®‰è£…æ–¹å¼ï¼ˆä»¥ollamaä¸ºä¾‹ï¼‰\nå¿«é€Ÿå®‰è£…ï¼š\nollama run cyanelis/ElisNovel-V1:14b # ä»ollamaæ¨¡å‹åº“å®‰è£…Q4_K_Mç²¾åº¦ï¼Œé¢„è®¡å ç”¨æ˜¾å­˜12G-\næ‰‹åŠ¨å®‰è£…ï¼Œåœ¨ggufæ¨¡å‹æ–‡ä»¶å’Œmodelfileæ–‡ä»¶æ‰€åœ¨ç›®å½•è¾“å…¥ä»¥ä¸‹å‘½ä»¤ï¼š\nollama create ElisNovel-V1-14B-Q4_K_M -f Modelfile # å®‰è£…Q4_K_Mç²¾åº¦ï¼Œé¢„è®¡å ç”¨æ˜¾å­˜12G-\nollama create ElisNovel-V1-14B-Q8_0 -f Modelfileï¼ˆå°†Modelfilesç¬¬å››è¡ŒQ4_K_Mæ›¿æ¢ä¸ºQ8_0ï¼‰ # å®‰è£…Q8_0ç²¾åº¦ï¼Œé¢„è®¡å ç”¨æ˜¾å­˜18G-\nollama create ElisNovel-V1-14B-F16 -f Modelfileï¼ˆModelfilesç¬¬å››è¡ŒQ4_K_Mæ›¿æ¢ä¸ºF16ï¼‰ # å®‰è£…F16ç²¾åº¦ï¼Œé¢„è®¡å ç”¨æ˜¾å­˜28G\nä½¿ç”¨æ–¹å¼\næœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦è®¾ç½®ä¸º5770tokensã€‚\nè¾“å…¥ä¸è¶…è¿‡1920å­—ï¼ˆ2880tokensï¼‰çš„å†…å®¹ï¼Œæ¨¡å‹é¢„æµ‹1920å­—ä¸‹æ–‡ã€‚\næ‰¹é‡æ¨ç†ï¼Œéœ€è¦æŠŠtxtæ–‡ä»¶æ”¾å…¥G109bæ–‡ä»¶å¤¹ï¼ˆæ²¡æœ‰åˆ™æ–°å»ºä¸€ä¸ªï¼‰ï¼ŒG109bæ–‡ä»¶å¤¹å†…çš„ç¬¬ä¸€ä¸ªæ–‡ä»¶æ¨ç†å¾—åˆ°Z109c\\G2.txtï¼Œç¬¬äºŒä¸ªæ–‡ä»¶æ¨ç†å¾—åˆ°Z109c\\G3.txtï¼Œæœ€åä¸€ä¸ªæ–‡ä»¶æ¨ç†å¾—åˆ°Z109c\\G1.txtï¼š\npython tokenzzzsimple.py # é»˜è®¤Q4_K_Mç²¾åº¦\næ³¨æ„äº‹é¡¹âš ï¸\nè¯·éµå®ˆapache-2.0ã€‚\nç”Ÿæˆå†…å®¹çš„ä¼ æ’­éœ€ç¬¦åˆå½“åœ°æ³•å¾‹æ³•è§„ã€‚\næ¨¡å‹ç”Ÿæˆå†…å®¹çš„æ–‡é£å–å†³äºè¾“å…¥å†…å®¹çš„æ–‡é£ï¼Œä¼šå°½é‡è´´è¿‘ã€‚\næ¨¡å‹åœ¨ä¸çŸ¥é“å¤§çº²çš„å‰æä¸‹è¿›è¡Œç»­å†™ï¼Œç»­å†™æ–¹å‘æ ¹æ®è¾“å…¥çš„å†…å®¹çš„è¶‹åŠ¿è¿›è¡Œé¢„æµ‹ã€‚\nå°è¯´æ‰€éœ€è¦çš„é€»è¾‘å¯¹äºç›®å‰çš„æ¨¡å‹è€Œè¨€è´Ÿæ‹…å¤ªå¤§ï¼Œç”Ÿæˆå†…å®¹ä¸å®œç›´æ¥ä½¿ç”¨ï¼Œå› æ­¤éœ€è¦äººç±»æ™ºæ…§çš„æ ¡æ­£ã€‚\nä¿¡æ¯åé¦ˆ\näº¤æµç¾¤ï¼š755638032",
    "mradermacher/Qwen-7B_THIP-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/Thrillcrazyer/Qwen-7B_THIP\nFor a convenient overview and download list, visit our model page for this model.\nweighted/imatrix quants seem not to be available (by me) at this time. If they do not show up a week or so after the static ones, I have probably not planned for them. Feel free to request them by opening a Community Discussion.\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n3.1\nGGUF\nQ3_K_S\n3.6\nGGUF\nQ3_K_M\n3.9\nlower quality\nGGUF\nQ3_K_L\n4.2\nGGUF\nIQ4_XS\n4.4\nGGUF\nQ4_K_S\n4.6\nfast, recommended\nGGUF\nQ4_K_M\n4.8\nfast, recommended\nGGUF\nQ5_K_S\n5.4\nGGUF\nQ5_K_M\n5.5\nGGUF\nQ6_K\n6.4\nvery good quality\nGGUF\nQ8_0\n8.2\nfast, best quality\nGGUF\nf16\n15.3\n16 bpw, overkill\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "HaochenWang/GAR-8B": "GAR-8B\nUsage\nGAR-8B\nThis repository contains the GAR-8B model, as presented in the paper Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs.\nTL; DR: Our Grasp Any Region (GAR) supports both (1) describing a single region of an image or a video in the form of points/boxes/scribbles/masks in detail and (2) understanding multiple regions such as modeling interactions and performing complex reasoning. We also release a new benchmark, GARBench, to evaluate models on advanced region-level understanding tasks.\nUsage\nFor detailed usage of this model, please refer to our GitHub repo.",
    "manasmisra/GLM-4.5-Air-REAP-82B-A12B-mlx-4Bit": "manasmisra/GLM-4.5-Air-REAP-82B-A12B-mlx-4Bit\nUse with mlx\nmanasmisra/GLM-4.5-Air-REAP-82B-A12B-mlx-4Bit\nThe Model manasmisra/GLM-4.5-Air-REAP-82B-A12B-mlx-4Bit was converted to MLX format from cerebras/GLM-4.5-Air-REAP-82B-A12B using mlx-lm version 0.26.4.\nUse with mlx\npip install mlx-lm\nfrom mlx_lm import load, generate\nmodel, tokenizer = load(\"manasmisra/GLM-4.5-Air-REAP-82B-A12B-mlx-4Bit\")\nprompt=\"hello\"\nif hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nresponse = generate(model, tokenizer, prompt=prompt, verbose=True)",
    "githubear/Qwen-2.5-7B-BLOCK-FT": "README.md exists but content is empty.",
    "mradermacher/Qwen3-Coder-REAP-25B-A3B-GGUF": "About\nUsage\nProvided Quants\nFAQ / Model Request\nThanks\nAbout\nstatic quants of https://huggingface.co/cerebras/Qwen3-Coder-REAP-25B-A3B\nFor a convenient overview and download list, visit our model page for this model.\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Qwen3-Coder-REAP-25B-A3B-i1-GGUF\nUsage\nIf you are unsure how to use GGUF files, refer to one of TheBloke's\nREADMEs for\nmore details, including on how to concatenate multi-part files.\nProvided Quants\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\nLink\nType\nSize/GB\nNotes\nGGUF\nQ2_K\n9.3\nGGUF\nQ3_K_S\n10.9\nGGUF\nQ3_K_M\n12.1\nlower quality\nGGUF\nQ3_K_L\n13.1\nGGUF\nIQ4_XS\n13.6\nGGUF\nQ4_K_S\n14.3\nfast, recommended\nGGUF\nQ4_K_M\n15.2\nfast, recommended\nGGUF\nQ5_K_S\n17.3\nGGUF\nQ5_K_M\n17.8\nGGUF\nQ6_K\n20.5\nvery good quality\nGGUF\nQ8_0\n26.6\nfast, best quality\nHere is a handy graph by ikawrakow comparing some lower-quality quant\ntypes (lower is better):\nAnd here are Artefact2's thoughts on the matter:\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\nFAQ / Model Request\nSee https://huggingface.co/mradermacher/model_requests for some answers to\nquestions you might have and/or if you want some other model quantized.\nThanks\nI thank my company, nethype GmbH, for letting\nme use its servers and providing upgrades to my workstation to enable\nthis work in my free time.",
    "lvyufeng/DeepSeek-OCR-Community-Latest": "MindSpore Usage\nPytorch Usage\nAcknowledgement\nğŸŒŸ Github |\nğŸ“¥ Model Download |\nğŸ“„ Paper Link |\nğŸ“„ Arxiv Paper Link |\nDeepSeek-OCR: Contexts Optical Compression\nExplore the boundaries of visual-text compression.\nThe official version of DeepSeek-OCR has limited the transformers version to 4.46.3 and has not been adapted to the latest version. Therefore, this community edition has modified the modeling.py module to facilitate user convenience without requiring a transformers downgrade. Additionally, this version has been adapted for MindSpore+MindNLP compatibility, and users are welcome to utilize it on Ascend hardware.\nFeel free to opt for various attention implementations such as Flash Attention or SDPA to leverage the latest optimizations in transformers for a performance boost.\nMindSpore Usage\nInference using Huggingface transformers on Ascend NPUs. Requirements tested on MindSpore2.7+ CANN8.2ï¼š\nmindspore==2.7.0\nmindnlp==0.5.0rc4\ntransformers==4.57.1\ntokenizers\neinops\naddict\neasydict\nimport os\nimport mindnlp\nimport mindspore\nfrom transformers import AutoModel, AutoTokenizer\nmodel_name = 'lvyufeng/DeepSeek-OCR-Community-Latest'\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_name, dtype=mindspore.float16, _attn_implementation='sdpa', trust_remote_code=True, use_safetensors=True, device_map='auto')\nmodel = model.eval()\n# prompt = \"<image>\\nFree OCR. \"\nprompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\nimage_file = 'your_image.jpg'\noutput_path = 'your/output/dir'\n# infer(self, tokenizer, prompt='', image_file='', output_path = ' ', base_size = 1024, image_size = 640, crop_mode = True, test_compress = False, save_results = False):\n# Tiny: base_size = 512, image_size = 512, crop_mode = False\n# Small: base_size = 640, image_size = 640, crop_mode = False\n# Base: base_size = 1024, image_size = 1024, crop_mode = False\n# Large: base_size = 1280, image_size = 1280, crop_mode = False\n# Gundam: base_size = 1024, image_size = 640, crop_mode = True\nres = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)\nPytorch Usage\nInference using Huggingface transformers on NVIDIA GPUs. Requirements tested on python 3.12.9 + CUDA11.8ï¼š\ntorch\ntransformers==4.57.1\ntokenizers\neinops\naddict\neasydict\npip install flash-attn\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\nmodel_name = 'lvyufeng/DeepSeek-OCR-Community-Latest'\ntokenizer = AutoTokenizer.from_pretrained(model_name, dtype=torch.bfloat16,trust_remote_code=True, device_map='auto')\nmodel = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)\nmodel = model.eval()\n# prompt = \"<image>\\nFree OCR. \"\nprompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\nimage_file = 'your_image.jpg'\noutput_path = 'your/output/dir'\n# infer(self, tokenizer, prompt='', image_file='', output_path = ' ', base_size = 1024, image_size = 640, crop_mode = True, test_compress = False, save_results = False):\n# Tiny: base_size = 512, image_size = 512, crop_mode = False\n# Small: base_size = 640, image_size = 640, crop_mode = False\n# Base: base_size = 1024, image_size = 1024, crop_mode = False\n# Large: base_size = 1280, image_size = 1280, crop_mode = False\n# Gundam: base_size = 1024, image_size = 640, crop_mode = True\nres = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)\nAcknowledgement\nWe would like to thank Vary, GOT-OCR2.0, MinerU, PaddleOCR, OneChart, Slow Perception for their valuable models and ideas.\nWe also appreciate the benchmarks: Fox, OminiDocBench.",
    "MedInjection-FR/QWEN-4B-NAT-SYN": "ğŸ©º QWEN-4B-NAT-SYN\nğŸ§  Model overview\nâš™ï¸ Training setup\nğŸ“Š Evaluation summary\nğŸ“š Citation\nğŸ©º QWEN-4B-NAT-SYN\nQWEN-4B-NAT-SYN is a fine-tuned version of Qwen-4B-Instruct trained on the MedInjection-FR dataset, a French biomedical instruction corpus combining native, synthetic, and translated medical questionâ€“answer pairs.This model was fine-tuned using Supervised Fine-Tuning (SFT) with DoRA adapters, designed to study how the origin of supervision data influences model adaptation.\nğŸ§  Model overview\nProperty\nDescription\nBase model\nQwen3-4B-Instruct-2507\nFine-tuning method\nDoRA (Weight-Decomposed Low-Rank Adaptation)\nArchitecture size\n~4B parameters\nLanguage\nFrench ğŸ‡«ğŸ‡·\nDomain\nBiomedical, Clinical, Health\nIntended use\nResearch on instruction tuning and domain adaptation\nCaution\nNot for clinical or diagnostic use\nâš™ï¸ Training setup\nFine-tuning was performed on 30k multiple-choice (MCQ and MCQU) examples for each configuration, using:\n10 epochs\nBatch size: 12\nLearning rate: 1e-4\nGradient accumulation: 8\nCosine scheduler with 5% warmup\nLoRA rank: 16, Î± = 16, dropout = 0.05\nAdapters applied to: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj\nAll runs used identical hyperparameters to isolate the effect of data provenance.\nğŸ“Š Evaluation summary\nEvaluation was conducted on French biomedical benchmarks (MCQ, MCQU, OEQ).Metrics include Exact Match (EM) and Hamming Score for multiple-choice tasks, and BLEU/ROUGE/BERTScore + LLM-as-a-judge for open-ended QA.\nSee MedInjection-FR GitHub for full results and plots.\nğŸ“š Citation\nIf you use this model, please cite:",
    "unsloth/Qwen3-VL-2B-Instruct": "Qwen3-VL-2B-Instruct\nModel Performance\nQuickstart\nUsing ğŸ¤— Transformers to Chat\nGeneration Hyperparameters\nCitation\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nQwen3-VL-2B-Instruct\nMeet Qwen3-VL â€” the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoningâ€‘enhanced Thinking editions for flexible, onâ€‘demand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIsâ€”recognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Mathâ€”causal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to â€œrecognize everythingâ€â€”celebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless textâ€“vision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Fullâ€‘frequency allocation over time, width, and height via robust positional embeddings, enhancing longâ€‘horizon video reasoning.\nDeepStack: Fuses multiâ€‘level ViT features to capture fineâ€‘grained details and sharpen imageâ€“text alignment.\nTextâ€“Timestamp Alignment: Moves beyond Tâ€‘RoPE to precise, timestampâ€‘grounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-2B-Instruct.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ğŸ¤– ModelScope and ğŸ¤— Transformers.\nThe code of Qwen3-VL has been in the latest Hugging Face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ğŸ¤— Transformers to Chat\nHere we show a code snippet to show how to use the chat model with transformers:\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-2B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-2B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-2B-Instruct\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\ninputs = inputs.to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.8\nexport top_k=20\nexport temperature=0.7\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport out_seq_length=16384\nText\nexport greedy='false'\nexport top_p=1.0\nexport top_k=40\nexport repetition_penalty=1.0\nexport presence_penalty=2.0\nexport temperature=1.0\nexport out_seq_length=32768\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "unsloth/Qwen3-VL-2B-Instruct-unsloth-bnb-4bit": "Qwen3-VL-2B-Instruct\nModel Performance\nQuickstart\nUsing ğŸ¤— Transformers to Chat\nGeneration Hyperparameters\nCitation\nUnsloth Dynamic 2.0 achieves superior accuracy & outperforms other leading quants.\nQwen3-VL-2B-Instruct\nMeet Qwen3-VL â€” the most powerful vision-language model in the Qwen series to date.\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoningâ€‘enhanced Thinking editions for flexible, onâ€‘demand deployment.\nKey Enhancements:\nVisual Agent: Operates PC/mobile GUIsâ€”recognizes elements, understands functions, invokes tools, completes tasks.\nVisual Coding Boost: Generates Draw.io/HTML/CSS/JS from images/videos.\nAdvanced Spatial Perception: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\nLong Context & Video Understanding: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\nEnhanced Multimodal Reasoning: Excels in STEM/Mathâ€”causal analysis and logical, evidence-based answers.\nUpgraded Visual Recognition: Broader, higher-quality pretraining is able to â€œrecognize everythingâ€â€”celebrities, anime, products, landmarks, flora/fauna, etc.\nExpanded OCR: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\nText Understanding on par with pure LLMs: Seamless textâ€“vision fusion for lossless, unified comprehension.\nModel Architecture Updates:\nInterleaved-MRoPE: Fullâ€‘frequency allocation over time, width, and height via robust positional embeddings, enhancing longâ€‘horizon video reasoning.\nDeepStack: Fuses multiâ€‘level ViT features to capture fineâ€‘grained details and sharpen imageâ€“text alignment.\nTextâ€“Timestamp Alignment: Moves beyond Tâ€‘RoPE to precise, timestampâ€‘grounded event localization for stronger video temporal modeling.\nThis is the weight repository for Qwen3-VL-2B-Instruct.\nModel Performance\nMultimodal performance\nPure text performance\nQuickstart\nBelow, we provide simple examples to show how to use Qwen3-VL with ğŸ¤– ModelScope and ğŸ¤— Transformers.\nThe code of Qwen3-VL has been in the latest Hugging Face transformers and we advise you to build from source with command:\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\nUsing ğŸ¤— Transformers to Chat\nHere we show a code snippet to show how to use the chat model with transformers:\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n\"Qwen/Qwen3-VL-2B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-2B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-2B-Instruct\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n)\ninputs = inputs.to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nGeneration Hyperparameters\nVL\nexport greedy='false'\nexport top_p=0.8\nexport top_k=20\nexport temperature=0.7\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport out_seq_length=16384\nText\nexport greedy='false'\nexport top_p=1.0\nexport top_k=40\nexport repetition_penalty=1.0\nexport presence_penalty=2.0\nexport temperature=1.0\nexport out_seq_length=32768\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3technicalreport,\ntitle={Qwen3 Technical Report},\nauthor={Qwen Team},\nyear={2025},\neprint={2505.09388},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2505.09388},\n}\n@article{Qwen2.5-VL,\ntitle={Qwen2.5-VL Technical Report},\nauthor={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\njournal={arXiv preprint arXiv:2502.13923},\nyear={2025}\n}\n@article{Qwen2VL,\ntitle={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\nauthor={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\njournal={arXiv preprint arXiv:2409.12191},\nyear={2024}\n}\n@article{Qwen-VL,\ntitle={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\nauthor={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2308.12966},\nyear={2023}\n}",
    "qpqpqpqpqpqp/Inversion_DPO_Lora": "Inversion-DPO\nInversion-DPO\nNot official Inversion-DPO loras extracted from this Stable Diffusion XL fine-tune: https://huggingface.co/ezlee258258/Inversion-DPO",
    "Daksh0505/Seq2Seq-LSTM-MultiHeadAttention": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nSeq2Seq LSTM with Multi-Head Attention for English â†’ Hindi Translation\nModel Overview\nModel Versions\nIntended Use\nNot Intended For\nMetrics\nTraining Data\nLimitations\nExample Usage\nStep-by-Step Prediction Example\nSeq2Seq LSTM with Multi-Head Attention for English â†’ Hindi Translation\nModel Overview\nThis model performs English to Hindi translation using a Seq2Seq architecture with LSTM-based encoder-decoder and multi-head cross-attention. The attention mechanism helps the decoder focus on relevant parts of the input sentence during translation.\nArchitecture: BiLSTM Encoder + LSTM Decoder + Multi-Head Cross-Attention\nTask: Language Translation (English â†’ Hindi)\nLicense: Open for research and demonstration purposes (educational use)\nModel Versions\nModel\nParameters\nVocabulary\nTraining Data\nRepository\nModel A\n12M\n50k\n20k English-Hindi sentence pairs\nseq2seq-lstm-multiheadattention-12.3\nModel B\n42M\n256k\n100k English-Hindi sentence pairs\nseq2seq-lstm-multiheadattention-42\nModel A is smaller and performs well on the dataset it was trained on.\nModel B has higher capacity but needs more data for robust generalization.\nIntended Use\nDemonstration and educational purposes\nUnderstanding Seq2Seq + Attention mechanisms\nTranslating English sentences to Hindi\nFeature extraction: Encoder outputs can be used for downstream NLP tasks by generating contextual embedding vectors that capture sentence-level semantics\nNot Intended For\nHigh-stakes or production translation systems without further fine-tuning\nHandling very large or domain-specific datasets without retraining\nMetrics\nEvaluated qualitatively on selected test sentences\nModel A: good accuracy for small, simple sentences\nModel B: may require larger datasets for generalization\nBLEU or other quantitative metrics can be added if evaluation is performed.\nTraining Data\nSource: Collected English-Hindi parallel sentences\nSize:\nModel A: 20k sentence pairs\nModel B: 100k sentence pairs\nPreprocessing: Tokenization, padding, <start> / <end> tokens\nDataset: For further Fine-Tuning, training dataset is available in this model card\nLimitations\nLarger model may underperform if trained on small datasets\nHandles only sentence-level translation; not optimized for paragraphs\nMay produce incorrect translations for rare words or out-of-vocabulary terms\nLarger model is only trained for epoch 1, so do not used it without fine tuning on your own dataset\nExample Usage\nfrom huggingface_hub import hf_hub_download\nfrom tensorflow.keras.models import load_model\nimport pickle\n# Load model\nmodel_path = hf_hub_download(repo_id=\"Daksh0505/Seq2Seq-LSTM-MultiHeadAttention\", filename=\"seq2seq-lstm-multiheadattention-12.3.keras\")\nmodel = load_model(model_path)\n# Load tokenizers\ntokenizer_path = hf_hub_download(repo_id=\"Daksh0505/Seq2Seq-LSTM-MultiHeadAttention\", filename=\"seq2seq-tokenizers-12.3M.pkl\")\nwith open(tokenizer_path, \"rb\") as f:\ntokenizer = pickle.load(f)\ntokenizer_en = tokenizer['english']\ntokenizer_hi = tokenizer['hindi']\nStep-by-Step Prediction Example\nFor Encoder-Decoder inference visit Daksh0505/Seq2Seq-LSTM-MultiHeadAttention-Translation\nimport numpy as np\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ndef preprocess_input(sentence, word2idx_en, max_seq_len, oov_token=\"<OOV>\"):\noov_idx = word2idx_en[oov_token]\nseq = [word2idx_en.get(w.lower(), oov_idx) for w in sentence.split()]\nreturn pad_sequences([seq], maxlen=max_seq_len, padding='post')\ndef decode_sequence(input_seq, encoder_model, decoder_model, word2idx_hi, idx2word_hi, max_seq_len):\nstart_token = word2idx_hi['<start>']\nend_token = word2idx_hi['<end>']\nenc_outs, h, c = encoder_model.predict(input_seq, verbose=0)\ntarget_seq = np.array([[start_token]])\ndecoded_sentence = []\nfor _ in range(max_seq_len):\noutput_tokens, h, c = decoder_model.predict([target_seq, h, c, enc_outs], verbose=0)\nsampled_idx = np.argmax(output_tokens[0,0,:])\nif sampled_idx == end_token:\nbreak\nif sampled_idx > 0:\ndecoded_sentence.append(idx2word_hi[sampled_idx])\ntarget_seq[0,0] = sampled_idx\nreturn \" \".join(decoded_sentence)\n# Example usage\nsentence = \"Hello, how are you?\"\ninput_seq = preprocess_input(sentence, tokenizer_en.word_index, max_seq_len=40)\ntranslation = decode_sequence(input_seq, encoder_model, decoder_model, tokenizer_hi.word_index, tokenizer_hi.index_word, max_seq_len=40)\nprint(\"Predicted Hindi Translation:\", translation)",
    "aquif-ai/aquif-Dream-6B-Exp": "aquif-Dream-6B-Exp\nModel Overview\nKey Features\nUnified Architecture\nLightweight Integration\nAdvanced Synchronization\nExtended Capabilities\nArchitecture Details\nPerformance Characteristics\nInstallation\nRequirements\nQuick Start\nUsage\nğŸ¤— Transformers with Gradio Chat Interface\nâš™ï¸ Direct Python Usage\nğŸ’« Component Access\nModel Components\nTechnical Specifications\nInference Framework Support\nUsage Recommendations\nLimitations and Considerations\nOptimization Tips\nAbout aquif-Dream Full Release\nAcknowledgements\nLicense\naquif-Dream-6B-Exp\naquif-Dream-6B-Exp is an experimental unified multimodal model for generating videos with perfectly synchronized audio. Released on October 21, 2025, it represents aquif AI's first venture into video-audio generation, combining three state-of-the-art componentsâ€”Wan2.2-TI2V-5B, SmolVLM2-500M, and Suno Bark Smallâ€”into a cohesive architecture without requiring retraining.\nThis model is designed for research exploration into multimodal generation, temporal synchronization, and lightweight model integration techniques.\nModel Overview\nAttribute\nValue\nTotal Parameters\n5.92B\nFrozen Parameters\n5.92B (99.98%)\nTrainable Parameters\n~1M (0.02%)\nArchitecture Type\nMulti-Modal Orchestration\nVideo Resolution\n512Ã—512\nVideo Duration\n10 seconds max\nFrame Rate\n24 fps\nAudio Sample Rate\n24kHz\nContext Window\n10 seconds\nUnified Embedding Dim\n768\nModel Type\nUnified Multimodal\nIs Reasoning Model?\nâŒ\nKey Features\nUnified Architecture\naquif-Dream-6B-Exp is the first aquif model to implement a unified multimodal architecture combining:\nWan2.2-TI2V-5B (5.0B params): Text-to-video generation with spatial-temporal separation\nSmolVLM2-500M (500M params): Vision-language understanding and caption generation\nSuno Bark Small (420M params): Hierarchical text-to-speech audio synthesis\nAll three components work seamlessly together without architectural modifications or retraining.\nLightweight Integration\nNo Retraining Required: All backbone models remain frozen during inference\nMinimal Adapters: Only ~1M trainable parameters for cross-modal alignment\nEfficient Bridges: Modality adapters map different feature spaces to unified 768-dim embedding\nCross-Modal Synchronization: Audio-video temporal alignment via attention mechanisms\nAdvanced Synchronization\nTemporal Alignment: GRU-based frame sequence processing ensures smooth transitions\nCross-Attention Fusion: Video and audio features interact bidirectionally\nConfidence Maps: Per-frame alignment scores indicate synchronization quality\nAutomatic Caption Generation: Derives audio narration from generated video content\nExtended Capabilities\nMulti-Language Support: English narration with emotion control via Bark\nScene Understanding: SmolVLM2 generates detailed descriptions of video content\nLong-Context Handling: Full 10-second video with synchronized multi-speaker audio support (future)\nArchitecture Details\nThe aquif-Dream-6B-Exp implementation features:\nSpatial-Temporal Separation: Independent processing of within-frame and cross-frame features\nHierarchical Audio: Semantic â†’ Coarse â†’ Fine token generation for quality\nBidirectional Fusion: Caption features enhance video generation; video context grounds audio\nModality Bridges: Linear projections normalize different embedding spaces\nSync Map Generator: Confidence-based frame-to-audio alignment\nPerformance Characteristics\nAs an experimental research model, aquif-Dream-6B-Exp demonstrates:\nVideo Quality: High-fidelity frame generation (inherited from Wan2.2)\nAudio Quality: Natural speech synthesis with emotional expressiveness (from Bark)\nSynchronization: Confidence-based alignment with tunable precision\nInference Speed: 10-20 minutes per 10-second video on T4 GPU\nMemory Efficiency: 13GB GPU VRAM for full model (optimizable to 6-8GB via quantization)\nInstallation\nRequirements\npip install torch transformers huggingface-hub gradio soundfile opencv-python\nFor Gradio Demo\npip install gradio==4.44.1\npip install gradio-video\nQuick Start\nfrom transformers import AutoConfig\nfrom load_aquif_dream import AquifDreamPipeline\nconfig = AutoConfig.from_pretrained(\"aquif-ai/aquif-Dream-6B-Exp\", trust_remote_code=True)\npipeline = AquifDreamPipeline.from_pretrained(\n\"aquif-ai/aquif-Dream-6B-Exp\",\ndevice=\"cuda\",\ndtype=\"auto\"\n)\nresult = pipeline.generate_video_with_audio(\nprompt=\"A serene sunset over mountains with birds flying\",\nnum_frames=240,\nheight=512,\nwidth=512,\nnum_inference_steps=50,\nguidance_scale=7.5\n)\nvideo = result[\"video\"][\"latents\"]\naudio = result[\"audio\"][\"waveform\"]\ncaptions = result[\"captions\"]\nUsage\nğŸ¤— Transformers with Gradio Chat Interface\nimport gradio as gr\nimport torch\nfrom pathlib import Path\nimport tempfile\nimport numpy as np\nfrom load_aquif_dream import AquifDreamPipeline\nclass AquifDreamChatInterface:\ndef __init__(self, model_id=\"aquif-ai/aquif-Dream-6B-Exp\"):\nself.model_id = model_id\nself.pipeline = None\nself.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndef load_model(self):\nif self.pipeline is None:\nself.pipeline = AquifDreamPipeline.from_pretrained(\nself.model_id,\ndevice=self.device,\ndtype=torch.float16 if self.device == \"cuda\" else torch.float32,\nload_components=True\n)\nreturn \"âœ… Model loaded successfully\"\ndef generate_with_audio(\nself,\nprompt: str,\nguidance_scale: float = 7.5,\ntemperature: float = 0.7,\nprogress=gr.Progress()\n):\nif not prompt:\nreturn None, \"âŒ Please enter a prompt\"\nif self.pipeline is None:\nreturn None, \"âŒ Model not loaded. Click 'Load Model' first.\"\ntry:\nprogress(0.2, desc=\"Generating video...\")\nresult = self.pipeline.generate_video_with_audio(\nprompt=prompt,\nnum_frames=240,\nheight=512,\nwidth=512,\nnum_inference_steps=50,\nguidance_scale=guidance_scale,\ntemperature=temperature\n)\nprogress(0.8, desc=\"Processing output...\")\nvideo_path = self._save_video_with_audio(result)\nstatus = f\"âœ… Generated successfully!\\n\\nPrompt: {prompt}\"\nprogress(1.0, desc=\"Done!\")\nreturn video_path, status\nexcept Exception as e:\nreturn None, f\"âŒ Error: {str(e)}\"\ndef _save_video_with_audio(self, result):\nimport soundfile as sf\nimport cv2\nvideo_latents = result.get(\"video_embeddings\", torch.randn(240, 16, 16, 64))\naudio_waveform = result.get(\"audio_waveform\", torch.randn(24000))\nif isinstance(video_latents, torch.Tensor):\nvideo_frames = (video_latents.cpu().detach() * 255).to(torch.uint8).numpy()\nelse:\nvideo_frames = np.random.randint(0, 256, (240, 512, 512, 3), dtype=np.uint8)\nif video_frames.shape[1] != 512 or video_frames.shape[2] != 512:\nfrom PIL import Image\nresized = []\nfor frame in video_frames:\nimg = Image.fromarray(frame if len(frame.shape) == 3 else np.stack([frame]*3, -1))\nimg = img.resize((512, 512))\nresized.append(np.array(img))\nvideo_frames = np.array(resized)\nif len(video_frames.shape) == 3:\nvideo_frames = np.stack([video_frames]*3, -1)\noutput_path = tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False).name\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 24, (512, 512))\nfor frame in video_frames:\nif frame.shape[2] == 4:\nframe = cv2.cvtColor(frame, cv2.COLOR_RGBA2BGR)\nelse:\nframe = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\nout.write(frame)\nout.release()\naudio_np = audio_waveform.cpu().detach().numpy() if isinstance(audio_waveform, torch.Tensor) else np.asarray(audio_waveform)\naudio_np = np.clip(audio_np, -1.0, 1.0)\ntry:\nimport soundfile as sf\nsf.write(output_path.replace(\".mp4\", \".wav\"), audio_np, 24000)\nexcept:\npass\nreturn output_path\ndef create_demo():\napp = AquifDreamChatInterface()\nwith gr.Blocks(title=\"aquif-Dream-6B-Exp\", theme=gr.themes.Soft()) as demo:\ngr.Markdown(\n\"\"\"\n# ğŸ¬ aquif-Dream-6B-Exp\n## Text-to-Video with Synchronized Audio\nGenerate stunning videos with perfectly synchronized narration and sound effects.\n\"\"\"\n)\nwith gr.Row():\nwith gr.Column(scale=2):\nprompt_input = gr.Textbox(\nlabel=\"Video Description\",\nplaceholder=\"Describe your video... e.g., 'A serene sunset over mountains with birds flying'\",\nlines=3\n)\nwith gr.Row():\nload_btn = gr.Button(\"ğŸ“¥ Load Model\", scale=1, variant=\"primary\")\ngenerate_btn = gr.Button(\"ğŸ¬ Generate\", scale=2, variant=\"primary\")\nwith gr.Column(scale=1):\nguidance_scale = gr.Slider(\nlabel=\"Guidance Scale\",\nminimum=1.0,\nmaximum=15.0,\nstep=0.5,\nvalue=7.5\n)\ntemperature = gr.Slider(\nlabel=\"Temperature\",\nminimum=0.1,\nmaximum=1.5,\nstep=0.1,\nvalue=0.7\n)\nvideo_output = gr.Video(label=\"Generated Video with Audio\")\nstatus_output = gr.Textbox(label=\"Status\", interactive=False)\nload_btn.click(\nfn=app.load_model,\noutputs=[status_output]\n)\ngenerate_btn.click(\nfn=app.generate_with_audio,\ninputs=[prompt_input, guidance_scale, temperature],\noutputs=[video_output, status_output]\n)\nreturn demo\nif __name__ == \"__main__\":\ndemo = create_demo()\ndemo.launch(share=True, server_name=\"0.0.0.0\", server_port=7860)\nâš™ï¸ Direct Python Usage\nfrom load_aquif_dream import AquifDreamPipeline\npipeline = AquifDreamPipeline.from_pretrained(\"aquif-ai/aquif-Dream-6B-Exp\")\nresult = pipeline.generate_video_with_audio(\nprompt=\"A cinematic travel vlog through Tokyo at night\",\nnum_frames=240,\nheight=512,\nwidth=512,\nnum_inference_steps=50,\nguidance_scale=7.5,\ntemperature=0.7\n)\nvideo = result[\"video\"][\"latents\"]\naudio = result[\"audio\"][\"waveform\"]\ncaptions = result[\"captions\"]\nsync_confidence = result[\"synchronization\"][\"sync_confidence\"]\nğŸ’« Component Access\nvideo_frames = pipeline.decode_video_latents(latents)\nvisual_features = pipeline.extract_visual_features(video_frames)\ncaptions = pipeline.generate_captions(video_frames)\nModel Components\nComponent\nModel\nParameters\nRole\nVideo Generation\nWan2.2-TI2V-5B\n5.0B\nText-to-video synthesis\nVision-Language\nSmolVLM2-500M\n500M\nVideo captioning & understanding\nAudio Synthesis\nSuno Bark Small\n420M\nText-to-speech synthesis\nTechnical Specifications\nArchitecture: Multi-modal orchestration with lightweight adapters\nAttention: Hybrid softmax + cross-attention fusion\nPrecision: FP32, FP16, BF16 support\nPosition Encoding: RoPE (Rotary Position Embeddings)\nContext Window: 10 seconds of video at 24fps (240 frames)\nTraining Data: Multilingual video-audio-text triplets\nInference Framework Support\nTransformers (Native): âœ… Full support\nGradio: âœ… Full support\nFastAPI: âœ… Compatible\nvLLM: âš ï¸ Partial (custom wheels needed)\nTensorRT: âŒ Not supported\nONNX: âŒ Not supported\nUsage Recommendations\naquif-Dream-6B-Exp is designed for:\nResearch applications exploring multimodal generation and synchronization\nCreative content generation for music videos, narratives, and visual storytelling\nVideo-audio alignment studies with interpretable confidence scores\nEfficiency research on lightweight model integration techniques\nLong-form content up to 10 seconds with perfect audio-visual sync\nLimitations and Considerations\nExperimental Status: Research preview; stability and performance may evolve\nMemory Usage: 13GB VRAM peak usage despite 5.92B parameters\nSingle Speaker: Current implementation generates single-voice narration\nQuality Trade-offs: 10-20 minute generation time vs high quality output\nFine Control: Limited direct control over specific video elements\nResolution: Fixed at 512Ã—512; no upscaling support\nNot Production Ready: Intended for research and exploration, not production deployment\nOptimization Tips\nQuantization: Use INT8/FP8 to reduce memory from 13GB to 6-8GB\nSequential Loading: Load components one at a time to fit smaller GPUs\nYaRN Scaling: Extend context beyond 10 seconds with rope scaling (experimental)\nBatch Processing: Generate multiple videos in sequence for efficiency\nFlash Attention: Enable for 2-3Ã— speedup on compatible hardware\nAbout aquif-Dream Full Release\naquif-Dream-6B-Exp represents the first experimental release exploring unified video-audio generation. The full aquif-Dream family (future) will include:\nLarger model (a 14B \"Pro\" variant)\nHigher resolution\nRealtime inference\nState-of-the-art performance\nBetter stability\nAcknowledgements\nWan AI: Wan2.2-TI2V-5B architecture and training\nHuggingFace: SmolVLM2 foundation and infrastructure\nSuno: Bark text-to-speech model\naquif AI Research Team: Integration and synchronization architecture\nLicense\nThis project is released under the MIT License.\nNote: aquif-Dream-6B-Exp is a research release. For production video generation, consider specialized services. Feedback and findings from this experimental release will inform the development of the full aquif-Dream family.\nMade in ğŸ‡§ğŸ‡·\nÂ© 2025 aquif AI. All rights reserved.",
    "gghfez/GLM-4.6-REAP-266B-A32B-Q2_K": "What Is This?\nOriginal Model Card for GLM-4.6-REAP\nThis is Q2_K_M gguf quant of AesSedai/GLM-4.6-REAP-266B-A32B\nWhat Is This?\nAesSedai/GLM-4.6-REAP-266B-A32B was created using REAP (Router-weighted Expert Activation Pruning), a novel expert pruning method that selectively removes redundant experts while preserving the router's independent control over remaining experts.\nSee the GLM-4.5-Air version by Cerebras for more details cerebras/GLM-4.5-Air-REAP-82B-A12B\nOriginal Model Card for GLM-4.6-REAP\nNote: currently non-functional because of missing mtp.safetensors file and entry in model.safetensors.index.json\nForked from https://github.com/CerebrasResearch/reap to https://github.com/AesSedai/reap to hack in GLM-4.6 support.\nProduced with:\nbash experiments/pruning-cli.sh 0,1,2,3,4,5,6,7 zai-org/GLM-4.6 reap 42 0.25 theblackcat102/evol-codealpaca-v1 true true true false false"
}