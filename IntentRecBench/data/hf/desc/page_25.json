{
    "bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF": "Llamacpp imatrix Quantizations of Cydonia-24B-v4.2.0 by TheDrummer\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nARM/AVX information\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of Cydonia-24B-v4.2.0 by TheDrummer\nUsing llama.cpp release b6714 for quantization.\nOriginal model: https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0\nAll quants made using imatrix option with dataset from here combined with a subset of combined_all_small.parquet from Ed Addario here\nRun them in LM Studio\nRun them directly with llama.cpp, or any other llama.cpp based project\nPrompt format\nNo chat template specified so default is used. This may be incorrect, check original model card for details.\n<s>[SYSTEM_PROMPT]{system_prompt}[/SYSTEM_PROMPT][INST]{prompt}[/INST]\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nCydonia-24B-v4.2.0-bf16.gguf\nbf16\n47.15GB\nfalse\nFull BF16 weights.\nCydonia-24B-v4.2.0-Q8_0.gguf\nQ8_0\n25.05GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\nCydonia-24B-v4.2.0-Q6_K_L.gguf\nQ6_K_L\n19.67GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\nCydonia-24B-v4.2.0-Q6_K.gguf\nQ6_K\n19.35GB\nfalse\nVery high quality, near perfect, recommended.\nCydonia-24B-v4.2.0-Q5_K_L.gguf\nQ5_K_L\n17.18GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\nCydonia-24B-v4.2.0-Q5_K_M.gguf\nQ5_K_M\n16.76GB\nfalse\nHigh quality, recommended.\nCydonia-24B-v4.2.0-Q5_K_S.gguf\nQ5_K_S\n16.30GB\nfalse\nHigh quality, recommended.\nCydonia-24B-v4.2.0-Q4_1.gguf\nQ4_1\n14.87GB\nfalse\nLegacy format, similar performance to Q4_K_S but with improved tokens/watt on Apple silicon.\nCydonia-24B-v4.2.0-Q4_K_L.gguf\nQ4_K_L\n14.83GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\nCydonia-24B-v4.2.0-Q4_K_M.gguf\nQ4_K_M\n14.33GB\nfalse\nGood quality, default size for most use cases, recommended.\nCydonia-24B-v4.2.0-Q4_K_S.gguf\nQ4_K_S\n13.55GB\nfalse\nSlightly lower quality with more space savings, recommended.\nCydonia-24B-v4.2.0-Q4_0.gguf\nQ4_0\n13.49GB\nfalse\nLegacy format, offers online repacking for ARM and AVX CPU inference.\nCydonia-24B-v4.2.0-IQ4_NL.gguf\nIQ4_NL\n13.47GB\nfalse\nSimilar to IQ4_XS, but slightly larger. Offers online repacking for ARM CPU inference.\nCydonia-24B-v4.2.0-Q3_K_XL.gguf\nQ3_K_XL\n12.99GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nCydonia-24B-v4.2.0-IQ4_XS.gguf\nIQ4_XS\n12.76GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nCydonia-24B-v4.2.0-Q3_K_L.gguf\nQ3_K_L\n12.40GB\nfalse\nLower quality but usable, good for low RAM availability.\nCydonia-24B-v4.2.0-Q3_K_M.gguf\nQ3_K_M\n11.47GB\nfalse\nLow quality.\nCydonia-24B-v4.2.0-IQ3_M.gguf\nIQ3_M\n10.65GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nCydonia-24B-v4.2.0-Q3_K_S.gguf\nQ3_K_S\n10.40GB\nfalse\nLow quality, not recommended.\nCydonia-24B-v4.2.0-IQ3_XS.gguf\nIQ3_XS\n9.91GB\nfalse\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nCydonia-24B-v4.2.0-Q2_K_L.gguf\nQ2_K_L\n9.55GB\nfalse\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\nCydonia-24B-v4.2.0-IQ3_XXS.gguf\nIQ3_XXS\n9.28GB\nfalse\nLower quality, new method with decent performance, comparable to Q3 quants.\nCydonia-24B-v4.2.0-Q2_K.gguf\nQ2_K\n8.89GB\nfalse\nVery low quality but surprisingly usable.\nCydonia-24B-v4.2.0-IQ2_M.gguf\nIQ2_M\n8.11GB\nfalse\nRelatively low quality, uses SOTA techniques to be surprisingly usable.\nCydonia-24B-v4.2.0-IQ2_S.gguf\nIQ2_S\n7.48GB\nfalse\nLow quality, uses SOTA techniques to be usable.\nCydonia-24B-v4.2.0-IQ2_XS.gguf\nIQ2_XS\n7.21GB\nfalse\nLow quality, uses SOTA techniques to be usable.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nDownloading using huggingface-cli\nClick to view download instructions\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF --include \"TheDrummer_Cydonia-24B-v4.2.0-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/TheDrummer_Cydonia-24B-v4.2.0-GGUF --include \"TheDrummer_Cydonia-24B-v4.2.0-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (TheDrummer_Cydonia-24B-v4.2.0-Q8_0) or download them all in place (./)\nARM/AVX information\nPreviously, you would download Q4_0_4_4/4_8/8_8, and these would have their weights interleaved in memory in order to improve performance on ARM and AVX machines by loading up more data in one pass.\nNow, however, there is something called \"online repacking\" for weights. details in this PR. If you use Q4_0 and your hardware would benefit from repacking weights, it will do it automatically on the fly.\nAs of llama.cpp build b4282 you will not be able to run the Q4_0_X_X files and will instead need to use Q4_0.\nAdditionally, if you want to get slightly better quality for , you can use IQ4_NL thanks to this PR which will also repack the weights for ARM, though only the 4_4 for now. The loading time may be slower but it will result in an overall speed incrase.\nClick to view Q4_0_X_X information (deprecated\nI'm keeping this section to show the potential theoretical uplift in performance from using the Q4_0 with online repacking.\nClick to view benchmarks on an AVX2 system (EPYC7702)\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n% (vs Q4_0)\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp512\n204.03 ¬± 1.03\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp1024\n282.92 ¬± 0.19\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp2048\n259.49 ¬± 0.44\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg128\n39.12 ¬± 0.27\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg256\n39.31 ¬± 0.69\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg512\n40.52 ¬± 0.03\n100%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp512\n301.02 ¬± 1.74\n147%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp1024\n287.23 ¬± 0.20\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp2048\n262.77 ¬± 1.81\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg128\n18.80 ¬± 0.99\n48%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg256\n24.46 ¬± 3.04\n83%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg512\n36.32 ¬± 3.59\n90%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp512\n271.71 ¬± 3.53\n133%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp1024\n279.86 ¬± 45.63\n100%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp2048\n320.77 ¬± 5.00\n124%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg128\n43.51 ¬± 0.05\n111%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg256\n43.35 ¬± 0.09\n110%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg512\n42.60 ¬± 0.31\n105%\nQ4_0_8_8 offers a nice bump to prompt processing and a small bump to text generation\nWhich file should I choose?\nClick here for details\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset.\nThank you ZeroWw for the inspiration to experiment with embed/output.\nThank you to LM Studio for sponsoring my work.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "VeryAladeen/Sec-4B": "SeC-4B Model Files - Multiple Precision Formats\nModel Formats\nWhat is SeC?\nUsage\nOriginal Model\nCredits\nLicense\nCitation\nSeC-4B Model Files - Multiple Precision Formats\nSingle-file model formats for the SeC (Segment Concept) video object segmentation model, optimized for use with ComfyUI SeC Nodes.\nModel Formats\nFormat\nSize\nDescription\nGPU Requirements\nSeC-4B-fp16.safetensors\n7.35 GB\nRecommended - Best balance of quality and size\nAll CUDA GPUs\nSeC-4B-fp8.safetensors\n3.97 GB\nVRAM-constrained systems (saves 1.5-2GB VRAM)\nRTX 30 series or newer\nSeC-4B-bf16.safetensors\n7.35 GB\nAlternative to FP16\nAll CUDA GPUs\nSeC-4B-fp32.safetensors\n14.14 GB\nFull precision\nAll CUDA GPUs\nWhat is SeC?\nSeC (Segment Concept) uses Large Vision-Language Models for video object segmentation, achieving +11.8 points improvement over SAM 2.1 on complex semantic scenarios (SeCVOS benchmark).\nKey features:\nConcept-driven tracking with semantic understanding\nHandles occlusions and appearance changes\nBidirectional tracking support\nState-of-the-art performance on multiple benchmarks\nUsage\nThese models are designed for use with the ComfyUI SeC Nodes custom nodes.\nInstallation:\nDownload your preferred model format\nPlace in ComfyUI/models/sams/\nInstall ComfyUI SeC Nodes\nThe model will be automatically detected and available in the SeC Model Loader\nOriginal Model\nThese are converted single-file versions of the original model:\nOriginal Repository: OpenIXCLab/SeC-4B\nPaper: arXiv:2507.15852\nOfficial Implementation: github.com/OpenIXCLab/SeC\nCredits\nOriginal Model: Developed by OpenIXCLab\nModel architecture and weights: Apache 2.0 License\nPaper: Zhang et al. \"SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction\"\nSingle-File Conversions: Created for ComfyUI SeC Nodes\nConversion script and ComfyUI integration: 9nate-drake\nFP8 quantization support via torchao\nLicense\nApache 2.0 (same as original SeC-4B model)\nCitation\nIf you use this model in your research, please cite the original SeC paper:\n@article{zhang2025sec,\ntitle     = {SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction},\nauthor    = {Zhixiong Zhang and Shuangrui Ding and Xiaoyi Dong and Songxin He and\nJianfan Lin and Junsong Tang and Yuhang Zang and Yuhang Cao and\nDahua Lin and Jiaqi Wang},\njournal   = {arXiv preprint arXiv:2507.15852},\nyear      = {2025}\n}",
    "nightmedia/VCoder-120b-1.0-qx86-hi-mlx": "VCoder-120b-1.0-qx86-hi-mlx\nUse with mlx\nVCoder-120b-1.0-qx86-hi-mlx\nKey Insights from Benchmark Performance\nComparing model with the unsloth-gpt-oss-120b-qx86-mxfp4, a similar quant\nBenchmark\t  unsloth\tVCoder\tWinner\narc_challenge\t0.334\t0.323\tunsloth (slight edge)\narc_easy\t    0.335\t0.366\tVCoder\nboolq\t        0.378\t0.429\tVCoder\nhellaswag\t    0.264\t0.538\tVCoder\nopenbookqa\t    0.354\t0.360\tVCoder\npiqa\t        0.559\t0.694\tVCoder\nwinogrande\t    0.512\t0.544\tVCoder\n‚úÖ Overall Winner: VCoder\nVCoder outperforms unsloth in 6/7 benchmarks, with particularly strong gains in:\nHellaSwag (0.538 vs. 0.264): ~103% improvement in commonsense reasoning (e.g., completing everyday scenarios).\nPIQA (0.694 vs. 0.559): ~24% better at physical commonsense (e.g., understanding real-world physics).\nBoolQ (0.429 vs. 0.378): ~13% improvement in binary question answering (e.g., yes/no reasoning over text).\nThe only minor exception is arc_challenge, where unsloth has a slightly higher score (0.334 vs. 0.323), but VCoder dominates in arc_easy (0.366 vs. 0.335), suggesting it handles easier reasoning tasks better despite a small gap in the hardest ARC questions.\nWhy VCoder Excels & What \"High Resolution Attention\" Means\nBoth models are structurally similar MoEs (Mixture of Experts), but VCoder uses high-resolution attention paths and heads.\nThis likely enables finer-grained contextual understanding, especially for tasks requiring nuanced reasoning (e.g., HellaSwag/PIQA).\nHigher attention resolution improves the model's ability to:\nTrack relationships between distant tokens in long contexts.\nResolve ambiguous pronouns (Winogrande).\nApply physical commonsense (PIQA) or everyday scenarios (HellaSwag).\nPerplexity Confirmation:\nVCoder's perplexity of 4.677 ¬±0.032 is exceptionally low for language modeling.\nLower perplexity = better at predicting text sequences (e.g., GPT-3 has ~20+ on standard datasets).\nThis aligns with VCoder's superior performance across most benchmarks, as strong language modeling correlates with general reasoning abilities.\nPractical Implications\nFor tasks requiring commonsense physics (PIQA), everyday reasoning (HellaSwag), or binary question understanding (BoolQ), VCoder is significantly stronger.\nIf your use case involves high-stakes scientific reasoning (arc_challenge), unsloth might edge out slightly‚Äîbut this is negligible compared to VCoder‚Äôs broader strengths.\nRecommendation: Prioritize VCoder unless you have a specific need for arc_challenge (which is rare in real-world applications).\nVCoder‚Äôs high-resolution attention architecture delivers superior cognitive abilities across nearly all evaluated tasks, especially in commonsense reasoning and physical understanding. Its low perplexity further confirms robust language modeling skills, making it the more capable model for general-purpose reasoning. The unsloth model‚Äôs slight edge in arc_challenge is overshadowed by VCoder‚Äôs dominance elsewhere.\nQuantization   Perplexity      tok/sec\nbf16           4.669 ¬± 0.032   68.85\nq8-hi          4.675 ¬± 0.032   70.32\nqx86-hi        4.677 ¬± 0.032   71.47\nPeak memory: 68.85 GB\nThis model VCoder-120b-1.0-qx86-hi-mlx was\nconverted to MLX format from EpistemeAI/VCoder-120b-1.0\nusing mlx-lm version 0.28.2.\nUse with mlx\npip install mlx-lm\nfrom mlx_lm import load, generate\nmodel, tokenizer = load(\"VCoder-120b-1.0-qx86-hi-mlx\")\nprompt = \"hello\"\nif tokenizer.chat_template is not None:\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\nmessages, add_generation_prompt=True\n)\nresponse = generate(model, tokenizer, prompt=prompt, verbose=True)",
    "cerebras/Qwen3-Coder-REAP-363B-A35B-FP8": "Qwen3-Coder-REAP-363B-A35B-FP8\n‚ú® Highlights\nüìã Model Overview\nüìä Evaluations\nüöÄ Deployment\nüß© Model Creation\nHow REAP Works\nKey Advantages\nCalibration\n‚öñÔ∏è License\nüßæ Citation\nìå≥ REAPìå≥  the Experts: Why Pruning Prevails for One-Shot MoE Compression\nQwen3-Coder-REAP-363B-A35B-FP8\n‚ú® Highlights\nIntroducing Qwen3-Coder-REAP-363B-A35B-FP8, a memory-efficient compressed variant of Qwen3-Coder-480B-A35B-Instruct-FP8 that maintains near-identical performance while being 25% lighter.\nThis model was created using REAP (Router-weighted Expert Activation Pruning), a novel expert pruning method that selectively removes redundant experts while preserving the router's independent control over remaining experts. Key features include:\nNear-Lossless Performance: Maintains almost identical accuracy on code generation, agentic coding, and function calling tasks compared to the full 480B model\n25% Memory Reduction: Compressed from 480B to 363B parameters, significantly lowering deployment costs and memory requirements\nPreserved Capabilities: Retains all core functionalities including code generation, agentic workflows, repository-scale understanding, and function calling\nDrop-in Compatibility: Works with vanilla vLLM - no source modifications or custom patches required\nOptimized for Real-World Use: Particularly effective for resource-constrained environments, local deployments, and academic research\nüìã Model Overview\nQwen3-Coder-REAP-363B-A35B-FP8 has the following specifications:\nBase Model: Qwen3-Coder-480B-A35B-Instruct\nCompression Method: REAP (Router-weighted Expert Activation Pruning)\nCompression Ratio: 25% expert pruning\nType: Sparse Mixture-of-Experts (SMoE) Causal Language Model\nNumber of Parameters: 363B total, 35B activated per token\nNumber of Layers: 62\nNumber of Attention Heads (GQA): 96 for Q and 8 for KV\nNumber of Experts: 120 (uniformly pruned from 160)\nNumber of Activated Experts: 8 per token\nContext Length: 262,144 tokens natively (extendable to 1M with YaRN)\nQuantization: FP8\nLicense: Apache 2.0\nüìä Evaluations\nBenchmark\nQwen3-Coder-480B-A35B-Instruct-FP8\nQwen3-Coder-REAP-363B-A35B-FP8\nQwen3-Coder-REAP-246B-A35B-FP8\nCompression\n‚Äî\n25%\n50%\nHumanEval\n95.1\n95.7\n93.9\nHumanEval+\n89.0\n89.0\n87.2\nMBPP\n92.3\n91.7\n91.0\nMBPP+\n79.1\n77.2\n77.2\nLiveCodeBench (25.01 - 25.05)\n43.1\n41.6\n41.5\nSWE-Bench-Verified (w/ mini-swe-agent)\n54.0\n54.0\n52.2\nBFCL-v3 (Non-Live)\n86.6\n87.8\n84.9\nBFCL-v3 (Live)\n82.5\n82.3\n80.1\nBFCL-v3 (Multi-Turn)\n38.0\n39.2\n37.1\nBFCL-v3 (Overall)\n69.0\n69.8\n67.4\nùúè¬≤-bench (Airline)\n46.0\n48.7\n44.7\nùúè¬≤-bench (Retail)\n64.3\n66.1\n63.2\nùúè¬≤-bench (Telecom)\n50.0\n52.9\n47.1\nTerminalBench 0.1.1 (Terminus agent)\n30.5\n30.5\n30.0\nüü© This checkpoint maintains almost identical performance while being 25% lighter.\nFor more details on the evaluation setup, refer to the REAP arXiv preprint.\nüöÄ Deployment\nYou can deploy the model directly using the latest vLLM (v0.11.0), no source modifications or custom patches required.\nvllm serve cerebras/Qwen3-Coder-REAP-363B-A35B-FP8 \\\n--tensor-parallel-size 8 \\\n--tool-call-parser qwen3_coder \\\n--enable-auto-tool-choice \\\n--enable-expert-parallel\nIf you encounter insufficient memory when running this model, you might need to set a lower value for --max-num-seqs flag (e.g. set to 64).\nüß© Model Creation\nThis checkpoint was created by applying the REAP (Router-weighted Expert Activation Pruning) method uniformly across all Mixture-of-Experts (MoE) blocks of Qwen3-Coder-480B-A35B-Instruct, with a 25% pruning rate.\nHow REAP Works\nREAP selects experts to prune based on a novel saliency criterion that considers both:\nRouter gate values: How frequently and strongly the router activates each expert\nExpert activation norms: The magnitude of each expert's output contributions\nThis dual consideration ensures that experts contributing minimally to the layer's output are pruned, while preserving those that play critical roles in the model's computations.\nKey Advantages\nOne-Shot Compression: No fine-tuning required after pruning - the model is immediately ready for deployment\nPreserved Router Control: Unlike expert merging methods, REAP maintains the router's independent, input-dependent control over remaining experts, avoiding \"functional subspace collapse\"\nGenerative Task Superiority: REAP significantly outperforms expert merging approaches on generative benchmarks (code generation, creative writing, mathematical reasoning) while maintaining competitive performance on discriminative tasks\nCalibration\nThe model was calibrated using a diverse mixture of domain-specific datasets including:\nCode generation samples (evol-codealpaca)\nFunction calling examples (xlam-function-calling)\nAgentic multi-turn trajectories (SWE-smith-trajectories)\nüìö For more details, refer to the following resources:\nüßæ arXiv Preprint\nüßæ REAP Blog\nüíª REAP Codebase (GitHub)\n‚öñÔ∏è License\nThis model is derived from\nQwen/Qwen3-Coder-480B-A35B-Instruct\nand distributed under the Apache 2.0 License.\nüîó View License File ‚Üí\nüßæ Citation\nIf you use this checkpoint, please cite the REAP paper:\n@article{lasby-reap,\ntitle={REAP the Experts: Why Pruning Prevails for One-Shot MoE compression},\nauthor={Lasby, Mike and Lazarevich, Ivan and Sinnadurai, Nish and Lie, Sean and Ioannou, Yani and Thangarasa, Vithursan},\njournal={arXiv preprint arXiv:2510.13999},\nyear={2025}\n}",
    "WithAnyone/WithAnyone": "WithAnyone: Towards Controllable and ID Consistent Image Generation\nAbstract\nModel Zoo\nüìë Introduction\n‚ö°Ô∏è Quick Start\nüè∞ Model Zoo\nüîß Requirements\nüîß Model Checkpoints\n‚ö°Ô∏è Gradio Demo\nüí° Tips for Better Results\n‚öôÔ∏è Batch Inference\nDownload MultiID-Bench\nRun Batch Inference\n‚öôÔ∏è Face Edit with FLUX.1 Kontext\nüìú License and Disclaimer\nüåπ Acknowledgement\nüìë Citation\nWithAnyone: Towards Controllable and ID Consistent Image Generation\nThe model was presented in the paper WithAnyone: Towards Controllable and ID Consistent Image Generation.\nAbstract\nIdentity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.\nModel Zoo\nModel\nDescription\nDownload\nWithAnyone 1.0 - FLUX.1 [dev]\nJust use this one.\nHuggingFace\nWithAnyone.K Preview - FLUX.1 Kontext [dev]\nFor t2i generation with FLUX.1 Kontext\nHuggingFace\nWithAnyone.Ke Preview - FLUX.1 Kontext [dev]\nFor face-editing with FLUX.1 Kontext\nHuggingFace\nIf you just want to try it out, please use the base model WithAnyone - FLUX.1 [dev]. The other models are for the following use cases:\nWithAnyone.K\nThis is a preliminary version of WithAnyone with FLUX.1 Kontext. It can be used for text-to-image generation with multiple given identities. However, stability and quality are not as good as the base model. Please use it with caution. We are working on improving it.\nWithAnyone.Ke\nThis is a face editing version of WithAnyone with FLUX.1 Kontext, leveraging the editing capabilities of FLUX.1 Kontext. Please use it with `gradio_edit.py` instead of `gradio_app.py`. It is still a preliminary version, and we are working on improving it.\nüìë Introduction\nHighlight of WithAnyone\nControllable: WithAnyone aims to mitigate the \"copy-paste\" artifacts in face generation. Previous methods have a tendency to directly copy and paste the reference face onto the generated image, leading poor controllability of expressions, hairstyles, accessories, and even poses. They falls into a clear trade-off between similarity and copy-paste. The more similar the generated face is to the reference, the more copy-paste artifacts it has. WithAnyone is an attampt to break this trade-off.\nMulti-ID Generation: WithAnyone can generate multiple given identities in a single image. With the help of controllable face generation, all generated faces can fit harmoniously in one group photo.\n‚ö°Ô∏è Quick Start\nüè∞ Model Zoo\nModel\nDescription\nDownload\nWithAnyone 1.0 - FLUX.1\nMain model with FLUX.1\nHuggingFace\nWithAnyone.K.preview - FLUX.1 Kontext\nFor t2i generation with FLUX.1 Kontext\nHuggingFace\nWithAnyone.Ke.preview - FLUX.1 Kontext\nFor face-editing with FLUX.1 Kontext\nHuggingFace\nIf you just want to try it out, please use the base model WithAnyone - FLUX.1. The other models are for the following use cases:\nWithAnyone.K\nThis is a preliminary version of WithAnyone with FLUX.1 Kontext. It can be used for text-to-image generation with multiple given identities. However, stability and quality are not as good as the base model. Please use it with caution. We are working on improving it.\nWithAnyone.Ke\nThis is a face editing version of WithAnyone with FLUX.1 Kontext, leveraging the editing capabilities of FLUX.1 Kontext. Please use it with `gradio_edit.py` instead of `gradio_app.py`. It is still a preliminary version, and we are working on improving it.\nüîß Requirements\nUse pip install -r requirements.txt to install the necessary packages.\nüîß Model Checkpoints\nYou can download the necessary model checkpoints in one of the two ways:\nDirectly run the inference scripts. The checkpoints will be downloaded automatically by the hf_hub_download function in the code to your $HF_HOME (default: ~/.cache/huggingface).\nUse huggingface-cli download <repo name> to download:\nblack-forest-labs/FLUX.1-dev\nxlabs-ai/xflux_text_encoders\nopenai/clip-vit-large-patch14\ngoogle/siglip-base-patch16-256-i18n\nwithanyone/withanyoneThen run the inference scripts. You can download only the checkpoints you need to speed up setup and save disk space.Example for black-forest-labs/FLUX.1-dev:\nhuggingface-cli download black-forest-labs/FLUX.1-dev flux1-dev.safetensors\nhuggingface-cli download black-forest-labs/FLUX.1-dev ae.safetensorsIgnore the text encoder in the black-forest-labs/FLUX.1-dev model repo (it is there for diffusers calls). All checkpoints together require about 37 GB of disk space.\nAfter downloading, set the following arguments in the inference script to the local paths of the downloaded checkpoints:\n--flux_path <path to flux1-dev.safetensors>\n--clip_path <path to clip-vit-large-patch14>\n--t5_path <path to xflux_text_encoders>\n--siglip_path <path to siglip-base-patch16-256-i18n>\n--ipa_path <path to withanyone>\nWe need to use the ArcFace model for face embedding. It will automatically be downloaded to `./models/`. However, there is an original bug. If you see an error like `assert 'detection' in self.models`, please manually move the model directory:\nmv models/antelopev2/ models/antelopev2_\nmv models/antelopev2_/antelopev2/ models/antelopev2/\nrm -rf models/antelopev2_, antelopev2.zip\n‚ö°Ô∏è Gradio Demo\nThe Gradio GUI demo is a good starting point to experiment with WithAnyone. Run it with:\npython gradio_app.py --flux_path <path to flux1-dev directory> --ipa_path <path to withanyone directory> \\\n--clip_path <path to clip-vit-large-patch14> \\\n--t5_path <path to xflux_text_encoders> \\\n--siglip_path <path to siglip-base-patch16-256-i18n> \\\n--model_type \"flux-dev\" # or \"flux-kontext\" for WithAnyone.K\n‚ùó WithAnyone requires face bounding boxes (bboxes). You should provide them to indicate where faces are. You can provide face bboxes in two ways:\nUpload an example image with desired face locations in Mask Configuration (Option 1: Automatic). The face bboxes will be extracted automatically, and faces will be generated in the same locations. Do not worry if the given image has a different resolution or aspect ratio; the face bboxes will be resized accordingly.\nInput face bboxes directly in Mask Configuration (Option 2: Manual). The format is x1,y1,x2,y2 for each face, one per line.\n(NOT recommended) leave both options empty, and the face bboxes will be randomly chosen from a pre-defined set.\n‚≠ï WithAnyone works well with LoRA. If you have any stylized LoRA checkpoints, use --additional_lora_ckpt <path to lora checkpoint> when launching the demo. The LoRA will be merged into the diffusion model.\npython gradio_app.py --flux_path <path to flux1-dev directory> --ipa_path <path to withanyone directory> \\\n--additional_lora_ckpt <path to lora checkpoint> \\\n--lora_scale 0.8 # adjust the weight as needed\n‚≠ï In Advanced Options, there is a slider controlling whether outputs are more \"similar in spirit\" or \"similar in form\" to the reference faces.\nMove the slider to the right to preserve more details in the reference image (expression, makeup, accessories, hairstyle, etc.). Identity will also be better preserved.\nMove it to the left for more freedom and creativity. Stylization can be stronger, hair style and makeup can be changed.\nHow the slider works and some tips\nThe slider actually controlls the weight of SigLIP embedding and ArcFace embedding. The former preserves more mid-level semantic details, while the latter preserves more high-level identity information.\nSigLIP is a general image embedding model, capturing more than just faces, while ArcFace is a face-specific embedding model, capturing only identity information.\nWhen using high arcface weight (slider to the left), please add more description of the identity in the prompt, since arcface embedding may lose information like hairstyle, skin color, body build, age, etc.\nüí° Tips for Better Results\nBe prepared for the first few runs as it may not be very satisfying.\nProvide detailed prompts describing the identity. WithAnyone is \"controllable\", so it needs more information to be controlled. Here are something that might go wrong if not specified:\nSkin color (generally the race is fine, but for asain descent, if not specified, it may generate darker skin tone);\nAge (e.g., intead of \"a man\", try \"a young man\". If not specified, it may generate an older figure);\nBody build;\nHairstyle;\nAccessories (glasses, hats, earrings, etc.);\nMakeup\nUse the slider to balance between \"Resemblance in Spirit\" and \"Resemblance in Form\" according to your needs. If you want to preserve more details in the reference image, move the slider to the right; if you want more freedom and creativity, move it to the left.\nTry it with LoRAs from community. They are usually fantastic.\n‚öôÔ∏è Batch Inference\nYou can use infer_withanyone.py for batch inference. The script supports generating multiple images with MultiID-Bench.\nDownload MultiID-Bench\nDownload from HuggingFace.\nhuggingface-cli download WithAnyone/MultiID-Bench --repo-type dataset --local-dir <path to MultiID-Bench directory>\nAnd convert the parquet file to a folder of images and a json file using MultiID_Bench/parquet2bench.py:\npython MultiID_Bench/parquet2bench.py --parquet <path to parquet file> --output_dir <path to output directory>\nYou will get a folder with the following structure:\n<output_dir>/\n‚îú‚îÄ‚îÄ p1/untar\n‚îú‚îÄ‚îÄ p2/untar\n‚îú‚îÄ‚îÄ p3/\n‚îú‚îÄ‚îÄ p1.json\n‚îú‚îÄ‚îÄ p2.json\n‚îî‚îÄ‚îÄ p3.json\nRun Batch Inference\npython infer_withanyone.py \\\n--eval_json_path <path to MultiID-Bench subset json> \\\n--data_root <path to MultiID-Bench subset images> \\\n--save_path <path to save results>  \\\n--use_matting True \\ # set to True when siglip_weight > 0.0\n--siglip_weight 0.0 \\ # Resemblance in Spirit vs Resemblance in Form, higher means more similar to reference\n--id_weight 1.0 \\ # usually, set it to 1 - id_weight, higher means more controllable\n--t5_path <path to xflux_text_encoders> \\\n--clip_path <path to clip-vit-large-patch14> \\\n--ipa_path <path to withanyone> \\\n--flux_path <path to flux1-dev>\nWhere the data_root should be p1/untar, p2/untar, or p3/ depending on which subset you want to evaluate. The eval_json_path should be the corresponding json file converted from the parquet file.\n‚öôÔ∏è Face Edit with FLUX.1 Kontext\nYou can use gradio_edit.py for face editing with FLUX.1 Kontext and WithAnyone.Ke.\nRun it with:\npython gradio_edit.py --flux_path <path to flux1-dev directory> --ipa_path <path to withanyone directory> \\\n--clip_path <path to clip-vit-large-patch14> \\\n--t5_path <path to xflux_text_encoders> \\\n--siglip_path <path to siglip-base-patch16-256-i18n> \\\n--model_type \"flux-kontext\"\nüìú License and Disclaimer\nThe code of WithAnyone is released under the Apache License 2.0, while the WithAnyone model and associated datasets are made available solely for non-commercial academic research purposes.\nLicense Terms:The WithAnyone model is distributed under the FLUX.1 [dev] Non-Commercial License v1.1.1. All underlying base models remain governed by their respective original licenses and terms, which shall continue to apply in full. Users must comply with all such applicable licenses when using this project.\nPermitted Use:This project may be used for lawful academic research, analysis, and non-commercial experimentation only. Any form of commercial use, redistribution for profit, or application that violates applicable laws, regulations, or ethical standards is strictly prohibited.\nUser Obligations:Users are solely responsible for ensuring that their use of the model and dataset complies with all relevant laws, regulations, institutional review policies, and third-party license terms.\nDisclaimer of Liability:The authors, developers, and contributors make no warranties, express or implied, regarding the accuracy, reliability, or fitness of this project for any particular purpose. They shall not be held liable for any damages, losses, or legal claims arising from the use or misuse of this project, including but not limited to violations of law or ethical standards by end users.\nAcceptance of Terms:By downloading, accessing, or using this project, you acknowledge and agree to be bound by the applicable license terms and legal requirements, and you assume full responsibility for all consequences resulting from your use.\nüåπ Acknowledgement\nWe thank the following prior art for their excellent open source work:\nPuLID\nUNO\nUniPortrait\nInfiniteYou\nDreamO\nUMO\nüìë Citation\nIf you find this project useful in your research, please consider citing:\n@article{xu2025withanyone,\ntitle={WithAnyone: Towards Controllable and ID-Consistent Image Generation},\nauthor={Hengyuan Xu and Wei Cheng and Peng Xing and Yixiao Fang and Shuhan Wu and Rui Wang and Xianfang Zeng and Gang Yu and Xinjun Ma and Yu-Gang Jiang},\njournal={arXiv preprint arxiv:2510.14975},\nyear={2025}\n}",
    "akhaliq/veo3.1-fast": "veo 3.1 inference provider integration, see docs: https://huggingface.co/docs/inference-providers/en/index",
    "arcprize/trm_arc_prize_verification": "Replication Results\nEnvironment Setup\nDataset preprocessing\nARC-AGI-1\nARC-AGI-2\nTraining\nARC-AGI-2\nARC-AGI-2\nFor multi-node training:\nThis repository contains the TinyRecursiveModels checkpoints for arc v1 public eval and arc v2 public eval that were trained for the performance verification. They were trained using the code and recipe of the official TRM repository. We had to adapt the environment setup as detailed below. We provide these checkpoints for transparency and to facilitate further research. We did not contribute to the TRM reserach nor maintain the TRM code. For any questions, please reach out to the TRM maintainers.\nTRM writes checkpoints as torch state_dicts. The subdirectories arc_v1_public and arc_v2_public contain the final checkpoints step_<final-step>, which can be loaded with the load_checkpoint or by providing the checkpoint path as load_checkpoint=path/to/checkpoint. For reference, see the PretrainConfig in pretrain.py.\nReplication Results\nTiny Recursion Model (TRM) results on ARC-AGI\nARC-AGI-1: 40%, $1.76/task\nARC-AGI-2: 6.2%, $2.10/task\nTweet: https://x.com/arcprize/status/1978872651180577060\nLeaderboard: https://arcprize.org/leaderboard\nEnvironment Setup\n# use uv for venv\nsudo snap install astral-uv --classic\nuv venv .venv -p 3.12\nsource .venv/bin/activate\n# install python-dev for adam atan2\nsudo apt install python3-dev -y\n# install torch\nPYTORCH_INDEX_URL=https://download.pytorch.org/whl/cu128\nuv pip install torch torchvision torchaudio --index-url $PYTORCH_INDEX_URL\n# install dependencies + adam atan\nuv pip install packaging ninja wheel setuptools setuptools-scm\nuv pip install --no-cache-dir --no-build-isolation adam-atan2\n# test torch, cuda and AdamAtan2\npython\nimport torch\nt = torch.tensor([0,1,2]).to('cuda')\nfrom adam_atan2 import AdamATan2\n# install remaining dependencies\nuv pip install -r requirements.txt\nDataset preprocessing\nThe repository already contains the raw data, but it needs to be preprocessed. Run the following commands to preprocess the v1 and v2 datasets to make predictions for the public eval datasets.\nARC-AGI-1\npython -m dataset.build_arc_dataset \\\n--input-file-prefix kaggle/combined/arc-agi \\\n--output-dir data/arc1concept-aug-1000 \\\n--subsets training evaluation concept \\\n--test-set-name evaluation\nARC-AGI-2\npython -m dataset.build_arc_dataset \\\n--input-file-prefix kaggle/combined/arc-agi \\\n--output-dir data/arc2concept-aug-1000 \\\n--subsets training2 evaluation2 concept \\\n--test-set-name evaluation2\nTraining\nTo reproduce the checkpoints, run the following two training runs on a single 8:H100 node. Each run takes ~20-30h. To speed it up, instructions for multi-node training are below.\nARC-AGI-2\nrun_name=\"trm_arc_v1_public\"\ntorchrun --nproc-per-node 8 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc1concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\nARC-AGI-2\nrun_name=\"trm_arc_v2_public\"\ntorchrun --nproc-per-node 8 --rdzv_backend=c10d --rdzv_endpoint=localhost:0 --nnodes=1 pretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc2concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True\nFor multi-node training:\nexport MAIN_ADDR=<MAIN_IP>\nexport MAIN_PORT=29500\nexport NNODES=2\nexport GPUS_PER_NODE=8\nexport OMP_NUM_THREADS=8\nexport NCCL_PORT_RANGE=40000-40050\nrun_name=\"arc_v1_public_2_nodes\"\n# on each node:\nexport NODE_RANK=0\ntorchrun \\\n--nnodes $NNODES \\\n--node_rank $NODE_RANK \\\n--nproc_per_node $GPUS_PER_NODE \\\n--rdzv_backend c10d \\\n--rdzv_endpoint $MAIN_ADDR:$MAIN_PORT \\\npretrain.py \\\narch=trm \\\ndata_paths=\"[data/arc1concept-aug-1000]\" \\\narch.L_layers=2 \\\narch.H_cycles=3 arch.L_cycles=4 \\\n+run_name=${run_name} ema=True \\\neval_interval=50000",
    "lightx2v/Wan2.1-Distill-Models": "üé¨ Wan2.1 Distilled Models\nüåü What's Special?\n‚ö° Ultra-Fast Generation\nüéØ Flexible Options\nüíæ Memory Efficient\nüîß Easy Integration\nüì¶ Model Catalog\nüé• Model Types\nüéØ Precision Variants\nüìù Naming Convention\nüöÄ Usage\nCommunity\n‚ö†Ô∏è Important Notes\nüé¨ Wan2.1 Distilled Models\n‚ö° High-Performance Video Generation with 4-Step Inference\nDistillation-accelerated versions of Wan2.1 - Dramatically faster while maintaining exceptional quality\nüåü What's Special?\n‚ö° Ultra-Fast Generation\n4-step inference (vs traditional 50+ steps)\nUp to 2x faster than ComfyUI\nReal-time video generation capability\nüéØ Flexible Options\nMultiple resolutions (480P/720P)\nVarious precision formats (BF16/FP8/INT8)\nI2V and T2V support\nüíæ Memory Efficient\nFP8/INT8: ~50% size reduction\nCPU offload support\nOptimized for consumer GPUs\nüîß Easy Integration\nCompatible with LightX2V framework\nComfyUI support available\nSimple configuration files\nüì¶ Model Catalog\nüé• Model Types\nüñºÔ∏è Image-to-Video (I2V)\nTransform still images into dynamic videos\nüì∫ 480P Resolution\nüé¨ 720P Resolution\nüìù Text-to-Video (T2V)\nGenerate videos from text descriptions\nüöÄ 14B Parameters\nüé® High-quality synthesis\nüéØ Precision Variants\nPrecision\nModel Identifier\nModel Size\nFramework\nQuality vs Speed\nüèÜ BF16\nlightx2v_4step\n~28-32 GB\nLightX2V\n‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Highest quality\n‚ö° FP8\nscaled_fp8_e4m3_lightx2v_4step\n~15-17 GB\nLightX2V\n‚≠ê‚≠ê‚≠ê‚≠ê Excellent balance\nüéØ INT8\nint8_lightx2v_4step\n~15-17 GB\nLightX2V\n‚≠ê‚≠ê‚≠ê‚≠ê Fast & efficient\nüî∑ FP8 ComfyUI\nscaled_fp8_e4m3_lightx2v_4step_comfyui\n~15-17 GB\nComfyUI\n‚≠ê‚≠ê‚≠ê ComfyUI ready\nüìù Naming Convention\n# Pattern: wan2.1_{task}_{resolution}_{precision}.safetensors\n# Examples:\nwan2.1_i2v_720p_lightx2v_4step.safetensors                          # 720P I2V - BF16\nwan2.1_i2v_720p_scaled_fp8_e4m3_lightx2v_4step.safetensors         # 720P I2V - FP8\nwan2.1_i2v_480p_int8_lightx2v_4step.safetensors                    # 480P I2V - INT8\nwan2.1_t2v_14b_scaled_fp8_e4m3_lightx2v_4step_comfyui.safetensors  # T2V - FP8 ComfyUI\nüí° Explore all models: Browse Full Model Collection ‚Üí\nüöÄ Usage\nLightX2V is a high-performance inference framework optimized for these models, approximately 2x faster than ComfyUI with better quantization accuracy. Highly recommended!\nQuick Start\nDownload model (720P I2V FP8 example)\nhuggingface-cli download lightx2v/Wan2.1-Distill-Models \\\n--local-dir ./models/wan2.1_i2v_720p \\\n--include \"wan2.1_i2v_720p_scaled_fp8_e4m3_lightx2v_4step.safetensors\"\nClone LightX2V repository\ngit clone https://github.com/ModelTC/LightX2V.git\ncd LightX2V\nInstall dependencies\npip install -r requirements.txt\nOr refer to Quick Start Documentation to use docker\nSelect and modify configuration file\nChoose the appropriate configuration based on your GPU memory:\nFor 80GB+ GPU (A100/H100)\nI2V: wan_i2v_distill_4step_cfg.json\nT2V: wan_t2v_distill_4step_cfg.json\nFor 24GB+ GPU (RTX 4090)\nI2V: wan_i2v_distill_4step_cfg_4090.json\nT2V: wan_t2v_distill_4step_cfg_4090.json\nRun inference\ncd scripts\nbash wan/run_wan_i2v_distill_4step_cfg.sh\nDocumentation\nQuick Start Guide: LightX2V Quick Start\nComplete Usage Guide: LightX2V Model Structure Documentation\nConfiguration Guide: Configuration Files\nQuantization Usage: Quantization Documentation\nParameter Offload: Offload Documentation\nPerformance Advantages\n‚ö° Fast: Approximately 2x faster than ComfyUI\nüéØ Optimized: Deeply optimized for distilled models\nüíæ Memory Efficient: Supports CPU offload and other memory optimization techniques\nüõ†Ô∏è Flexible: Supports multiple quantization formats and configuration options\nCommunity\nIssues: https://github.com/ModelTC/LightX2V/issues\n‚ö†Ô∏è Important Notes\nAdditional Components: These models only contain DIT weights. You also need:\nT5 text encoder\nCLIP vision encoder\nVAE encoder/decoder\nTokenizers\nRefer to LightX2V Documentation for how to organize the complete model directory.\nIf you find this project helpful, please give us a ‚≠ê on GitHub",
    "infly/Infinity-Parser-7B": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nInfinity-Parser-7B\nIntroduction\nKey Features\nArchitecture\nPerformance\nolmOCR-bench\nOmniDocBench\nTable Recognition\nQuick Start\nVllm Inference\nUsing Transformers to Inference\nVisualization\nComparison Examples\nCitation\nLicense\nInfinity-Parser-7B\nüíª Github |\nüìä Dataset |\nüìÑ Paper |\nüöÄ Demo\nIntroduction\nWe develop Infinity-Parser, an end-to-end scanned document parsing model trained with reinforcement learning. By incorporating verifiable rewards based on layout and content, Infinity-Parser maintains the original document's structure and content with high fidelity. Extensive evaluations on benchmarks in cluding OmniDocBench, olmOCR-Bench, PubTabNet, and FinTabNet show that Infinity-Parser consistently achieves state-of-the-art performance across a broad range of document types, languages, and structural complexities, substantially outperforming both specialized document parsing systems and general-purpose vision-language models while preserving the model‚Äôs general multimodal understanding capability.\nKey Features\nLayoutRL Framework: a reinforcement learning framework that explicitly trains models to be layout-aware through verifiable multi-aspect rewards combining edit distance, paragraph accuracy, and reading order preservation.\nInfinity-Doc-400K Dataset: a large-scale dataset of 400K scanned documents that integrates high-quality synthetic data with diverse real-world samples, featuring rich layout variations and comprehensive structural annotations.\nInfinity-Parser Model: a VLM-based parser that achieves new state-of-the-art performance on OCR, table and formula extraction, and reading-order detection benchmarks in both English and Chinese, while maintaining nearly the same general multimodal understanding capability as the base model.\nArchitecture\nOverview of Infinity-Parser training framework. Our model is optimized via reinforcement finetuning with edit distance, layout, and order-based rewards.\nPerformance\nolmOCR-bench\nOmniDocBench\nTable Recognition\nQuick Start\nVllm Inference\nWe recommend using the vLLM backend for accelerated inference.\nIt supports image and PDF inputs, automatically parses the document content, and exports the results in Markdown format to a specified directory.\nBefore starting, make sure that PyTorch is correctly installed according to the official installation guide at https://pytorch.org/.\npip install .\nparser --model /path/model --input dir/PDF/Image --output output_folders --batch_size 128 --tp 1\nAdjust the tensor parallelism (tp) value ‚Äî 1, 2, or 4 ‚Äî and the batch size according to the number of GPUs and the available memory.\n[The information of result folder]\nThe result folder contains the following contents:\noutput_folders/\n‚îú‚îÄ‚îÄ <file_name>/output.md\n‚îú‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ ...\nUsing Transformers to Inference\nTransformers Inference Example\nimport torch\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nmodel_path = \"infly/Infinity-Parser-7B\"\nprompt = \"Please transform the document‚Äôs contents into Markdown format.\"\nprint(\"Loading model and processor...\")\n# Default: Load the model on the available device(s)\n# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n#     model_path, torch_dtype=\"auto\", device_map=\"auto\"\n# )\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\nmodel_path,\ntorch_dtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\",\ndevice_map=\"auto\",\n)\n# Default processor\n# processor = AutoProcessor.from_pretrained(model_path)\n# Recommended processor\nmin_pixels = 256 * 28 * 28   # 448 * 448\nmax_pixels = 2304 * 28 * 28  # 1344 * 1344\nprocessor = AutoProcessor.from_pretrained(model_path, min_pixels=min_pixels, max_pixels=max_pixels)\nprint(\"Preparing messages for inference...\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": \"https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png\",\n},\n{\"type\": \"text\", \"text\": prompt},\n],\n}\n]\ntext = processor.apply_chat_template(\nmessages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\ntext=[text],\nimages=image_inputs,\nvideos=video_inputs,\npadding=True,\nreturn_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\nprint(\"Generating results...\")\ngenerated_ids = model.generate(**inputs, max_new_tokens=4096)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nVisualization\nComparison Examples\nCitation\n@misc{wang2025infinityparserlayoutaware,\ntitle={Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing},\nauthor={Baode Wang and Biao Wu and Weizhen Li and Meng Fang and Zuming Huang and Jun Huang and Haozhe Wang and Yanjie Liang and Ling Chen and Wei Chu and Yuan Qi},\nyear={2025},\neprint={2506.03197},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2506.03197},\n}\nLicense\nThis model is licensed under apache-2.0.",
    "nvidia/nemoretriever-graphic-elements-v1": "Nemoretriever Graphic Element v1\nModel Overview\nDescription\nLicense/Terms of use\nTeam\nDeployment Geography\nUse Case\nRelease Date\nReferences\nModel Architecture\nInput\nOutput\nUsage\nModel Version(s):\nTraining and Evaluation Datasets:\nTraining Dataset\nEvaluation Dataset\nEthical Considerations\nBias\nExplainability\nPrivacy\nSafety\nNemoretriever Graphic Element v1\nModel Overview\nPreview of the model output on the example image.\nThe input of this model is expected to be a chart image. You can use the Nemoretriever Page Element v3 to detect and crop such images.\nDescription\nThe NeMo Retriever Graphic Elements v1 model is a specialized object detection system designed to identify and extract key elements from charts and graphs. Based on YOLOX, an anchor-free version of YOLO (You Only Look Once), this model combines a simpler architecture with enhanced performance. While the underlying technology builds upon work from Megvii Technology, we developed our own base model through complete retraining rather than using pre-trained weights.\nThe model excels at detecting and localizing various graphic elements within chart images, including titles, axis labels, legends, and data point annotations. This capability makes it particularly valuable for document understanding tasks and automated data extraction from visual content.\nThis model is ready for commercial/non-commercial use.\nWe are excited to announce the open sourcing of this commercial model. For users interested in deploying this model in production environments, it is also available via the model API in NVIDIA Inference Microservices (NIM) at nemoretriever-graphic-elements-v1.\nLicense/Terms of use\nThe use of this model is governed by the NVIDIA Open Model License Agreement and the use of the post-processing scripts are licensed under Apache 2.0.\nTeam\nTheo Viel\nBo Liu\nDarragh Hanley\nEven Oldridge\nCorrespondence to Theo Viel (tviel@nvidia.com) and Bo Liu (boli@nvidia.com)\nDeployment Geography\nGlobal\nUse Case\nThe NeMo Retriever Graphic Elements v1 is designed for automating extraction of graphic elements of charts in enterprise documents. Key applications include:\nEnterprise document extraction, embedding and indexing\nAugmenting Retrieval Augmented Generation (RAG) workflows with multimodal retrieval\nData extraction from legacy documents and reports\nRelease Date\n10/23/2025 via https://huggingface.co/nvidia/nemoretriever-graphic-elements-v1\nReferences\nYOLOX paper: https://arxiv.org/abs/2107.08430\nYOLOX repo: https://github.com/Megvii-BaseDetection/YOLOX\nCACHED paper: https://arxiv.org/abs/2305.04151\nCACHED repo : https://github.com/pengyu965/ChartDete\nTechnical blog: https://developer.nvidia.com/blog/approaches-to-pdf-data-extraction-for-information-retrieval/\nModel Architecture\nArchitecture Type: YOLOX\nNetwork Architecture: DarkNet53 Backbone + FPN Decoupled head (one 1x1 convolution + 2 parallel 3x3 convolutions (one for the classification and one for the bounding box prediction). YOLOX is a single-stage object detector that improves on Yolo-v3.\nThis model was developed based on the Yolo architecture\nNumber of model parameters: 5.4e7\nInput\nInput Type(s): Image\nInput Format(s): Red, Green, Blue (RGB)\nInput Parameters: Two-Dimensional (2D)\nOther Properties Related to Input: Image size resized to (1024, 1024)\nOutput\nOutput Type(s): Array\nOutput Format: A dictionary of dictionaries containing np.ndarray objects. The outer dictionary has entries for each sample (page), and the inner dictionary contains a list of dictionaries, each with a bounding box (np.ndarray), class label, and confidence score for that page.\nOutput Parameters: One-Dimensional (1D)\nOther Properties Related to Output: The output contains bounding boxes, detection confidence scores, and object classes (chart title, x/y axis titles and labels, legend title and labels, marker labels, value labels and other texts). The thresholds used for non-maximum suppression are conf_thresh=0.01 and iou_thresh=0.25.\nOutput Classes:\nChart title\nTitle or caption associated to the chart\nx-axis title\nTitle associated to the x axis\ny-axis title\nTitle associated to the y axis\nx-axis label(s)\nLabels associated to the x axis\ny-axis label(s)\nLabels associated to the y axis\nLegend title\nTitle of the legend\nLegend label(s)\nLabels associated to the legend\nMarker label(s)\nLabels associated to markers\nValue label(s)\nLabels associated to values\nOther\nMiscellaneous other text components\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA‚Äôs hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nUsage\nThe model requires torch, and the custom code available in this repository.\nClone the repository\nMake sure git-lfs is installed (https://git-lfs.com)\ngit lfs install\nUsing https\ngit clone https://huggingface.co/nvidia/nemoretriever-graphic-elements-v1\nOr using ssh\ngit clone git@hf.co:nvidia/nemoretriever-graphic-elements-v1\nRun the model using the following code:\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom model import define_model\nfrom utils import plot_sample, postprocess_preds_graphic_element, reformat_for_plotting\n# Load image\npath = \"./example.png\"\nimg = Image.open(path).convert(\"RGB\")\nimg = np.array(img)\n# Load model\nmodel = define_model(\"graphic_element_v1\")\n# Inference\nwith torch.inference_mode():\nx = model.preprocess(img)\npreds = model(x, img.shape)[0]\nprint(preds)\n# Post-processing\nboxes, labels, scores = postprocess_preds_graphic_element(preds, model.threshold, model.labels)\n# Plot\nboxes_plot, confs = reformat_for_plotting(boxes, labels, scores, img.shape, model.num_classes)\nplt.figure(figsize=(15, 10))\nplot_sample(img, boxes_plot, confs, labels=model.labels)\nplt.show()\nNote that this repository only provides minimal code to infer the model.\nIf you wish to do additional training, refer to the original repo.\nAdvanced post-processing\nAdditional post-processing might be required to use the model as part of a data extraction pipeline.\nWe provide examples in the notebook Demo.ipynb.\nModel Version(s):\nnemoretriever-graphic-elements-v1\nTraining and Evaluation Datasets:\nTraining Dataset\nData Modality: Image\nImage Training Data Size: Less than a Million Images\nData collection method by dataset: Automated\nLabeling method by dataset: Hybrid: Automated, Human\nPretraining (by NVIDIA): 118,287 images of the COCO train2017 dataset\nFinetuning (by NVIDIA): 5,614 images from the PubMed Central (PMC) Chart Dataset. 9,091 images from the DeepRule Dataset with annotations obtained using the CACHED model\nNumber of bounding boxes per class:\nLabel\nImages\nBoxes\nchart_title\n9,487\n18,754\nx_title\n5,995\n9,152\ny_title\n8,487\n12,893\nxlabel\n13,227\n217,820\nylabel\n12,983\n172,431\nlegend_title\n168\n209\nlegend_label\n9,812\n59,044\nmark_label\n660\n2,887\nvalue_label\n3,573\n65,847\nother\n3,717\n29,565\nTotal\n14,143\n588,602\nEvaluation Dataset\nResults were evaluated using the PMC Chart dataset. The Mean Average Precision (mAP) was used as the evaluation metric to measure the model's ability to correctly identify and localize objects across different confidence thresholds.\nNumber of bounding boxes and images per class:\nLabel\nImages\nBoxes\nchart_title\n38\n38\nx_title\n404\n437\ny_title\n502\n505\nxlabel\n553\n4,091\nylabel\n534\n3,944\nlegend_title\n17\n19\nlegend_label\n318\n1,077\nmark_label\n42\n219\nvalue_label\n52\n726\nother\n113\n464\nTotal\n560\n11,520\nData collection method by dataset: Hybrid: Automated, Human\nLabeling method by dataset: Hybrid: Automated, Human\nProperties: The validation dataset is the same as the PMC Chart dataset.\nPer-class Performance Metrics:\nClass\nAP (%)\nAR (%)\nchart_title\n82.38\n93.16\nx_title\n88.77\n92.31\ny_title\n89.48\n92.32\nxlabel\n85.04\n88.93\nylabel\n86.22\n89.40\nother\n55.14\n79.48\nlegend_label\n84.09\n88.07\nlegend_title\n60.61\n68.42\nmark_label\n49.31\n73.61\nvalue_label\n62.66\n68.32\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nFor more detailed information on ethical considerations for this model, please see the Explainability, Bias, Safety & Security, and Privacy sections below.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nBias\nField\nResponse\nParticipation considerations from adversely impacted groups protected classes in model design and testing\nNone\nMeasures taken to mitigate against unwanted bias\nNone\nExplainability\nField\nResponse\nIntended Application & Domain:\nObject Detection\nModel Type:\nYOLOX-architecture for detection of graphic elements within images of charts.\nIntended User:\nEnterprise developers, data scientists, and other technical users who need to extract textual elements from charts and graphs.\nOutput:\nAfter post-processing, the output is three numpy array that contains the detections: boxes [N x 4] (format is normalized (x_min, y_min, x_max, y_max)), associated classes: labels [N] and confidence scores: scores [N].\nDescribe how the model works:\nFinds and identifies objects in images by first dividing the image into a grid. For each section of the grid, the model uses a series of neural networks to extract visual features and simultaneously predict what objects are present (in this case \"chart title\" or \"axis label\" etc.) and exactly where they are located in that section, all in a single pass through the image.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of:\nNot Applicable\nTechnical Limitations & Mitigation:\nThe model may not generalize to unknown chart types/formats. Further fine-tuning might be required for such images.\nVerified to have met prescribed NVIDIA quality standards:\nYes\nPerformance Metrics:\nMean Average Precision, detectionr recall and visual inspection\nPotential Known Risks:\nThis model may not always detect all elements in a document.\nLicensing & Terms of Use:\nUse of this model is governed by NVIDIA Open Model License Agreement and the use of the post-processing scripts are licensed under Apache 2.0.\nPrivacy\nField\nResponse\nGeneratable or reverse engineerable personal data?\nNo\nPersonal data used to create this model?\nNo\nWas consent obtained for any personal data used?\nNot Applicable\nHow often is the dataset reviewed?\nBefore Release\nIs there provenance for all datasets used in training?\nYes\nDoes data labeling (annotation, metadata) comply with privacy laws?\nYes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?\nNo, not possible with externally-sourced data.\nApplicable Privacy Policy\nhttps://www.nvidia.com/en-us/about-nvidia/privacy-policy/\nSafety\nField\nResponse\nModel Application Field(s):\nObject Detection for Retrieval, focused on Enterprise\nDescribe the life critical impact (if present).\nNot Applicable\nUse Case Restrictions:\nAbide by NVIDIA Open Model License Agreement and the use of the post-processing scripts are licensed under Apache 2.0.\nModel and dataset restrictions:\nThe Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training, and dataset license constraints adhered to.",
    "huawei-csl/Qwen3-1.7B-3bit-ASINQ": "A-SINQ 3-bit Quantized Qwen3-1.7B model\nModel Details\nQuantization Details\nüöÄ Usage\nPrerequisite\nUsage example\nüßæ How to Cite This Work\nüêô Github¬†¬† | ¬†¬†üìÑ Paper\nA-SINQ 3-bit Quantized Qwen3-1.7B model\nThis repository contains the official 3-bit quantized version of the Qwen3-1.7B model using the calibrated version of SINQ (Sinkhorn-Normalized Quantization) method.SINQ is a novel, fast and high-quality quantization method designed to make any Large Language Models smaller while keeping their accuracy almost intact.\nTo support the project please put a star ‚≠ê in the official SINQ github repository.\nModel Details\nModel Name: Qwen3-1.7B-3bit-ASINQ\nBase Model: Qwen/Qwen3-1.7B\nTask: Text Generation\nFramework: PyTorch / Transformers\nLicense: Apache-2.0\nQuantized By: Huawei - Computing Systems Lab\nQuantization Details\nQuantization Method:  A-SINQ (Sinkhorn-Normalized Quantization)\nPrecision: INT3\nGroup Size:  64\nFramework:  PyTorch\nQuantization Library: sinq\nüöÄ Usage\nPrerequisite\nBefore running the quantization script, make sure the SINQ library is installed.\nInstallation instructions and setup details are available in the SINQ official github repository.\nUsage example\nYou can load and use the model with our wrapper based on the ü§ó Transformers library:\nfrom transformers import AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nmodel_name = \"huawei-csl/Qwen3-1.7B-3bit-ASINQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsinq_model = AutoSINQHFModel.from_quantized_safetensors(\nmodel_name,\ndevice=\"cuda:0\",\ncompute_dtype=torch.bfloat16\n)\nprompt = \"Explain neural network quantization in one sentence.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\nwith torch.inference_mode():\nout_ids = sinq_model.generate(**inputs, max_new_tokens=32, do_sample=False)\nprint(tokenizer.decode(out_ids[0], skip_special_tokens=True))\nüß© Quantization Process\nThe quantized model was obtained using the SINQ quantization library, following the steps below:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nfrom sinq.sinqlinear import BaseQuantizeConfig\n# Load base model\nbase_model_name = \"Qwen/Qwen3-1.7B\"\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=\"float16\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n# Apply 3-bit SINQ quantization\nquant_cfg = BaseQuantizeConfig(\nnbits=3,            # quantization bit-width\ngroup_size=64,     # group size\ntiling_mode=\"1D\",   # tiling strategy\nmethod=\"asinq\"       # quantization method (\"asinq\" for the calibrated version)\n)\nqmodel = AutoSINQHFModel.quantize_model(\nmodel,\ntokenizer=tokenizer,\nquant_config=quant_cfg,\ncompute_dtype=torch.bfloat16,\ndevice=\"cuda:0\"\n)\nReproducibility Note: This model was quantized using the SINQ implementation from commit 14ad847 of the SINQ repository.\nüßæ How to Cite This Work\nIf you find SINQ useful in your research or applications, please\nPut a star ‚≠ê in the official SINQ github repository.\nCite our paper:\n@misc{muller2025sinq,\ntitle={SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights},\nauthor={Lorenz K. Muller and Philippe Bich and Jiawei Zhuang and Ahmet Celik and Luca Benfenati and Lukas Cavigelli},\nyear={2025},\neprint={2509.22944},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={http://arxiv.org/abs/2509.22944}\n}",
    "huawei-csl/Qwen3-32B-4bit-SINQ": "SINQ 4-bit Quantized Qwen3-32B model\nModel Details\nQuantization Details\nüöÄ Usage\nPrerequisite\nUsage example\nüßæ How to Cite This Work\nüêô Github¬†¬† | ¬†¬†üìÑ Paper\nSINQ 4-bit Quantized Qwen3-32B model\nThis repository contains the official 4-bit quantized version of the Qwen3-32B model using the SINQ (Sinkhorn-Normalized Quantization) method.SINQ is a novel, fast and high-quality quantization method designed to make any Large Language Models smaller while keeping their accuracy almost intact.\nTo support the project please put a star ‚≠ê in the official SINQ github repository.\nModel Details\nModel Name: Qwen3-32B-4bit-SINQ\nBase Model: Qwen/Qwen3-32B\nTask: Text Generation\nFramework: PyTorch / Transformers\nLicense: Apache-2.0\nQuantized By: Huawei - Computing Systems Lab\nQuantization Details\nQuantization Method:  SINQ (Sinkhorn-Normalized Quantization)\nPrecision: INT4\nGroup Size:  64\nFramework:  PyTorch\nQuantization Library: sinq\nüöÄ Usage\nPrerequisite\nBefore running the quantization script, make sure the SINQ library is installed.\nInstallation instructions and setup details are available in the SINQ official github repository.\nUsage example\nYou can load and use the model with our wrapper based on the ü§ó Transformers library:\nfrom transformers import AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nmodel_name = \"huawei-csl/Qwen3-32B-4bit-SINQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsinq_model = AutoSINQHFModel.from_quantized_safetensors(\nmodel_name,\ndevice=\"cuda:0\",\ncompute_dtype=torch.bfloat16\n)\nprompt = \"Explain neural network quantization in one sentence.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\nwith torch.inference_mode():\nout_ids = sinq_model.generate(**inputs, max_new_tokens=32, do_sample=False)\nprint(tokenizer.decode(out_ids[0], skip_special_tokens=True))\nüß© Quantization Process\nThe quantized model was obtained using the SINQ quantization library, following the steps below:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nfrom sinq.sinqlinear import BaseQuantizeConfig\n# Load base model\nbase_model_name = \"Qwen/Qwen3-32B\"\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=\"float16\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n# Apply 4-bit SINQ quantization\nquant_cfg = BaseQuantizeConfig(\nnbits=4,            # quantization bit-width\ngroup_size=64,     # group size\ntiling_mode=\"1D\",   # tiling strategy\nmethod=\"sinq\"       # quantization method (\"asinq\" for the calibrated version)\n)\nqmodel = AutoSINQHFModel.quantize_model(\nmodel,\ntokenizer=tokenizer,\nquant_config=quant_cfg,\ncompute_dtype=torch.bfloat16,\ndevice=\"cuda:0\"\n)\nReproducibility Note: This model was quantized using the SINQ implementation from commit 14ad847 of the SINQ repository.\nüßæ How to Cite This Work\nIf you find SINQ useful in your research or applications, please\nPut a star ‚≠ê in the official SINQ github repository.\nCite our paper:\n@misc{muller2025sinq,\ntitle={SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights},\nauthor={Lorenz K. Muller and Philippe Bich and Jiawei Zhuang and Ahmet Celik and Luca Benfenati and Lukas Cavigelli},\nyear={2025},\neprint={2509.22944},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={http://arxiv.org/abs/2509.22944}\n}",
    "huawei-csl/Qwen3-32B-4bit-ASINQ": "A-SINQ 4-bit Quantized Qwen3-32B model\nModel Details\nQuantization Details\nüöÄ Usage\nPrerequisite\nUsage example\nüßæ How to Cite This Work\nüêô Github¬†¬† | ¬†¬†üìÑ Paper\nA-SINQ 4-bit Quantized Qwen3-32B model\nThis repository contains the official 4-bit quantized version of the Qwen3-32B model using the calibrated version of SINQ (Sinkhorn-Normalized Quantization) method.SINQ is a novel, fast and high-quality quantization method designed to make any Large Language Models smaller while keeping their accuracy almost intact.\nTo support the project please put a star ‚≠ê in the official SINQ github repository.\nModel Details\nModel Name: Qwen3-32B-4bit-ASINQ\nBase Model: Qwen/Qwen3-32B\nTask: Text Generation\nFramework: PyTorch / Transformers\nLicense: Apache-2.0\nQuantized By: Huawei - Computing Systems Lab\nQuantization Details\nQuantization Method:  A-SINQ (Sinkhorn-Normalized Quantization)\nPrecision: INT4\nGroup Size:  64\nFramework:  PyTorch\nQuantization Library: sinq\nüöÄ Usage\nPrerequisite\nBefore running the quantization script, make sure the SINQ library is installed.\nInstallation instructions and setup details are available in the SINQ official github repository.\nUsage example\nYou can load and use the model with our wrapper based on the ü§ó Transformers library:\nfrom transformers import AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nmodel_name = \"huawei-csl/Qwen3-32B-4bit-ASINQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsinq_model = AutoSINQHFModel.from_quantized_safetensors(\nmodel_name,\ndevice=\"cuda:0\",\ncompute_dtype=torch.bfloat16\n)\nprompt = \"Explain neural network quantization in one sentence.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\nwith torch.inference_mode():\nout_ids = sinq_model.generate(**inputs, max_new_tokens=32, do_sample=False)\nprint(tokenizer.decode(out_ids[0], skip_special_tokens=True))\nüß© Quantization Process\nThe quantized model was obtained using the SINQ quantization library, following the steps below:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sinq.patch_model import AutoSINQHFModel\nfrom sinq.sinqlinear import BaseQuantizeConfig\n# Load base model\nbase_model_name = \"Qwen/Qwen3-32B\"\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=\"float16\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n# Apply 4-bit SINQ quantization\nquant_cfg = BaseQuantizeConfig(\nnbits=4,            # quantization bit-width\ngroup_size=64,     # group size\ntiling_mode=\"1D\",   # tiling strategy\nmethod=\"asinq\"       # quantization method (\"asinq\" for the calibrated version)\n)\nqmodel = AutoSINQHFModel.quantize_model(\nmodel,\ntokenizer=tokenizer,\nquant_config=quant_cfg,\ncompute_dtype=torch.bfloat16,\ndevice=\"cuda:0\"\n)\nReproducibility Note: This model was quantized using the SINQ implementation from commit 14ad847 of the SINQ repository.\nüßæ How to Cite This Work\nIf you find SINQ useful in your research or applications, please\nPut a star ‚≠ê in the official SINQ github repository.\nCite our paper:\n@misc{muller2025sinq,\ntitle={SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights},\nauthor={Lorenz K. Muller and Philippe Bich and Jiawei Zhuang and Ahmet Celik and Luca Benfenati and Lukas Cavigelli},\nyear={2025},\neprint={2509.22944},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={http://arxiv.org/abs/2509.22944}\n}",
    "ostris/qwen_image_edit_2509_shirt_design": "Qwen Image Edit 2509 - Shirt Design LoRA\nModel description\nTrigger words\nDownload model\nQwen Image Edit 2509 - Shirt Design LoRA\nPrompt\nput this design on their shirt\nPrompt\nput this design on their shirt\nPrompt\nput this design on their shirt\nPrompt\nput this design on their shirt\nModel description\nThis LoRA will put designs on people's shirts. This LoRA was trained while filming a tutorial Train a Qwen Image Edit 2509 LoRA with AI Toolkit <10GB VRAM. Check out that video for more info.\nTrigger words\nYou should use put this design on their shirt to trigger the image generation.\nDownload model\nDownload them in the Files & versions tab.",
    "patientxtr/WAN2.2-14B-Rapid-AllInOne-GGUF": "gguf version of https://huggingface.co/Phr00t/WAN2.2-14B-Rapid-AllInOne/blob/main/Mega-v7/wan2.2-rapid-mega-aio-nsfw-v7.safetensors",
    "nineninesix/kani-tts-370m-expo2025-osaka-ja": "KaniTTS EXPO2025 Osaka japanese\nOverview\nPerformance\nAudio Examples\nUse Cases\nLimitations\nOptimization Tips\nResources\nAcknowledgments\nResponsible Use\nContact\nKaniTTS EXPO2025 Osaka japanese\nA high-speed, high-fidelity Text-to-Speech model optimized for real-time conversational AI applications.\n„Äé„ÅÑ„ÅÆ„Å°Ëºù„ÅèÊú™Êù•Á§æ‰ºö„ÅÆ„Éá„Ç∂„Ç§„É≥„Äè„Å®„ÅÑ„ÅÜÂ§ßÈò™„ÉªÈñ¢Ë•ø‰∏áÂçö2025„ÅÆ„ÉÜ„Éº„Éû„ÇíÁ•ù„Åó„ÄÅ„Ç≠„É´„ÇÆ„Çπ„ÅÆ‰∫∫„ÄÖ„Åã„ÇâÊó•Êú¨„ÅÆÁöÜ„Åï„Åæ„Å∏ --ÂøÉ„Å®ÂøÉ „Çí„Å§„Å™„ÅêË¥à„ÇäÁâ©„Å®„Åó„Å¶„ÄÅ„Å©„ÅÜ„Åû„ÅäÂèó„ÅëÂèñ„Çä „Åè„Å†„Åï„ÅÑ„ÄÇ\nIn honor of Expo Osaka 2025 and its motto 'Designing Future Society for Our Lives,' we humbly present this gift from the people of the Kyrgyz Republic to the people of Japan - heart to heart.\nOverview\nKaniTTS uses a two-stage pipeline combining a large language model with an efficient audio codec for exceptional speed and audio quality. The architecture generates compressed token representations through a backbone LLM, then rapidly synthesizes waveforms via neural audio codec, achieving extremely low latency.\nKey Specifications:\nModel Size: 370M parameters\nSample Rate: 22kHz\nLanguages: Japanese\nLicense: Apache 2.0\nPerformance\nNvidia RTX 5090 Benchmarks:\nLatency: ~1 second to generate 15 seconds of audio\nMemory: 2GB GPU VRAM\nQuality Metrics: MOS 4.3/5 (naturalness), WER <5% (accuracy)\nPretraining:\nDataset: ~80k hours from LibriTTS, Common Voice, and Emilia\nHardware: 8x H100 GPUs, 45 hours training time on Lambda AI\nVoices Datasets\nhttps://huggingface.co/datasets/MomoyamaSawa/Voice-KusanagiNene\nAudio Examples\nText\nAudio\n„Åì„Çì„Å´„Å°„ÅØÔºÅ„Ç´„Éã„Å®Áî≥„Åó„Åæ„Åô„ÄÇÁßÅ„ÅØ„Éú„Ç§„Çπ„É¢„Éá„É´„Åß„ÅôÔºÅ‰Ωï„Å´„Å§„ÅÑ„Å¶„ÅäË©±„Åó„Åó„Åæ„Åó„Çá„ÅÜ„ÅãÔºü\n2025Âπ¥„ÅÆÂ§ßÈò™„ÉªÈñ¢Ë•ø‰∏áÂçö„ÅØÁ¥†Êô¥„Çâ„Åó„ÅÑ„Ç§„Éô„É≥„Éà„Åß„Åó„Åü„ÄÇ\n„Äå„ÅÑ„ÅÆ„Å°Ëºù„ÅèÊú™Êù•Á§æ‰ºö„ÅÆ„Éá„Ç∂„Ç§„É≥„Äç„Å®„ÅÑ„ÅÜ„ÉÜ„Éº„Éû„ÅåÂ§ö„Åè„ÅÆ‰∫∫„ÅÆÂøÉ„Å´ÊÆã„Çä„Åæ„Åó„Åü„ÄÇ\n‰∏ñÁïå‰∏≠„ÅÆÂõΩ„ÄÖ„ÅåÊú™Êù•„ÅÆÊäÄË°ì„ÇíÁ¥π‰ªã„Åó„Åæ„Åó„Åü„ÄÇ\nÂ∞è„Åï„Å™‰∏ÄÊ≠©„Åß„ÇÇ„ÄÅÂâç„Å´ÈÄ≤„ÇÅ„Å∞ÊôØËâ≤„ÅåÂ§â„Çè„Çä„Åæ„Åô„ÄÇ\n‰ΩïÊ∞ó„Å™„ÅÑÊó•Â∏∏„ÅÆ‰∏≠„Å´„ÇÇ„ÄÅÂøÉ„ÅåÊ∏©„Åæ„ÇãÁû¨Èñì„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\nUse Cases\nConversational AI: Real-time speech for chatbots and virtual assistants\nEdge/Server Deployment: Resource-efficient inference on affordable hardware\nAccessibility: Screen readers and language learning applications\nResearch: Fine-tuning for specific voices, accents, or emotions\nLimitations\nPerformance degrades with inputs exceeding 2000 tokens\nLimited expressivity without fine-tuning for specific emotions\nMay inherit biases from training data in prosody or pronunciation\nOptimization Tips\nMultilingual Performance: Continually pretrain on target language datasets and fine-tune NanoCodec\nBatch Processing: Use batches of 8-16 for high-throughput scenarios\nHardware: Optimized for NVIDIA Blackwell architecture GPUs\nResources\nModels:\nPretrained Model\nFine-tuned Model\nHuggingFace Space\nExamples:\nInference Example\nFine-tuning-code\nExample Dataset\nGitHub Repository\nLinks:\nWebsite\nContact Form\nAcknowledgments\nBuilt on top of LiquidAI LFM2 350M as the backbone and Nvidia NanoCodec for audio processing.\nResponsible Use\nProhibited activities include:\nIllegal content or harmful, threatening, defamatory, or obscene material\nHate speech, harassment, or incitement of violence\nGenerating false or misleading information\nImpersonating individuals without consent\nMalicious activities such as spamming, phishing, or fraud\nBy using this model, you agree to comply with these restrictions and all applicable laws.\nContact\nHave a question, feedback, or need support? Please fill out our contact form and we'll get back to you as soon as possible.",
    "ilintar/Qwen3-Next-80B-A3B-Instruct-GGUF": "Preliminary quants for the model (Q2_K_S is an early quant and is not imatrixed, the rest are).\nPerplexity scores (20 chunks):\nQ8_0: Final estimate: PPL = 8.1500 +/- 0.30810\nIQ4_NL: Final estimate: PPL = 8.2485 +/- 0.31326\nIQ3_XS: Final estimate: PPL = 8.3266 +/- 0.30716\nIQ2_M: Final estimate: PPL = 9.1081 +/- 0.33962\nIQ2_XXS: Final estimate: PPL = 10.2483 +/- 0.38654 (I'd guess for the desperate)",
    "taobao-mnn/Qwen3-VL-2B-Instruct-MNN": "Qwen3-VL-2B-Instruct-MNN\nIntroduction\nDownload\nUsage\nDocument\nQwen3-VL-2B-Instruct-MNN\nIntroduction\nThis model is a 4-bit quantized version of the MNN model exported from Qwen3-VL-2B-Instruct using llmexport.\nDownload\n# install huggingface\npip install huggingface\n# shell download\nhuggingface download --model 'taobao-mnn/Qwen3-VL-2B-Instruct-MNN' --local_dir 'path/to/dir'\n# SDK download\nfrom huggingface_hub import snapshot_download\nmodel_dir = snapshot_download('taobao-mnn/Qwen3-VL-2B-Instruct-MNN')\n# git clone\ngit clone https://www.modelscope.cn/taobao-mnn/Qwen3-VL-2B-Instruct-MNN\nUsage\n# clone MNN source\ngit clone https://github.com/alibaba/MNN.git\n# compile\ncd MNN\nmkdir build && cd build\ncmake .. -DMNN_LOW_MEMORY=true -DMNN_CPU_WEIGHT_DEQUANT_GEMM=true -DMNN_BUILD_LLM=true -DMNN_SUPPORT_TRANSFORMER_FUSE=true\nmake -j\n# run\n./llm_demo /path/to/Qwen3-VL-2B-Instruct-MNN/config.json prompt.txt\nDocument\nMNN-LLM",
    "alphaXiv/trm-model-sudoku": "TRM Model for Sudoku Solving\nModel Description\nIntended Use\nPrimary Use\nOut-of-Scope Use\nLimitations and Bias\nTraining Data\nEvaluation Results\nRepository\nTRM Model for Sudoku Solving\nModel Description\nThis is a Tiny Recursive Model (TRM) fine-tuned for solving Sudoku puzzles. The model uses recursive reasoning to fill in missing numbers in Sudoku grids.\nDeveloped by: alphaXiv\nModel type: TRM-MLP\nLanguage(s) (NLP): N/A (grid-based reasoning)\nLicense: MIT\nFinetuned from model: Custom TRM architecture\nIntended Use\nPrimary Use\nThis model is designed to solve Sudoku puzzles by predicting the correct numbers for empty cells in standard 9x9 Sudoku grids.\nOut-of-Scope Use\nNot intended for general NLP tasks, image processing, or other puzzle types.\nLimitations and Bias\nTrained only on standard 9x9 Sudoku puzzles\nMay not handle non-standard Sudoku variants\nPerformance depends on puzzle difficulty\nTraining Data\nThe model was trained on a dataset of Sudoku puzzles with extreme difficulty levels. The dataset includes:\nPartially filled 9x9 grids\nCorrect solutions\nDifficulty ratings\nEvaluation Results\nVariant\nMetric\nClaimed\nAchieved\nTRM-MLP\nAccuracy\n87.4%\n79.37% ¬± 0.12%\nTRM-Attention\nAccuracy\n74.7%\n73.66% ¬± 0.13%\nResults from independent reproduction study.\nRepository\nhttps://github.com/alphaXiv/TinyRecursiveModels",
    "alphaXiv/trm-model-arc-agi-1": "TRM Model for ARC-AGI-1\nModel Description\nIntended Use\nPrimary Use\nOut-of-Scope Use\nLimitations and Bias\nTraining Data\nEvaluation Results\nRepository\nTRM Model for ARC-AGI-1\nModel Description\nThis is a Tiny Recursive Model (TRM) fine-tuned for solving Abstract Reasoning Challenge (ARC-AGI) tasks. The model performs abstract reasoning to predict output grids from input grids.\nDeveloped by: alphaXiv\nModel type: TRM-Attention\nLanguage(s) (NLP): N/A (grid-based reasoning)\nLicense: MIT\nFinetuned from model: Custom TRM architecture\nIntended Use\nPrimary Use\nThis model is designed to solve ARC-AGI tasks by predicting the correct output grid transformation based on input grid patterns.\nOut-of-Scope Use\nNot intended for general NLP tasks, image generation, or other reasoning domains.\nLimitations and Bias\nTrained only on ARC-AGI training and evaluation sets\nMay not generalize to novel abstract reasoning tasks\nPerformance limited by training data diversity\nTraining Data\nThe model was trained on the ARC-AGI dataset, which includes:\nInput-output grid pairs\nVarious transformation patterns\nTraining and evaluation splits\nEvaluation Results\nMetric\nClaimed\nAchieved\nPass@2\n44.6%\n43.00% ¬± 0.16%\nResults from independent reproduction study.\nRepository\nhttps://github.com/alphaXiv/TinyRecursiveModels",
    "onnx-community/nanochat-d32-ONNX": "Transformers.js\nTransformers.js\nIf you haven't already, you can install the Transformers.js JavaScript library from NPM using:\nnpm i @huggingface/transformers\nYou can then use the model like this:\nimport { pipeline, TextStreamer } from \"@huggingface/transformers\";\n// Create a text generation pipeline\nconst generator = await pipeline(\n\"text-generation\",\n\"onnx-community/nanochat-d32-ONNX\",\n{ dtype: \"q4\" },\n);\n// Define the list of messages\nconst messages = [\n{ role: \"system\", content: \"You are a helpful assistant.\" },\n{ role: \"user\", content: \"What is the capital of France?\" },\n];\n// Generate a response\nconst output = await generator(messages, {\nmax_new_tokens: 512,\ndo_sample: false,\nstreamer: new TextStreamer(generator.tokenizer, { skip_prompt: true, skip_special_tokens: true}),\n});\nconsole.log(output[0].generated_text.at(-1).content);",
    "Thrillcrazyer/Qwen-7B_THIP": "Model Card for Qwen-7B_THIP\nQuick start\nTraining procedure\nFramework versions\nCitations\nModel Card for Qwen-7B_THIP\nThis model is a fine-tuned version of deepseek-ai/DeepSeek-R1-Distill-Qwen-7B on the DeepMath-103k dataset.\nIt has been trained using TRL.\nQuick start\nfrom transformers import pipeline\nquestion = \"If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?\"\ngenerator = pipeline(\"text-generation\", model=\"Thrillcrazyer/Qwen-7B_THIP\", device=\"cuda\")\noutput = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=128, return_full_text=False)[0]\nprint(output[\"generated_text\"])\nTraining procedure\nThis model was trained with GRPO, a method introduced in DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.\nFramework versions\nTRL: 0.24.0\nTransformers: 4.57.1\nPytorch: 2.8.0+cu128\nDatasets: 4.2.0\nTokenizers: 0.22.1\nCitations\nCite GRPO as:\n@article{shao2024deepseekmath,\ntitle        = {{DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}},\nauthor       = {Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},\nyear         = 2024,\neprint       = {arXiv:2402.03300},\n}\nCite TRL as:\n@misc{vonwerra2022trl,\ntitle        = {{TRL: Transformer Reinforcement Learning}},\nauthor       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallou{\\'e}dec},\nyear         = 2020,\njournal      = {GitHub repository},\npublisher    = {GitHub},\nhowpublished = {\\url{https://github.com/huggingface/trl}}\n}",
    "bartowski/inclusionAI_Ling-flash-2.0-GGUF": "Llamacpp imatrix Quantizations of Ling-flash-2.0 by inclusionAI\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nDownloading using huggingface-cli\nARM/AVX information\nWhich file should I choose?\nCredits\nLlamacpp imatrix Quantizations of Ling-flash-2.0 by inclusionAI\nUsing llama.cpp release b6810 for quantization.\nOriginal model: https://huggingface.co/inclusionAI/Ling-flash-2.0\nAll quants made using imatrix option with dataset from here combined with a subset of combined_all_small.parquet from Ed Addario here\nRun them in LM Studio\nRun them directly with llama.cpp, or any other llama.cpp based project\nPrompt format\n<role>SYSTEM</role>{system_prompt}\ndetailed thinking off<|role_end|><role>HUMAN</role>{prompt}<|role_end|><role>ASSISTANT</role><|role_end|><role>ASSISTANT</role>\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\nLing-flash-2.0-Q8_0.gguf\nQ8_0\n109.42GB\ntrue\nExtremely high quality, generally unneeded but max available quant.\nLing-flash-2.0-Q6_K.gguf\nQ6_K\n84.61GB\ntrue\nVery high quality, near perfect, recommended.\nLing-flash-2.0-Q5_K_M.gguf\nQ5_K_M\n73.32GB\ntrue\nHigh quality, recommended.\nLing-flash-2.0-Q5_K_S.gguf\nQ5_K_S\n71.03GB\ntrue\nHigh quality, recommended.\nLing-flash-2.0-Q4_1.gguf\nQ4_1\n64.64GB\ntrue\nLegacy format, similar performance to Q4_K_S but with improved tokens/watt on Apple silicon.\nLing-flash-2.0-Q4_K_L.gguf\nQ4_K_L\n63.10GB\ntrue\nUses Q8_0 for embed and output weights. Good quality, recommended.\nLing-flash-2.0-Q4_K_M.gguf\nQ4_K_M\n62.62GB\ntrue\nGood quality, default size for most use cases, recommended.\nLing-flash-2.0-Q4_K_S.gguf\nQ4_K_S\n60.37GB\ntrue\nSlightly lower quality with more space savings, recommended.\nLing-flash-2.0-Q4_0.gguf\nQ4_0\n59.29GB\ntrue\nLegacy format, offers online repacking for ARM and AVX CPU inference.\nLing-flash-2.0-IQ4_NL.gguf\nIQ4_NL\n58.35GB\ntrue\nSimilar to IQ4_XS, but slightly larger. Offers online repacking for ARM CPU inference.\nLing-flash-2.0-IQ4_XS.gguf\nIQ4_XS\n55.18GB\ntrue\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\nLing-flash-2.0-Q3_K_XL.gguf\nQ3_K_XL\n49.57GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\nLing-flash-2.0-Q3_K_L.gguf\nQ3_K_L\n49.01GB\nfalse\nLower quality but usable, good for low RAM availability.\nLing-flash-2.0-Q3_K_M.gguf\nQ3_K_M\n47.13GB\nfalse\nLow quality.\nLing-flash-2.0-IQ3_M.gguf\nIQ3_M\n47.13GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nLing-flash-2.0-Q3_K_S.gguf\nQ3_K_S\n44.90GB\nfalse\nLow quality, not recommended.\nLing-flash-2.0-IQ3_XS.gguf\nIQ3_XS\n42.48GB\nfalse\nLower quality, new method with decent performance, slightly better than Q3_K_S.\nLing-flash-2.0-IQ3_XXS.gguf\nIQ3_XXS\n40.85GB\nfalse\nLower quality, new method with decent performance, comparable to Q3 quants.\nLing-flash-2.0-Q2_K_L.gguf\nQ2_K_L\n36.88GB\nfalse\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\nLing-flash-2.0-Q2_K.gguf\nQ2_K\n36.25GB\nfalse\nVery low quality but surprisingly usable.\nLing-flash-2.0-IQ2_M.gguf\nIQ2_M\n32.41GB\nfalse\nRelatively low quality, uses SOTA techniques to be surprisingly usable.\nLing-flash-2.0-IQ2_S.gguf\nIQ2_S\n28.66GB\nfalse\nLow quality, uses SOTA techniques to be usable.\nLing-flash-2.0-IQ2_XS.gguf\nIQ2_XS\n28.53GB\nfalse\nLow quality, uses SOTA techniques to be usable.\nLing-flash-2.0-IQ2_XXS.gguf\nIQ2_XXS\n25.82GB\nfalse\nVery low quality, uses SOTA techniques to be usable.\nLing-flash-2.0-IQ1_M.gguf\nIQ1_M\n22.22GB\nfalse\nExtremely low quality, not recommended.\nLing-flash-2.0-IQ1_S.gguf\nIQ1_S\n21.45GB\nfalse\nExtremely low quality, not recommended.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nDownloading using huggingface-cli\nClick to view download instructions\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/inclusionAI_Ling-flash-2.0-GGUF --include \"inclusionAI_Ling-flash-2.0-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/inclusionAI_Ling-flash-2.0-GGUF --include \"inclusionAI_Ling-flash-2.0-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (inclusionAI_Ling-flash-2.0-Q8_0) or download them all in place (./)\nARM/AVX information\nPreviously, you would download Q4_0_4_4/4_8/8_8, and these would have their weights interleaved in memory in order to improve performance on ARM and AVX machines by loading up more data in one pass.\nNow, however, there is something called \"online repacking\" for weights. details in this PR. If you use Q4_0 and your hardware would benefit from repacking weights, it will do it automatically on the fly.\nAs of llama.cpp build b4282 you will not be able to run the Q4_0_X_X files and will instead need to use Q4_0.\nAdditionally, if you want to get slightly better quality for , you can use IQ4_NL thanks to this PR which will also repack the weights for ARM, though only the 4_4 for now. The loading time may be slower but it will result in an overall speed incrase.\nClick to view Q4_0_X_X information (deprecated\nI'm keeping this section to show the potential theoretical uplift in performance from using the Q4_0 with online repacking.\nClick to view benchmarks on an AVX2 system (EPYC7702)\nmodel\nsize\nparams\nbackend\nthreads\ntest\nt/s\n% (vs Q4_0)\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp512\n204.03 ¬± 1.03\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp1024\n282.92 ¬± 0.19\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\npp2048\n259.49 ¬± 0.44\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg128\n39.12 ¬± 0.27\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg256\n39.31 ¬± 0.69\n100%\nqwen2 3B Q4_0\n1.70 GiB\n3.09 B\nCPU\n64\ntg512\n40.52 ¬± 0.03\n100%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp512\n301.02 ¬± 1.74\n147%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp1024\n287.23 ¬± 0.20\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\npp2048\n262.77 ¬± 1.81\n101%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg128\n18.80 ¬± 0.99\n48%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg256\n24.46 ¬± 3.04\n83%\nqwen2 3B Q4_K_M\n1.79 GiB\n3.09 B\nCPU\n64\ntg512\n36.32 ¬± 3.59\n90%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp512\n271.71 ¬± 3.53\n133%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp1024\n279.86 ¬± 45.63\n100%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\npp2048\n320.77 ¬± 5.00\n124%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg128\n43.51 ¬± 0.05\n111%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg256\n43.35 ¬± 0.09\n110%\nqwen2 3B Q4_0_8_8\n1.69 GiB\n3.09 B\nCPU\n64\ntg512\n42.60 ¬± 0.31\n105%\nQ4_0_8_8 offers a nice bump to prompt processing and a small bump to text generation\nWhich file should I choose?\nClick here for details\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset.\nThank you ZeroWw for the inspiration to experiment with embed/output.\nThank you to LM Studio for sponsoring my work.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "ByteDance/Sa2VA-Qwen3-VL-4B": "Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos\nIntroduction\nSa2VA Family\nSa2VA Performance\nQuick Start\nCitation\nSa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos\n[üìÇ GitHub]\n[üìú Sa2VA paper]\n[üöÄ Quick Start]\nIntroduction\nSa2VA is an MLLM capable of question answering, visual prompt understanding, and dense object segmentation at both image and video levels. It achieves comparable performance to SOTA MLLMs Qwen2.5-VL and InternVL3 on question-answering benchmarks. Additionally, Sa2VA possesses the visual prompt understanding and dense object segmentation capabilities that SOTA MLLMs Qwen2.5-VL and InternVL3 lack. Sa2VA achieves SOTA performance on both image and video grounding and segmentation benchmarks.\nSa2VA Family\nWe built the Sa2VA series based on Qwen2.5/3-VL and InternVL2.5/3. In the following table, we provide some Sa2VA models built on Qwen2.5/3-VL and InternVL3.\nModel Name\nBase MLLM\nLanguage Part\nHF Link\nSa2VA-InternVL3-2B\nInternVL3-2B\nQwen2.5-1.5B\nü§ó link\nSa2VA-InternVL3-8B\nInternVL3-8B\nQwen2.5-7B\nü§ó link\nSa2VA-InternVL3-14B\nInternVL3-14B\nQwen2.5-14B\nü§ó link\nSa2VA-Qwen2_5-VL-3B\nQwen2.5-VL-3B-Instruct\nQwen2.5-3B\nü§ó link\nSa2VA-Qwen2_5-VL-7B\nQwen2.5-VL-7B-Instruct\nQwen2.5-7B\nü§ó link\nSa2VA-Qwen3-VL-4B\nQwen3-VL-4B-Instruct\nQwen3-4B\nü§ó link\nSa2VA Performance\nModel Name\nMME\nMMBench\nRefCOCO\nRefCOCO+\nRefCOCOg\nMeVIS (val_u)\nDAVIS\nSa2VA-InternVL3-2B\n1631/559\n79.8\n81.4\n75.7\n80.3\n53.9\n74.5\nSa2VA-InternVL3-8B\n1743/633\n83.0\n83.3\n78.9\n81.8\n56.4\n76.3\nSa2VA-InternVL3-14B\n1746/724\n84.3\n83.6\n79.9\n83.6\n59.2\n76.6\nSa2VA-Qwen2_5-VL-3B\n1533/572\n78.4\n79.6\n74.0\n77.1\n51.6\n73.4\nSa2VA-Qwen2_5-VL-7B\n1552/676\n84.5\n82.4\n77.5\n81.5\n56.4\n79.4\nSa2VA-Qwen3-VL-4B\n1660/655\n86.3\n81.7\n77.4\n80.0\n57.1\n75.9\nQuick Start\nWe provide an example code to run Sa2VA using transformers.\nimport torch\nfrom transformers import AutoProcessor, AutoModel\nfrom PIL import Image\nimport numpy as np\nimport os\n# load the model and processor\npath = \"ByteDance/Sa2VA-Qwen3-VL-4B\"\nmodel = AutoModel.from_pretrained(\npath,\ntorch_dtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\nuse_flash_attn=True,\ntrust_remote_code=True).eval().cuda()\nprocessor = AutoProcessor.from_pretrained(path, trust_remote_code=True, use_fast=False)\n# for image chat\nimage_path = \"/PATH/TO/IMAGE\"\ntext_prompts = \"<image>Please describe the image.\"\nimage = Image.open(image_path).convert('RGB')\ninput_dict = {\n'image': image,\n'text': text_prompts,\n'past_text': '',\n'mask_prompts': None,\n'processor': processor,\n}\nreturn_dict = model.predict_forward(**input_dict)\nanswer = return_dict[\"prediction\"] # the text format answer\n# for image chat with segmentation output\nimage_path = \"/PATH/TO/IMAGE\"\ntext_prompts = \"<image>Could you please give me a brief description of the image? Please respond with interleaved segmentation masks for the corresponding parts of the answer.\"\nimage = Image.open(image_path).convert('RGB')\ninput_dict = {\n'image': image,\n'text': text_prompts,\n'past_text': '',\n'mask_prompts': None,\n'processor': processor,\n}\nreturn_dict = model.predict_forward(**input_dict)\nanswer = return_dict[\"prediction\"] # the text format answer\nmasks = return_dict['prediction_masks']  # segmentation masks, list(np.array(1, h, w), ...)\n# for chat with visual prompt (mask format) input\nmask_prompts = np.load('/PATH/TO/pred_masks.npy') # np.array(n_prompts, h, w)\nimage_path = \"/PATH/TO/IMAGE\"\ntext_prompts = \"<image>Can you provide me with a detailed description of the region in the picture marked by region1.\"\nimage = Image.open(image_path).convert('RGB')\ninput_dict = {\n'image': image,\n'text': text_prompts,\n'past_text': '',\n'mask_prompts': mask_prompts,\n'processor': processor,\n}\nreturn_dict = model.predict_forward(**input_dict)\nanswer = return_dict[\"prediction\"] # the text format answer\n# for video chat\nvideo_folder = \"/PATH/TO/VIDEO_FOLDER\"\nimages_paths = os.listdir(video_folder)\nimages_paths = [os.path.join(video_folder, image_path) for image_name in images_paths]\nif len(images_paths) > 5:  # uniformly sample 5 frames\nstep = (len(images_paths) - 1) // (5 - 1)\nimages_paths = [images_paths[0]] + images_paths[1:-1][::step][1:] + [images_paths[-1]]\ntext_prompts = \"<image>Please describe the video.\"\ninput_dict = {\n'video': images_paths,\n'text': text_prompts,\n'past_text': '',\n'mask_prompts': None,\n'processor': processor,\n}\nreturn_dict = model.predict_forward(**input_dict)\nanswer = return_dict[\"prediction\"] # the text format answer\n# for video chat with segmentation mask output\nvideo_folder = \"/PATH/TO/VIDEO_FOLDER\"\nimages_paths = os.listdir(video_folder)\nimages_paths = [os.path.join(video_folder, image_path) for image_name in images_paths]\ntext_prompts = \"<image>Please segment the person.\"\ninput_dict = {\n'video': images_paths,\n'text': text_prompts,\n'past_text': '',\n'mask_prompts': None,\n'processor': processor,\n}\nreturn_dict = model.predict_forward(**input_dict)\nanswer = return_dict[\"prediction\"] # the text format answer\nmasks = return_dict['prediction_masks']  # segmentation masks, list(np.array(n_frames, h, w), ...)\nCitation\nIf you find this project useful in your research, please consider citing:\n@article{sa2va,\ntitle={Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos},\nauthor={Yuan, Haobo and Li, Xiangtai and Zhang, Tao and Huang, Zilong Huang and Xu, Shilin and Ji, Shunping and Tong, Yunhai and Qi, Lu and Feng, Jiashi and Yang, Ming-Hsuan},\njournal={arXiv preprint},\nyear={2025}\n}",
    "itsomk/chexpert-densenet121": "DenseNet121 CheXpert Multi-label (chexpert-densenet121-v1)\nModel Description\nIntended Use\nTraining Summary\nPer-class AUC (validation)\nQuick Usage (local safetensors)\nDenseNet121 CheXpert Multi-label (chexpert-densenet121-v1)\nModel Description\nThis model is a fine-tuned DenseNet-121 (PyTorch) for multi-label classification of chest X-rays, trained on the Stanford CheXpert v1.0 dataset.It predicts the presence of the following 14 labels (order preserved):\nNo Finding\nEnlarged Cardiomediastinum\nCardiomegaly\nLung Opacity\nLung Lesion\nEdema\nConsolidation\nPneumonia\nAtelectasis\nPneumothorax\nPleural Effusion\nPleural Other\nFracture\nSupport Devices\nAuthor: Om Kumar (Hugging Face: @itsomk)\nModel files included:\nchexpert_pytorch.safetensors ‚Äî model weights saved with safetensors\nconfig.json ‚Äî minimal config (backbone, num_labels, transforms)\ntraining_history.png ‚Äî training curves\n‚ö†Ô∏è Important:  This model is provided for research and educational purposes only. Not for clinical use.\nIntended Use\nResearch in medical imaging and multi-label classification\nEducational use and reproducible baseline for further fine-tuning or adaptation\nNOT intended for clinical diagnosis or patient care. Use with caution; validate thoroughly before any downstream application.\nTraining Summary\nBackbone: DenseNet-121 (PyTorch torchvision.models.densenet121)\nDataset: CheXpert v1.0 (Stanford)\nUncertainty handling: U-Zeros (replace -1 with 0)\nImage size: 224 √ó 224\nEpochs: 20\nBatch size: 32\nOptimizer: Adam (lr=1e-4, weight_decay=1e-4)\nLoss: BCEWithLogitsLoss with per-class pos_weight\nBest validation mean AUC: 0.8176\nPer-class AUC (validation)\nNo Finding : 0.8762\nEnlarged Cardiomediastinum : 0.5959\nCardiomegaly : 0.8165\nLung Opacity : 0.8083\nLung Lesion : 0.8230\nEdema : 0.8779\nConsolidation : 0.8527\nPneumonia : 0.7559\nAtelectasis : 0.7117\nPneumothorax : 0.8546\nPleural Effusion : 0.9021\nPleural Other : 0.9157\nFracture : 0.7936\nSupport Devices : 0.8622\nQuick Usage (local safetensors)\nimport torch\nfrom torchvision import models, transforms\nfrom safetensors.torch import load_file\nfrom huggingface_hub import hf_hub_download\nfrom PIL import Image\nREPO_ID = \"itsomk/chexpert-densenet121\"\nFILENAME = \"pytorch_model.safetensors\"\nlocal_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\nclass DenseNet121_CheXpert(torch.nn.Module):\ndef __init__(self, num_labels=14, pretrained=False):\nsuper().__init__()\nself.densenet = models.densenet121(pretrained=pretrained)\nnum_features = self.densenet.classifier.in_features\nself.densenet.classifier = torch.nn.Linear(num_features, num_labels)\ndef forward(self, x):\nreturn self.densenet(x)\nstate = load_file(local_path)\nmodel = DenseNet121_CheXpert(num_labels=14, pretrained=False)\nmodel.load_state_dict(state, strict=False)\nmodel.eval()\npreprocess = transforms.Compose([\ntransforms.Resize((224, 224)),\ntransforms.ToTensor(),\ntransforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n])\nlabels = [\n\"No Finding\",\"Enlarged Cardiomediastinum\",\"Cardiomegaly\",\"Lung Opacity\",\n\"Lung Lesion\",\"Edema\",\"Consolidation\",\"Pneumonia\",\"Atelectasis\",\n\"Pneumothorax\",\"Pleural Effusion\",\"Pleural Other\",\"Fracture\",\"Support Devices\"\n]\n# inference\nimg = Image.open(\"path/to/xray.jpg\").convert(\"RGB\")\nx = preprocess(img).unsqueeze(0)\nwith torch.no_grad():\nlogits = model(x)\nprobs = torch.sigmoid(logits).squeeze().tolist()\nresults = {labels[i]: float(probs[i]) for i in range(len(labels))}\nprint(results)",
    "CodeGoat24/UnifiedReward-Edit-qwen-7b": "UnifiedReward-Edit-qwen-7B\nCitation\nUnifiedReward-Edit-qwen-7B\n[2025/10/23] üî•üî•üî• We release UnifiedReward-Edit-7b, a unified reward model for both Text-to-Image and Image-to-Image generation!!\nFor image editing reward task, our models support:\nPairwise Rank ‚Äî directly judge which of two edited images is better.\nPairwise Score ‚Äî assign a separate score to each image in a pair.\nPointwise Score ‚Äî rate a single image on two axes: instruction-following and overall image quality.\nüöÄ The image editing reward inference code is available at UnifiedReward-Edit/ directory, while T2I inference code is unchanged from previous models. The editing training data is preprocessed from EditScore and EditReward and will be released soon. We sincerely appreciate all contributors!!\nFor further details, please refer to the following resources:\nüì∞ Paper: https://arxiv.org/pdf/2503.05236\nü™ê Project Page: https://codegoat24.github.io/UnifiedReward/\nü§ó Model Collections: https://huggingface.co/collections/CodeGoat24/unifiedreward-models-67c3008148c3a380d15ac63a\nü§ó Dataset Collections: https://huggingface.co/collections/CodeGoat24/unifiedreward-training-data-67c300d4fd5eff00fa7f1ede\nüëã Point of Contact: Yibin Wang\nCitation\n@article{unifiedreward,\ntitle={Unified reward model for multimodal understanding and generation},\nauthor={Wang, Yibin and Zang, Yuhang and Li, Hao and Jin, Cheng and Wang, Jiaqi},\njournal={arXiv preprint arXiv:2503.05236},\nyear={2025}\n}",
    "Salesforce/Llama-Fin-8b": "üí∞ Demystifying Domain-adaptive Post-training for Financial LLMs\nCitation\nüí∞ Demystifying Domain-adaptive Post-training for Financial LLMs\nThis is the finance-specific large language model trained using the recipe described in our paper:üìÑ Demystifying Domain-adaptive Post-training for Financial LLMs\nFor more details, please check the following resources:\nüåê Project Page: https://vincent950129.github.io/adapt-llm/\nüìö Training Data: https://huggingface.co/datasets/Salesforce/FinTrain\nüß† Evaluation Data: https://huggingface.co/datasets/Salesforce/FinEval\nüíª Code Repository: https://github.com/SalesforceAIResearch/FinDAP\nEthical Considerations\nUsers need to make their own assessment regarding any obligations or responsibilities under the corresponding licenses or terms and conditions pertaining to the original datasets and data. This release is for research purposes only in support of an academic paper.\nCitation\nIf you find our project helpful, please consider citing our paper üòä\n@misc{ke2025demystifyingdomainadaptiveposttrainingfinancial,\ntitle={Demystifying Domain-adaptive Post-training for Financial LLMs},\nauthor={Zixuan Ke and Yifei Ming and Xuan-Phi Nguyen and Caiming Xiong and Shafiq Joty},\nyear={2025},\neprint={2501.04961},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2501.04961},\n}",
    "svjack/Qwen_Image_Edit_2509_Sref_Lora": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)\nLoRA Fine-tuned Qwen Model for Style Reference Image Edit\n‚ú® Key Features\nüé® Technical Approach\nStyle Analysis Pipeline\nMethod Comparison\nüñºÔ∏è Transformation Examples\nExample 1: Red Parrot\nExample 1: Cat Playing with Yarn Ball (Stamp Style)\nExample 2: Summer Swimming Pool Cube Model\nExample 3: Golden Retriever in Bubbles\nExample 4: Yellow-Green Firefighter\nExample 5: Arab Knight in Constantinople (Comic Style)\n‚öôÔ∏è Usage Instructions\nüîß Technical Details\nüéØ Best Practices\nLoRA Fine-tuned Qwen Model for Style Reference Image Edit\nRepository: https://huggingface.co/svjack/Qwen_Image_Edit_2509_Sref_Lora\nThis repository provides a LoRA (Low-Rank Adaptation) fine-tuned variant of the Qwen image model,\nspecialized in generating new images that match the artistic style of reference images while allowing for content transformation.\nThe model combines Qwen3-VL's ability to analyze and describe artistic styles with LoRA's efficient adaptation for style transfer.\n‚ú® Key Features\nStyle Preservation\nAccurately extracts and replicates artistic characteristics from reference images including color palette, brush strokes, texture, and overall aesthetic style\nMaintains stylistic consistency across different content subjects and compositions\nSupports various art styles including illustration, painting, digital art, and photography\nContent Flexibility\nAllows complete content transformation while preserving reference style characteristics\nEnables creative recombination of different subjects within consistent artistic styles\nSupports complex style descriptions and parameter controls for fine-grained adjustments\nPrompt Efficiency\nSimple commands trigger precise style transfers and content adaptations\nSupports detailed style weight adjustments for controlling influence strength\nüé® Technical Approach\nStyle Analysis Pipeline\nQwen3-VL Analysis: The model first uses Qwen3-VL to analyze reference images and generate detailed style description prompts\nLoRA Adaptation: The fine-tuned LoRA parameters specialize in style transfer and adaptation\nContent-Style Fusion: Combines target content descriptions with extracted style characteristics for final image generation\nMethod Comparison\nThis approach differs from traditional style transfer methods by leveraging advanced vision-language understanding for more accurate style characterization and more flexible content-style separation.\nüñºÔ∏è Transformation Examples\nExample 1: Red Parrot\nReference Style Analysis: Red Parrot transformed from a red man with glasses\ntarget prompt\nÈπ¶Èπâ\nQwen3 VL Edit prompt\nËøõË°å‰∏ãÈù¢ÁöÑ‰øÆÊîπÔºö\n- Â∞ÜÂéüÂõæ‰∏≠ÁöÑ‰∫∫Áâ©Â§¥ÈÉ®ÂèäË∫ØÂπ≤ÈÉ®ÂàÜÊõøÊç¢‰∏∫‰∏ÄÂè™Èπ¶ÈπâÁöÑÂΩ¢Ë±°Ôºå‰øùÁïôÂÖ∂ËΩÆÂªìÁªìÊûÑÂπ∂Âª∂Áª≠ÂéüÊúâÊûÑÂõæÊØî‰æãÔºõ\n- Èπ¶ÈπâË∫´‰ΩìË¶ÜÁõñÁùÄÁ±ª‰ººÂéüÂõæÊúçË£ÖÁ∫πÁêÜÁöÑÈªëËâ≤Â∫ïËâ≤ÔºåÂπ∂Âú®È¢àÈÉ®Ëá≥ËÉåÈÉ®Âå∫ÂüüÊ∑ªÂä†Á∫¢Ëâ≤Êù°Á∫πË£ÖÈ•∞ÔºåÈ¢úËâ≤Ëµ∞ÂêëÂèÇÁÖßÂéüÂõæË°£È¢ÜËÆæËÆ°Ôºõ\n- ‰øùÊåÅÂ§¥ÁõîÂºèË£ÖÁΩÆ‰∏çÂèòÂΩ¢‰ΩÜË∞ÉÊï¥ÂΩ¢ÊÄÅÈÄÇÈÖçÈπ¶ÈπâÂ§¥ÈÉ®ÔºåÊñ∞Â¢ûÈªÑËâ≤ÂèëÂÖâÂ∏¶ÁéØÁªïÁúºÈÉ®‰ΩçÁΩÆÔºàÂëºÂ∫îÂéüÂõæÊä§ÁõÆÈïúÂÖâÊïàÔºâÔºåÂêåÊó∂‰∏§‰æßËÄ≥ÁΩ©Êîπ‰∏∫È∏üÂñô‰æßÁøºÈÄ†ÂûãÔºõ\n- ÂéüÂõæËìùËâ≤ÁöÆËÇ§Ë¥®ÊÑüËΩ¨Âåñ‰∏∫Èπ¶ÈπâÁæΩÊØõÂ±ÇÊ¨°ÔºåÂú®Èù¢ÈÉ®‰∏éÈ¢àËÇ©Â§ÑÂëàÁé∞Á¥´Ëâ≤ËøáÊ∏°Ë∞ÉÂπ∂‰∏éÁ∫¢Ê©ôËÉåÊôØÂΩ¢ÊàêÈ´òÂØπÊØîÂ∫¶ËßÜËßâÊïàÊûúÔºõ\n- ËÉåÊôØ‰ªçÁª¥ÊåÅÁ∫ØÊ≠£È•±ÂíåÊ©ôÁ∫¢Ëâ≤ÂùóÈù¢È£éÊ†ºÔºåÊó†‰ªª‰ΩïÁéØÂ¢ÉÂÖÉÁ¥†‰ªãÂÖ•ÔºõÊâÄÊúâËæπÁºòÁ∫øÁ≤óÁªÜÁªü‰∏Ä‰∏îÂÖ∑Âπ≥Èù¢Ââ™ÂΩ±ÊÑüÔºåÂº∫ÂåñÁü¢ÈáèÊèíÁîªÁâπÂæÅÔºõ\n- ÊâÄÊúâÂÖâÂΩ±Â§ÑÁêÜÈááÁî®Âπ≥Ê∂Ç+Âº∫ÊèèËæπÊâãÊ≥ïÔºåÈÅøÂÖçÊ∏êÂèòÈò¥ÂΩ±ÔºåÁ°Æ‰øùÁîªÈù¢ÂëàÁé∞Âá∫‰∏ÄËá¥ÊÄßÁöÑÂ§çÂè§ËµõÂçöÊúãÂÖãÊ≥¢ÊôÆËâ∫ÊúØË°®Áé∞ÂΩ¢Âºè„ÄÇ\nReference Style\nStyle-Transformed Output\nExample 1: Cat Playing with Yarn Ball (Stamp Style)\nReference Style Analysis: Vintage stamp design with perforated borders, teal background, and retro texture effects.\ntarget prompt\nÁå´Âí™Áé©ÊØõÁ∫øÁêÉ\nQwen3 VL Edit prompt\nËøõË°å‰∏ãÈù¢ÁöÑ‰øÆÊîπÔºö\n- Â∞ÜÂéüÂõæ‰∏≠ÁöÑÁãêÁã∏ÊõøÊç¢‰∏∫‰∏ÄÂè™Ëú∑Áº©ÁùÄË∫´‰Ωì„ÄÅÊ≠£Âú®Áé©ËÄçÊØõÁ∫øÁêÉÁöÑÂ∞èÁå´Ôºõ\n- ‰øùÊåÅÂéüÊúâÈÇÆÁ•®ËæπÊ°ÜÂèäÈΩøÂ≠îÊ†∑Âºè‰∏çÂèòÔºåÂπ∂Âª∂Áª≠ÈùíÁªøËâ≤ËÉåÊôØÂü∫Ë∞ÉÔºõ\n- ÂéüÁ∫¢Ëâ≤ÂúÜÂΩ¢Â§™Èò≥ÂõæÊ°à‰øùÁïôÂπ∂Ë∞ÉÊï¥Â§ßÂ∞èÈÄÇÈÖçÊñ∞‰∏ª‰Ωì‰ΩçÁΩÆÔºåÂú®Â∞èÁå´ÂêéÊñπ‰Ωú‰∏∫ËßÜËßâÁÑ¶ÁÇπÔºõ\n- ÊâÄÊúâÊñáÂ≠óÊ†áÁ≠æÔºàÂ∑¶‰∏äËßíÁ∫¢ÂúÜÂÜÖÊ±âÂ≠ó„ÄÅÂè≥‰∏äËßíÁ´ñÊéíÁü©ÂΩ¢Â≠óÂùóÁ≠âÔºâÂùáÊ≤øÁî®Áõ∏ÂêåÂ≠ó‰ΩìÈ£éÊ†º‰∏éÂ∏ÉÂ±ÄÁªìÊûÑÔºå‰ΩÜÂÜÖÂÆπÈúÄÊîπ‰∏∫Á¨¶Âêà‚ÄúÁå´Âí™Áé©ÊØõÁ∫øÁêÉ‚Äù‰∏ªÈ¢òÁöÑÊñ∞Â≠óÊ†∑ÔºåÂ¶Ç‚ÄúÂñµÊòü‰∫∫‚Äù„ÄÅ‚ÄúÁªíÁêÉ‰πêÂõ≠‚ÄùÁ≠âÔºõ\n- ËÉåÊôØÁ∫πÁêÜÁª¥ÊåÅÈ¢óÁ≤íÊÑüË¥®ÊÑüÔºåÁ°Æ‰øùÁîªÈù¢Áªü‰∏ÄÂ§çÂè§Ë¥¥Á∫∏ÊïàÊûúÔºõ\n- ÊØõÁ∫øÁêÉÂΩ¢ÊÄÅ‰ªéÂéüÊú¨Êó†ÊòéÁ°ÆÂØπË±°Âèò‰∏∫Ê∏ÖÊô∞ÂèØËßÅ‰∏îË¢´Áå´Âí™‰∫íÂä®Áº†ÁªïÁöÑÁä∂ÊÄÅÔºõ\n- È¢úËâ≤Ë∞ÉÊÄßÈÄÇÂ∫¶ÂæÆË∞É‰Ωø‰∏ªËâ≤Ë∞ÉÊõ¥ÂÅèÂêëÊöñÊ©ò/Á±≥ÁôΩÁ≥ªÁ™ÅÂá∫ÂèØÁà±Ê∞õÂõ¥ÔºåÂêåÊó∂‰øùËØÅÊ©ôËâ≤Êó•ËΩÆ‰ªçÂÖ∑Ëæ®ËØÜÂ∫¶Ôºõ\n- Áª¥ÊåÅÊâÄÊúâÈò¥ÂΩ±ËøáÊ∏°‰∏éÁ¨îËß¶ËÇåÁêÜ‰∏ÄËá¥ÔºåÂº∫ÂåñÂõæÂÉèÂêåË¥®ÂåñËâ∫ÊúØË°®ËææÊâãÊ≥ï„ÄÇ\n‚Äî‚ÄîÊúÄÁªàÊàêÂìÅÂú®ÊûÑÂõæ„ÄÅÊùêË¥®Ë°®Áé∞Âäõ„ÄÅË£ÖÈ•∞ÂÖÉÁ¥†ÈÖçÁΩÆÁ≠âÊñπÈù¢ÂÆåÂÖ®ÁªßÊâøÂéüÂßãÁîªÈ£éÁâπÂæÅËÄåÂÆûÁé∞‰∏ªÈ¢òËΩ¨Êç¢„ÄÇ\nReference Style\nStyle-Transformed Output\nExample 2: Summer Swimming Pool Cube Model\nReference Style Analysis: 3D miniature landscape with precise lighting, material textures, and spatial composition.\ntarget prompt\nÂ§èÂ≠£Ê∏∏Ê≥≥Ê±†ÊñπÂùóÊ®°Âûã\nQwen3 VL Edit prompt\nËøõË°å‰∏ãÈù¢ÁöÑ‰øÆÊîπÔºö\n- Â∞ÜÈõ™Âú∞Âú∫ÊôØÂÆåÂÖ®ÊõøÊç¢‰∏∫‰∏Ä‰∏™Ê∏ÖÊæàÊπõËìùÁöÑÊ∏∏Ê≥≥Ê±†Ê∞¥Èù¢ÔºåÂπ∂‰øùÁïôÂéüÂõæ‰∏≠ÁöÑÁ´ãÊñπ‰ΩìÂú∞ÂΩ¢ÁªìÊûÑÔºõ\n- ‰øùÊåÅÊàøÂ±ãÂª∫Á≠ëÂΩ¢ÊÄÅ‰∏çÂèòÔºå‰ΩÜÂ∞ÜÂÖ∂Â±ãÈ°∂Êîπ‰∏∫Ê≥≥Ê±†ËæπÊ≤øÈ£éÊ†ºÂπ∂ÁßªÈô§ÁßØÈõ™Ë¶ÜÁõñÂ±ÇÔºõÁ™óÊà∑ÂÜÖÊöñÂÖâ‰ªçÂèØÁª¥ÊåÅÔºå‰Ωú‰∏∫ËßÜËßâÁÑ¶ÁÇπÂª∂Áª≠Ê∏©ÊöñÊ∞õÂõ¥Ôºõ\n- ÂéüÊúâÊùæÊ†ëÂÖ®ÈÉ®ÊõøÊç¢ÊàêÁÉ≠Â∏¶Ê£ïÊ¶àÊ†ëÔºåÂú®Ê∞¥‰∏≠ÂÄíÂΩ±Â§ÑÂ¢ûÂä†Ê≥¢Á∫πÊïàÊûúÊ®°ÊãüÊ∞¥Âä®ÊÑüÔºõ\n- Á∫¢Ëâ≤Âç°ËΩ¶Ë¢´ÊõøÊç¢‰∏∫Á©∫ÁôΩÁôΩËâ≤Â∞èËàπÊºÇÊµÆ‰∫éÊ≥≥Ê±†‰∏≠Â§Æ‰ΩçÁΩÆÔºåËΩ¶ËΩÆÈÉ®ÂàÜÂèò‰∏∫ËàπÊ°®Ê†∑ÂºèÔºõ\n- ÊõøÊç¢Â§©Á©∫ËÉåÊôØÁî±ÊòüÁÇπÂ§úÁ©∫ËΩ¨‰∏∫Êô¥ÊúóËìùÂ§©Êó†‰∫ëÁä∂ÊÄÅÔºåÂêåÊó∂Âú®‰∏äÊñπÊ∑ªÂä†Âá†ÁºïËΩªÁõàÁôΩ‰∫ëÁÇπÁºÄÔºõ\n- Âú∞Èù¢ÂâñÂºÄÈú≤Âá∫‰∏ãÊñπÂúüË¥®ÁªìÊûÑÊó∂ÔºåÂéüÊúâÊ∑±ÁÅ∞Ëâ≤Â≤©Áü≥Á∫πÁêÜÊîπÁªòÊàêÊµÖÈªÑËâ≤Ê≤ôÊª©Ë¥®Âú∞Ôºõ\n- ÊâÄÊúâÂÖâÂΩ±Â§ÑÁêÜÁªü‰∏ÄÈááÁî®È´ò‰∫ÆÂèçÂ∑ÆË°®Áé∞ÊâãÊ≥ïÔºàÂ¶ÇÂèçÂ∞ÑÂÖâÊ≥ΩÔºâÔºåÁ°Æ‰øùÁîªÈù¢ÂëàÁé∞‰∏ÄËá¥ÊÄßÁöÑ3DÁ´ã‰ΩìË¥®ÊÑüÂèäÊùêË¥®ËøáÊ∏°„ÄÇ\nÊâÄÊúâÂÖÉÁ¥†Âùá‰æùÊçÆÂéüÂßãÂõæÂÉèÁöÑÁ©∫Èó¥Â∏ÉÂ±ÄÊØî‰æãË∞ÉÊï¥Â∞∫ÂØ∏ÂÖ≥Á≥ªÔºå‰ΩøÊñ∞ÊûÑÂõæÂÆåÊï¥Â•ëÂêàÂêå‰∏ÄÊûÅÁÆÄ‰∏ª‰πâÂæÆÁº©ÊôØËßÇËâ∫ÊúØÈ£éÊ†º„ÄÇ\nReference Style\nStyle-Transformed Output\nExample 3: Golden Retriever in Bubbles\nReference Style Analysis: Dreamy soft-focus photography with bokeh effects, ethereal lighting, and translucent elements.\ntarget prompt\nÊ≥°Ê≤´‰∏≠ÁöÑÈáëÊØõÁä¨\nQwen3 VL Edit prompt\nËøõË°å‰∏ãÈù¢ÁöÑ‰øÆÊîπÔºö\n- Â∞ÜÂéüÂõæ‰∏≠ÁöÑ‰∫∫Áâ©‰∏ª‰ΩìÊõøÊç¢‰∏∫‰∏ÄÂè™ÈáëÊØõÁä¨ÔºåÂÖ∂Ë∫´‰ΩìÂßøÊÄÅ‰øùÊåÅÁ±ª‰ººËàûËπàËà¨ÁöÑËàíÂ±ïÂºßÂ∫¶ÔºåÂπ∂ÁΩÆ‰∫éÁîªÈù¢‰∏≠Â§ÆÂÅèÂè≥‰ΩçÁΩÆÔºõ\n- ‰øùÁïôÂéüÊúâÁöÑÊ¢¶ÂπªÊüîÂÖâÊïàÊûúÂèäËÉåÊôØÊï£ÊôØÔºàbokehÔºâÁªìÊûÑÔºåÂú®ÁãóÁãóÂë®Âõ¥Ê∑ªÂä†Â§ßÈáèÂçäÈÄèÊòéÊ∞îÊ≥°Áä∂ÂÖÉÁ¥†Ôºå‰ΩøÂÖ∂‰ªø‰ΩõÊÇ¨ÊµÆ‰∫éÊ∞¥‰∏≠ÊàñÈõæÈú≠‰πã‰∏≠Ôºõ\n- Ë∞ÉÊï¥ÊúçË£ÖÈÉ®ÂàÜ‚Äî‚ÄîÂéüÊú¨Á≤âËâ≤Á∫±Ë¥®ËàûË£ôË¢´ÊõøÊç¢ÊàêÂÖ∑ÊúâÂÖâÊ≥ΩË¥®ÊÑüÁöÑÊµÖÈáëËâ≤ÁªíÈù¢ÊùêË¥®ÔºåË¥¥ÂêàÁãóÁãó‰ΩìÂûãÂπ∂Ë¶ÜÁõñÂÖ®Ë∫´ÔºõË¢ñÂè£ËÆæËÆ°Êîπ‰∏∫Áü≠ËÄåËì¨ÊùæÁöÑÊµÅËãèÊ†∑ÂºèÔºåÂª∂Áª≠ËΩªÁõàÈ£òÈÄ∏ÊÑüÔºõ\n- È¢úËâ≤Âü∫Ë∞É‰ªéÁ≤âËìùÊ∏êÂèòËøáÊ∏°Ëá≥ÊöñÊ©ôË∞É‰∏∫‰∏ªÂØºÔºå‰ΩÜ‰æùÁÑ∂Áª¥ÊåÅÊüîÂíåÊú¶ËÉßÊ∞õÂõ¥ÔºåÂº∫ÂåñÂÖâÂΩ±Âú®ÁöÆÊØõ‰∏äÁöÑÂèçÂ∞ÑÈ´òÂÖâÂå∫ÂüüÔºõ\n- ÂéüÊúâÈù¢ÈÉ®ËΩÆÂªìÊ∂àÂ§±ÂêéÊñ∞Â¢ûÊãü‰∫∫Âåñ‰æßËÑ∏Á∫øÊù°ÊöóÁ§∫ÊñπÂêëÊÄßÂä®‰ΩúÔºåÂêåÊó∂Â¢ûÂº∫ÁúºÈÉ®ÂèëÂÖâÁâπÊïàÊ®°ÊãüÂÖâÊ∫êÁ©øÈÄèÊ∞¥‰ΩìÊó∂ÂΩ¢ÊàêÁöÑÁ≤íÂ≠êÊäòÂ∞ÑÁé∞Ë±°Ôºõ\n- Êï¥‰∏™ÊûÑÂõæ‰ªçÈááÁî®‰ΩéËßíÂ∫¶‰ª∞ÊãçËßÜËßíÂä†Âº∫ËßÜËßâÁ∫µÊ∑±ÔºåËæπÁºòÊ®°Á≥äÂ§ÑÁêÜ‰æùÊóßÂ≠òÂú®‰∏î‰øùÊåÅ‰∏ÄËá¥ËôöÂÆûÂØπÊØîÂ±ÇÊ¨°„ÄÇ\nÊâÄÊúâÊîπÂä®ÂùáÊ≤øË¢≠ÂéüÂßãÂõæÂÉèÊâÄÂëàÁé∞ÁöÑËâ∫ÊúØÈ£éÊ†ºÁâπÂæÅÔºåÂåÖÊã¨ÂÖâÁ∫øËøêÁî®ÊñπÂºè„ÄÅÁ∫πÁêÜË°®Áé∞ÊâãÊ≥ï‰ª•ÂèäÊÉÖÁª™Ê∏≤ÊüìÊâãÊÆµÁ≠âÂÖ≥ÈîÆË¶ÅÁ¥†Êú™‰ΩúÂÅèÁ¶ª„ÄÇ\nReference Style\nStyle-Transformed Output\nExample 4: Yellow-Green Firefighter\nReference Style Analysis: Surreal portrait style with blurred edges, chromatic transitions, and fluorescent color effects.\ntarget prompt\nÈªÑÁªøËâ≤ÁöÑÊ∂àÈò≤Âëò\nQwen3 VL Edit prompt\nËøõË°å‰∏ãÈù¢ÁöÑ‰øÆÊîπÔºö\n- Â∞ÜÂéüÂõæ‰∏≠ÁöÑËßíËâ≤‰∏ª‰ΩìÊõøÊç¢‰∏∫‰∏Ä‰∏™Á©øÁùÄÈªÑËâ≤Âà∂ÊúçÂπ∂ÈÖçÊúâÁ∫¢Ëâ≤Êù°Á∫πÂèäËÇ©Á´†ÁöÑÊ∂àÈò≤ÂëòÂΩ¢Ë±°ÔºåÂÖ∂ÊúçË£ÖÁªìÊûÑÂåÖÊã¨ËÉ∏ÂâçÂæΩÁ´†ÂºèÂõæÊ°àÔºàÁ±ª‰ººÊòüÂΩ¢‰ΩÜÈ£éÊ†ºÂåñÔºâ„ÄÅËÖ∞Â∏¶‰ª•ÂèäËáÇÈÉ®Êä§Áî≤ËÆæËÆ°Ôºõ\n- ÂØπËØ•Ê∂àÈò≤ÂëòÁöÑÂΩ¢Ë±°Âú®ËßÜËßâÂëàÁé∞‰∏ä‰øùÊåÅÂéüÊúâÊ®°Á≥äËæπÁºòÂ§ÑÁêÜÔºåÂπ∂Âª∂Áª≠ÁîªÈù¢‰∏≠ÂøÉËÅöÁÑ¶‰∫é‰∫∫Áâ©ÁöÑÂßøÊÄÅÁâπÂæÅÔºõ\n- ËÉåÊôØ‰øùÁïôÂéüÊúâÁöÑÊ¢¶ÂπªËà¨ÂÖâÊôïÊïàÊûúÔºåÂú®Ê≠§Âü∫Á°Ä‰∏äÂ¢ûÂº∫ÂΩ©ËôπËâ≤Ë∞±ËøáÊ∏°‚Äî‚Äî‰ªéÂ∑¶‰∏äÊñπËìùËâ≤Ê∏êÂèòËá≥Âè≥‰∏ãÊñπÊ©ôÁ∫¢Á¥´Ê∑∑ÂêàÂå∫ÂüüÔºõÂêåÊó∂Âº∫ÂåñÂâçÊôØÂú∞Èù¢Â§ÑÊµÅÂä®Ëà¨ÁöÑÂΩ©Ëâ≤ÊäòÂ∞ÑÁ∫πÁêÜÔºå‰ΩøÂÖ∂ÂÖ∑ÊúâÈÄèÊòéËñÑËÜúË¥®ÊÑüÔºõ\n- ÈÄöËøáË∞ÉÊï¥È•±ÂíåÂ∫¶ÊèêÂçáÈÉ®ÂàÜÈ´ò‰∫ÆÂå∫Â¶ÇËÉ∏Âè£ÂæΩÁ´†‰∏éË¢ñÂè£ÂèçÂÖâÈÉ®‰ΩçÁöÑÈ¢úËâ≤ÊµìÂ∫¶Ôºå‰ΩøÊï¥‰∏™ÂõæÂÉèÂëàÁé∞Âá∫Êõ¥Âº∫ÁÉàÁöÑËçßÂÖâÊÑüÔºõ\n- Âú®ÊâÄÊúâÈ¢úËâ≤Â±ÇÈù¢‰∏äÁªü‰∏ÄÂ∫îÁî®ÂÅèÂÜ∑Ëâ≤Ë∞É‰∏ªË∞ÉÂü∫Á°Ä‰∏äÂè†Âä†ÊöñËâ≤ÁÇπÁºÄÁöÑÊñπÂºèÔºåÁ°Æ‰øùÊñ∞Âä†ÂÖ•ÁöÑ‰∫∫Áâ©ÂèäÂÖ∂ÊúçÈ•∞ÂÖÉÁ¥†‰ªçÁ¨¶ÂêàÂéüÂßãÂú∫ÊôØÊâÄÂÖ∑ÊúâÁöÑË∂ÖÁé∞ÂÆûÂÖâÂΩ±Ê∞õÂõ¥„ÄÇ\n‰ª•‰∏äÊìç‰ΩúÂÆåÂÖ®Ê≤øË¢≠‰∫ÜÊ∫êÂõæÁöÑËâ∫ÊúØË°®Áé∞ÊâãÊ≥ïÔºåÊú™ÂºïÂÖ•‰ªª‰ΩïÈùûÂêåË¥®ÂåñÁöÑÁâπÊïàÊàñÊûÑÂõæÊñπÂºè„ÄÇ\nReference Style\nStyle-Transformed Output\nExample 5: Arab Knight in Constantinople (Comic Style)\nReference Style Analysis: Bold comic art style with sharp edges, flat color areas, and strong contrast.\ntarget prompt\nÊº´ÁîªÈòøÊãâ‰ºØÈ™ëÂ£´Âú®ÂêõÂ£´Âù¶‰∏ÅÂ†°\nQwen3 VL Edit prompt\nËøõË°å‰∏ãÈù¢ÁöÑ‰øÆÊîπÔºö\n- Â∞ÜÂéüÂõæ‰∏≠ÁöÑ‰øÆÂ•≥ÊúçÈ•∞ÊõøÊç¢‰∏∫Â∏¶ÊúâÁõîÁî≤ÂÖÉÁ¥†ÁöÑÈòøÊãâ‰ºØÈ£éÊ†ºÈ™ëÂ£´Ë£ÖÊùüÔºåÂåÖÊã¨ËÇ©ÈÉ®Êä§Áî≤„ÄÅËÉ∏Áî≤ÂèäËÖøÈÉ®Èì†ÁâáÔºåÂπ∂‰øùÁïôÂéüÊúâÁ¥´Ëâ≤Ë∞É‰Ωú‰∏∫‰∏ªËâ≤Á≥ªÔºõ\n- ÂéüÂõæ‰∏≠‰Ω©Êà¥ÁöÑÁúºÈïú‰øùÊåÅ‰∏çÂèò‰ΩÜË∞ÉÊï¥‰ΩçÁΩÆ‰ΩøÂÖ∂Êõ¥Á¨¶ÂêàÈ™ëÂ£´ÂΩ¢Ë±°ÔºõÂ§¥Â∑æÊîπ‰∏∫Ë£ÖÈ•∞ÊÄßÂ§¥È•∞Âπ∂Â¢ûÂä†ÈáëËâ≤Á∫πÊ†∑ÁªÜËäÇÔºõ\n- ÊâãÂäø‰ªéÊØîËÄ∂ÊâãÂäøÂèò‰∏∫Êè°ÂâëÂßøÊÄÅÔºàÂè≥ÊâãÊåÅÂâëÔºâÔºåÂ∑¶ÊâãËá™ÁÑ∂‰∏ãÂûÇ‰∏î‰ªçÂ∏¶ÊâãÂ•óÔºåÂâëÊüÑÈÉ®ÂàÜÈááÁî®ÈáëËìùËâ≤Èï∂ÂµåËÆæËÆ°Âπ∂‰∏éËßíËâ≤ÊúçË£ÖÈ¢úËâ≤ÂëºÂ∫îÔºõ\n- ËÉåÊôØÁî±ÊïôÂ†ÇÂ∞ñÂ°îÁªìÊûÑÊîπÁªòÊàêÂÖ∑Êúâ‰ºäÊñØÂÖ∞Âª∫Á≠ëÁâπÂæÅÁöÑÂüéÂ∏ÇËΩÆÂªì‚Äî‚ÄîÂ¶ÇÊã±ÂΩ¢Èó®Âªä„ÄÅÂúÜÈ°∂ÂºèÂ±ãÈ°∂‰ª•ÂèäÂ§öËßíÂ°îÊ•ºÂΩ¢ÊÄÅÔºåÂú®Â∑¶‰æßËÉåÊôØÂä†ÂÖ•Âñ∑Ê∫ÖÁä∂Á∫¢ËìùÈ¢úÊñôÊïàÊûúÂ¢ûÂº∫ËßÜËßâÂº†ÂäõÔºõ\n- ÁîªÈù¢Âè≥‰æßÊñ∞Â¢û‰∏ÄÂ∞äÁ±ª‰ººÊ∏ÖÁúüÂØ∫ÂÆ£Á§ºÂ°îÁöÑÈ´òËÄ∏Âª∫Á≠ëÁâ©Ââ™ÂΩ±ÔºåÂÖ∂È°∂ÈÉ®ÂëàÁé∞Êñ∞ÊúàÁ¨¶Âè∑ÈÄ†ÂûãÔºõ\n- Êï¥‰ΩìÁ∫øÊù°‰æùÊóßÂª∂Áª≠ÈîêÂà©ËæπÁºò+Âπ≥Ê∂ÇÂùóÈù¢ÊûÑÊàêÁöÑÂä®Êº´ÊèíÁîªË¥®ÊÑüÔºå‰ªÖÈÄöËøáÂ±ÄÈÉ®ÊûÑÂõæÈáçÁªÑÂÆûÁé∞‰∏ªÈ¢òËΩ¨Êç¢ËÄå‰∏çÁ†¥ÂùèÂéüÊúâÁöÑËâ∫ÊúØËØ≠Ë®Ä‰∏ÄËá¥ÊÄß„ÄÇ\nÊâÄÊúâÊîπÂä®ÂùáÂõ¥ÁªïÂêå‰∏ÄÂ•óÈ≤úÊòéÂØπÊØîÂº∫ÁÉàÁöÑÂÜ∑ÊöñÊíûËâ≤‰ΩìÁ≥ªÂ±ïÂºÄÔºåÁ°Æ‰øùÊúÄÁªàÂõæÂÉèÂëàÁé∞Âá∫Áªü‰∏ÄËÄåÂØåÊúâÂÜ≤ÂáªÊÑüÁöÑÊñ∞Âú∫ÊôØËÆæÂÆö„ÄÇ\nReference Style\nStyle-Transformed Output\n‚öôÔ∏è Usage Instructions\nrefer on workflow in\nhttps://huggingface.co/svjack/Qwen_Image_Edit_2509_Sref_Lora/resolve/main/qwen_image_edit_2509_vl_sref.json\nüîß Technical Details\nBase Model: Qwen Image Edit 2509 Model\nLoRA Rank: 32\nTraining Data: 2,000+ style-content pairs across diverse artistic genres\nStyle Analysis: Qwen3-VL for automated style characterization and prompt generation\nSpecialized Capabilities:\nAccurate style element extraction and replication\nContent-style disentanglement and recombination\nMulti-style blending and interpolation\nStyle strength modulation\nOptimized for: Single-GPU inference (NVIDIA 20GB+ recommended) with batch processing support for style-consistent image series generation.\nüéØ Best Practices\nReference Image Quality\nUse high-quality, stylistically consistent reference images\nEnsure reference images have clear stylistic characteristics\nAvoid references with conflicting or ambiguous styles\nPrompt Formulation\nClearly separate style and content instructions\nUse the model's specialized style vocabulary\nSpecify style strength parameters based on desired outcome\nThis model is particularly valuable for creators, designers, and artists looking to maintain stylistic consistency across different subjects or explore creative variations within established visual styles.\nNote: This model is designed for creative style transfer applications. All copyrights for reference images and generated content belong to their respective owners.",
    "Nanbeige/CoSineVerifier-Tool-4B": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nCoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions.\n‚öôÔ∏è Installation\nüöÄ Run the Demo\nüòä Note\nüß© Example\nüîç Performance\nüìñ Introduction\n‚öôÔ∏è Installation\nüöÄ Run the Demo\nüòä Note\nüß© Example\nüîç Performance\nüåü Key Features:\n‚öôÔ∏è Installation\nüöÄ Run the Demo\nüòä Note\nüß© Example\nüîç Performance\nüß™ Quick Start\n‚öôÔ∏è Installation\nüöÄ Run the Demo\nüòä Note\nüß© Example\nüîç Performance\nü§ñ Limitations\n‚úèÔ∏è Citation\nüìÆ Contact\nCoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions.\nüìñ Introduction\nWe present CoSineVerifier-Tool, a compact, tool-augmented verifier for Computation-Oriented ScIeNtific sEnario answer verification. It equips LLM reasoning with external tools‚Äîe.g.,  Python interpreter,‚Äîto accurately verify answers in computation-oriented scenarios such as algebraic equivalence and  physical-constant alignment. We also release the CoSineVerifier series: efficient single-token labeling verifiers with performance comparable to CoSineVerifier-Tool. Our methods achieve state-of-the-art results on VerifyBench and SCI-VerifyBench. They also show clear improvements on RLVR tasks over other verification methods.\nPicture 1: RLVR performance with different verification methods.\nüåü Key Features:\nTool-augmented verification for computation-oriented scientific scenarios:\nCoSineVerifier-Tool evaluates multi-step reasoning in math and science where correctness depends on precise intermediate calculations. It also normalizes answers to a common dimension/unit for consistent numeric checks. It integrates:\nüßÆPython Interpreter: executes snippets to validate algebra/calculus steps, algorithmic logic, and data operations.\nüî¨Scientific Unit Converter: verifies and normalizes unit conversions (e.g., km/h ‚Üí m/s; joule ‚Üí calorie).\nConcise, low-latency reasoning:\nA 4B-parameter CoSinVerifier-Tool with ‚â§100 output tokens enables real-time evaluation and large-scale batch processing.\nGeneral applicability across tasks and domains:\nAccurately assesses equivalence across math, physics, chemistry, biology, and logical reasoning; supports short-answer and multiple-choice formats; and handles both brief answers and long-form responses.\nüß™ Quick Start\nThis repository provides a minimal working demo for playing around the verifier conveniently.\n‚öôÔ∏è Installation\npip install torch transformers pint\nüöÄ Run the Demo\nYou can run the verifier with local or Hugging Face model checkpoint:\npython demo.py --model /path/to/the/verifier\nThe script will:\nLoad the model and tokenizer.\nRun several evaluation cases (e.g., numerical equivalence, physics derivations, symbolic math).\nPrint intermediate reasoning, any tool calls, and the final judgement for each case.\nüòä Note\nUse the exact system and user prompt templates. Do not change whitespace; preserve line breaks identical.\nThe final judgement is parsed from the token \\boxed{Correct} or \\boxed{Incorrect} in the model‚Äôs reply.\nüß© Example\nHamilton‚ÄìJacobi Projectile Motion Verification\nUser:\n{\n\"Question\":\"A particle m undergoes projectile motion with an initial velocity v‚ÇÄ and a horizontal angle of elevation Œ±. Solve using the Hamilton‚ÄìJacobi equation.\",\n\"Ground_Truth\":\"['y = x tan Œ± - (g x¬≤) / (2 v‚ÇÄ¬≤ cos¬≤ Œ±)']\",\n\"Model_Prediction\":\"y = x tan Œ± - (g x¬≤ tan¬≤ Œ±) / (2 v‚ÇÄ¬≤) - (g x¬≤) / (2 v‚ÇÄ¬≤)\"\n}\nAssistant:\n<tool_call>\n{\n\"name\":\"python_interpreter\",\n\"arguments\":{\n\"code\":\"\"\n}\n}\n</tool_call>\nUser:\n<tool_response>\n{\n\"name\":\"python_interpreter\",\n\"status\":\"Success\",\n\"run_result\":{\n\"stdout\":\"1.3895573024349792 1.3895573024349792\",\n\"stderr\":\"\",\n\"exit_success\":true,\n\"return_code\":0,\n\"status\":\"Finished\"\n}\n}\n</tool_response>\nAssistant:\nGold: \\(y=x\\tan\\alpha-\\frac{g x^2}{2v_0^2\\cos^2\\alpha}\\).\nUsing \\(\\sec^2\\alpha=1+\\tan^2\\alpha\\): \\(\\frac{g x^2}{2v_0^2\\cos^2\\alpha}=\\frac{g x^2}{2v_0^2}+\\frac{g x^2\\tan^2\\alpha}{2v_0^2}\\).\nHence \\(y=x\\tan\\alpha-\\frac{g x^2}{2v_0^2}-\\frac{g x^2\\tan^2\\alpha}{2v_0^2}\\equiv\\texttt{pred}\\).\nFinal Verdict: [Correct]\nüîç Performance\nWe conduct a systematic evaluation on VerifyBench and SCI-VerifyBench for CoSineVerifier-Tool-4B and CoSineVerifier-32B, trained from Qwen3-4B-Instruct-2507 and Qwen3-32B, respectively. We report accuracy as mean@3 and efficiency as average output tokens per verdict on these benchmarks.\nModel\nVerifyBench\nVerifyBench (Hard)\nSCI-VerifyBench\nAvg Tokens\nCoT Verifier\no396.188.787.5206.7\nGPT-oss-20B92.284.785.0221\nQwen3-4B92.680.382.01156.7\nQwen3-8B93.783.683.9926.6\nQwen3-32B94.785.283.5798.8\nQwen3-4B-Instruct-250794.784.182.4869.7\nQwen3-235B-A22B-250794.487.782.61885.3\nCoSineVerifier-Tool-4B96.691.989.795.3\nLabeling Verifier\nXVerify-8B-I92.583.378.11.0\nCompassVerifier-7B93.585.285.71.0\nCompassVerifier-32B96.388.985.31.0\nCoSineVerifier-4B95.084.686.23.0\nCoSineVerifier-32B94.289.786.63.0\nWe further evaluate answer-verification methods in an RLVR setting to demonstrate the efficacy of the CoSineVerifier series. Using an on-policy GRPO algorithm, we train Qwen3-4B-Instruct-2507 on competition-math problems with 42K training data drawn from DAPO-Math-17k, OpenR1-Math-220k, and DeepScaleR-Preview. We train 3 epochs for each experiment, with batch size of 128 and rollout_num = 8.\nWe compare CoSineVerifier-Tool-4B and CoSineVerifier-32B against Math-verify, CompassVerifier, and Xverify. We report mean@32 accuracy on AIME 2024 and AIME 2025 as reported in Figure 1.\nü§ñ Limitations\nDespite its high accuracy, CoSinVerifier-Tool-4B invokes external tools in only ~10% of cases and still struggles on the hardest verification items. We will improve its capability by increasing tool-use coverage and developing a unified external-tool suite. Current CoSineVerifier Labeling models output three tokens instead of one token and we will update it into 1 token in next version.\n‚úèÔ∏è Citation\nIf you find our verifiers useful or want to use it in your projects, please kindly cite this Huggingface project.\n@article{nbg_team_2025_cosineverifier,\ntitle   = {CoSineVerifier: Tool-Augmented Answer Verification for Computation-Oriented Scientific Questions},\nauthor  = {{Nanbeige Team}},\nyear    = {2025},\n}\nüìÆ Contact\nIf you have any questions, please raise an issue or contact us at nanbeige@126.com.",
    "huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated": "huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated\nGGUF\nChat with Image\nUsage Warnings\nDonation\nhuihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated\nThis is an uncensored version of Qwen/Qwen3-VL-2B-Instruct created with abliteration (see remove-refusals-with-transformers to know more about it).\nIt was only the text part that was processed, not the image part.\nThe abliterated model will no longer say \"I can‚Äôt describe or analyze this image.\"\nGGUF\nllama.cpp.tr-qwen3-vl-6-b7106-495c611 now supports conversion to GGUF format and can be tested using  llama-mtmd-cli.\nThe GGUF file has been uploaded.\nllama-mtmd-cli -m huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated/GGUF/ggml-model-f16.gguf --mmproj huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated/GGUF/mmproj-model-f16.gguf -c 4096 --image png/cc.jpg -p \"Describe this image.\"\nIf it's just for chatting, you can use llama-cli.\nllama-cli -m huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated/GGUF/ggml-model-f16.gguf -c 40960\nChat with Image\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\nimport os\nimport torch\ncpu_count = os.cpu_count()\nprint(f\"Number of CPU cores in the system: {cpu_count}\")\nhalf_cpu_count = cpu_count // 2\nos.environ[\"MKL_NUM_THREADS\"] = str(half_cpu_count)\nos.environ[\"OMP_NUM_THREADS\"] = str(half_cpu_count)\ntorch.set_num_threads(half_cpu_count)\nMODEL_ID = \"huihui-ai/Huihui-Qwen3-VL-2B-Instruct-abliterated\"\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\nMODEL_ID,\ndevice_map=\"auto\",\ntrust_remote_code=True,\ndtype=torch.bfloat16,\nlow_cpu_mem_usage=True,\n)\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen3-VL-235B-A22B-Instruct\",\n#     dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\nprocessor = AutoProcessor.from_pretrained(MODEL_ID)\nimage_path = \"/png/cars.jpg\"\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\", \"image\": f\"{image_path}\",\n},\n{\"type\": \"text\", \"text\": \"Describe this image.\"},\n],\n}\n]\n# Preparation for inference\ninputs = processor.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_dict=True,\nreturn_tensors=\"pt\"\n).to(model.device)\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\nout_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\ngenerated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\nUsage Warnings\nRisk of Sensitive or Controversial Outputs: This model‚Äôs safety filtering has been significantly reduced, potentially generating sensitive, controversial, or inappropriate content. Users should exercise caution and rigorously review generated outputs.\nNot Suitable for All Audiences: Due to limited content filtering, the model‚Äôs outputs may be inappropriate for public settings, underage users, or applications requiring high security.\nLegal and Ethical Responsibilities: Users must ensure their usage complies with local laws and ethical standards. Generated content may carry legal or ethical risks, and users are solely responsible for any consequences.\nResearch and Experimental Use: It is recommended to use this model for research, testing, or controlled environments, avoiding direct use in production or public-facing commercial applications.\nMonitoring and Review Recommendations: Users are strongly advised to monitor model outputs in real-time and conduct manual reviews when necessary to prevent the dissemination of inappropriate content.\nNo Default Safety Guarantees: Unlike standard models, this model has not undergone rigorous safety optimization. huihui.ai bears no responsibility for any consequences arising from its use.\nDonation\nYour donation helps us continue our further development and improvement, a cup of coffee can do it.\nbitcoin:\nbc1qqnkhuchxw0zqjh2ku3lu4hq45hc6gy84uk70ge\nSupport our work on Ko-fi!"
}