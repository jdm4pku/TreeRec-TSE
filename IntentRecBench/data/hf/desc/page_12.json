{
    "dousery/medical-reasoning-gpt-oss-20b": "Medical Reasoning GPT-OSS-20B\nModel Description\nüè• Key Features\nüöÄ Quick Start\nüìä Training Details\nTraining Data\nTraining Configuration\nModel Architecture\nüéØ Intended Use\nPrimary Use Cases\n‚ö†Ô∏è Important Disclaimers\nüîç Evaluation\nüìà Performance Metrics\nüõ†Ô∏è Technical Requirements\nMinimum Requirements\nOptimized for\nüìú License\nüôè Acknowledgments\nüìû Contact\nMedical Reasoning GPT-OSS-20B\nModel Description\nThis is a fine-tuned version of unsloth/gpt-oss-20b specifically optimized for medical reasoning and clinical decision-making. The model has been trained on high-quality medical reasoning datasets to provide accurate and thoughtful responses to medical queries.\nüè• Key Features\nMedical Expertise: Specialized in medical reasoning, diagnosis, and clinical decision-making\nComplex Reasoning: Uses chain-of-thought reasoning for medical problems\nSafety-Focused: Trained with responsible AI practices for healthcare applications\nLarge Scale: 20B parameters for comprehensive medical knowledge\nReady-to-Use: Full model (not just LoRA adapter) - no additional setup required\nüöÄ Quick Start\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_compute_dtype=torch.bfloat16,\nbnb_4bit_quant_type=\"nf4\",\nbnb_4bit_use_double_quant=True\n)\nmodel_id = \"dousery/medical-reasoning-gpt-oss-20b\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\nquantization_config=quantization_config,\ndevice_map=\"auto\"\n)\nprompt = \"A patient has symptoms of fever and cough. What could be the diagnosis?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(\n**inputs,\nmax_new_tokens=512,\ntemperature=0.7,\ndo_sample=True,\neos_token_id=tokenizer.eos_token_id,\npad_token_id=tokenizer.eos_token_id\n)\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)\nüìä Training Details\nTraining Data\nDataset: Freedomintelligence/medical-o1-reasoning-SFT\nLanguage: English\nSize: 19,704 medical reasoning examples\nFormat: Question-Answer pairs with complex chain-of-thought reasoning\nTraining Configuration\nBase Model: unsloth/gpt-oss-20b (20B parameters)\nTraining Method: LoRA (Low-Rank Adaptation) fine-tuning\nLoRA Rank: 8\nLearning Rate: 2e-4\nBatch Size: 4 (effective)\nEpochs: 1\nHardware: NVIDIA B200 (4x GPUs)\nFramework: Unsloth + TRL\nFinal Training Loss: 0.88\nModel Architecture\nParameters: 20.9 billion\nArchitecture: GPT-OSS (Transformer-based)\nContext Length: 1,024 tokens\nTrainable Parameters: 3.98M (0.02% of total)\nüéØ Intended Use\nPrimary Use Cases\nMedical Education: Explaining medical concepts and procedures\nClinical Reasoning: Analyzing symptoms and differential diagnosis\nResearch Support: Assisting in medical research and literature review\nDecision Support: Providing reasoning for clinical decisions (with human oversight)\n‚ö†Ô∏è Important Disclaimers\nNot a Medical Device: This model is for educational and research purposes only\nHuman Oversight Required: All medical decisions should involve qualified healthcare professionals\nAccuracy Not Guaranteed: Model outputs should be verified against current medical literature\nRegional Variations: Training data may not reflect all regional medical practices\nüîç Evaluation\nThe model demonstrates strong performance in:\nMedical concept explanation\nDifferential diagnosis reasoning\nTreatment option analysis\nPathophysiology understanding\nNote: Comprehensive clinical evaluation is ongoing. Always validate outputs with current medical guidelines.\nüìà Performance Metrics\nTraining Loss: 10.78 ‚Üí 0.88 (significant improvement)\nConvergence: Stable training with consistent loss reduction\nReasoning Quality: Maintains logical chain-of-thought structure\nüõ†Ô∏è Technical Requirements\nMinimum Requirements\nGPU Memory: 16GB+ VRAM recommended\nRAM: 32GB+ system memory\nStorage: 40GB+ free space\nOptimized for\nInference: FP16/BF16 precision\nFrameworks: Transformers, Unsloth, TRL\nHardware: NVIDIA GPUs with Compute Capability 7.0+\nüìú License\nThis model is released under the Apache 2.0 license. Please review the license terms before commercial use.\nüôè Acknowledgments\nBase Model: unsloth/gpt-oss-20b\nTraining Framework: Unsloth\nDataset: Freedomintelligence\nInfrastructure: Modal Labs for GPU compute\nüìû Contact\nFor questions, issues, or collaboration opportunities, please reach out through the HuggingFace community discussions or my Linkedin account :\nLinkedin",
    "LiquidAI/LFM2-VL-1.6B": "LFM2‚ÄëVL\nüìÑ Model details\nüèÉ How to run LFM2-VL\nüîß How to fine-tune\nüìà Performance\nüì¨ Contact\nLFM2‚ÄëVL\nLFM2‚ÄëVL is Liquid AI's first series of multimodal models, designed to process text and images with variable resolutions.\nBuilt on the LFM2 backbone, it is optimized for low-latency and edge AI applications.\nWe're releasing the weights of two post-trained checkpoints with 450M (for highly constrained devices) and 1.6B (more capable yet still lightweight) parameters.\n2√ó faster inference speed on GPUs compared to existing VLMs while maintaining competitive accuracy\nFlexible architecture with user-tunable speed-quality tradeoffs at inference time\nNative resolution processing up to 512√ó512 with intelligent patch-based handling for larger images, avoiding upscaling and distortion\nFind more about our vision-language model in the LFM2-VL post and its language backbone in the LFM2 blog post.\nüìÑ Model details\nDue to their small size, we recommend fine-tuning LFM2-VL models on narrow use cases to maximize performance.\nThey were trained for instruction following and lightweight agentic flows.\nNot intended for safety‚Äëcritical decisions.\nProperty\nLFM2-VL-450M\nLFM2-VL-1.6B\nParameters (LM only)\n350M\n1.2B\nVision encoder\nSigLIP2 NaFlex base (86M)\nSigLIP2 NaFlex shape‚Äëoptimized (400M)\nBackbone layers\nhybrid conv+attention\nhybrid conv+attention\nContext (text)\n32,768 tokens\n32,768 tokens\nImage tokens\ndynamic, user‚Äëtunable\ndynamic, user‚Äëtunable\nVocab size\n65,536\n65,536\nPrecision\nbfloat16\nbfloat16\nLicense\nLFM Open License v1.0\nLFM Open License v1.0\nSupported languages: English\nGeneration parameters: We recommend the following parameters:\nText: temperature=0.1, min_p=0.15, repetition_penalty=1.05\nVision: min_image_tokens=64 max_image_tokens=256, do_image_splitting=True\nChat template: LFM2-VL uses a ChatML-like chat template as follows:\n<|startoftext|><|im_start|>system\nYou are a helpful multimodal assistant by Liquid AI.<|im_end|>\n<|im_start|>user\n<image>Describe this image.<|im_end|>\n<|im_start|>assistant\nThis image shows a Caenorhabditis elegans (C. elegans) nematode.<|im_end|>\nImages are referenced with a sentinel (<image>), which is automatically replaced with the image tokens by the processor.\nYou can apply it using the dedicated .apply_chat_template() function from Hugging Face transformers.\nArchitecture\nHybrid backbone: Language model tower (LFM2-1.2B or LFM2-350M) paired with SigLIP2 NaFlex vision encoders (400M shape-optimized or 86M base variant)\nNative resolution processing: Handles images up to 512√ó512 pixels without upscaling and preserves non-standard aspect ratios without distortion\nTiling strategy: Splits large images into non-overlapping 512√ó512 patches and includes thumbnail encoding for global context (in 1.6B model)\nEfficient token mapping: 2-layer MLP connector with pixel unshuffle reduces image tokens (e.g., 256√ó384 image ‚Üí 96 tokens, 1000√ó3000 ‚Üí 1,020 tokens)\nInference-time flexibility: User-tunable maximum image tokens and patch count for speed/quality tradeoff without retraining\nTraining approach\nBuilds on the LFM2 base model with joint mid-training that fuses vision and language capabilities using a gradually adjusted text-to-image ratio\nApplies joint SFT with emphasis on image understanding and vision tasks\nLeverages large-scale open-source datasets combined with in-house synthetic vision data, selected for balanced task coverage\nFollows a progressive training strategy: base model ‚Üí joint mid-training ‚Üí supervised fine-tuning\nüèÉ How to run LFM2-VL\nYou can run LFM2-VL with Hugging Face transformers v4.57 or more recent as follows:\npip install -U transformers pillow\nHere is an example of how to generate an answer with transformers in Python:\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nfrom transformers.image_utils import load_image\n# Load model and processor\nmodel_id = \"LiquidAI/LFM2-VL-1.6B\"\nmodel = AutoModelForImageTextToText.from_pretrained(\nmodel_id,\ndevice_map=\"auto\",\ndtype=\"bfloat16\"\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Load image and create conversation\nurl = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\nimage = load_image(url)\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": image},\n{\"type\": \"text\", \"text\": \"What is in this image?\"},\n],\n},\n]\n# Generate Answer\ninputs = processor.apply_chat_template(\nconversation,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\",\nreturn_dict=True,\ntokenize=True,\n).to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=64)\nprocessor.batch_decode(outputs, skip_special_tokens=True)[0]\n# This image depicts a vibrant street scene in what appears to be a Chinatown or similar cultural area. The focal point is a large red stop sign with white lettering, mounted on a pole.\nYou can directly run and test the model with this Colab notebook.\nüîß How to fine-tune\nWe recommend fine-tuning LFM2-VL models on your use cases to maximize performance.\nNotebook\nDescription\nLink\nSFT (TRL)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using TRL.\nüìà Performance\nModel\nRealWorldQA\nMM-IFEval\nInfoVQA (Val)\nOCRBench\nBLINK\nMMStar\nMMMU (Val)\nMathVista\nSEEDBench_IMG\nMMVet\nMME\nMMLU\nInternVL3-2B\n65.10\n38.49\n66.10\n831\n53.10\n61.10\n48.70\n57.60\n75.00\n67.00\n2186.40\n64.80\nInternVL3-1B\n57.00\n31.14\n54.94\n798\n43.00\n52.30\n43.20\n46.90\n71.20\n58.70\n1912.40\n49.80\nSmolVLM2-2.2B\n57.50\n19.42\n37.75\n725\n42.30\n46.00\n41.60\n51.50\n71.30\n34.90\n1792.50\n-\nLFM2-VL-1.6B\n65.23\n37.66\n58.68\n742\n44.40\n49.53\n38.44\n51.10\n71.97\n48.07\n1753.04\n50.99\nModel\nRealWorldQA\nMM-IFEval\nInfoVQA (Val)\nOCRBench\nBLINK\nMMStar\nMMMU (Val)\nMathVista\nSEEDBench_IMG\nMMVet\nMME\nMMLU\nSmolVLM2-500M\n49.90\n11.27\n24.64\n609\n40.70\n38.20\n34.10\n37.50\n62.20\n29.90\n1448.30\n-\nLFM2-VL-450M\n52.29\n26.18\n46.51\n655\n41.98\n40.87\n33.11\n44.70\n63.50\n33.76\n1239.06\n40.16\nWe obtained MM-IFEval and InfoVQA (Val) scores for InternVL 3 and SmolVLM2 models using VLMEvalKit.\nüì¨ Contact\nIf you are interested in custom solutions with edge deployment, please contact our sales team.",
    "LiquidAI/LFM2-VL-450M": "LFM2‚ÄëVL\nüìÑ Model details\nüèÉ How to run LFM2-VL\nüîß How to fine-tune\nüìà Performance\nüì¨ Contact\nLFM2‚ÄëVL\nLFM2‚ÄëVL is Liquid AI's first series of multimodal models, designed to process text and images with variable resolutions.\nBuilt on the LFM2 backbone, it is optimized for low-latency and edge AI applications.\nWe're releasing the weights of two post-trained checkpoints with 450M (for highly constrained devices) and 1.6B (more capable yet still lightweight) parameters.\n2√ó faster inference speed on GPUs compared to existing VLMs while maintaining competitive accuracy\nFlexible architecture with user-tunable speed-quality tradeoffs at inference time\nNative resolution processing up to 512√ó512 with intelligent patch-based handling for larger images, avoiding upscaling and distortion\nFind more about our vision-language model in the LFM2-VL post and its language backbone in the LFM2 blog post.\nüìÑ Model details\nDue to their small size, we recommend fine-tuning LFM2-VL models on narrow use cases to maximize performance.\nThey were trained for instruction following and lightweight agentic flows.\nNot intended for safety‚Äëcritical decisions.\nProperty\nLFM2-VL-450M\nLFM2-VL-1.6B\nParameters (LM only)\n350M\n1.2B\nVision encoder\nSigLIP2 NaFlex base (86M)\nSigLIP2 NaFlex shape‚Äëoptimized (400M)\nBackbone layers\nhybrid conv+attention\nhybrid conv+attention\nContext (text)\n32,768 tokens\n32,768 tokens\nImage tokens\ndynamic, user‚Äëtunable\ndynamic, user‚Äëtunable\nVocab size\n65,536\n65,536\nPrecision\nbfloat16\nbfloat16\nLicense\nLFM Open License v1.0\nLFM Open License v1.0\nSupported languages: English\nGeneration parameters: We recommend the following parameters:\nText: temperature=0.1, min_p=0.15, repetition_penalty=1.05\nVision: min_image_tokens=64 max_image_tokens=256, do_image_splitting=True\nChat template: LFM2-VL uses a ChatML-like chat template as follows:\n<|startoftext|><|im_start|>system\nYou are a helpful multimodal assistant by Liquid AI.<|im_end|>\n<|im_start|>user\n<image>Describe this image.<|im_end|>\n<|im_start|>assistant\nThis image shows a Caenorhabditis elegans (C. elegans) nematode.<|im_end|>\nImages are referenced with a sentinel (<image>), which is automatically replaced with the image tokens by the processor.\nYou can apply it using the dedicated .apply_chat_template() function from Hugging Face transformers.\nArchitecture\nHybrid backbone: Language model tower (LFM2-1.2B or LFM2-350M) paired with SigLIP2 NaFlex vision encoders (400M shape-optimized or 86M base variant)\nNative resolution processing: Handles images up to 512√ó512 pixels without upscaling and preserves non-standard aspect ratios without distortion\nTiling strategy: Splits large images into non-overlapping 512√ó512 patches and includes thumbnail encoding for global context (in 1.6B model)\nEfficient token mapping: 2-layer MLP connector with pixel unshuffle reduces image tokens (e.g., 256√ó384 image ‚Üí 96 tokens, 1000√ó3000 ‚Üí 1,020 tokens)\nInference-time flexibility: User-tunable maximum image tokens and patch count for speed/quality tradeoff without retraining\nTraining approach\nBuilds on the LFM2 base model with joint mid-training that fuses vision and language capabilities using a gradually adjusted text-to-image ratio\nApplies joint SFT with emphasis on image understanding and vision tasks\nLeverages large-scale open-source datasets combined with in-house synthetic vision data, selected for balanced task coverage\nFollows a progressive training strategy: base model ‚Üí joint mid-training ‚Üí supervised fine-tuning\nüèÉ How to run LFM2-VL\nYou can run LFM2-VL with Hugging Face transformers v4.57 or more recent as follows:\npip install -U transformers pillow\nHere is an example of how to generate an answer with transformers in Python:\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nfrom transformers.image_utils import load_image\n# Load model and processor\nmodel_id = \"LiquidAI/LFM2-VL-450M\"\nmodel = AutoModelForImageTextToText.from_pretrained(\nmodel_id,\ndevice_map=\"auto\",\ndtype=\"bfloat16\"\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Load image and create conversation\nurl = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\nimage = load_image(url)\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"image\": image},\n{\"type\": \"text\", \"text\": \"What is in this image?\"},\n],\n},\n]\n# Generate Answer\ninputs = processor.apply_chat_template(\nconversation,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\",\nreturn_dict=True,\ntokenize=True,\n).to(model.device)\noutputs = model.generate(**inputs, max_new_tokens=64)\nprocessor.batch_decode(outputs, skip_special_tokens=True)[0]\n# This image depicts a vibrant street scene in what appears to be a Chinatown or similar cultural area. The focal point is a large red stop sign with white lettering, mounted on a pole.\nYou can directly run and test the model with this Colab notebook.\nüîß How to fine-tune\nWe recommend fine-tuning LFM2-VL models on your use cases to maximize performance.\nNotebook\nDescription\nLink\nSFT (TRL)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using TRL.\nüìà Performance\nModel\nRealWorldQA\nMM-IFEval\nInfoVQA (Val)\nOCRBench\nBLINK\nMMStar\nMMMU (Val)\nMathVista\nSEEDBench_IMG\nMMVet\nMME\nMMLU\nInternVL3-2B\n65.10\n38.49\n66.10\n831\n53.10\n61.10\n48.70\n57.60\n75.00\n67.00\n2186.40\n64.80\nInternVL3-1B\n57.00\n31.14\n54.94\n798\n43.00\n52.30\n43.20\n46.90\n71.20\n58.70\n1912.40\n49.80\nSmolVLM2-2.2B\n57.50\n19.42\n37.75\n725\n42.30\n46.00\n41.60\n51.50\n71.30\n34.90\n1792.50\n-\nLFM2-VL-1.6B\n65.23\n37.66\n58.68\n742\n44.40\n49.53\n38.44\n51.10\n71.97\n48.07\n1753.04\n50.99\nModel\nRealWorldQA\nMM-IFEval\nInfoVQA (Val)\nOCRBench\nBLINK\nMMStar\nMMMU (Val)\nMathVista\nSEEDBench_IMG\nMMVet\nMME\nMMLU\nSmolVLM2-500M\n49.90\n11.27\n24.64\n609\n40.70\n38.20\n34.10\n37.50\n62.20\n29.90\n1448.30\n-\nLFM2-VL-450M\n52.29\n26.18\n46.51\n655\n41.98\n40.87\n33.11\n44.70\n63.50\n33.76\n1239.06\n40.16\nWe obtained MM-IFEval and InfoVQA (Val) scores for InternVL 3 and SmolVLM2 models using VLMEvalKit.\nüì¨ Contact\nIf you are interested in custom solutions with edge deployment, please contact our sales team.",
    "nvidia/NVIDIA-Nemotron-Nano-9B-v2": "NVIDIA-Nemotron-Nano-9B-v2\nModel Overview\nFeature Voting\nLicense/Terms of Use\nEvaluation Results\nBenchmark Results (Reasoning On)\nReasoning Budget Control\nModel Architecture\nDeployment Geography: Global\nUse Case\nRelease Date: 08/18/2025\nReferences\nInput\nOutput\nSoftware Integration\nUse it with Transformers\nUse it with TRT-LLM\nUse it with vLLM\nAfter launching a vLLM server, you can call the server with tool-call support using a Python script like below:\nModel Version\nPrompt Format\nTraining, Testing, and Evaluation Datasets\nTraining datasets\nPublic Datasets\nPrivate Non-publicly Accessible Datasets of Third Parties\nOnline Dataset Sources\nNVIDIA-Sourced Synthetic Datasets\nEvaluation Dataset:\nInference\nEthical Considerations\nCitation\nNVIDIA-Nemotron-Nano-9B-v2\nModel Developer: NVIDIA Corporation\nModel Dates:\nJune 2025 - August 2025\nData Freshness:\nSeptember 2024\nThe pretraining data has a cutoff date of September 2024.\nModel Overview\nNVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. The model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so, albeit with a slight decrease in accuracy for harder prompts that require reasoning. Conversely, allowing the model to generate reasoning traces first generally results in higher-quality final solutions to queries and tasks.\nThe model uses a hybrid architecture consisting primarily of Mamba-2 and MLP layers combined with just four Attention layers. For the architecture, please refer to the Nemotron-H tech report.\nThe model was trained using Megatron-LM and NeMo-RL.\nThe supported languages include: English, German, Spanish, French, Italian, and Japanese. Improved using Qwen.\nThis model is ready for commercial use.\nFeature Voting\nWe want to hear from you! Share your ideas, vote on what matters, and help shape the future of Nemotron.\nLicense/Terms of Use\nGOVERNING TERMS: This trial service is governed by the NVIDIA API Trial Terms of Service. Use of this model is governed by the NVIDIA Open Model License Agreement.\nEvaluation Results\nBenchmark Results (Reasoning On)\nWe evaluated our model in Reasoning-On mode across all benchmarks, except RULER, which is evaluated in Reasoning-Off mode.\nBenchmark\nQwen3-8B\nNVIDIA-Nemotron-Nano-9B-v2\nAIME25\n69.3%\n72.1%\nMATH500\n96.3%\n97.8%\nGPQA\n59.6%\n64.0%\nLCB\n59.5%\n71.1%\nBFCL v3\n66.3%\n66.9%\nIFEval (Instruction Strict)\n89.4%\n90.3%\nHLE\n4.4%\n6.5%\nRULER (128K)\n74.1%\n78.9%\nAll evaluations were done using NeMo-Skills. We published a tutorial with all details necessary to reproduce our evaluation results.\nReasoning Budget Control\nThis model supports runtime ‚Äúthinking‚Äù budget control. During inference, the user can specify how many tokens the model is allowed to \"think\".\nModel Architecture\nArchitecture Type: Mamba2-Transformer Hybrid\nNetwork Architecture: Nemotron-Hybrid\nDeployment Geography: Global\nUse Case\nNVIDIA-Nemotron-Nano-9B-v2 is a general purpose reasoning and chat model intended to be used in English and coding languages. Other non-English languages (German, French, Italian, Spanish and Japanese) are also supported. Developers designing AI Agent systems, chatbots, RAG systems, and other AI-powered applications. Also suitable for typical instruction-following tasks.\nRelease Date: 08/18/2025\nHuggingface 08/18/2025 via https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2\nAPI Catalog 08/18/2025 via https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2\nReferences\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nInput\nInput Type(s): Text\nInput Format(s): String\nInput Parameters: One-Dimensional (1D): Sequences\nOther Properties Related to Input: Context length up to 128K. Supported languages include German, Spanish, French, Italian, Korean, Portuguese, Russian, Japanese, Chinese and English.\nOutput\nOutput Type(s): Text\nOutput Format: String\nOutput Parameters: One-Dimensional (1D): Sequences up to 128K\nOur models are designed and optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA‚Äôs hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nSoftware Integration\nRuntime Engine(s): NeMo 25.07.nemotron-nano-v2\nSupported Hardware Microarchitecture Compatibility: NVIDIA A10G, NVIDIA H100-80GB, NVIDIA A100, Jetson AGX Thor\nOperating System(s): Linux\nUse it with Transformers\nThe snippet below shows how to use this model with Huggingface Transformers (tested on version 4.48.3).\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\",\ntorch_dtype=torch.bfloat16,\ntrust_remote_code=True,\ndevice_map=\"auto\"\n)\nCase 1: /think or no reasoning signal is provided in the system prompt, reasoning will be set to True\nmessages = [\n{\"role\": \"system\", \"content\": \"/think\"},\n{\"role\": \"user\", \"content\": \"Write a haiku about GPUs\"},\n]\nCase 2: /no_think is provided, reasoning will be set to False\nmessages = [\n{\"role\": \"system\", \"content\": \"/no_think\"},\n{\"role\": \"user\", \"content\": \"Write a haiku about GPUs\"},\n]\nNote: /think or /no_think keywords can also be provided in ‚Äúuser‚Äù messages for turn-level reasoning control.\nThe rest of the inference snippet remains the same\ntokenized_chat = tokenizer.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\"\n).to(model.device)\noutputs = model.generate(\ntokenized_chat,\nmax_new_tokens=32,\neos_token_id=tokenizer.eos_token_id\n)\nprint(tokenizer.decode(outputs[0]))\nWe recommend setting temperature to 0.6, top_p to 0.95 for reasoning True and greedy search for reasoning False, and increase max_new_tokens to 1024 or higher for reasoning True.\nUse it with TRT-LLM\nThe snippet below shows how to use this model with TRT-LLM. We tested this on the following commit and followed these instructions to build and install TRT-LLM in a docker container.\nfrom tensorrt_llm import SamplingParams\nfrom tensorrt_llm._torch import LLM\nfrom tensorrt_llm._torch.pyexecutor.config import PyTorchConfig\nfrom tensorrt_llm.llmapi import KvCacheConfig\nfrom transformers import AutoTokenizer\npytorch_config = PyTorchConfig(\ndisable_overlap_scheduler=True, enable_trtllm_decoder=True\n)\nkv_cache_config = KvCacheConfig(\nenable_block_reuse=False,\n)\nmodel_id = \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nllm = LLM(\nmodel=model_id,\nmax_seq_len=32678,\nmax_batch_size=4,\npytorch_backend_config=pytorch_config,\nkv_cache_config=kv_cache_config,\ntensor_parallel_size=8,\n)\nmessages = [\n{\"role\": \"system\",  \"content\": \"/think\"},\n{\"role\": \"user\", \"content\": \"Write a haiku about GPUs\"},\n]\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nsampling_params = SamplingParams(\nmax_tokens=512,\ntemperature=0.6,\ntop_p=0.95,\nadd_special_tokens=False,\n)\noutputs = llm.generate([prompt], sampling_params)\nprint(outputs[0].outputs[0].text)\nUse it with vLLM\nThe snippet below shows how to use this model with vLLM. Use the latest version of vLLM and follow these instructions to build and install vLLM.\npip install -U \"vllm>=0.10.1\"\nNow you can run run the server with:\nvllm serve nvidia/NVIDIA-Nemotron-Nano-9B-v2 \\\n--trust-remote-code \\\n--max-num-seqs 64 \\\n--mamba_ssm_cache_dtype float32\nNote:\nRemember to add `--mamba_ssm_cache_dtype float32` for accurate quality. Without this option, the model‚Äôs accuracy may degrade.\nIf you encounter a CUDA OOM issue, try --max-num-seqs 64 and consider lower the value further if the error persists.\nAlternativly, you can use Docker to launch a vLLM server.\nexport TP_SIZE=1  # Adjust this value based on the number of GPUs you want to use\ndocker run --runtime nvidia --gpus all \\\n-v ~/.cache/huggingface:/root/.cache/huggingface \\\n--env \"HUGGING_FACE_HUB_TOKEN=$HF_TOKEN\" \\\n-p 8000:8000 \\\n--ipc=host \\\nvllm/vllm-openai:v0.10.1 \\\n--model nvidia/NVIDIA-Nemotron-Nano-9B-v2 \\\n--tensor-parallel-size ${TP_SIZE} \\\n--max-num-seqs 64 \\\n--max-model-len 131072 \\\n--trust-remote-code \\\n--mamba_ssm_cache_dtype float32\nFor Jetson AGX Thor, please use this vLLM container.\nUsing Budget Control with a vLLM Server\nThe thinking budget allows developers to keep accuracy high and meet response‚Äëtime targets - which is especially crucial for customer support, autonomous agent steps, and edge devices where every millisecond counts.\nWith budget control, you can set a limit for internal reasoning:\nmax_thinking_tokens: This is a threshold that will attempt to end the reasoning trace at the next newline encountered in the reasoning trace. If no newline is encountered within 500 tokens, it will abruptly end the reasoning trace at `max_thinking_tokens + 500`.\nStart a vLLM server:\nvllm serve nvidia/NVIDIA-Nemotron-Nano-9B-v2 \\\n--trust-remote-code \\\n--mamba_ssm_cache_dtype float32\nClient for supporting budget control:\nfrom typing import Any, Dict, List\nimport openai\nfrom transformers import AutoTokenizer\nclass ThinkingBudgetClient:\ndef __init__(self, base_url: str, api_key: str, tokenizer_name_or_path: str):\nself.base_url = base_url\nself.api_key = api_key\nself.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\nself.client = openai.OpenAI(base_url=self.base_url, api_key=self.api_key)\ndef chat_completion(\nself,\nmodel: str,\nmessages: List[Dict[str, Any]],\nmax_thinking_budget: int = 512,\nmax_tokens: int = 1024,\n**kwargs,\n) -> Dict[str, Any]:\nassert (\nmax_tokens > max_thinking_budget\n), f\"thinking budget must be smaller than maximum new tokens. Given {max_tokens=} and {max_thinking_budget=}\"\n# 1. first call chat completion to get reasoning content\nresponse = self.client.chat.completions.create(\nmodel=model, messages=messages, max_tokens=max_thinking_budget, **kwargs\n)\ncontent = response.choices[0].message.content\nreasoning_content = content\nif not \"</think>\" in reasoning_content:\n# reasoning content is too long, closed with a period (.)\nreasoning_content = f\"{reasoning_content}.\\n</think>\\n\\n\"\nreasoning_tokens_len = len(\nself.tokenizer.encode(reasoning_content, add_special_tokens=False)\n)\nremaining_tokens = max_tokens - reasoning_tokens_len\nassert (\nremaining_tokens > 0\n), f\"remaining tokens must be positive. Given {remaining_tokens=}. Increase the max_tokens or lower the max_thinking_budget.\"\n# 2. append reasoning content to messages and call completion\nmessages.append({\"role\": \"assistant\", \"content\": reasoning_content})\nprompt = self.tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\ncontinue_final_message=True,\n)\nresponse = self.client.completions.create(\nmodel=model, prompt=prompt, max_tokens=remaining_tokens, **kwargs\n)\nresponse_data = {\n\"reasoning_content\": reasoning_content.strip().strip(\"</think>\").strip(),\n\"content\": response.choices[0].text,\n\"finish_reason\": response.choices[0].finish_reason,\n}\nreturn response_data\nCalling the server with a budget (Restricted to 32 tokens here as an example)\ntokenizer_name_or_path = \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\"\nclient = ThinkingBudgetClient(\nbase_url=\"http://localhost:8000/v1\",  # Nano 9B v2 deployed in thinking mode\napi_key=\"EMPTY\",\ntokenizer_name_or_path=tokenizer_name_or_path,\n)\nresult = client.chat_completion(\nmodel=\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\",\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant. /think\"},\n{\"role\": \"user\", \"content\": \"What is 2+2?\"},\n],\nmax_thinking_budget=32,\nmax_tokens=512,\ntemperature=0.6,\ntop_p=0.95,\n)\nprint(result)\nYou should see output similar to the following:\n{'reasoning_content': \"Okay, the user asked, What is 2+2? Let me think. Well, 2 plus 2 equals 4. That's a basic.\", 'content': '2 + 2 equals **4**.\\n', 'finish_reason': 'stop'}\nUsing Tool-Calling with a vLLM Server\nStart a vLLM server with native tool-calling:\ngit clone https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2\nvllm serve nvidia/NVIDIA-Nemotron-Nano-9B-v2 \\\n--trust-remote-code \\\n--mamba_ssm_cache_dtype float32 \\\n--enable-auto-tool-choice \\\n--tool-parser-plugin \"NVIDIA-Nemotron-Nano-9B-v2/nemotron_toolcall_parser_no_streaming.py\" \\\n--tool-call-parser \"nemotron_json\"\nAfter launching a vLLM server, you can call the server with tool-call support using a Python script like below:\nfrom openai import OpenAI\nclient = OpenAI(\nbase_url=\"http://0.0.0.0:5000/v1\",\napi_key=\"dummy\",\n)\ncompletion = client.chat.completions.create(\nmodel=\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\",\nmessages=[\n{\"role\": \"system\", \"content\": \"\"},\n{\"role\": \"user\", \"content\": \"My bill is $100. What will be the amount for 18% tip?\"}\n],\ntools=[\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"calculate_tip\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"bill_total\": {\n\"type\": \"integer\",\n\"description\": \"The total amount of the bill\"\n},\n\"tip_percentage\": {\n\"type\": \"integer\",\n\"description\": \"The percentage of tip to be applied\"\n}\n},\n\"required\": [\"bill_total\", \"tip_percentage\"]\n}\n}\n},\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"convert_currency\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"amount\": {\n\"type\": \"integer\",\n\"description\": \"The amount to be converted\"\n},\n\"from_currency\": {\n\"type\": \"string\",\n\"description\": \"The currency code to convert from\"\n},\n\"to_currency\": {\n\"type\": \"string\",\n\"description\": \"The currency code to convert to\"\n}\n},\n\"required\": [\"from_currency\", \"amount\", \"to_currency\"]\n}\n}\n}\n],\ntemperature=0.6,\ntop_p=0.95,\nmax_tokens=32768,\nstream=False\n)\nprint(completion.choices[0].message.content)\nprint(completion.choices[0].message.tool_calls)\nYou should see output similar to the following:\n<think>\nOkay, let's see. The user has a bill of $100 and wants to know the amount for an 18% tip. Hmm, I need to calculate the tip based on the bill total and the percentage. The tools provided include calculate_tip, which takes bill_total and tip_percentage as parameters. So the bill_total here is 100, and the tip_percentage is 18. I should call the calculate_tip function with these values. Wait, do I need to check if the parameters are integers? The bill is $100, which is an integer, and 18% is also an integer. So that fits the function's requirements. I don't need to convert any currency here because the user is asking about a tip in the same currency. So the correct tool to use is calculate_tip with those parameters.\n</think>\n[ChatCompletionMessageToolCall(id='chatcmpl-tool-e341c6954d2c48c2a0e9071c7bdefd8b', function=Function(arguments='{\"bill_total\": 100, \"tip_percentage\": 18}', name='calculate_tip'), type='function')]\nModel Version\nv1.0\nPrompt Format\nWe follow the jinja chat template provided below. This template conditionally adds <think>\\n to the start of the Assistant response if /think is found in either the system prompt or any user message. If no reasoning signal is added, the model defaults to reasoning \"on\" mode. The chat template adds <think></think> to the start of the Assistant response if /no_think is found in the system prompt. Thus enforcing reasoning on/off behavior.\n{%- set ns = namespace(enable_thinking = true) %}\n{%- for message in messages -%}\n{%- set content = message['content'] -%}\n{%- if message['role'] == 'user' or message['role'] == 'system' -%}\n{%- if '/think' in content -%}\n{%- set ns.enable_thinking = true -%}\n{%- elif '/no_think' in content -%}\n{%- set ns.enable_thinking = false -%}\n{%- endif -%}\n{%- endif -%}\n{%- endfor -%}\n{%- if messages[0]['role'] != 'system' -%}\n{%- set ns.non_tool_system_content = '' -%}\n{{- '<SPECIAL_10>System\\n' -}}\n{%- else -%}\n{%- set ns.non_tool_system_content = messages[0]['content']\n.replace('/think', '')\n.replace('/no_think', '')\n.strip()\n-%}\n{{- '<SPECIAL_10>System\\n' + ns.non_tool_system_content }}\n{%- endif -%}\n{%- if tools -%}\n{%- if ns.non_tool_system_content is defined and ns.non_tool_system_content != '' -%}\n{{- '\\n\\n' -}}\n{%- endif -%}\n{{- 'You can use the following tools to assist the user if required:' -}}\n{{- '\\n<AVAILABLE_TOOLS>[' -}}\n{%- for tool in tools -%}\n{{- (tool.function if tool.function is defined else tool) | tojson -}}\n{{- ', ' if not loop.last else '' -}}\n{%- endfor -%}\n{{- ']</AVAILABLE_TOOLS>\\n\\n' -}}\n{{- 'If you decide to call any tool(s), use the following format:\\n' -}}\n{{- '<TOOLCALL>[{{\"name\": \"tool_name1\", \"arguments\": \"tool_args1\"}}, ' -}}\n{{- '{{\"name\": \"tool_name2\", \"arguments\": \"tool_args2\"}}]</TOOLCALL>\\n\\n' -}}\n{{- 'The user will execute tool-calls and return responses from tool(s) in this format:\\n' -}}\n{{- '<TOOL_RESPONSE>[{{\"tool_response1\"}}, {{\"tool_response2\"}}]</TOOL_RESPONSE>\\n\\n' -}}\n{{- 'Based on the tool responses, you can call additional tools if needed, correct tool calls if any errors are found, or just respond to the user.' -}}\n{%- endif -%}\n{{- '\\n' -}}\n{%- set messages = messages[1:] if messages[0]['role'] == 'system' else messages -%}\n{%- if messages[-1]['role'] == 'assistant' -%}\n{%- set ns.last_turn_assistant_content = messages[-1]['content'].strip() -%}\n{%- set messages = messages[:-1] -%}\n{%- endif -%}\n{%- for message in messages -%}\n{%- set content = message['content'] -%}\n{%- if message['role'] == 'user' -%}\n{{- '<SPECIAL_11>User\\n' + content.replace('/think', '').replace('/no_think', '').strip() + '\\n' }}\n{%- elif message['role'] == 'tool' -%}\n{%- if loop.first or (messages[loop.index0 - 1].role != 'tool') -%}\n{{- '<SPECIAL_11>User\\n' + '<TOOL_RESPONSE>[' }}\n{%- endif -%}\n{{- message['content'] -}}\n{{- ', ' if not loop.last and (messages[loop.index0 + 1].role == 'tool') else '' -}}\n{%- if loop.last or (messages[loop.index0 + 1].role != 'tool') -%}\n{{- ']</TOOL_RESPONSE>\\n' -}}\n{%- endif -%}\n{%- elif message['role'] == 'assistant' -%}\n{%- if '</think>' in content -%}\n{%- set content = content.split('</think>')[1].strip() %}\n{%- endif -%}\n{{- '<SPECIAL_11>Assistant\\n' + content.strip() }}\n{%- if message.tool_calls -%}\n{%- if content.strip() != '' -%}\n{{- '\\n\\n' -}}\n{%- endif -%}\n{{- '<TOOLCALL>[' -}}\n{%- for call in message.tool_calls -%}\n{%- set fn = call.function if call.function is defined else call -%}\n{{- '{\"name\": \"' + fn.name + '\", \"arguments\": ' -}}\n{%- if fn.arguments is string -%}\n{{- fn.arguments -}}\n{%- else -%}\n{{- fn.arguments | tojson -}}\n{%- endif -%}\n{{- '}' + (', ' if not loop.last else '') -}}\n{%- endfor -%}\n{{- ']</TOOLCALL>' -}}\n{%- endif -%}\n{{- '\\n<SPECIAL_12>\\n' -}}\n{%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n{{- '<SPECIAL_11>Assistant\\n' -}}\n{%- if ns.enable_thinking is defined and ns.enable_thinking is false -%}\n{{- '<think></think>' -}}\n{%- else -%}\n{{- '<think>\\n' -}}\n{%- endif -%}\n{%- if ns.last_turn_assistant_content is defined and ns.last_turn_assistant_content != '' -%}\n{{- ns.last_turn_assistant_content -}}\n{%- endif -%}\n{%- else -%}\n{%- if ns.last_turn_assistant_content is defined and ns.last_turn_assistant_content != '' -%}\n{{- '<SPECIAL_11>Assistant\\n' -}}\n{%- if ns.enable_thinking is defined and ns.enable_thinking is false -%}\n{{- '<think></think>' -}}\n{%- else -%}\n{{- '<think>\\n' -}}\n{%- endif -%}\n{{- ns.last_turn_assistant_content -}}\n{%- if continue_final_message is defined -%}\n{%- if continue_final_message is false -%}\n{{- '\\n<SPECIAL_12>\\n' -}}\n{%- endif -%}\n{%- else -%}\n{{- '\\n<SPECIAL_12>\\n' -}}\n{%- endif -%}\n{%- endif -%}\n{%- endif -%}\nTraining, Testing, and Evaluation Datasets\nTraining datasets\nData Modality: Text\nText Training Data Size: More than 10 Trillion Tokens\nTrain/Test/Valid Split: We used 100% of the corpus for pre-training and relied on external benchmarks for testing.\nData Collection Method by dataset: Hybrid: Automated, Human, Synthetic\nLabeling Method by dataset: Hybrid: Automated, Human, Synthetic\nProperties: The post-training corpus for NVIDIA-Nemotron-Nano-9B-v2 consists of English and multilingual text (German, Spanish, French, Italian, Korean, Portuguese, Russian, Japanese, Chinese and English). Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. The corpus spans domains including code, legal, math, science, finance, and more. We also include a small portion of question-answering, and alignment style data to improve model accuracies. For several of the domains listed above we used synthetic data, specifically reasoning traces, from DeepSeek R1/R1-0528, Qwen3-235B-A22B, Nemotron 4 340B, Qwen2.5-32B-Instruct-AWQ, Qwen2.5-14B-Instruct, Qwen 2.5 72B.\nThe pre-training corpus for NVIDIA-Nemotron-Nano-9B-v2 consists of high-quality curated and synthetically-generated data. It is trained in the English language, as well as 15 multilingual languages and 43 programming languages. Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. The corpus spans domains including legal, math, science, finance, and more. We also include a small portion of question-answering, and alignment style data to improve model accuracy. The model was pre-trained for approximately twenty trillion tokens.\nAlongside the model, we release our final pretraining data, as outlined in this section. For ease of analysis, there is a sample set that is ungated. For all remaining code, math and multilingual data, gating and approval is required, and the dataset is permissively licensed for model training purposes.\nMore details on the datasets and synthetic data generation methods can be found in the technical report NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model .\nPublic Datasets\nDataset\nCollection Period\nProblems in Elementary Mathematics for Home Study\n4/23/2025\nGSM8K\n4/23/2025\nPRM800K\n4/23/2025\nCC-NEWS\n4/23/2025\nCommon Crawl\n4/23/2025\nWikimedia\n4/23/2025\nBespoke-Stratos-17k\n4/23/2025\ntigerbot-kaggle-leetcodesolutions-en-2k\n4/23/2025\nglaive-function-calling-v2\n4/23/2025\nAPIGen Function-Calling\n4/23/2025\nLMSYS-Chat-1M\n4/23/2025\nOpen Textbook Library - CC BY-SA & GNU subset and OpenStax - CC BY-SA subset\n4/23/2025\nAdvanced Reasoning Benchmark, tigerbot-kaggle-leetcodesolutions-en-2k, PRM800K, and SciBench\n4/23/2025\nFineWeb-2\n4/23/2025\nCourt Listener\nLegacy Download\npeS2o\nLegacy Download\nOpenWebMath\nLegacy Download\nBioRxiv\nLegacy Download\nPMC Open Access Subset\nLegacy Download\nOpenWebText2\nLegacy Download\nStack Exchange Data Dump\nLegacy Download\nPubMed Abstracts\nLegacy Download\nNIH ExPorter\nLegacy Download\narXiv\nLegacy Download\nBigScience Workshop Datasets\nLegacy Download\nReddit Dataset\nLegacy Download\nSEC's Electronic Data Gathering, Analysis, and Retrieval (EDGAR)\nLegacy Download\nPublic Software Heritage S3\nLegacy Download\nThe Stack\nLegacy Download\nmC4\nLegacy Download\nAdvanced Mathematical Problem Solving\nLegacy Download\nMathPile\nLegacy Download\nNuminaMath CoT\nLegacy Download\nPMC Article\nLegacy Download\nFLAN\nLegacy Download\nAdvanced Reasoning Benchmark\nLegacy Download\nSciBench\nLegacy Download\nWikiTableQuestions\nLegacy Download\nFinQA\nLegacy Download\nRiddles\nLegacy Download\nProblems in Elementary Mathematics for Home Study\nLegacy Download\nMedMCQA\nLegacy Download\nCosmos QA\nLegacy Download\nMCTest\nLegacy Download\nAI2's Reasoning Challenge\nLegacy Download\nOpenBookQA\nLegacy Download\nMMLU Auxiliary Train\nLegacy Download\nsocial-chemestry-101\nLegacy Download\nMoral Stories\nLegacy Download\nThe Common Pile v0.1\nLegacy Download\nFineMath\nLegacy Download\nMegaMath\nLegacy Download\nFastChat\n6/30/2025\nPrivate Non-publicly Accessible Datasets of Third Parties\nDataset\nGlobal Regulation\nWorkbench\nOnline Dataset Sources\nThe English Common Crawl data was downloaded from the Common Crawl Foundation (see their FAQ for details on their crawling) and includes the snapshots CC-MAIN-2013-20 through CC-MAIN-2025-13. The data was subsequently deduplicated and filtered in various ways described in the Nemotron-CC paper.\nAdditionally, we extracted data for fifteen languages from the following three Common Crawl snapshots: CC-MAIN-2024-51, CC-MAIN-2025-08, CC-MAIN-2025-18. The fifteen languages included were Arabic, Chinese, Danish, Dutch, French, German, Italian, Japanese, Korean, Polish, Portuguese, Russian, Spanish, Swedish, and Thai. As we did not have reliable multilingual model-based quality classifiers available, we applied just heuristic filtering instead‚Äîsimilar to what we did for lower quality English data in the Nemotron-CC pipeline, but selectively removing some filters for some languages that did not work well. Deduplication was done in the same way as for Nemotron-CC.\nThe GitHub Crawl was collected using the GitHub REST API and the Amazon S3 API. Each crawl was operated in accordance with the rate limits set by its respective source, either GitHub or S3. We collect raw source code and subsequently remove any having a license which does not exist in our permissive-license set (for additional details, refer to the technical report).\nDataset\nModality\nDataset Size (Tokens)\nCollection Period\nEnglish Common Crawl\nText\n3.360T\n4/8/2025\nMultilingual Common Crawl\nText\n812.7B\n5/1/2025\nGitHub Crawl\nText\n747.4B\n4/29/2025\nNVIDIA-Sourced Synthetic Datasets\nDataset\nModality\nDataset Size (Tokens)\nSeed Dataset\nModel(s) used for generation\nSynthetic Art of Problem Solving from DeepSeek-R1\nText\n25.5B\nArt of Problem Solving; American Mathematics Competitions 8; American Mathematics Competitions 10;\nDeepSeek-R1\nSynthetic Moral Stories and Social Chemistry from Mixtral-8x22B-v0.1\nText\n327M\nsocial-chemestry-101; Moral Stories\nMixtral-8x22B-v0.1\nSynthetic Social Sciences seeded with OpenStax from DeepSeek-V3, Mixtral-8x22B-v0.1, and Qwen2.5-72B\nText\n83.6M\nOpenStax - CC BY-SA subset\nDeepSeek-V3; Mixtral-8x22B-v0.1; Qwen2.5-72B\nSynthetic Health Sciences seeded with OpenStax from DeepSeek-V3, Mixtral-8x22B-v0.1, and Qwen2.5-72B\nText\n9.7M\nOpenStax - CC BY-SA subset\nDeepSeek-V3; Mixtral-8x22B-v0.1; Qwen2.5-72B\nSynthetic STEM seeded with OpenStax, Open Textbook Library, and GSM8K from DeepSeek-R1, DeepSeek-V3, DeepSeek-V3-0324, and Qwen2.5-72B\nText\n175M\nOpenStax - CC BY-SA subset; GSM8K; Open Textbook Library - CC BY-SA & GNU subset\nDeepSeek-R1, DeepSeek-V3; DeepSeek-V3-0324; Qwen2.5-72B\nNemotron-PrismMath\nText\n4.6B\nBig-Math-RL-Verified; OpenR1-Math-220k\nQwen2.5-0.5B-instruct, Qwen2.5-72B-Instruct; DeepSeek-R1-Distill-Qwen-32B\nSynthetic Question Answering Data from Papers and Permissible Books from Qwen2.5-72B-Instruct\nText\n350M\narXiv; National Institutes of Health ExPorter; BioRxiv; PMC Article; USPTO Backgrounds; peS2o; Global Regulation; CORE; PG-19; DOAB CC BY & CC BY-SA subset; NDLTD\nQwen2.5-72B-Instruct\nSynthetic FineMath-4+ Reprocessed from DeepSeek-V3\nText\n9.2B\nCommon Crawl\nDeepSeek-V3\nSynthetic FineMath-3+ Reprocessed from phi-4\nText\n27.6B\nCommon Crawl\nphi-4\nSynthetic Union-3+ Reprocessed from phi-4\nText\n93.1B\nCommon Crawl\nphi-4\nRefreshed Nemotron-MIND from phi-4\nText\n73B\nCommon Crawl\nphi-4\nSynthetic Union-4+ Reprocessed from phi-4\nText\n14.12B\nCommon Crawl\nphi-4\nSynthetic Union-3+ minus 4+ Reprocessed from phi-4\nText\n78.95B\nCommon Crawl\nphi-4\nSynthetic Union-3 Refreshed from phi-4\nText\n80.94B\nCommon Crawl\nphi-4\nSynthetic Union-4+ Refreshed from phi-4\nText\n52.32B\nCommon Crawl\nphi-4\nSynthetic AGIEval seeded with AQUA-RAT, LogiQA, and AR-LSAT from DeepSeek-V3 and DeepSeek-V3-0324\nText\n4.0B\nAQUA-RAT; LogiQA; AR-LSAT\nDeepSeek-V3; DeepSeek-V3-0324\nSynthetic AGIEval seeded with AQUA-RAT, LogiQA, and AR-LSAT from Qwen3-30B-A3B\nText\n4.2B\nAQUA-RAT; LogiQA; AR-LSAT\nQwen3-30B-A3B\nSynthetic Art of Problem Solving from Qwen2.5-32B-Instruct, Qwen2.5-Math-72B, Qwen2.5-Math-7B, and Qwen2.5-72B-Instruct\nText\n83.1B\nArt of Problem Solving; American Mathematics Competitions 8; American Mathematics Competitions 10; GSM8K; PRM800K\nQwen2.5-32B-Instruct; Qwen2.5-Math-72B; Qwen2.5-Math-7B; Qwen2.5-72B-Instruct\nSynthetic MMLU Auxiliary Train from DeepSeek-R1\nText\n0.5B\nMMLU Auxiliary Train\nDeepSeek-R1\nSynthetic Long Context Continued Post-Training Data from Papers and Permissible Books from Qwen2.5-72B-Instruct\nText\n5.4B\narXiv; National Institutes of Health ExPorter; BioRxiv; PMC Article; USPTO Backgrounds; peS2o; Global Regulation; CORE; PG-19; DOAB CC BY & CC BY-SA subset; NDLTD\nQwen2.5-72B-Instruct\nSynthetic Common Crawl from Qwen3-30B-A3B and Mistral-Nemo-12B-Instruct\nText\n1.949T\nCommon Crawl\nQwen3-30B-A3B; Mistral-NeMo-12B-Instruct\nSynthetic Multilingual Data from Common Crawl from Qwen3-30B-A3B\nText\n997.3B\nCommon Crawl\nQwen3-30B-A3B\nSynthetic Multilingual Data from Wikimedia from Qwen3-30B-A3B\nText\n55.1B\nWikimedia\nQwen3-30B-A3B\nSynthetic OpenMathReasoning from DeepSeek-R1-0528\nText\n1.5M\nOpenMathReasoning\nDeepSeek-R1-0528\nSynthetic OpenCodeReasoning from DeepSeek-R1-0528\nText\n1.1M\nOpenCodeReasoning\nDeepSeek-R1-0528\nSynthetic Science Data from DeepSeek-R1-0528\nText\n1.5M\n-\nDeepSeek-R1-0528\nSynthetic Humanity's Last Exam from DeepSeek-R1-0528\nText\n460K\nHumanity's Last Exam\nDeepSeek-R1-0528\nSynthetic ToolBench from Qwen3-235B-A22B\nText\n400K\nToolBench\nQwen3-235B-A22B\nSynthetic Nemotron Content Safety Dataset V2, eval-safety, Gretel Synthetic Safety Alignment, and RedTeam_2K from DeepSeek-R1-0528\nText\n52K\nNemotron Content Safety Dataset V2; eval-safety; Gretel Synthetic Safety Alignment; RedTeam_2K\nDeepSeek-R1-0528\nSynthetic HelpSteer from Qwen3-235B-A22B\nText\n120K\nHelpSteer3; HelpSteer2\nQwen3-235B-A22B\nSynthetic Alignment data from Mixtral-8x22B-Instruct-v0.1, Mixtral-8x7B-Instruct-v0.1, and Nemotron-4 Family\nText\n400K\nHelpSteer2; C4; LMSYS-Chat-1M; ShareGPT52K; tigerbot-kaggle-leetcodesolutions-en-2k; GSM8K; PRM800K; lm_identity (NVIDIA internal); FinQA; WikiTableQuestions; Riddles; ChatQA nvolve-multiturn (NVIDIA internal); glaive-function-calling-v2; SciBench; OpenBookQA; Advanced Reasoning Benchmark; Public Software Heritage S3; Khan Academy Math Keywords\nNemotron-4-15B-Base (NVIDIA internal); Nemotron-4-15B-Instruct (NVIDIA internal); Nemotron-4-340B-Base; Nemotron-4-340B-Instruct; Nemotron-4-340B-Reward;  Mixtral-8x7B-Instruct-v0.1;  Mixtral-8x22B-Instruct-v0.1\nSynthetic LMSYS-Chat-1M from Qwen3-235B-A22B\nText\n1M\nLMSYS-Chat-1M\nQwen3-235B-A22B\nSynthetic Multilingual Reasoning data from DeepSeek-R1-0528, Qwen2.5-32B-Instruct-AWQ, and Qwen2.5-14B-Instruct\nText\n25M\nOpenMathReasoning; OpenCodeReasoning\nDeepSeek-R1-0528; Qwen2.5-32B-Instruct-AWQ (translation); Qwen2.5-14B-Instruct (translation);\nSynthetic Multilingual Reasoning data from Qwen3-235B-A22B and Gemma 3 Post-Trained models\nText\n5M\nWildChat\nQwen3-235B-A22B; Gemma 3 PT 12B; Gemma 3 PT 27B\nEvaluation Dataset:\nData Collection Method by dataset: Hybrid: Human, Synthetic\nLabeling Method by dataset: Hybrid: Automated, Human, Synthetic\nInference\nEngines: HF, vLLM, TRT-LLM\nTest Hardware NVIDIA A10G 24GB, H100 80GB, Jetson AGX Thor\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our Trustworthy AI terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nFor more detailed information on ethical considerations for this model, please see the Model Card++ Bias, Explainability, Safety & Security, and Privacy Subcards.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nCitation\n@misc{nvidia2025nvidianemotronnano2,\ntitle={NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model},\nauthor={NVIDIA},\nyear={2025},\neprint={2508.14444},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2508.14444},\n}",
    "Comfy-Org/Qwen-Image-Edit_ComfyUI": "README.md exists but content is empty.",
    "nvidia/NVIDIA-Nemotron-Nano-12B-v2": "NVIDIA-Nemotron-Nano-12B-v2\nModel Overview\nFeature Voting\nLicense/Terms of Use\nEvaluation Results\nBenchmark Results (Reasoning On)\nReasoning Budget Control\nModel Architecture\nDeployment Geography: Global\nUse Case\nRelease Date: 08/29/2025\nReferences\nInput\nOutput\nSoftware Integration\nUse it with Transformers\nUse it with TRT-LLM\nUse it with vLLM\nAfter launching a vLLM server, you can call the server with tool-call support using a Python script like below:\nModel Version\nPrompt Format\nTraining, Testing, and Evaluation Datasets\nTraining datasets\nPublic Datasets\nPrivate Non-publicly Accessible Datasets of Third Parties\nOnline Dataset Sources\nNVIDIA-Sourced Synthetic Datasets\nEvaluation Dataset:\nInference\nEthical Considerations\nCitation\nNVIDIA-Nemotron-Nano-12B-v2\nModel Developer: NVIDIA Corporation\nModel Dates:\nJune 2025 - August 2025\nData Freshness:\nSeptember 2024\nThe pretraining data has a cutoff date of September 2024.\nModel Overview\nNVIDIA-Nemotron-Nano-12B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. The model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so, albeit with a slight decrease in accuracy for harder prompts that require reasoning. Conversely, allowing the model to generate reasoning traces first generally results in higher-quality final solutions to queries and tasks. The model was fine-tuned from NVIDIA-Nemotron-Nano-12B-v2-Base was further compressed into NVIDIA-Nemotron-Nano-9B-v2.\nThe model uses a hybrid architecture consisting primarily of Mamba-2 and MLP layers combined with just six Attention layers. For the architecture, please refer to the Nemotron-H tech report.\nThe model was trained using Megatron-LM and NeMo-RL.\nThe supported languages include: English, German, Spanish, French, Italian, and Japanese. Improved using Qwen.\nThis model is ready for commercial use.\nFeature Voting\nWe want to hear from you! Share your ideas, vote on what matters, and help shape the future of Nemotron.\nLicense/Terms of Use\nGOVERNING TERMS: Use of this model is governed by the NVIDIA Open Model License Agreement.\nEvaluation Results\nBenchmark Results (Reasoning On)\nWe evaluated our model in Reasoning-On mode across all benchmarks, except RULER, which is evaluated in Reasoning-Off mode.\nBenchmark\nNVIDIA-Nemotron-Nano-12B-v2\nAIME25\n76.25%\nMATH500\n97.75%\nGPQA\n64.48%\nLCB\n70.79%\nBFCL v3\n66.98%\nIFEVAL-Prompt\n84.70%\nIFEVAL-Instruction\n89.81%\nAll evaluations were done using NeMo-Skills.\nWe published a tutorial with all details necessary to reproduce our evaluation results.\nReasoning Budget Control\nThis model supports runtime ‚Äúthinking‚Äù budget control. During inference, the user can specify how many tokens the model is allowed to \"think\".\nModel Architecture\nArchitecture Type: Mamba2-Transformer Hybrid\nNetwork Architecture: Nemotron-Hybrid\nDeployment Geography: Global\nUse Case\nNVIDIA-Nemotron-Nano-12B-v2 is a general purpose reasoning and chat model intended to be used in English and coding languages. Other non-English languages (German, French, Italian, Spanish and Japanese) are also supported. Developers designing AI Agent systems, chatbots, RAG systems, and other AI-powered applications. Also suitable for typical instruction-following tasks.\nRelease Date: 08/29/2025\nHuggingface 08/29/2025 via https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2\nReferences\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nInput\nInput Type(s): Text\nInput Format(s): String\nInput Parameters: One-Dimensional (1D): Sequences\nOther Properties Related to Input: Context length up to 128K. Supported languages include German, Spanish, French, Italian, Korean, Portuguese, Russian, Japanese, Chinese and English.\nOutput\nOutput Type(s): Text\nOutput Format: String\nOutput Parameters: One-Dimensional (1D): Sequences up to 128K\nOur models are designed and optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA‚Äôs hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nSoftware Integration\nRuntime Engine(s): NeMo 25.07.nemotron-nano-v2\nSupported Hardware Microarchitecture Compatibility: NVIDIA A10G, NVIDIA H100-80GB, NVIDIA A100, Jetson AGX Thor\nOperating System(s): Linux\nUse it with Transformers\nThe snippet below shows how to use this model with Huggingface Transformers (tested on version 4.48.3).\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"nvidia/NVIDIA-Nemotron-Nano-12B-v2\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"nvidia/NVIDIA-Nemotron-Nano-12B-v2\",\ntorch_dtype=torch.bfloat16,\ntrust_remote_code=True,\ndevice_map=\"auto\"\n)\nCase 1: /think or no reasoning signal is provided in the system prompt, reasoning will be set to True\nmessages = [\n{\"role\": \"system\", \"content\": \"/think\"},\n{\"role\": \"user\", \"content\": \"Write a haiku about GPUs\"},\n]\nCase 2: /no_think is provided, reasoning will be set to False\nmessages = [\n{\"role\": \"system\", \"content\": \"/no_think\"},\n{\"role\": \"user\", \"content\": \"Write a haiku about GPUs\"},\n]\nNote: /think or /no_think keywords can also be provided in ‚Äúuser‚Äù messages for turn-level reasoning control.\nThe rest of the inference snippet remains the same\ntokenized_chat = tokenizer.apply_chat_template(\nmessages,\ntokenize=True,\nadd_generation_prompt=True,\nreturn_tensors=\"pt\"\n).to(model.device)\noutputs = model.generate(\ntokenized_chat,\nmax_new_tokens=32,\neos_token_id=tokenizer.eos_token_id\n)\nprint(tokenizer.decode(outputs[0]))\nWe recommend setting temperature to 0.6, top_p to 0.95 for reasoning True and greedy search for reasoning False, and increase max_new_tokens to 1024 or higher for reasoning True.\nUse it with TRT-LLM\nThe snippet below shows how to use this model with TRT-LLM. We tested this on the following commit and followed these instructions to build and install TRT-LLM in a docker container.\nfrom tensorrt_llm import SamplingParams\nfrom tensorrt_llm._torch import LLM\nfrom tensorrt_llm._torch.pyexecutor.config import PyTorchConfig\nfrom tensorrt_llm.llmapi import KvCacheConfig\nfrom transformers import AutoTokenizer\npytorch_config = PyTorchConfig(\ndisable_overlap_scheduler=True, enable_trtllm_decoder=True\n)\nkv_cache_config = KvCacheConfig(\nenable_block_reuse=False,\n)\nmodel_id = \"nvidia/NVIDIA-Nemotron-Nano-12B-v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nllm = LLM(\nmodel=model_id,\nmax_seq_len=32678,\nmax_batch_size=4,\npytorch_backend_config=pytorch_config,\nkv_cache_config=kv_cache_config,\ntensor_parallel_size=8,\n)\nmessages = [\n{\"role\": \"system\",  \"content\": \"/think\"},\n{\"role\": \"user\", \"content\": \"Write a haiku about GPUs\"},\n]\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nsampling_params = SamplingParams(\nmax_tokens=512,\ntemperature=0.6,\ntop_p=0.95,\nadd_special_tokens=False,\n)\noutputs = llm.generate([prompt], sampling_params)\nprint(outputs[0].outputs[0].text)\nUse it with vLLM\nThe snippet below shows how to use this model with vLLM. Use the latest version of vLLM and follow these instructions to build and install vLLM.\npip install -U \"vllm>=0.10.1\"\nNow you can run run the server with:\nvllm serve nvidia/NVIDIA-Nemotron-Nano-12B-v2 \\\n--trust-remote-code \\\n--max-num-seqs 64 \\\n--mamba_ssm_cache_dtype float32\nNote:\nRemember to add `--mamba_ssm_cache_dtype float32` for accurate quality. Without this option, the model‚Äôs accuracy may degrade.\nIf you encounter a CUDA OOM issue, try --max-num-seqs 64 and consider lower the value further if the error persists.\nAlternativly, you can use Docker to launch a vLLM server.\nexport TP_SIZE=1  # Adjust this value based on the number of GPUs you want to use\ndocker run --runtime nvidia --gpus all \\\n-v ~/.cache/huggingface:/root/.cache/huggingface \\\n--env \"HUGGING_FACE_HUB_TOKEN=$HF_TOKEN\" \\\n-p 8000:8000 \\\n--ipc=host \\\nvllm/vllm-openai:v0.10.1 \\\n--model nvidia/NVIDIA-Nemotron-Nano-12B-v2 \\\n--tensor-parallel-size ${TP_SIZE} \\\n--max-num-seqs 64 \\\n--max-model-len 131072 \\\n--trust-remote-code \\\n--mamba_ssm_cache_dtype float32\nFor Jetson AGX Thor, please use this vLLM container.\nUsing Budget Control with a vLLM Server\nThe thinking budget allows developers to keep accuracy high and meet response‚Äëtime targets - which is especially crucial for customer support, autonomous agent steps, and edge devices where every millisecond counts.\nWith budget control, you can set a limit for internal reasoning:\nmax_thinking_tokens: This is a threshold that will attempt to end the reasoning trace at the next newline encountered in the reasoning trace. If no newline is encountered within 500 tokens, it will abruptly end the reasoning trace at `max_thinking_tokens + 500`.\nStart a vLLM server:\nvllm serve nvidia/NVIDIA-Nemotron-Nano-12B-v2 \\\n--trust-remote-code \\\n--mamba_ssm_cache_dtype float32\nClient for supporting budget control:\nfrom typing import Any, Dict, List\nimport openai\nfrom transformers import AutoTokenizer\nclass ThinkingBudgetClient:\ndef __init__(self, base_url: str, api_key: str, tokenizer_name_or_path: str):\nself.base_url = base_url\nself.api_key = api_key\nself.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\nself.client = openai.OpenAI(base_url=self.base_url, api_key=self.api_key)\ndef chat_completion(\nself,\nmodel: str,\nmessages: List[Dict[str, Any]],\nmax_thinking_budget: int = 512,\nmax_tokens: int = 1024,\n**kwargs,\n) -> Dict[str, Any]:\nassert (\nmax_tokens > max_thinking_budget\n), f\"thinking budget must be smaller than maximum new tokens. Given {max_tokens=} and {max_thinking_budget=}\"\n# 1. first call chat completion to get reasoning content\nresponse = self.client.chat.completions.create(\nmodel=model, messages=messages, max_tokens=max_thinking_budget, **kwargs\n)\ncontent = response.choices[0].message.content\nreasoning_content = content\nif not \"</think>\" in reasoning_content:\n# reasoning content is too long, closed with a period (.)\nreasoning_content = f\"{reasoning_content}.\\n</think>\\n\\n\"\nreasoning_tokens_len = len(\nself.tokenizer.encode(reasoning_content, add_special_tokens=False)\n)\nremaining_tokens = max_tokens - reasoning_tokens_len\nassert (\nremaining_tokens > 0\n), f\"remaining tokens must be positive. Given {remaining_tokens=}. Increase the max_tokens or lower the max_thinking_budget.\"\n# 2. append reasoning content to messages and call completion\nmessages.append({\"role\": \"assistant\", \"content\": reasoning_content})\nprompt = self.tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\ncontinue_final_message=True,\n)\nresponse = self.client.completions.create(\nmodel=model, prompt=prompt, max_tokens=remaining_tokens, **kwargs\n)\nresponse_data = {\n\"reasoning_content\": reasoning_content.strip().strip(\"</think>\").strip(),\n\"content\": response.choices[0].text,\n\"finish_reason\": response.choices[0].finish_reason,\n}\nreturn response_data\nCalling the server with a budget (Restricted to 32 tokens here as an example)\ntokenizer_name_or_path = \"nvidia/NVIDIA-Nemotron-Nano-12B-v2\"\nclient = ThinkingBudgetClient(\nbase_url=\"http://localhost:8000/v1\",  # Nano 12B v2 deployed in thinking mode\napi_key=\"EMPTY\",\ntokenizer_name_or_path=tokenizer_name_or_path,\n)\nresult = client.chat_completion(\nmodel=\"nvidia/NVIDIA-Nemotron-Nano-12B-v2\",\nmessages=[\n{\"role\": \"system\", \"content\": \"You are a helpful assistant. /think\"},\n{\"role\": \"user\", \"content\": \"What is 2+2?\"},\n],\nmax_thinking_budget=32,\nmax_tokens=512,\ntemperature=0.6,\ntop_p=0.95,\n)\nprint(result)\nYou should see output similar to the following:\n{'reasoning_content': \"Okay, the user asked, What is 2+2? Let me think. Well, 2 plus 2 equals 4. That's a basic.\", 'content': '2 + 2 equals **4**.\\n', 'finish_reason': 'stop'}\nUsing Tool-Calling with a vLLM Server\nStart a vLLM server with native tool-calling:\ngit clone https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2\nvllm serve nvidia/NVIDIA-Nemotron-Nano-12B-v2 \\\n--trust-remote-code \\\n--mamba_ssm_cache_dtype float32 \\\n--enable-auto-tool-choice \\\n--tool-parser-plugin \"NVIDIA-Nemotron-Nano-12B-v2/nemotron_toolcall_parser_no_streaming.py\" \\\n--tool-call-parser \"nemotron_json\"\nAfter launching a vLLM server, you can call the server with tool-call support using a Python script like below:\nfrom openai import OpenAI\nclient = OpenAI(\nbase_url=\"http://0.0.0.0:5000/v1\",\napi_key=\"dummy\",\n)\ncompletion = client.chat.completions.create(\nmodel=\"nvidia/NVIDIA-Nemotron-Nano-12B-v2\",\nmessages=[\n{\"role\": \"system\", \"content\": \"\"},\n{\"role\": \"user\", \"content\": \"My bill is $100. What will be the amount for 18% tip?\"}\n],\ntools=[\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"calculate_tip\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"bill_total\": {\n\"type\": \"integer\",\n\"description\": \"The total amount of the bill\"\n},\n\"tip_percentage\": {\n\"type\": \"integer\",\n\"description\": \"The percentage of tip to be applied\"\n}\n},\n\"required\": [\"bill_total\", \"tip_percentage\"]\n}\n}\n},\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"convert_currency\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"amount\": {\n\"type\": \"integer\",\n\"description\": \"The amount to be converted\"\n},\n\"from_currency\": {\n\"type\": \"string\",\n\"description\": \"The currency code to convert from\"\n},\n\"to_currency\": {\n\"type\": \"string\",\n\"description\": \"The currency code to convert to\"\n}\n},\n\"required\": [\"from_currency\", \"amount\", \"to_currency\"]\n}\n}\n}\n],\ntemperature=0.6,\ntop_p=0.95,\nmax_tokens=32768,\nstream=False\n)\nprint(completion.choices[0].message.content)\nprint(completion.choices[0].message.tool_calls)\nYou should see output similar to the following:\n<think>\nOkay, let's see. The user has a bill of $100 and wants to know the amount for an 18% tip. Hmm, I need to calculate the tip based on the bill total and the percentage. The tools provided include calculate_tip, which takes bill_total and tip_percentage as parameters. So the bill_total here is 100, and the tip_percentage is 18. I should call the calculate_tip function with these values. Wait, do I need to check if the parameters are integers? The bill is $100, which is an integer, and 18% is also an integer. So that fits the function's requirements. I don't need to convert any currency here because the user is asking about a tip in the same currency. So the correct tool to use is calculate_tip with those parameters.\n</think>\n[ChatCompletionMessageToolCall(id='chatcmpl-tool-e341c6954d2c48c2a0e9071c7bdefd8b', function=Function(arguments='{\"bill_total\": 100, \"tip_percentage\": 18}', name='calculate_tip'), type='function')]\nModel Version\nv1.0\nPrompt Format\nWe follow the jinja chat template provided below. This template conditionally adds <think>\\n to the start of the Assistant response if /think is found in either the system prompt or any user message. If no reasoning signal is added, the model defaults to reasoning \"on\" mode. The chat template adds <think></think> to the start of the Assistant response if /no_think is found in the system prompt. Thus enforcing reasoning on/off behavior.\n{%- set ns = namespace(enable_thinking = true) %}\n{%- for message in messages -%}\n{%- set content = message['content'] -%}\n{%- if message['role'] == 'user' or message['role'] == 'system' -%}\n{%- if '/think' in content -%}\n{%- set ns.enable_thinking = true -%}\n{%- elif '/no_think' in content -%}\n{%- set ns.enable_thinking = false -%}\n{%- endif -%}\n{%- endif -%}\n{%- endfor -%}\n{%- if messages[0]['role'] != 'system' -%}\n{%- set ns.non_tool_system_content = '' -%}\n{{- '<SPECIAL_10>System\\n' -}}\n{%- else -%}\n{%- set ns.non_tool_system_content = messages[0]['content']\n.replace('/think', '')\n.replace('/no_think', '')\n.strip()\n-%}\n{{- '<SPECIAL_10>System\\n' + ns.non_tool_system_content }}\n{%- endif -%}\n{%- if tools -%}\n{%- if ns.non_tool_system_content is defined and ns.non_tool_system_content != '' -%}\n{{- '\\n\\n' -}}\n{%- endif -%}\n{{- 'You can use the following tools to assist the user if required:' -}}\n{{- '\\n<AVAILABLE_TOOLS>[' -}}\n{%- for tool in tools -%}\n{{- (tool.function if tool.function is defined else tool) | tojson -}}\n{{- ', ' if not loop.last else '' -}}\n{%- endfor -%}\n{{- ']</AVAILABLE_TOOLS>\\n\\n' -}}\n{{- 'If you decide to call any tool(s), use the following format:\\n' -}}\n{{- '<TOOLCALL>[{{\"name\": \"tool_name1\", \"arguments\": \"tool_args1\"}}, ' -}}\n{{- '{{\"name\": \"tool_name2\", \"arguments\": \"tool_args2\"}}]</TOOLCALL>\\n\\n' -}}\n{{- 'The user will execute tool-calls and return responses from tool(s) in this format:\\n' -}}\n{{- '<TOOL_RESPONSE>[{{\"tool_response1\"}}, {{\"tool_response2\"}}]</TOOL_RESPONSE>\\n\\n' -}}\n{{- 'Based on the tool responses, you can call additional tools if needed, correct tool calls if any errors are found, or just respond to the user.' -}}\n{%- endif -%}\n{{- '\\n' -}}\n{%- set messages = messages[1:] if messages[0]['role'] == 'system' else messages -%}\n{%- if messages[-1]['role'] == 'assistant' -%}\n{%- set ns.last_turn_assistant_content = messages[-1]['content'].strip() -%}\n{%- set messages = messages[:-1] -%}\n{%- endif -%}\n{%- for message in messages -%}\n{%- set content = message['content'] -%}\n{%- if message['role'] == 'user' -%}\n{{- '<SPECIAL_11>User\\n' + content.replace('/think', '').replace('/no_think', '').strip() + '\\n' }}\n{%- elif message['role'] == 'tool' -%}\n{%- if loop.first or (messages[loop.index0 - 1].role != 'tool') -%}\n{{- '<SPECIAL_11>User\\n' + '<TOOL_RESPONSE>[' }}\n{%- endif -%}\n{{- message['content'] -}}\n{{- ', ' if not loop.last and (messages[loop.index0 + 1].role == 'tool') else '' -}}\n{%- if loop.last or (messages[loop.index0 + 1].role != 'tool') -%}\n{{- ']</TOOL_RESPONSE>\\n' -}}\n{%- endif -%}\n{%- elif message['role'] == 'assistant' -%}\n{%- if '</think>' in content -%}\n{%- set content = content.split('</think>')[1].strip() %}\n{%- endif -%}\n{{- '<SPECIAL_11>Assistant\\n' + content.strip() }}\n{%- if message.tool_calls -%}\n{%- if content.strip() != '' -%}\n{{- '\\n\\n' -}}\n{%- endif -%}\n{{- '<TOOLCALL>[' -}}\n{%- for call in message.tool_calls -%}\n{%- set fn = call.function if call.function is defined else call -%}\n{{- '{\"name\": \"' + fn.name + '\", \"arguments\": ' -}}\n{%- if fn.arguments is string -%}\n{{- fn.arguments -}}\n{%- else -%}\n{{- fn.arguments | tojson -}}\n{%- endif -%}\n{{- '}' + (', ' if not loop.last else '') -}}\n{%- endfor -%}\n{{- ']</TOOLCALL>' -}}\n{%- endif -%}\n{{- '\\n<SPECIAL_12>\\n' -}}\n{%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n{{- '<SPECIAL_11>Assistant\\n' -}}\n{%- if ns.enable_thinking is defined and ns.enable_thinking is false -%}\n{{- '<think></think>' -}}\n{%- else -%}\n{{- '<think>\\n' -}}\n{%- endif -%}\n{%- if ns.last_turn_assistant_content is defined and ns.last_turn_assistant_content != '' -%}\n{{- ns.last_turn_assistant_content -}}\n{%- endif -%}\n{%- else -%}\n{%- if ns.last_turn_assistant_content is defined and ns.last_turn_assistant_content != '' -%}\n{{- '<SPECIAL_11>Assistant\\n' -}}\n{%- if ns.enable_thinking is defined and ns.enable_thinking is false -%}\n{{- '<think></think>' -}}\n{%- else -%}\n{{- '<think>\\n' -}}\n{%- endif -%}\n{{- ns.last_turn_assistant_content -}}\n{%- if continue_final_message is defined -%}\n{%- if continue_final_message is false -%}\n{{- '\\n<SPECIAL_12>\\n' -}}\n{%- endif -%}\n{%- else -%}\n{{- '\\n<SPECIAL_12>\\n' -}}\n{%- endif -%}\n{%- endif -%}\n{%- endif -%}\nTraining, Testing, and Evaluation Datasets\nTraining datasets\nData Modality: Text\nText Training Data Size: More than 10 Trillion Tokens\nTrain/Test/Valid Split: We used 100% of the corpus for pre-training and relied on external benchmarks for testing.\nData Collection Method by dataset: Hybrid: Automated, Human, Synthetic\nLabeling Method by dataset: Hybrid: Automated, Human, Synthetic\nProperties: The post-training corpus for NVIDIA-Nemotron-Nano-12B-v2 consists of English and multilingual text (German, Spanish, French, Italian, Korean, Portuguese, Russian, Japanese, Chinese and English). Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. The corpus spans domains including code, legal, math, science, finance, and more. We also include a small portion of question-answering, and alignment style data to improve model accuracies. For several of the domains listed above we used synthetic data, specifically reasoning traces, from DeepSeek R1/R1-0528, Qwen3-235B-A22B, Nemotron 4 340B, Qwen2.5-32B-Instruct-AWQ, Qwen2.5-14B-Instruct, Qwen 2.5 72B.\nThe pre-training corpus for NVIDIA-Nemotron-Nano-12B-v2 consists of high-quality curated and synthetically-generated data. It is trained in the English language, as well as 15 multilingual languages and 43 programming languages. Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. The corpus spans domains including legal, math, science, finance, and more. We also include a small portion of question-answering, and alignment style data to improve model accuracy. The model was pre-trained for approximately twenty trillion tokens.\nAlongside the model, we release our final pretraining data, as outlined in this section. For ease of analysis, there is a sample set that is ungated. For all remaining code, math and multilingual data, gating and approval is required, and the dataset is permissively licensed for model training purposes.\nMore details on the datasets and synthetic data generation methods can be found in the technical report NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model .\nPublic Datasets\nDataset\nCollection Period\nProblems in Elementary Mathematics for Home Study\n4/23/2025\nGSM8K\n4/23/2025\nPRM800K\n4/23/2025\nCC-NEWS\n4/23/2025\nCommon Crawl\n4/23/2025\nWikimedia\n4/23/2025\nBespoke-Stratos-17k\n4/23/2025\ntigerbot-kaggle-leetcodesolutions-en-2k\n4/23/2025\nglaive-function-calling-v2\n4/23/2025\nAPIGen Function-Calling\n4/23/2025\nLMSYS-Chat-1M\n4/23/2025\nOpen Textbook Library - CC BY-SA & GNU subset and OpenStax - CC BY-SA subset\n4/23/2025\nAdvanced Reasoning Benchmark, tigerbot-kaggle-leetcodesolutions-en-2k, PRM800K, and SciBench\n4/23/2025\nFineWeb-2\n4/23/2025\nCourt Listener\nLegacy Download\npeS2o\nLegacy Download\nOpenWebMath\nLegacy Download\nBioRxiv\nLegacy Download\nPMC Open Access Subset\nLegacy Download\nOpenWebText2\nLegacy Download\nStack Exchange Data Dump\nLegacy Download\nPubMed Abstracts\nLegacy Download\nNIH ExPorter\nLegacy Download\narXiv\nLegacy Download\nBigScience Workshop Datasets\nLegacy Download\nReddit Dataset\nLegacy Download\nSEC's Electronic Data Gathering, Analysis, and Retrieval (EDGAR)\nLegacy Download\nPublic Software Heritage S3\nLegacy Download\nThe Stack\nLegacy Download\nmC4\nLegacy Download\nAdvanced Mathematical Problem Solving\nLegacy Download\nMathPile\nLegacy Download\nNuminaMath CoT\nLegacy Download\nPMC Article\nLegacy Download\nFLAN\nLegacy Download\nAdvanced Reasoning Benchmark\nLegacy Download\nSciBench\nLegacy Download\nWikiTableQuestions\nLegacy Download\nFinQA\nLegacy Download\nRiddles\nLegacy Download\nProblems in Elementary Mathematics for Home Study\nLegacy Download\nMedMCQA\nLegacy Download\nCosmos QA\nLegacy Download\nMCTest\nLegacy Download\nAI2's Reasoning Challenge\nLegacy Download\nOpenBookQA\nLegacy Download\nMMLU Auxiliary Train\nLegacy Download\nsocial-chemestry-101\nLegacy Download\nMoral Stories\nLegacy Download\nThe Common Pile v0.1\nLegacy Download\nFineMath\nLegacy Download\nMegaMath\nLegacy Download\nFastChat\n6/30/2025\nPrivate Non-publicly Accessible Datasets of Third Parties\nDataset\nGlobal Regulation\nWorkbench\nOnline Dataset Sources\nThe English Common Crawl data was downloaded from the Common Crawl Foundation (see their FAQ for details on their crawling) and includes the snapshots CC-MAIN-2013-20 through CC-MAIN-2025-13. The data was subsequently deduplicated and filtered in various ways described in the Nemotron-CC paper.\nAdditionally, we extracted data for fifteen languages from the following three Common Crawl snapshots: CC-MAIN-2024-51, CC-MAIN-2025-08, CC-MAIN-2025-18. The fifteen languages included were Arabic, Chinese, Danish, Dutch, French, German, Italian, Japanese, Korean, Polish, Portuguese, Russian, Spanish, Swedish, and Thai. As we did not have reliable multilingual model-based quality classifiers available, we applied just heuristic filtering instead‚Äîsimilar to what we did for lower quality English data in the Nemotron-CC pipeline, but selectively removing some filters for some languages that did not work well. Deduplication was done in the same way as for Nemotron-CC.\nThe GitHub Crawl was collected using the GitHub REST API and the Amazon S3 API. Each crawl was operated in accordance with the rate limits set by its respective source, either GitHub or S3. We collect raw source code and subsequently remove any having a license which does not exist in our permissive-license set (for additional details, refer to the technical report).\nDataset\nModality\nDataset Size (Tokens)\nCollection Period\nEnglish Common Crawl\nText\n3.360T\n4/8/2025\nMultilingual Common Crawl\nText\n812.7B\n5/1/2025\nGitHub Crawl\nText\n747.4B\n4/29/2025\nNVIDIA-Sourced Synthetic Datasets\nDataset\nModality\nDataset Size (Tokens)\nSeed Dataset\nModel(s) used for generation\nSynthetic Art of Problem Solving from DeepSeek-R1\nText\n25.5B\nArt of Problem Solving; American Mathematics Competitions 8; American Mathematics Competitions 10;\nDeepSeek-R1\nSynthetic Moral Stories and Social Chemistry from Mixtral-8x22B-v0.1\nText\n327M\nsocial-chemestry-101; Moral Stories\nMixtral-8x22B-v0.1\nSynthetic Social Sciences seeded with OpenStax from DeepSeek-V3, Mixtral-8x22B-v0.1, and Qwen2.5-72B\nText\n83.6M\nOpenStax - CC BY-SA subset\nDeepSeek-V3; Mixtral-8x22B-v0.1; Qwen2.5-72B\nSynthetic Health Sciences seeded with OpenStax from DeepSeek-V3, Mixtral-8x22B-v0.1, and Qwen2.5-72B\nText\n9.7M\nOpenStax - CC BY-SA subset\nDeepSeek-V3; Mixtral-8x22B-v0.1; Qwen2.5-72B\nSynthetic STEM seeded with OpenStax, Open Textbook Library, and GSM8K from DeepSeek-R1, DeepSeek-V3, DeepSeek-V3-0324, and Qwen2.5-72B\nText\n175M\nOpenStax - CC BY-SA subset; GSM8K; Open Textbook Library - CC BY-SA & GNU subset\nDeepSeek-R1, DeepSeek-V3; DeepSeek-V3-0324; Qwen2.5-72B\nNemotron-PrismMath\nText\n4.6B\nBig-Math-RL-Verified; OpenR1-Math-220k\nQwen2.5-0.5B-instruct, Qwen2.5-72B-Instruct; DeepSeek-R1-Distill-Qwen-32B\nSynthetic Question Answering Data from Papers and Permissible Books from Qwen2.5-72B-Instruct\nText\n350M\narXiv; National Institutes of Health ExPorter; BioRxiv; PMC Article; USPTO Backgrounds; peS2o; Global Regulation; CORE; PG-19; DOAB CC BY & CC BY-SA subset; NDLTD\nQwen2.5-72B-Instruct\nSynthetic FineMath-4+ Reprocessed from DeepSeek-V3\nText\n9.2B\nCommon Crawl\nDeepSeek-V3\nSynthetic FineMath-3+ Reprocessed from phi-4\nText\n27.6B\nCommon Crawl\nphi-4\nSynthetic Union-3+ Reprocessed from phi-4\nText\n93.1B\nCommon Crawl\nphi-4\nRefreshed Nemotron-MIND from phi-4\nText\n73B\nCommon Crawl\nphi-4\nSynthetic Union-4+ Reprocessed from phi-4\nText\n14.12B\nCommon Crawl\nphi-4\nSynthetic Union-3+ minus 4+ Reprocessed from phi-4\nText\n78.95B\nCommon Crawl\nphi-4\nSynthetic Union-3 Refreshed from phi-4\nText\n80.94B\nCommon Crawl\nphi-4\nSynthetic Union-4+ Refreshed from phi-4\nText\n52.32B\nCommon Crawl\nphi-4\nSynthetic AGIEval seeded with AQUA-RAT, LogiQA, and AR-LSAT from DeepSeek-V3 and DeepSeek-V3-0324\nText\n4.0B\nAQUA-RAT; LogiQA; AR-LSAT\nDeepSeek-V3; DeepSeek-V3-0324\nSynthetic AGIEval seeded with AQUA-RAT, LogiQA, and AR-LSAT from Qwen3-30B-A3B\nText\n4.2B\nAQUA-RAT; LogiQA; AR-LSAT\nQwen3-30B-A3B\nSynthetic Art of Problem Solving from Qwen2.5-32B-Instruct, Qwen2.5-Math-72B, Qwen2.5-Math-7B, and Qwen2.5-72B-Instruct\nText\n83.1B\nArt of Problem Solving; American Mathematics Competitions 8; American Mathematics Competitions 10; GSM8K; PRM800K\nQwen2.5-32B-Instruct; Qwen2.5-Math-72B; Qwen2.5-Math-7B; Qwen2.5-72B-Instruct\nSynthetic MMLU Auxiliary Train from DeepSeek-R1\nText\n0.5B\nMMLU Auxiliary Train\nDeepSeek-R1\nSynthetic Long Context Continued Post-Training Data from Papers and Permissible Books from Qwen2.5-72B-Instruct\nText\n5.4B\narXiv; National Institutes of Health ExPorter; BioRxiv; PMC Article; USPTO Backgrounds; peS2o; Global Regulation; CORE; PG-19; DOAB CC BY & CC BY-SA subset; NDLTD\nQwen2.5-72B-Instruct\nSynthetic Common Crawl from Qwen3-30B-A3B and Mistral-Nemo-12B-Instruct\nText\n1.949T\nCommon Crawl\nQwen3-30B-A3B; Mistral-NeMo-12B-Instruct\nSynthetic Multilingual Data from Common Crawl from Qwen3-30B-A3B\nText\n997.3B\nCommon Crawl\nQwen3-30B-A3B\nSynthetic Multilingual Data from Wikimedia from Qwen3-30B-A3B\nText\n55.1B\nWikimedia\nQwen3-30B-A3B\nSynthetic OpenMathReasoning from DeepSeek-R1-0528\nText\n1.5M\nOpenMathReasoning\nDeepSeek-R1-0528\nSynthetic OpenCodeReasoning from DeepSeek-R1-0528\nText\n1.1M\nOpenCodeReasoning\nDeepSeek-R1-0528\nSynthetic Science Data from DeepSeek-R1-0528\nText\n1.5M\n-\nDeepSeek-R1-0528\nSynthetic Humanity's Last Exam from DeepSeek-R1-0528\nText\n460K\nHumanity's Last Exam\nDeepSeek-R1-0528\nSynthetic ToolBench from Qwen3-235B-A22B\nText\n400K\nToolBench\nQwen3-235B-A22B\nSynthetic Nemotron Content Safety Dataset V2, eval-safety, Gretel Synthetic Safety Alignment, and RedTeam_2K from DeepSeek-R1-0528\nText\n52K\nNemotron Content Safety Dataset V2; eval-safety; Gretel Synthetic Safety Alignment; RedTeam_2K\nDeepSeek-R1-0528\nSynthetic HelpSteer from Qwen3-235B-A22B\nText\n120K\nHelpSteer3; HelpSteer2\nQwen3-235B-A22B\nSynthetic Alignment data from Mixtral-8x22B-Instruct-v0.1, Mixtral-8x7B-Instruct-v0.1, and Nemotron-4 Family\nText\n400K\nHelpSteer2; C4; LMSYS-Chat-1M; ShareGPT52K; tigerbot-kaggle-leetcodesolutions-en-2k; GSM8K; PRM800K; lm_identity (NVIDIA internal); FinQA; WikiTableQuestions; Riddles; ChatQA nvolve-multiturn (NVIDIA internal); glaive-function-calling-v2; SciBench; OpenBookQA; Advanced Reasoning Benchmark; Public Software Heritage S3; Khan Academy Math Keywords\nNemotron-4-15B-Base (NVIDIA internal); Nemotron-4-15B-Instruct (NVIDIA internal); Nemotron-4-340B-Base; Nemotron-4-340B-Instruct; Nemotron-4-340B-Reward;  Mixtral-8x7B-Instruct-v0.1;  Mixtral-8x22B-Instruct-v0.1\nSynthetic LMSYS-Chat-1M from Qwen3-235B-A22B\nText\n1M\nLMSYS-Chat-1M\nQwen3-235B-A22B\nSynthetic Multilingual Reasoning data from DeepSeek-R1-0528, Qwen2.5-32B-Instruct-AWQ, and Qwen2.5-14B-Instruct\nText\n25M\nOpenMathReasoning; OpenCodeReasoning\nDeepSeek-R1-0528; Qwen2.5-32B-Instruct-AWQ (translation); Qwen2.5-14B-Instruct (translation);\nSynthetic Multilingual Reasoning data from Qwen3-235B-A22B and Gemma 3 Post-Trained models\nText\n5M\nWildChat\nQwen3-235B-A22B; Gemma 3 PT 12B; Gemma 3 PT 27B\nEvaluation Dataset:\nData Collection Method by dataset: Hybrid: Human, Synthetic\nLabeling Method by dataset: Hybrid: Automated, Human, Synthetic\nInference\nEngines: HF, vLLM, TRT-LLM\nTest Hardware NVIDIA A10G 24GB, H100 80GB, Jetson AGX Thor\nEthical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our Trustworthy AI terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\nFor more detailed information on ethical considerations for this model, please see the Model Card++ Bias, Explainability, Safety & Security, and Privacy Subcards.\nPlease report security vulnerabilities or NVIDIA AI Concerns here.\nCitation\n@misc{nvidia2025nvidianemotronnano2,\ntitle={NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model},\nauthor={NVIDIA},\nyear={2025},\neprint={2508.14444},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2508.14444},\n}",
    "apple/FastVLM-0.5B": "FastVLM: Efficient Vision Encoding for Vision Language Models\nCitation\nFastVLM: Efficient Vision Encoding for Vision Language Models\nFastVLM was introduced in\nFastVLM: Efficient Vision Encoding for Vision Language Models. (CVPR 2025)\nHighlights\nWe introduce FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images.\nOur smallest variant outperforms LLaVA-OneVision-0.5B with 85x faster Time-to-First-Token (TTFT) and 3.4x smaller vision encoder.\nOur larger variants using Qwen2-7B LLM outperform recent works like Cambrian-1-8B while using a single image encoder with a 7.9x faster TTFT.\nEvaluations\nBenchmark\nFastVLM-0.5B\nFastVLM-1.5B\nFastVLM-7B\nAi2D\n68.0\n77.4\n83.6\nScienceQA\n85.2\n94.4\n96.7\nMMMU\n33.9\n37.8\n45.4\nVQAv2\n76.3\n79.1\n80.8\nChartQA\n76.0\n80.1\n85.0\nTextVQA\n64.5\n70.4\n74.9\nInfoVQA\n46.4\n59.7\n75.8\nDocVQA\n82.5\n88.3\n93.2\nOCRBench\n63.9\n70.2\n73.1\nRealWorldQA\n56.1\n61.2\n67.2\nSeedBench-Img\n71.0\n74.2\n75.4\nUsage Example\nTo run inference of PyTorch checkpoint, follow the instruction in the official repo:\nDownload the model\nhuggingface-cli download apple/FastVLM-0.5B\nRun inference using predict.py from the official repo.\npython predict.py --model-path /path/to/checkpoint-dir \\\n--image-file /path/to/image.png \\\n--prompt \"Describe the image.\"\nRun inference with Transformers (Remote Code)\nTo run inference with transformers we can leverage trust_remote_code along with the following snippet:\nimport torch\nfrom PIL import Image\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nMID = \"apple/FastVLM-0.5B\"\nIMAGE_TOKEN_INDEX = -200  # what the model code looks for\n# Load\ntok = AutoTokenizer.from_pretrained(MID, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\nMID,\ntorch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\ndevice_map=\"auto\",\ntrust_remote_code=True,\n)\n# Build chat -> render to string (not tokens) so we can place <image> exactly\nmessages = [\n{\"role\": \"user\", \"content\": \"<image>\\nDescribe this image in detail.\"}\n]\nrendered = tok.apply_chat_template(\nmessages, add_generation_prompt=True, tokenize=False\n)\npre, post = rendered.split(\"<image>\", 1)\n# Tokenize the text *around* the image token (no extra specials!)\npre_ids  = tok(pre,  return_tensors=\"pt\", add_special_tokens=False).input_ids\npost_ids = tok(post, return_tensors=\"pt\", add_special_tokens=False).input_ids\n# Splice in the IMAGE token id (-200) at the placeholder position\nimg_tok = torch.tensor([[IMAGE_TOKEN_INDEX]], dtype=pre_ids.dtype)\ninput_ids = torch.cat([pre_ids, img_tok, post_ids], dim=1).to(model.device)\nattention_mask = torch.ones_like(input_ids, device=model.device)\n# Preprocess image via the model's own processor\nimg = Image.open(\"test-2.jpg\").convert(\"RGB\")\npx = model.get_vision_tower().image_processor(images=img, return_tensors=\"pt\")[\"pixel_values\"]\npx = px.to(model.device, dtype=model.dtype)\n# Generate\nwith torch.no_grad():\nout = model.generate(\ninputs=input_ids,\nattention_mask=attention_mask,\nimages=px,\nmax_new_tokens=128,\n)\nprint(tok.decode(out[0], skip_special_tokens=True))\nCitation\nIf you found this model useful, please cite the following paper:\n@InProceedings{fastvlm2025,\nauthor = {Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari},\ntitle = {FastVLM: Efficient Vision Encoding for Vision Language Models},\nbooktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth = {June},\nyear = {2025},\n}",
    "alibaba-pai/Wan2.2-Fun-Reward-LoRAs": "Wan2.2-Fun-Reward-LoRAs\nIntroduction\nDemo\nWan2.2-Fun-A14B-InP\nQuick Start\nTraining\nLimitations\nReference\nWan2.2-Fun-Reward-LoRAs\nIntroduction\nWe explore the Reward Backpropagation technique 1 2 to optimized the generated videos by Wan2.2-Fun for better alignment with human preferences.\nWe provide the following pre-trained models (i.e. LoRAs) along with the training script. You can use these LoRAs to enhance the corresponding base model as a plug-in or train your own reward LoRA.\nFor more details, please refer to our GitHub repo.\nName\nBase Model\nReward Model\nHugging Face\nDescription\nWan2.2-Fun-A14B-InP-high-noise-HPS2.1.safetensors\nWan2.2-Fun-A14B-InP (high noise)\nHPS v2.1\nü§óLink\nOfficial HPS v2.1 reward LoRA (rank=128 and network_alpha=64) for Wan2.2-Fun-A14B-InP (high noise). It is trained with a batch size of 8 for 5,000 steps.\nWan2.2-Fun-A14B-InP-low-noise-HPS2.1.safetensors\nWan2.2-Fun-A14B-InP (low noise)\nMPS\nü§óLink\nOfficial HPS v2.1 reward LoRA (rank=128 and network_alpha=64) for Wan2.2-Fun-A14B-InP (low noise). It is trained with a batch size of 8 for 2,700 steps.\nWan2.2-Fun-A14B-InP-high-noise-MPS.safetensors\nWan2.2-Fun-A14B-InP (high noise)\nHPS v2.1\nü§óLink\nOfficial MPS reward LoRA (rank=128 and network_alpha=64) for Wan2.2-Fun-A14B-InP (high noise). It is trained with a batch size of 8 for 5,000 steps.\nWan2.2-Fun-A14B-InP-low-noise-MPS.safetensors\nWan2.2-Fun-A14B-InP (low noise)\nMPS\nü§óLink\nOfficial MPS reward LoRA (rank=128 and network_alpha=64) for Wan2.2-Fun-A14B-InP (low noise). It is trained with a batch size of 8 for 4,500 steps.\nWe found that, MPS reward LoRA for the low-noise model converges significantly more slowly than on the other models, and may not deliver satisfactory results. Therefore, for the low-noise model, we recommend using HPSv2.1 reward LoRA.\nDemo\nWan2.2-Fun-A14B-InP\nPrompt\nWan2.2-Fun-A14B-InP\nWan2.2-Fun-A14B-InP  high + low HPSv2.1 Reward LoRA\nWan2.2-Fun-A14B-InP  high MPS + low HPSv2.1 Reward LoRA\nA panda eats bamboo while a monkey swings from branch to branch\nExpanded\nIn a lush green forest, a panda sits comfortably against a tree, leisurely munching on bamboo stalks. Nearby, a lively monkey swings energetically from branch to branch, its tail curling around the limbs. Sunlight filters through the canopy, casting dappled shadows on the forest floor.\nA dog runs through a field while a cat climbs a tree\nExpanded\nIn a sunlit, expansive green field surrounded by tall trees, a playful golden retriever sprints energetically across the grass, its fur gleaming in the afternoon sun. Nearby, a nimble tabby cat gracefully climbs a sturdy tree, its claws gripping the bark effortlessly. The sky is clear blue with occasional birds flying.\nA penguin waddles on the ice, a camel treks by\nExpanded\nA small penguin waddles slowly across a vast, icy surface under a clear blue sky. The penguin's short, flipper-like wings sway at its sides as it moves. Nearby, a camel treks steadily, its long legs navigating the snowy terrain with ease. The camel's fur is thick, providing warmth in the cold environment.\nPig with wings flying above a diamond mountain\nExpanded\nA whimsical pig, complete with delicate feathered wings, soars gracefully above a shimmering diamond mountain. The pig's pink skin glistens in the sunlight as it flaps its wings. The mountain below sparkles with countless facets, reflecting brilliant rays of light into the clear blue sky.\nThe above test prompts are from T2V-CompBench and expanded into detailed prompts by Llama-3.3.\nVideos are generated with HPSv2.1 Reward LoRA weight 0.5 and MPS Reward LoRA weight 0.5.\nQuick Start\nSet lora_path along with lora_weight for the low noise reward LoRA, while specifying lora_high_path and lora_high_weight for high noise reward LoRA in examples/wan2.2_fun/predict_t2v.py.\nTraining\nPlease refer to README_TRAIN_REWARD.md\nLimitations\nWe observe after training to a certain extent, the reward continues to increase, but the quality of the generated videos does not further improve.\nThe model trickly learns some shortcuts (by adding artifacts in the background, i.e., adversarial patches) to increase the reward.\nCurrently, there is still a lack of suitable preference models for video generation. Directly using image preference models cannot\nevaluate preferences along the temporal dimension (such as dynamism and consistency). Further more, We find using image preference models leads to a decrease\nin the dynamism of generated videos. Although this can be mitigated by computing the reward using only the first frame of the decoded video, the impact still persists.\nReference\nClark, Kevin, et al. \"Directly fine-tuning diffusion models on differentiable rewards.\". In ICLR 2024.\nPrabhudesai, Mihir, et al. \"Aligning text-to-image diffusion models with reward backpropagation.\" arXiv preprint arXiv:2310.03739 (2023).",
    "LiquidAI/LFM2-1.2B-Tool": "LFM2-1.2B-Tool\nüìÑ Model details\nüìà Performance\nüèÉ How to run\nüì¨ Contact\nPlayground\nPlayground\nPlayground\nLeap\nLFM2-1.2B-Tool\nBased on LFM2-1.2B, LFM2-1.2B-Tool is designed for concise and precise tool calling. The key challenge was designing a non-thinking model that outperforms similarly sized thinking models for tool use.\nUse cases:\nMobile and edge devices requiring instant API calls, database queries, or system integrations without cloud dependency.\nReal-time assistants in cars, IoT devices, or customer support, where response latency is critical.\nResource-constrained environments like embedded systems or battery-powered devices needing efficient tool execution.\nYou can find more information about other task-specific models in this blog post.\nüìÑ Model details\nGeneration parameters: We recommend using greedy decoding with a temperature=0.\nSystem prompt: The system prompt must provide all the available tools\nSupported languages: English, Arabic, Chinese, French, German, Japanese, Korean, Portuguese, and Spanish.\nTool use: It consists of four main steps:\nFunction definition: LFM2 takes JSON function definitions as input (JSON objects between <|tool_list_start|> and <|tool_list_end|> special tokens), usually in the system prompt\nFunction call: LFM2 writes Pythonic function calls (a Python list between <|tool_call_start|> and <|tool_call_end|> special tokens), as the assistant answer.\nFunction execution: The function call is executed and the result is returned (string between <|tool_response_start|> and <|tool_response_end|> special tokens), as a \"tool\" role.\nFinal answer: LFM2 interprets the outcome of the function call to address the original user prompt in plain text.\nHere is a simple example of a conversation using tool use:\n<|startoftext|><|im_start|>system\nList of tools: <|tool_list_start|>[{\"name\": \"get_candidate_status\", \"description\": \"Retrieves the current status of a candidate in the recruitment process\", \"parameters\": {\"type\": \"object\", \"properties\": {\"candidate_id\": {\"type\": \"string\", \"description\": \"Unique identifier for the candidate\"}}, \"required\": [\"candidate_id\"]}}]<|tool_list_end|><|im_end|>\n<|im_start|>user\nWhat is the current status of candidate ID 12345?<|im_end|>\n<|im_start|>assistant\n<|tool_call_start|>[get_candidate_status(candidate_id=\"12345\")]<|tool_call_end|>Checking the current status of candidate ID 12345.<|im_end|>\n<|im_start|>tool\n<|tool_response_start|>{\"candidate_id\": \"12345\", \"status\": \"Interview Scheduled\", \"position\": \"Clinical Research Associate\", \"date\": \"2023-11-20\"}<|tool_response_end|><|im_end|>\n<|im_start|>assistant\nThe candidate with ID 12345 is currently in the \"Interview Scheduled\" stage for the position of Clinical Research Associate, with an interview date set for 2023-11-20.<|im_end|>\n‚ö†Ô∏è The model supports both single-turn and multi-turn conversations.\nüìà Performance\nFor edge inference, latency is a crucial factor in delivering a seamless and satisfactory user experience. Consequently, while test-time-compute inherently provides more accuracy, it ultimately compromises the user experience due to increased waiting times for function calls.\nTherefore, the goal was to develop a tool calling model that is competitive with thinking models, yet operates without any internal chain-of-thought process.\nWe evaluated each model on a proprietary benchmark that was specifically designed to prevent data contamination. The benchmark ensures that performance metrics reflect genuine tool-calling capabilities rather than memorized patterns from training data.\nüèÉ How to run\nHugging Face: LFM2-350M\nllama.cpp: LFM2-350M-Extract-GGUF\nLEAP: LEAP model library\nYou can use the following Colab notebooks for easy inference and fine-tuning:\nNotebook\nDescription\nLink\nInference\nRun the model with Hugging Face's transformers library.\nSFT (TRL)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using TRL.\nDPO (TRL)\nPreference alignment with Direct Preference Optimization (DPO) using TRL.\nSFT (Axolotl)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using Axolotl.\nSFT (Unsloth)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using Unsloth.\nüì¨ Contact\nIf you are interested in custom solutions with edge deployment, please contact our sales team.",
    "vibevoice/VibeVoice-7B": "VibeVoice: A Frontier Open-Source Text-to-Speech Model\nTraining Details\nModels\nInstallation and Usage\nResponsible Usage\nDirect intended uses\nOut-of-scope uses\nRisks and limitations\nRecommendations\nContact\nThanks to @aoi-ot for HF reupload, this is duplicated from their repo :)\nhttps://github.com/vibevoice-community/VibeVoice\nVibeVoice: A Frontier Open-Source Text-to-Speech Model\nVibeVoice is a novel framework designed for generating expressive, long-form, multi-speaker conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.\nA core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a next-token diffusion framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.\nThe model can synthesize speech up to 90 minutes long with up to 4 distinct speakers, surpassing the typical 1-2 speaker limits of many prior models.\n‚û°Ô∏è Technical Report: VibeVoice Technical Report\n‚û°Ô∏è Project Page: microsoft/VibeVoice\n‚û°Ô∏è Code: microsoft/VibeVoice-Code\nTraining Details\nTransformer-based Large Language Model (LLM) integrated with specialized acoustic and semantic tokenizers and a diffusion-based decoding head.\nLLM: Qwen2.5 for this release.\nTokenizers:\nAcoustic Tokenizer: Based on a œÉ-VAE variant (proposed in LatentLM), with a mirror-symmetric encoder-decoder structure featuring 7 stages of modified Transformer blocks. Achieves 3200x downsampling from 24kHz input. Encoder/decoder components are ~340M parameters each.\nSemantic Tokenizer: Encoder mirrors the Acoustic Tokenizer's architecture (without VAE components). Trained with an ASR proxy task.\nDiffusion Head: Lightweight module (4 layers, ~600M parameters) conditioned on LLM hidden states. Predicts acoustic VAE features using a Denoising Diffusion Probabilistic Models (DDPM) process. Uses Classifier-Free Guidance (CFG) and DPM-Solver (and variants) during inference.\nContext Length: Trained with a curriculum increasing up to 32,768 tokens.\nTraining Stages:\nTokenizer Pre-training: Acoustic and Semantic tokenizers are pre-trained separately.\nVibeVoice Training: Pre-trained tokenizers are frozen; only the LLM and diffusion head parameters are trained. A curriculum learning strategy is used for input sequence length (4k -> 16K -> 32K). Text tokenizer not explicitly specified, but the LLM (Qwen2.5) typically uses its own. Audio is \"tokenized\" via the acoustic and semantic tokenizers.\nModels\nModel\nContext Length\nGeneration Length\nWeight\nVibeVoice-0.5B-Streaming\n-\n-\nOn the way\nVibeVoice-1.5B\n64K\n~90 min\nHF link\nVibeVoice-Large\n32K\n~45 min\nYou are here.\nInstallation and Usage\nPlease refer to GitHub README\nResponsible Usage\nDirect intended uses\nThe VibeVoice model is limited to research purpose use exploring highly realistic audio dialogue generation detailed in the tech report.\nOut-of-scope uses\nUse in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by MIT License. Use to generate any text transcript. Furthermore, this release is not intended or licensed for any of the following scenarios:\nVoice impersonation without explicit, recorded consent ‚Äì cloning a real individual‚Äôs voice for satire, advertising, ransom, social‚Äëengineering, or authentication bypass.\nDisinformation or impersonation ‚Äì creating audio presented as genuine recordings of real people or events.\nReal‚Äëtime or low‚Äëlatency voice conversion ‚Äì telephone or video‚Äëconference ‚Äúlive deep‚Äëfake‚Äù applications.\nUnsupported language ‚Äì the model is trained only on English and Chinese data; outputs in other languages are unsupported and may be unintelligible or offensive.\nGeneration of background ambience, Foley, or music ‚Äì VibeVoice is speech‚Äëonly and will not produce coherent non‚Äëspeech audio.\nRisks and limitations\nWhile efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model.\nPotential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.\nEnglish and Chinese only: Transcripts in language other than English or Chinese may result in unexpected audio outputs.\nNon-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.\nOverlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.\nRecommendations\nWe do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.\nTo mitigate the risks of misuse, we have:\nEmbedded an audible disclaimer (e.g. ‚ÄúThis segment was generated by AI‚Äù) automatically into every synthesized audio file.\nAdded an imperceptible watermark to generated audio so third parties can verify VibeVoice provenance. Please see contact information at the end of this model card.\nLogged inference requests (hashed) for abuse pattern detection and publishing aggregated statistics quarterly.\nUsers are responsible for sourcing their datasets legally and ethically. This may include securing appropriate rights and/or anonymizing data prior to use with VibeVoice. Users are reminded to be mindful of data privacy concerns.\nContact\nThis project was conducted by members of Microsoft Research. We welcome feedback and collaboration from our audience. If you have suggestions, questions, or observe unexpected/offensive behavior in our technology, please contact us at VibeVoice@microsoft.com.\nIf the team receives reports of undesired behavior or identifies issues independently,‚ÄØwe will‚ÄØupdate this repository with appropriate mitigations.",
    "InstantX/Qwen-Image-ControlNet-Inpainting": "Qwen-Image-ControlNet-Inpainting\nModel Cards\nShowcases\nInference\nComfyUI Support\nCommunity Support\nLimitations\nAcknowledgements\nQwen-Image-ControlNet-Inpainting\nThis repository provides a ControlNet that supports mask-based image inpainting and outpainting for Qwen-Image.\nModel Cards\nThis ControlNet consists of 6 double blocks copied from the pretrained transformer layers.\nWe train the model from scratch for 65K steps using a dataset of 10M high-quality general and human images.\nWe train at 1328x1328 resolution in BFloat16, batch size=128, learning rate=4e-5. We set the text drop ratio to 0.10.\nThis model supports Object replacement, Text modification, Background replacement, Outpainting.\nShowcases\nYou can find more use cases in this blog.\nInference\nimport torch\nfrom diffusers.utils import load_image\n# pip install git+https://github.com/huggingface/diffusers\nfrom diffusers import QwenImageControlNetModel, QwenImageControlNetInpaintPipeline\nbase_model = \"Qwen/Qwen-Image\"\ncontrolnet_model = \"InstantX/Qwen-Image-ControlNet-Inpainting\"\ncontrolnet = QwenImageControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.bfloat16)\npipe = QwenImageControlNetInpaintPipeline.from_pretrained(\nbase_model, controlnet=controlnet, torch_dtype=torch.bfloat16\n)\npipe.to(\"cuda\")\nimage = load_image(\"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Inpainting/resolve/main/assets/images/image1.png\")\nmask_image = load_image(\"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Inpainting/resolve/main/assets/masks/mask1.png\")\nprompt = \"‰∏ÄËæÜÁªøËâ≤ÁöÑÂá∫ÁßüËΩ¶Ë°åÈ©∂Âú®Ë∑Ø‰∏ä\"\nimage = pipe(\nprompt=prompt,\nnegative_prompt=\" \",\ncontrol_image=image,\ncontrol_mask=mask_image,\ncontrolnet_conditioning_scale=controlnet_conditioning_scale,\nwidth=control_image.size[0],\nheight=control_image.size[1],\nnum_inference_steps=30,\ntrue_cfg_scale=4.0,\ngenerator=torch.Generator(device=\"cuda\").manual_seed(42),\n).images[0]\nimage.save(f\"qwenimage_cn_inpaint_result.png\")\nComfyUI Support\nComfyUI offers native support for Qwen-Image-ControlNet-Inpainting. The official workflow can be found here. Make sure your ComfyUI version is >=0.3.59.\nCommunity Support\nLiblib AI offers native support for Qwen-Image-ControlNet-Inpainting. Visit for online WebUI or ComfyUI inference.\nLimitations\nThis model is slightly sensitive to user prompts. Using detailed prompts that describe the entire image (both the inpainted area and the background) is highly recommended. Please use descriptive prompt instead of instructive prompt.\nAcknowledgements\nThis model is developed by InstantX Team. All copyright reserved.",
    "LLM360/K2-Think": "K2-Think: A Parameter-Efficient Reasoning System\nTransformers\nQuickstart\nTransformers\nEvaluation & Performance\nBenchmarks (pass@1, average over 16 runs)\nInference Speed\nSafety Evaluation\nTerms of Use\nCitation\nK2-Think: A Parameter-Efficient Reasoning System\nüìö Paper - üìù Code - üè¢ Project Page\nK2-Think is a 32 billion parameter open-weights general reasoning model with strong performance in competitive mathematical problem solving.\nQuickstart\nTransformers\nYou can use K2-Think with Transformers. If you use transformers.pipeline, it will apply the chat template automatically. If you use model.generate directly, you need to apply the chat template mannually.\nfrom transformers import pipeline\nimport torch\nmodel_id =  \"LLM360/K2-Think\"\npipe = pipeline(\n\"text-generation\",\nmodel=model_id,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\",\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"what is the next prime number after 2600?\"},\n]\noutputs = pipe(\nmessages,\nmax_new_tokens=32768,\n)\nprint(outputs[0][\"generated_text\"][-1])\nEvaluation & Performance\nDetailed evaluation results are reported in out Tech Report\nBenchmarks (pass@1, average over 16 runs)\nDomain\nBenchmark\nK2-Think\nMath\nAIME 2024\n90.83\nMath\nAIME 2025\n81.24\nMath\nHMMT 2025\n73.75\nMath\nOMNI-Math-HARD\n60.73\nCode\nLiveCodeBench v5\n63.97\nScience\nGPQA-Diamond\n71.08\nInference Speed\nWe deploy K2-THINK on Cerebras Wafer-Scale Engine (WSE) systems, leveraging the world‚Äôs largest processor and speculative decoding to achieve unprecedented inference speeds for our 32B reasoning system.\nPlatform\nThroughput (tokens/sec)\nExample: 32k-token response (time)\nCerebras WSE (our deployment)\n~2,000\n~16 s\nTypical Cloud Service setup\n~200\n~160 s\nSafety Evaluation\nAggregated across four safety dimensions (Safety-4):\nAspect\nMacro-Avg\nHigh-Risk Content Refusal\n0.83\nConversational Robustness\n0.89\nCybersecurity & Data Protection\n0.56\nJailbreak Resistance\n0.72\nSafety-4 Macro (avg)\n0.75\nTerms of Use\nWe have employed various techniques to reduce bias, harmful outputs, and other risks in the model. While these efforts help improve safety and reliability, the model, like all Large Language Models, may still generate inaccurate, misleading, biased, or otherwise undesirable content. By downloading, using, or interacting with this model, you acknowledge these limitations and agree to the following:\nProhibited Uses\nYou may not use this model for any illegal, unlawful, or harmful activities, including but not limited to fraud, abuse, harassment, privacy violations, or the creation/dissemination of malicious content.\nUser Responsibility\nYou are solely responsible for how you use the model and for any outcomes that result from its use.\nThe authors and institutions involved in releasing this model do not accept liability for any consequences arising from its use.\nNo Warranty\nThe model is provided ‚Äúas is‚Äù without any warranties or guarantees.\nCitation\n@misc{cheng2025k2thinkparameterefficientreasoning,\ntitle={K2-Think: A Parameter-Efficient Reasoning System},\nauthor={Zhoujun Cheng and Richard Fan and Shibo Hao and Taylor W. Killian and Haonan Li and Suqi Sun and Hector Ren and Alexander Moreno and Daqian Zhang and Tianjun Zhong and Yuxin Xiong and Yuanzhe Hu and Yutao Xie and Xudong Han and Yuqi Wang and Varad Pimpalkhute and Yonghao Zhuang and Aaryamonvikram Singh and Xuezhi Liang and Anze Xie and Jianshu She and Desai Fan and Chengqian Gao and Liqun Ma and Mikhail Yurochkin and John Maggs and Xuezhe Ma and Guowei He and Zhiting Hu and Zhengzhong Liu and Eric P. Xing},\nyear={2025},\neprint={2509.07604},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2509.07604},\n}",
    "lerobot/pi05_base": "œÄ‚ÇÄ.‚ÇÖ (Pi05)\nModel Overview\nThe Generalization Challenge\nCo-Training on Heterogeneous Data\nTraining\nCitation\nOriginal Repository\nLicense\nœÄ‚ÇÄ.‚ÇÖ (Pi05)\nThese weights directly come from the Pytorch conversion script of openpi and their pi05_base model.\nœÄ‚ÇÄ.‚ÇÖ is a Vision-Language-Action model with open-world generalization, from Physical Intelligence. The LeRobot implementation is adapted from their open source OpenPI repository.\nModel Overview\nœÄ‚ÇÄ.‚ÇÖ represents a significant evolution from œÄ‚ÇÄ, developed by Physical Intelligence to address a big challenge in robotics: open-world generalization. While robots can perform impressive tasks in controlled environments, œÄ‚ÇÄ.‚ÇÖ is designed to generalize to entirely new environments and situations that were never seen during training.\nThe Generalization Challenge\nAs Physical Intelligence explains, the fundamental challenge isn't performing tasks of agility or dexterity, but generalization, the ability to correctly perform tasks in new settings with new objects. Consider a robot cleaning different homes: each home has different objects in different places. Generalization must occur at multiple levels:\nPhysical Level: Understanding how to pick up a spoon (by the handle) or plate (by the edge), even with unseen objects in cluttered environments\nSemantic Level: Understanding task semantics, where to put clothes and shoes (laundry hamper, not on the bed), and what tools are appropriate for cleaning spills\nEnvironmental Level: Adapting to \"messy\" real-world environments like homes, grocery stores, offices, and hospitals\nCo-Training on Heterogeneous Data\nThe breakthrough innovation in œÄ‚ÇÄ.‚ÇÖ is co-training on heterogeneous data sources. The model learns from:\nMultimodal Web Data: Image captioning, visual question answering, object detection\nVerbal Instructions: Humans coaching robots through complex tasks step-by-step\nSubtask Commands: High-level semantic behavior labels (e.g., \"pick up the pillow\" for an unmade bed)\nCross-Embodiment Robot Data: Data from various robot platforms with different capabilities\nMulti-Environment Data: Static robots deployed across many different homes\nMobile Manipulation Data: ~400 hours of mobile robot demonstrations\nThis diverse training mixture creates a \"curriculum\" that enables generalization across physical, visual, and semantic levels simultaneously.\nTraining\nHere's a complete training command for finetuning the base œÄ‚ÇÄ.‚ÇÖ model on your own dataset:\npython src/lerobot/scripts/train.py \\\n--dataset.repo_id=your_dataset \\\n--policy.type=pi05 \\\n--output_dir=./outputs/pi05_training \\\n--job_name=pi05_training \\\n--policy.repo_id=your_repo_id \\\n--policy.pretrained_path=lerobot/pi05_base \\\n--policy.compile_model=true \\\n--policy.gradient_checkpointing=true \\\n--wandb.enable=true \\\n--policy.dtype=bfloat16 \\\n--steps=3000 \\\n--policy.scheduler_decay_steps=3000 \\\n--policy.device=cuda \\\n--batch_size=32\nCitation\nIf you use this model, please cite the original OpenPI work:\n@article{openpi2024,\ntitle={Open-World Robotic Manipulation with Vision-Language-Action Models},\nauthor={Physical Intelligence},\nyear={2024},\nurl={https://github.com/Physical-Intelligence/openpi}\n}\nOriginal Repository\nOpenPI GitHub Repository\nLicense\nThis model follows the same license as the original OpenPI repository.",
    "decart-ai/Lucy-Edit-Dev": "Lucy Edit Dev (5B)\nüé¨ Demos\nSample 2\nüî• Latest News\nüõ†Ô∏è Quickstart\nInstallation\nInference\nPrompting Guidelines & Supported Edits\nTrigger Words\nSupported Edit Types\nAdditional Notes\nüì¶ Integrations\nüß≠ Roadmap\nüîí License\nüì£ Citation\nüôè Acknowledgements\nüì¨ Contact\nLucy Edit Dev (5B)\nüß™ GitHub\n|¬† üìñ Playground\n|¬† üìë Technical Paper\n|¬† üí¨ Discord\nYour browser does not support the video tag.\nPut the woman in gothic black jeans and leather jacket and crop top under it.\nYour browser does not support the video tag.\n1.2) Put her in a clown outfit.\nYour browser does not support the video tag.\n1.3) Put the woman in a red bikini with an open thick coat above it.\nLucy Edit Dev is an open-weight video editing model that performs instruction-guided edits on videos using free-text prompts ‚Äî it supports a variety of edits, such as clothing & accessory changes, character changes, object insertions, and scene replacements while preserving the motion and composition perfectly.\nüöÄ First open-source instruction-guided video editing model\nüß© Built on Wan2.2 5B architecture ‚Äî inherits high-compression VAE + DiT stack, making adapting existing scripts and workflows easy.\nüèÉ‚Äç‚ôÇÔ∏è Motion Preservation - preserves the motion and composition of videos perfectly, allowing precise edits.\nüéØ Edit reliability ‚Äî edits are more robust when compared to common inference time methods.\nüß¢ Wardrobe & accessories ‚Äî change outfits, add glasses/earrings/hats/etc.\nüßå Character Changes ‚Äî replace characters with monsters, animals and known characters. (e.g., \"Replace the person with a polar bear\")\nüó∫Ô∏è Scenery swap ‚Äî move the scene (e.g., \"transform the scene into a 2D cartoon,\")\nüìù Pure text instructions ‚Äî no finetuning, no masks required for common edits\n‚ÑπÔ∏è Model size: ~5B params. Build on top of Wan2.2 5B.\nüé¨ Demos\n### Sample 1\nYour browser does not support the video tag.\n1.1) Turn the man into an alien\nYour browser does not support the video tag.\n1.2) Turn the man into a bear\nYour browser does not support the video tag.\n1.3) Make it snowy\nSample 2\nYour browser does not support the video tag.\n2.1) Turn the woman into Harley Quinn\nYour browser does not support the video tag.\n2.2) Turn the woman into Lego\nYour browser does not support the video tag.\n2.3) Turn the shirt into a sports jersey\nNote: The prompts above are not enriched, the model will react better to enriched prompts - as described in the prompt guideline section below.\nüî• Latest News\n[2025-09-18]: Initial Lucy Edit Dev weights & reference code released.\n[2025-09-16]: Diffusers integration PR opened and merged. PR #12340.\nüõ†Ô∏è Quickstart\nInstallation\npip install git+https://github.com/huggingface/diffusers\nInference\nPlease refer to the \"Prompting Guidelines & Supported Edits\" section for the best experience.\nfrom typing import List\nimport torch\nfrom PIL import Image\nfrom diffusers import AutoencoderKLWan, LucyEditPipeline\nfrom diffusers.utils import export_to_video, load_video\n# Arguments\nurl = \"https://d2drjpuinn46lb.cloudfront.net/painter_original_edit.mp4\"\nprompt = \"Change the apron and blouse to a classic clown costume: satin polka-dot jumpsuit in bright primary colors, ruffled white collar, oversized pom-pom buttons, white gloves, oversized red shoes, red foam nose; soft window light from left, eye-level medium shot, natural folds and fabric highlights.\"\nnegative_prompt = \"\"\nnum_frames = 81\nheight = 480\nwidth = 832\n# Load video\ndef convert_video(video: List[Image.Image]) -> List[Image.Image]:\nvideo = load_video(url)[:num_frames]\nvideo = [video[i].resize((width, height)) for i in range(num_frames)]\nreturn video\nvideo = load_video(url, convert_method=convert_video)\n# Load model\nmodel_id = \"decart-ai/Lucy-Edit-Dev\"\nvae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\npipe = LucyEditPipeline.from_pretrained(model_id, vae=vae, torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\n# Generate video\noutput = pipe(\nprompt=prompt,\nvideo=video,\nnegative_prompt=negative_prompt,\nheight=480,\nwidth=832,\nnum_frames=81,\nguidance_scale=5.0\n).frames[0]\n# Export video\nexport_to_video(output, \"output.mp4\", fps=24)\nPrompting Guidelines & Supported Edits\nLucy Edit is built for precise, realistic, and identity-preserving video edits.Prompts with ~20‚Äì30 descriptive words work best. Using the right trigger words helps the model understand your intent.\nTrigger Words\nChange ‚Üí Clothing or color modifications\nAdd ‚Üí Adding animals or objects\nReplace ‚Üí Object substitution or subject swap\nTransform to ‚Üí Global scene or style transformations\nSupported Edit Types\n1. Clothing Changes\n‚úÖ Best performance. Lucy Edit excels at swapping outfits while preserving motion, pose, and identity.Example: ‚ÄúChange the shirt to a kimono with wide sleeves and patterned fabric.‚Äù\n2. Human/Character Replacement\n‚úÖ Strong results. Works well for transforming people into new characters or creatures. Detailed prompts are key.Example: ‚ÄúReplace the person with a tiger, striped orange fur, muscular build, and glowing green eyes.‚ÄùExample: ‚ÄúReplace the person with an 2D anime character, big eyes, blue gown and battle scars.‚Äù\n3. Replace Objects\n‚úÖ Reliable for structure-preserving swaps. Ideal when replacing one object with another of similar scale.Example: ‚ÄúReplace the apple with a glowing crystal ball emitting blue light.‚Äù\n4. Color Changes\n‚ö†Ô∏è Mixed reliability. Sometimes subtle, sometimes exaggerated. Works best with precise descriptions.Example: ‚ÄúChange the jacket color to deep red leather with a glossy finish.‚Äù\n5. Add Objects\n‚ö†Ô∏è Often attaches to the subject. Works best for wearable or handheld props.Example: ‚ÄúAdd a golden crown on the person‚Äôs head, decorated with ornate jewels.‚Äù\n6. Global Transformations\n‚ö†Ô∏è Effective for backgrounds or scene-wide changes, might alter the subject Alter environment or style, might, Often changes the identity of the subject.\nExample: ‚ÄúTransform the sunny beach into a snowy tundra with falling snowflakes.‚Äù\nAdditional Notes\nStrengths: Lucy Edit excels at identity conservation, edit precision, realism, and prompt adherence.\nDetail matters: Longer prompts (20‚Äì30 words) describing style, appearance, and context improve results.\nFrame count: 81-frame generations produce better temporal consistency than shorter clips.\nüì¶ Integrations\n‚òÅÔ∏è Hosted API: You can access the model on our API and get 5000 free credits here.\nüß® Diffusers: Coming soon\nüß© ComfyUI: Coming soon\nüß≠ Roadmap\n‚úÖ Public Batch API.\n‚úÖ Diffusers pipeline (LucyEditPipeline)\n‚úÖ Remote ComfyUI custom nodes.\n‚úÖ Technical Report\nLocal Inference ComfyUI Nodes.\nLoRA and fine-tuning scripts.\nüîí License\nThis model falls under the  LUCY EDIT DEV MODEL Non-Commercial License v1.0\nüì£ Citation\n@article{decart2025lucyedit,\ntitle   = {Lucy Edit: Open-Weight Text-Guided Video Editing},\nauthor  = {DecartAI Team},\nyear    = {2025}\nurl     = { https://d2drjpuinn46lb.cloudfront.net/Lucy_Edit__High_Fidelity_Text_Guided_Video_Editing.pdf}\n}\nüôè Acknowledgements\nLucy Edit Dev builds on the excellent foundations of Wan2.2 (5B), and thanks the broader open-source community including diffusers and Hugging Face.\nüì¨ Contact\nGitHub Issues: DecartAI/lucy-edit.\nDiscord: Join our discord server, here.",
    "Manojb/Qwen3-4B-toolcalling-gguf-codex": "Specialized Qwen3 4B tool-calling\nOne-Command Setup\nüîß API Integration Made Easy\nüõ†Ô∏è Tool Selection Intelligence\nüìä Multi-Step Workflows\nSpecs\nQuick Start Examples\nBasic Function Calling\nAdvanced Tool Usage\nWhy Choose This Over Alternatives\nSystem Requirements\nBenchmark Results\nLicense\nSpecialized Qwen3 4B tool-calling\n‚úÖ Fine-tuned on 60K function calling examples\n‚úÖ 4B parameters (sweet spot for local deployment)\n‚úÖ GGUF format (optimized for CPU/GPU inference)\n‚úÖ 3.99GB download (fits on any modern system)\n‚úÖ Production-ready with 0.518 training loss\nOne-Command Setup\n# Download and run instantly\nollama create qwen3:toolcall -f ModelFile\nollama run qwen3:toolcall\nüîß API Integration Made Easy\n# Ask: \"Get weather data for New York and format it as JSON\"\n# Model automatically calls weather API with proper parameters\nüõ†Ô∏è Tool Selection Intelligence\n# Ask: \"Analyze this CSV file and create a visualization\"\n# Model selects appropriate tools: pandas, matplotlib, etc.\nüìä Multi-Step Workflows\n# Ask: \"Fetch stock data, calculate moving averages, and email me the results\"\n# Model orchestrates multiple function calls seamlessly\nSpecs\nBase Model: Qwen3-4B-Instruct\nFine-tuning: LoRA on function calling dataset\nFormat: GGUF (optimized for local inference)\nContext Length: 262K tokens\nPrecision: FP16 optimized\nMemory: Gradient checkpointing enabled\nQuick Start Examples\nBasic Function Calling\n# Load with Ollama\nimport requests\nresponse = requests.post('http://localhost:11434/api/generate', json={\n'model': 'qwen3:toolcall',\n'prompt': 'Get the current weather in San Francisco and convert to Celsius',\n'stream': False\n})\nprint(response.json()['response'])\nAdvanced Tool Usage\n# The model understands complex tool orchestration\nprompt = \"\"\"\nI need to:\n1. Fetch data from the GitHub API\n2. Process the JSON response\n3. Create a visualization\n4. Save it as a PNG file\nWhat tools should I use and how?\n\"\"\"\nBuilding AI agents that need tool calling\nCreating local coding assistants\nLearning function calling without cloud dependencies\nPrototyping AI applications on a budget\nPrivacy-sensitive development work\nWhy Choose This Over Alternatives\nFeature\nThis Model\nCloud APIs\nOther Local Models\nCost\nFree after download\n$0.01-0.10 per call\nOften larger/heavier\nPrivacy\n100% local\nData sent to servers\nVaries\nSpeed\nInstant\nNetwork dependent\nOften slower\nReliability\nAlways available\nService dependent\nDepends on setup\nCustomization\nFull control\nLimited\nVaries\nSystem Requirements\nGPU: 6GB+ VRAM (RTX 3060, RTX 4060, etc.)\nRAM: 8GB+ system RAM\nStorage: 5GB free space\nOS: Windows, macOS, Linux\nBenchmark Results\nFunction Call Accuracy: 94%+ on test set\nParameter Extraction: 96%+ accuracy\nTool Selection: 92%+ correct choices\nResponse Quality: Maintains conversational ability\nPERFECT for developers who want:\nLocal AI coding assistant (like Codex but private)\nFunction calling without API costs\n6GB VRAM compatibility (runs on most gaming GPUs)\nZero internet dependency once downloaded\nOllama integration (one-command setup)\n@model{Qwen3-4B-toolcalling-gguf-codex,\ntitle={Qwen3-4B-toolcalling-gguf-codex: Local Function Calling},\nauthor={Manojb},\nyear={2025},\nurl={https://huggingface.co/Manojb/Qwen3-4B-toolcalling-gguf-codex}\n}\nLicense\nApache 2.0 - Use freely for personal and commercial projects\nBuilt with ‚ù§Ô∏è for the developer community",
    "eddy1111111/Wan_toolkit": "YAML Metadata\nWarning:\nempty or missing yaml metadata in repo card\n(https://huggingface.co/docs/hub/model-cards#model-card-metadata)",
    "deepseek-ai/DeepSeek-V3.1-Terminus": "DeepSeek-V3.1-Terminus\nIntroduction\nHow to Run Locally\nLicense\nCitation\nContact\nDeepSeek-V3.1-Terminus\nIntroduction\nThis update maintains the model's original capabilities while addressing issues reported by users, including:\nLanguage consistency: Reducing instances of mixed Chinese-English text and occasional abnormal characters;\nAgent capabilities: Further optimizing the performance of the Code Agent and Search Agent.\nBenchmark\nDeepSeek-V3.1\nDeepSeek-V3.1-Terminus\nReasoning Mode w/o Tool Use\nMMLU-Pro\n84.8\n85.0\nGPQA-Diamond\n80.1\n80.7\nHumanity's Last Exam\n15.9\n21.7\nLiveCodeBench\n74.8\n74.9\nCodeforces\n2091\n2046\nAider-Polyglot\n76.3\n76.1\nAgentic Tool Use\nBrowseComp\n30.0\n38.5\nBrowseComp-zh\n49.2\n45.0\nSimpleQA\n93.4\n96.8\nSWE Verified\n66.0\n68.4\nSWE-bench Multilingual\n54.5\n57.8\nTerminal-bench\n31.3\n36.7\nThe template and tool-set of search agent have been updated, which is shown in assets/search_tool_trajectory.html.\nHow to Run Locally\nThe model structure of DeepSeek-V3.1-Terminus is the same as DeepSeek-V3. Please visit DeepSeek-V3 repo for more information about running this model locally.\nFor the model's chat template other than search agent, please refer to the DeepSeek-V3.1 repo.\nHere we also provide an updated inference demo code in the inference folder to help the community get started with running our model and understand the details of model architecture.\nNOTE: In the current model checkpoint, the parameters of self_attn.o_proj do not conform to the UE8M0 FP8 scale data format. This is a known issue and will be corrected in future model releases.\nLicense\nThis repository and the model weights are licensed under the MIT License.\nCitation\n@misc{deepseekai2024deepseekv3technicalreport,\ntitle={DeepSeek-V3 Technical Report},\nauthor={DeepSeek-AI},\nyear={2024},\neprint={2412.19437},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2412.19437},\n}\nContact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "Qwen/Qwen3Guard-Gen-0.6B": "Qwen3Guard-Gen-0.6B\nQuickstart\nModerating User Prompt\nModerating Model Response\nDeployment with SGLang and vLLM\nSafety Policy\nCitation\nQwen3Guard-Gen-0.6B\nQwen3Guard is a series of safety moderation models built upon Qwen3 and trained on a dataset of 1.19 million prompts and responses labeled for safety. The series includes models of three sizes (0.6B, 4B, and 8B) and features two specialized variants: Qwen3Guard-Gen, a generative model that frames safety classification as an instruction-following task, and Qwen3Guard-Stream, which incorporates a token-level classification head for real-time safety monitoring during incremental text generation.\nThis repository hosts Qwen3Guard-Gen, which offers the following key advantages:\nThree-Tiered Severity Classification: Enables detailed risk assessment by categorizing outputs into safe, controversial, and unsafe severity levels, supporting adaptation to diverse deployment scenarios.\nMultilingual Support: Qwen3Guard-Gen supports 119 languages and dialects, ensuring robust performance in global and cross-lingual applications.\nStrong Performance: Qwen3Guard-Gen achieves state-of-the-art performance on various safety benchmarks, excelling in both prompt and response classification across English, Chinese, and multilingual tasks.\nFor more details, please refer to our blog, GitHub, and Technical Report.\nQuickstart\nThe latest version of transformers is recommended and transformers>=4.51.0 is required.\nModerating User Prompt\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport re\nmodel_name = \"Qwen/Qwen3Guard-Gen-0.6B\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ndef extract_label_and_categories(content):\nsafe_pattern = r\"Safety: (Safe|Unsafe|Controversial)\"\ncategory_pattern = r\"(Violent|Non-violent Illegal Acts|Sexual Content or Sexual Acts|PII|Suicide & Self-Harm|Unethical Acts|Politically Sensitive Topics|Copyright Violation|Jailbreak|None)\"\nsafe_label_match = re.search(safe_pattern, content)\nlabel = safe_label_match.group(1) if safe_label_match else None\ncategories = re.findall(category_pattern, content)\nreturn label, categories\n# prepare the model input\nprompt = \"How can I make a bomb?\"\n# for prompt moderation\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=128\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(content)\n# '''\n# Safety: Unsafe\n# Categories: Violent\n# '''\nsafe_label, categories = extract_label_and_categories(content)\nprint(safe_label, categories)\nModerating Model Response\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport re\nmodel_name = \"Qwen/Qwen3Guard-4B-Gen\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ndef extract_label_categories_refusal(content):\nsafe_pattern = r\"Safety: (Safe|Unsafe|Controversial)\"\ncategory_pattern = r\"(Violent|Non-violent Illegal Acts|Sexual Content or Sexual Acts|PII|Suicide & Self-Harm|Unethical Acts|Politically Sensitive Topics|Copyright Violation|None)\"\nrefusal_pattern = r\"Refusal: (Yes|No)\"\nsafe_label_match = re.search(safe_pattern, content)\nrefusal_label_match = re.search(refusal_pattern, content)\nlabel = safe_label_match.group(1) if safe_label_match else None\nrefusal_label = refusal_label_match.group(1) if refusal_label_match else None\ncategories = re.findall(category_pattern, content)\nreturn label, categories, refusal_label\n# prepare the model input\nprompt = \"How can I make a bomb?\"\nresponse = \"As a responsible artificial intelligence system, I regret that I am unable to fulfill your request.\"\n# for response moderation\nmessages = [\n{\"role\": \"user\", \"content\": prompt},\n{\"role\": \"assistant\", \"content\": response},\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=128\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(content)\n# '''\n# Safety: Safe\n# Categories: None\n# Refusal: Yes\n# '''\nsafe_label, category_label, refusal_label = extract_label_categories_refusal(content)\nprint(safe_label, categories, refusal_label)\nDeployment with SGLang and vLLM\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.9.0 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3Guard-Gen-0.6B --port 30000 --context-length 32768\nvLLM:vllm serve Qwen/Qwen3Guard-Gen-0.6B --port 8000 --max-model-len 32768\nHere is an example API call using OpenAI-Compatible server:\nfrom openai import OpenAI\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nmodel = \"Qwen/Qwen3Guard-Gen-0.6B\"\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base)\n# Prompt Moderation\nprompt = \"How can I make a bomb?\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\nchat_completion = client.chat.completions.create(\nmessages=messages,\nmodel=model)\nprint(chat_completion.choices[0].message.content)\n# '''\n# Safety: Unsafe\n# Categories: Violent\n# '''\n# Response Moderation\nprompt = \"How can I make a bomb?\"\nresponse = \"As a responsible artificial intelligence system, I regret that I am unable to fulfill your request.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt},\n{\"role\": \"assistant\", \"content\": response}\n]\nprint(chat_completion.choices[0].message.content)\n# '''\n# Safety: Safe\n# Categories: None\n# Refusal: Yes\n# '''\nSafety Policy\nIn Qwen3Guard, potential harms are classified into three severity levels:\nUnsafe: Content generally considered harmful across most scenarios.\nControversial: Content whose harmfulness may be context-dependent or subject to disagreement across different applications.\nSafe: Content generally considered safe across most scenarios.\nIn the current version of Qwen3Guard, we consider the following safety categories:\nViolent: Content that provides detailed instructions, methods, or advice on how to commit acts of violence, including the manufacture, acquisition, or use of weapons. Also includes depictions of violence.\nNon-violent Illegal Acts: Content providing guidance or advice for non-violent illegal activities like hacking, unauthorized drug production, or stealing.\nSexual Content or Sexual Acts: Content offering any sexual imagery, references, or descriptions featuring individuals. Also includes content that describes explicit sexual imagery, references, or descriptions containing illegal or unethical sexual acts, such as rape, bestiality, incest, and sexual slavery.\nPersonally Identifiable Information: Content offering unauthorized sharing or disclosure of sensitive personal identifying information, such as name, ID number, address, phone number, medical records, financial details, and account passwords, etc.\nSuicide & Self-Harm: Content advocating, directly encouraging, or detailing methods for self-harm, suicide, or dangerous activities that could lead to serious injury or death.\nUnethical Acts: Any immoral or unethical content or acts, including but not limited to bias, discrimination, stereotype, injustice, hate speech, offensive language, harassment, insults, threat, defamation, extremism, misinformation regarding ethics, and other behaviors that while not illegal are still considered unethical.\nPolitically Sensitive Topics: The deliberate creation or spread of false information about government actions, historical events, or public figures that is demonstrably untrue and poses risk of public deception or social harm.\nCopyright Violation: Content offering unauthorized reproduction, distribution, public display, or derivative use of copyrighted materials, such as novels, scripts, lyrics, and other creative works protected by law, without the explicit permission of the copyright holder.\nJailbreak (Only for input): Content that explicitly attempts to override the model's system prompt or model conditioning.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3guard,\ntitle={Qwen3Guard Technical Report},\nauthor={Qwen Team},\nyear={2025},\nurl={http://arxiv.org/abs/2510.14276},\n}",
    "Qwen/Qwen3Guard-Gen-8B": "Qwen3Guard-Gen-8B\nQuickstart\nModerating User Prompt\nModerating Model Response\nDeployment with SGLang and vLLM\nSafety Policy\nCitation\nQwen3Guard-Gen-8B\nQwen3Guard is a series of safety moderation models built upon Qwen3 and trained on a dataset of 1.19 million prompts and responses labeled for safety. The series includes models of three sizes (0.6B, 4B, and 8B) and features two specialized variants: Qwen3Guard-Gen, a generative model that frames safety classification as an instruction-following task, and Qwen3Guard-Stream, which incorporates a token-level classification head for real-time safety monitoring during incremental text generation.\nThis repository hosts Qwen3Guard-Gen, which offers the following key advantages:\nThree-Tiered Severity Classification: Enables detailed risk assessment by categorizing outputs into safe, controversial, and unsafe severity levels, supporting adaptation to diverse deployment scenarios.\nMultilingual Support: Qwen3Guard-Gen supports 119 languages and dialects, ensuring robust performance in global and cross-lingual applications.\nStrong Performance: Qwen3Guard-Gen achieves state-of-the-art performance on various safety benchmarks, excelling in both prompt and response classification across English, Chinese, and multilingual tasks.\nFor more details, please refer to our blog, GitHub, and Technical Report.\nQuickstart\nThe latest version of transformers is recommended and transformers>=4.51.0 is required.\nModerating User Prompt\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport re\nmodel_name = \"Qwen/Qwen3Guard-Gen-8B\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ndef extract_label_and_categories(content):\nsafe_pattern = r\"Safety: (Safe|Unsafe|Controversial)\"\ncategory_pattern = r\"(Violent|Non-violent Illegal Acts|Sexual Content or Sexual Acts|PII|Suicide & Self-Harm|Unethical Acts|Politically Sensitive Topics|Copyright Violation|Jailbreak|None)\"\nsafe_label_match = re.search(safe_pattern, content)\nlabel = safe_label_match.group(1) if safe_label_match else None\ncategories = re.findall(category_pattern, content)\nreturn label, categories\n# prepare the model input\nprompt = \"How can I make a bomb?\"\n# for prompt moderation\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=128\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(content)\n# '''\n# Safety: Unsafe\n# Categories: Violent\n# '''\nsafe_label, categories = extract_label_and_categories(content)\nprint(safe_label, categories)\nModerating Model Response\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport re\nmodel_name = \"Qwen/Qwen3Guard-Gen-8B\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ndef extract_label_categories_refusal(content):\nsafe_pattern = r\"Safety: (Safe|Unsafe|Controversial)\"\ncategory_pattern = r\"(Violent|Non-violent Illegal Acts|Sexual Content or Sexual Acts|PII|Suicide & Self-Harm|Unethical Acts|Politically Sensitive Topics|Copyright Violation|None)\"\nrefusal_pattern = r\"Refusal: (Yes|No)\"\nsafe_label_match = re.search(safe_pattern, content)\nrefusal_label_match = re.search(refusal_pattern, content)\nlabel = safe_label_match.group(1) if safe_label_match else None\nrefusal_label = refusal_label_match.group(1) if refusal_label_match else None\ncategories = re.findall(category_pattern, content)\nreturn label, categories, refusal_label\n# prepare the model input\nprompt = \"How can I make a bomb?\"\nresponse = \"As a responsible artificial intelligence system, I regret that I am unable to fulfill your request.\"\n# for response moderation\nmessages = [\n{\"role\": \"user\", \"content\": prompt},\n{\"role\": \"assistant\", \"content\": response},\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=128\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(content)\n# '''\n# Safety: Safe\n# Categories: None\n# Refusal: Yes\n# '''\nsafe_label, category_label, refusal_label = extract_label_categories_refusal(content)\nprint(safe_label, categories, refusal_label)\nDeployment with SGLang and vLLM\nFor deployment, you can use sglang>=0.4.6.post1 or vllm>=0.9.0 or to create an OpenAI-compatible API endpoint:\nSGLang:python -m sglang.launch_server --model-path Qwen/Qwen3Guard-Gen-8B --port 30000 --context-length 32768\nvLLM:vllm serve Qwen/Qwen3Guard-Gen-8B --port 8000 --max-model-len 32768\nHere is an example API call using OpenAI-Compatible server:\nfrom openai import OpenAI\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nmodel = \"Qwen/Qwen3Guard-Gen-8B\"\nclient = OpenAI(\napi_key=openai_api_key,\nbase_url=openai_api_base)\n# Prompt Moderation\nprompt = \"How can I make a bomb?\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\nchat_completion = client.chat.completions.create(\nmessages=messages,\nmodel=model)\nprint(chat_completion.choices[0].message.content)\n# '''\n# Safety: Unsafe\n# Categories: Violent\n# '''\n# Response Moderation\nprompt = \"How can I make a bomb?\"\nresponse = \"As a responsible artificial intelligence system, I regret that I am unable to fulfill your request.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt},\n{\"role\": \"assistant\", \"content\": response}\n]\nprint(chat_completion.choices[0].message.content)\n# '''\n# Safety: Safe\n# Categories: None\n# Refusal: Yes\n# '''\nSafety Policy\nIn Qwen3Guard, potential harms are classified into three severity levels:\nUnsafe: Content generally considered harmful across most scenarios.\nControversial: Content whose harmfulness may be context-dependent or subject to disagreement across different applications.\nSafe: Content generally considered safe across most scenarios.\nIn the current version of Qwen3Guard, we consider the following safety categories:\nViolent: Content that provides detailed instructions, methods, or advice on how to commit acts of violence, including the manufacture, acquisition, or use of weapons. Also includes depictions of violence.\nNon-violent Illegal Acts: Content providing guidance or advice for non-violent illegal activities like hacking, unauthorized drug production, or stealing.\nSexual Content or Sexual Acts: Content offering any sexual imagery, references, or descriptions featuring individuals. Also includes content that describes explicit sexual imagery, references, or descriptions containing illegal or unethical sexual acts, such as rape, bestiality, incest, and sexual slavery.\nPersonally Identifiable Information: Content offering unauthorized sharing or disclosure of sensitive personal identifying information, such as name, ID number, address, phone number, medical records, financial details, and account passwords, etc.\nSuicide & Self-Harm: Content advocating, directly encouraging, or detailing methods for self-harm, suicide, or dangerous activities that could lead to serious injury or death.\nUnethical Acts: Any immoral or unethical content or acts, including but not limited to bias, discrimination, stereotype, injustice, hate speech, offensive language, harassment, insults, threat, defamation, extremism, misinformation regarding ethics, and other behaviors that while not illegal are still considered unethical.\nPolitically Sensitive Topics: The deliberate creation or spread of false information about government actions, historical events, or public figures that is demonstrably untrue and poses risk of public deception or social harm.\nCopyright Violation: Content offering unauthorized reproduction, distribution, public display, or derivative use of copyrighted materials, such as novels, scripts, lyrics, and other creative works protected by law, without the explicit permission of the copyright holder.\nJailbreak (Only for input): Content that explicitly attempts to override the model's system prompt or model conditioning.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen3guard,\ntitle={Qwen3Guard Technical Report},\nauthor={Qwen Team},\nyear={2025},\nurl={http://arxiv.org/abs/2510.14276},\n}",
    "ai21labs/AI21-Jamba-Reasoning-3B": "Introduction\nKey Advantages\nModel Details\nQuickstart\nRun the model locally\nRun the model with vLLM\nRun the model with Transformers\nTraining Details\nReinforcement ‚ÄúFine-Tuning‚Äù\nLicense\nCitation\nIntroduction\nAI21‚Äôs Jamba Reasoning 3B is a top-performing reasoning model that packs leading scores on intelligence benchmarks and highly-efficient processing into a compact 3B build.\nRead the full blog post here.\nKey Advantages\nFast: Optimized for efficient sequence processing\nThe hybrid design combines Transformer attention with Mamba (a state-space model). Mamba layers are more efficient for sequence processing, while attention layers capture complex dependencies. This mix reduces memory overhead, improves throughput, and makes the model run smoothly on laptops, GPUs, and even mobile devices, while maintainig impressive quality.\nSmart: Leading intelligence scores\nThe model outperforms competitors, such as Gemma 3 4B, Llama 3.2 3B, and Granite 4.0 Micro, on a combined intelligence score that averages 6 standard benchmarks.\nScalable: Handles very long contexts\nUnlike most compact models, Jamba Reasoning 3B supports extremely long contexts. Mamba layers allow the model to process inputs without storing massive attention caches, so it scales to 256K tokens while keeping inference practical. This makes it suitable for edge deployment as well as datacenter workloads.\nModel Details\nNumber of Parameters: 3B\nNumber of Layers: 28 (26 Mamba, 2 Attention)\nNumber of Attention Heads: 20 MQA (20 for Q, 1 for KV)\nVocabulary Size: 64K\nContext Length: 256k\nArchitecture: Hybrid Transformer‚ÄìMamba with efficient attention and long-context support\nDeveloped by:¬†AI21\nSupported languages:¬†English, Spanish, French, Portuguese, Italian, Dutch, German, Arabic and Hebrew\nIntelligence benchmark results:\nMMLU-Pro\nHumanity‚Äôs Last Exam\nIFBench\nDeepSeek R1 Distill Qwen 1.5B\n27.0%\n3.3%\n13.0%\nPhi-4 mini\n47.0%\n4.2%\n21.0%\nGranite 4.0 Micro\n44.7%\n5.1%\n24.8%\nLlama 3.2 3B\n35.0%\n5.2%\n26.0%\nGemma 3 4B\n42.0%\n5.2%\n28.0%\nQwen 3 1.7B\n57.0%\n4.8%\n27.0%\nQwen 3 4B\n70%\n5.1%\n33%\nJamba Reasoning 3B\n61.0%\n6.0%\n52.0%\nQuickstart\nRun the model locally\nPlease reference the GGUF model card here.\nRun the model with vLLM\nFor best results, we recommend using vLLM version 0.11.0 or higher and enabling --mamba-ssm-cache-dtype=float32\npip install vllm>=0.11.0\nUsing vllm in online server mode:\nvllm serve \"ai21labs/AI21-Jamba-Reasoning-3B\" --mamba-ssm-cache-dtype float32 --reasoning-parser deepseek_r1 --enable-auto-tool-choice --tool-call-parser hermes\nUsing vllm in offline mode:\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoTokenizer\nmodel = \"ai21labs/AI21-Jamba-Reasoning-3B\"\nllm = LLM(model=model,\ntensor_parallel_size=1,\nmamba_ssm_cache_dtype=\"float32\")\ntokenizer = AutoTokenizer.from_pretrained(model)\nmessages = [\n{\"role\": \"user\", \"content\": \"You are analyzing customer support tickets to decide which need escalation.\\nTicket 1: 'App crashes when uploading files >50MB.'\\nTicket 2: 'Forgot password, can‚Äôt log in.'\\nTicket 3: 'Billing page missing enterprise pricing.'\\nClassify each ticket as Critical, Medium, or Low and explain your reasoning.\\n\"},\n]\nprompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\nsampling_params = SamplingParams(temperature=0.6, max_tokens=4096)\noutputs = llm.generate(prompts, sampling_params)\ngenerated_text = outputs[0].outputs[0].text\nprint(generated_text)\nRun the model with Transformers\npip install transformers>=¬†4.54.0\npip install flash-attn --no-build-isolation\npip install causal-conv1d>=1.2.0\npip install mamba-ssm\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"ai21labs/AI21-Jamba-Reasoning-3B\",\ndtype=torch.bfloat16,\nattn_implementation=\"flash_attention_2\",\ndevice_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"ai21labs/AI21-Jamba-Reasoning-3B\")\nmessages = [\n{\"role\": \"user\", \"content\": \"You are analyzing customer support tickets to decide which need escalation.\\nTicket 1: 'App crashes when uploading files >50MB.'\\nTicket 2: 'Forgot password, can‚Äôt log in.'\\nTicket 3: 'Billing page missing enterprise pricing.'\\nClassify each ticket as Critical, Medium, or Low and explain your reasoning.\\n\"},\n]\nprompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\noutputs = model.generate(**tokenizer(prompts, return_tensors=\"pt\").to(model.device), do_sample=True, temperature=0.6, max_new_tokens=4096)\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)\nTraining Details\nWe trained the model in multiple stages, each designed to strengthen reasoning and long-context performance. The process began with large-scale pre-training on a diverse corpus of natural documents. We then mid-trained on ~0.5T tokens of math and code, while extending the context length to 32K tokens. During this stage we also applied a Mamba-specific long-context method, which we found to significantly improve long-context abilities.\nTo improve reasoning, tool use, and instruction following, we applied cold-start distillation: supervised fine-tuning with a 32K window and direct preference optimization with a 64K window. Finally, we enhanced reasoning performance further through online reinforcement learning with RLVR, targeting tasks such as code generation, mathematical problem solving, structured output, and information extraction.\nReinforcement ‚ÄúFine-Tuning‚Äù\nFull support for training Jamba through VeRL will be available soon. AI21 has introduced several improvements to the VeRL framework (https://github.com/volcengine/verl), including new capabilities for training hybrid models, and stability improvements for GRPO training. These improvements will soon be available to the open source community.\nLicense\nApache 2.0\nCitation\nBlog post- Read the full blog post here",
    "LiquidAI/LFM2-8B-A1B-GGUF": "LFM2-8B-A1B\nüèÉ How to run LFM2\nPlayground\nPlayground\nPlayground\nLeap\nLFM2-8B-A1B\nLFM2 is a new generation of hybrid models developed by Liquid AI, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.\nWe're releasing the weights of our first MoE based on LFM2, with 8.3B total parameters and 1.5B active parameters.\nLFM2-8B-A1B is the best on-device MoE in terms of both quality (comparable to 3-4B dense models) and speed (faster than Qwen3-1.7B).\nCode and knowledge capabilities are significantly improved compared to LFM2-2.6B.\nQuantized variants fit comfortably on high-end phones, tablets, and laptops.\nFind more information about LFM2-8B-A1B in our blog post.\nüèÉ How to run LFM2\nExample usage with llama.cpp:\nllama-cli -hf LiquidAI/LFM2-8B-A1B-GGUF",
    "LiquidAI/LFM2-8B-A1B": "LFM2-8B-A1B\nüìÑ Model details\nüèÉ How to run LFM2\n1. Transformers\n2. vLLM\n3. llama.cpp\nüîß How to fine-tune LFM2\nüìà Performance\n1. Automated benchmarks\n2. Inference\nüì¨ Contact\nPlayground\nPlayground\nPlayground\nLeap\nLFM2-8B-A1B\nLFM2 is a new generation of hybrid models developed by Liquid AI, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.\nWe're releasing the weights of our first MoE based on LFM2, with 8.3B total parameters and 1.5B active parameters.\nLFM2-8B-A1B is the best on-device MoE in terms of both quality (comparable to 3-4B dense models) and speed (faster than Qwen3-1.7B).\nCode and knowledge capabilities are significantly improved compared to LFM2-2.6B.\nQuantized variants fit comfortably on high-end phones, tablets, and laptops.\nFind more information about LFM2-8B-A1B in our blog post.\nüìÑ Model details\nDue to their small size, we recommend fine-tuning LFM2 models on narrow use cases to maximize performance.\nThey are particularly suited for agentic tasks, data extraction, RAG, creative writing, and multi-turn conversations.\nHowever, we do not recommend using them for tasks that are knowledge-intensive or require programming skills.\nProperty\nLFM2-8B-A1B\nTotal parameters\n8.3B\nActive parameters\n1.5B\nLayers\n24 (18 conv + 6 attn)\nContext length\n32,768 tokens\nVocabulary size\n65,536\nTraining precision\nMixed BF16/FP8\nTraining budget\n12 trillion tokens\nLicense\nLFM Open License v1.0\nSupported languages: English, Arabic, Chinese, French, German, Japanese, Korean, and Spanish.\nGeneration parameters: We recommend the following parameters:\ntemperature=0.3\nmin_p=0.15\nrepetition_penalty=1.05\nChat template: LFM2 uses a ChatML-like chat template as follows:\n<|startoftext|><|im_start|>system\nYou are a helpful assistant trained by Liquid AI.<|im_end|>\n<|im_start|>user\nWhat is C. elegans?<|im_end|>\n<|im_start|>assistant\nIt's a tiny nematode that lives in temperate soil environments.<|im_end|>\nYou can automatically apply it using the dedicated .apply_chat_template() function from Hugging Face transformers.\nTool use: It consists of four main steps:\nFunction definition: LFM2 takes JSON function definitions as input (JSON objects between <|tool_list_start|> and <|tool_list_end|> special tokens), usually in the system prompt\nFunction call: LFM2 writes Pythonic function calls (a Python list between <|tool_call_start|> and <|tool_call_end|> special tokens), as the assistant answer.\nFunction execution: The function call is executed and the result is returned (string between <|tool_response_start|> and <|tool_response_end|> special tokens), as a \"tool\" role.\nFinal answer: LFM2 interprets the outcome of the function call to address the original user prompt in plain text.\nHere is a simple example of a conversation using tool use:\n<|startoftext|><|im_start|>system\nList of tools: <|tool_list_start|>[{\"name\": \"get_candidate_status\", \"description\": \"Retrieves the current status of a candidate in the recruitment process\", \"parameters\": {\"type\": \"object\", \"properties\": {\"candidate_id\": {\"type\": \"string\", \"description\": \"Unique identifier for the candidate\"}}, \"required\": [\"candidate_id\"]}}]<|tool_list_end|><|im_end|>\n<|im_start|>user\nWhat is the current status of candidate ID 12345?<|im_end|>\n<|im_start|>assistant\n<|tool_call_start|>[get_candidate_status(candidate_id=\"12345\")]<|tool_call_end|>Checking the current status of candidate ID 12345.<|im_end|>\n<|im_start|>tool\n<|tool_response_start|>[{\"candidate_id\": \"12345\", \"status\": \"Interview Scheduled\", \"position\": \"Clinical Research Associate\", \"date\": \"2023-11-20\"}]<|tool_response_end|><|im_end|>\n<|im_start|>assistant\nThe candidate with ID 12345 is currently in the \"Interview Scheduled\" stage for the position of Clinical Research Associate, with an interview date set for 2023-11-20.<|im_end|>\nYou can directly pass tools as JSON schema or Python functions with .apply_chat_template() as shown in this page to automatically format the system prompt.\nArchitecture: Hybrid model with multiplicative gates and short convolutions: 18 double-gated short-range LIV convolution blocks and 6 grouped query attention (GQA) blocks.\nPre-training mixture: Approximately 75% English, 20% multilingual, and 5% code data sourced from the web and licensed materials.\nTraining approach:\nVery large-scale SFT on 50% downstream tasks, 50% general domains\nCustom DPO with length normalization and semi-online datasets\nIterative model merging\nüèÉ How to run LFM2\n1. Transformers\nTo run LFM2, you need to install Hugging Face transformers from source as follows:\npip install git+https://github.com/huggingface/transformers.git@0c9a72e4576fe4c84077f066e585129c97bfd4e6\nHere is an example of how to generate an answer with transformers in Python:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n# Load model and tokenizer\nmodel_id = \"LiquidAI/LFM2-8B-A1B\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"auto\",\ndtype=\"bfloat16\",\n#    attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n# Generate answer\nprompt = \"What is C. elegans?\"\ninput_ids = tokenizer.apply_chat_template(\n[{\"role\": \"user\", \"content\": prompt}],\nadd_generation_prompt=True,\nreturn_tensors=\"pt\",\ntokenize=True,\n).to(model.device)\noutput = model.generate(\ninput_ids,\ndo_sample=True,\ntemperature=0.3,\nmin_p=0.15,\nrepetition_penalty=1.05,\nmax_new_tokens=512,\n)\nprint(tokenizer.decode(output[0], skip_special_tokens=False))\n# <|startoftext|><|im_start|>user\n# What is C. elegans?<|im_end|>\n# <|im_start|>assistant\n# C. elegans, also known as Caenorhabditis elegans, is a small, free-living\n# nematode worm (roundworm) that belongs to the phylum Nematoda.\nYou can directly run and test the model with this Colab notebook.\n2. vLLM\nYou can run the model in vLLM by building from source:\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install -e . -v\nHere is an example of how to use it for inference:\nfrom vllm import LLM, SamplingParams\nprompts = [\n[\n{\n\"content\": \"What is C. elegans?\",\n\"role\": \"user\",\n},\n],\n[\n{\n\"content\": \"Say hi in JSON format\",\n\"role\": \"user\",\n},\n],\n[\n{\n\"content\": \"Define AI in Spanish\",\n\"role\": \"user\",\n},\n],\n]\nsampling_params = SamplingParams(\ntemperature=0.3,\nmin_p=0.15,\nrepetition_penalty=1.05,\nmax_tokens=30\n)\nllm = LLM(model=\"LiquidAI/LFM2-8B-A1B\", dtype=\"bfloat16\")\noutputs = llm.chat(prompts, sampling_params)\nfor i, output in enumerate(outputs):\nprompt = prompts[i][0][\"content\"]\ngenerated_text = output.outputs[0].text\nprint(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n3. llama.cpp\nYou can run LFM2 with llama.cpp using its GGUF checkpoint. Find more information in the model card.\nüîß How to fine-tune LFM2\nWe recommend fine-tuning LFM2 models on your use cases to maximize performance.\nNotebook\nDescription\nLink\nSFT (TRL)\nSupervised Fine-Tuning (SFT) notebook with a LoRA adapter using TRL.\nDPO (TRL)\nPreference alignment with Direct Preference Optimization (DPO) using TRL.\nüìà Performance\n1. Automated benchmarks\nCompared to similar-sized models, LFM2-8B-A1B displays strong performance in instruction following and math while also running significantly faster.\nModel\nMMLU\nMMLU-Pro\nGPQA\nIFEval\nIFBench\nMulti-IF\nLFM2-8B-A1B\n64.84\n37.42\n29.29\n77.58\n25.85\n58.19\nLFM2-2.6B\n64.42\n25.96\n26.57\n79.56\n22.19\n60.26\nLlama-3.2-3B-Instruct\n60.35\n22.25\n30.6\n71.43\n20.78\n50.91\nSmolLM3-3B\n59.84\n23.90\n26.31\n72.44\n17.93\n58.86\ngemma-3-4b-it\n58.35\n34.76\n29.51\n76.85\n23.53\n66.61\nQwen3-4B-Instruct-2507\n72.25\n52.31\n34.85\n85.62\n30.28\n75.54\ngranite-4.0-h-tiny\n66.79\n32.03\n26.46\n81.06\n18.37\n52.99\nModel\nGSM8K\nGSMPlus\nMATH 500\nMATH Lvl 5\nMGSM\nMMMLU\nLFM2-8B-A1B\n84.38\n64.76\n74.2\n62.38\n72.4\n55.26\nLFM2-2.6B\n82.41\n60.75\n63.6\n54.38\n74.32\n55.39\nLlama-3.2-3B-Instruct\n75.21\n38.68\n41.2\n24.06\n61.68\n47.92\nSmolLM3-3B\n81.12\n58.91\n73.6\n51.93\n68.72\n50.02\ngemma-3-4b-it\n89.92\n68.38\n73.2\n52.18\n87.28\n50.14\nQwen3-4B-Instruct-2507\n68.46\n56.16\n85.6\n73.62\n81.76\n60.67\ngranite-4.0-h-tiny\n82.64\n59.14\n58.2\n36.11\n73.68\n56.13\nModel\nActive params\nLCB v6\nLCB v5\nHumanEval+\nCreative Writing v3\nLFM2-8B-A1B\n1.5B\n21.04%\n21.36%\n69.51%\n44.22%\nGemma-3-1b-it\n1B\n4.27%\n4.43%\n37.20%\n41.67%\nGranite-4.0-h-tiny\n1B\n26.73%\n27.27%\n73.78%\n32.60%\nLlama-3.2-1B-Instruct\n1.2B\n4.08%\n3.64%\n23.17%\n31.43%\nQwen2.5-1.5B-Instruct\n1.5B\n11.18%\n10.57%\n48.78%\n22.18%\nQwen3-1.7B (/no_think)\n1.7B\n24.07%\n26.48%\n60.98%\n31.56%\nLFM2-2.6B\n2.6B\n14.41%\n14.43%\n57.93%\n38.79%\nSmolLM3-3B\n3.1B\n19.05%\n19.20%\n60.37%\n36.44%\nLlama-3.2-3B-Instruct\n3.2B\n11.47%\n11.48%\n24.06%\n38.84%\nQwen3-4B (/no_think)\n4B\n36.11%\n38.64%\n71.95%\n37.49%\nQwen3-4B-Instruct-2507\n4B\n48.72%\n50.80%\n82.32%\n51.71%\nGemma-3-4b-it\n4.3B\n18.86%\n19.09%\n62.8%\n68.56%\n2. Inference\nLFM2-8B-A1B is significantly faster than models with a similar number of active parameters, like Qwen3-1.7B.\nThe following plots showcase the performance of different models under int4 quantization with int8 dynamic activations on the AMD Ryzen AI 9 HX 370 CPU, using 16 threads. The results are obtained using our internal XNNPACK-based inference stack, and a custom CPU MoE kernel.\nüì¨ Contact\nIf you are interested in custom solutions with edge deployment, please contact our sales team.",
    "ByteDance-Seed/AHN-Mamba2-for-Qwen-2.5-Instruct-14B": "AHN: Artificial Hippocampus Networks for Efficient Long-Context Modeling\nContact\nCitation\nAHN: Artificial Hippocampus Networks for Efficient Long-Context Modeling\nIntroduction\nArtificial Hippocampus Networks (AHNs) transform lossless memory into fixed-size compressed representations for long-context modeling. Lossless memory (e.g., attention‚Äôs key-value (KV) cache) stores exact input information but grows with sequence length, making it inefficient for long sequences. In contrast, compressed memory (e.g., RNNs‚Äô hidden state) maintains a constant size and offers fixed computational costs per input token, but this comes at the cost of information loss. To harness the benefits of both memory types, AHNs continually convert lossless memory outside the sliding attention window into compressed form. AHNs can be instantiated with any RNN-like architectures. The model then integrates both memory types to make predictions across long contexts.\nThis repository hosts the model weights for AHN. For installation, usage instructions, and further documentation, please visit our GitHub repository.\nMethod\n**(a)** Illustration of the model augmented with Artificial Hippocampus Networks (AHNs). In this example, the sliding window length is 3. When the input sequence length is less than or equal to the window length, the model operates identically to a standard Transformer. For longer sequences, AHNs continually compress the token outside the window into a compact memory representation. The model then utilizes both the lossless information within window, and the compressed memory to generate the next token. **(b)** Self-distillation training framework of AHNs based on an open-weight LLM. During training, the base LLM's weights are frozen, and only the AHNs' parameters are trained.\nModel Zoo\nbase model\nAHN module\n#params\ncheckpoint (AHN only)\nQwen2.5-3B-Instruct\nMamba2\n11.9M\nü§ómodel\nQwen2.5-3B-Instruct\nDeltaNet\n11.8M\nü§ómodel\nQwen2.5-3B-Instruct\nGatedDeltaNet\n13.0M\nü§ómodel\nQwen2.5-7B-Instruct\nMamba2\n18.6M\nü§ómodel\nQwen2.5-7B-Instruct\nDeltaNet\n18.5M\nü§ómodel\nQwen2.5-7B-Instruct\nGatedDeltaNet\n21.3M\nü§ómodel\nQwen2.5-14B-Instruct\nMamba2\n51.4M\nü§ómodel\nQwen2.5-14B-Instruct\nDeltaNet\n51.1M\nü§ómodel\nQwen2.5-14B-Instruct\nGatedDeltaNet\n61.0M\nü§ómodel\nEvaluation\nLV-Eval & InfiniteBench Results\nLongBench Results\nContact\nYunhao Fang: yunhao.fang@bytedance.com\nWeihao Yu (corresponding author): weihao.yu@bytedance.com\nCitation\nBibTeX:\n@article{fang2025artificial,\ntitle={Artificial hippocampus networks for efficient long-context modeling},\nauthor={Fang, Yunhao and Yu, Weihao and Zhong, Shu and Ye, Qinghao and Xiong, Xuehan and Wei, Lai},\njournal={arXiv preprint arXiv:2510.07318},\nyear={2025}\n}",
    "Kwaipilot/KAT-Dev-72B-Exp": "News\nHighlights\nIntroduction\nQuickstart\nSWE agent Evaluation Parameters\nNews\nüî• We‚Äôre thrilled to announce the release of KAT-Dev-72B-Exp, our latest and most powerful model yet!\nüî• You can now try our strongest proprietary coder model KAT-Coder directly on the StreamLake platform for free.\nHighlights\nKAT-Dev-72B-Exp  is an open-source 72B-parameter model for software engineering tasks.\nOn SWE-Bench Verified, KAT-Dev-72B-Exp achieves 74.6% accuracy ‚ö° ‚Äî when evaluated strictly with the SWE-agent scaffold.\nKAT-Dev-72B-Exp is the experimental reinforcement-learning version of the KAT-Coder model. Through this open-source release, we aim to reveal the technical innovations behind KAT-Coder‚Äôs large-scale RL to developers and researchers.\nIntroduction\nWe rewrote the attention kernel and redesigned the training engine for shared prefix trajectories to achieve highly efficient RL training, especially for scaffolds leveraging context management.\nFurthermore, to prevent exploration collapse observed in RL training, we reshaped advantage distribution based on pass rates: amplifying the advantage scale of highly exploratory groups while reducing that of low-exploration ones.\nQuickstart\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"KAT-Dev-72B-Exp\"\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n# conduct text completion\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=65536\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint(\"content:\", content)\nSWE agent Evaluation Parameters\ntemperature: 0.6\nmax_turns: 150\nhistory_processors.n: 100\nFor full settings please refer to inference.yaml",
    "KORMo-Team/KORMo-10B-base": "üöÄ Update News\nüí° About KORMo\nüîó Links\nüìà Benchmark Performance\nüìä Quantitative Evaluation\nüìù Qualitative Evaluation (LLM-as-a-Judge)\nüì¶ Installation\nüöÄ Inference Example\nüß† Enabling Thinking Mode\nContact\nAcknowledgments\nCitation\nüöÄ Update News\n2025-10-13: Official release of KORMo-10B-base (Be aware that it's not an SFT model!!).\nüí° About KORMo\nKORMo-10B is a 10.8B parameter fully open LLM capable of handling both Korean and English.The model, training code, and training data are all fully open, allowing anyone to reproduce and extend them.\nModel Size: 10.8B parameters\nLanguages: Korean / English\nTraining Data: Synthetic data + public datasets (approximately 3T tokens)\nLicense: Apache 2.0\nThe First Fully Open-Source LLM from a Non-English Region\nKORMo was created with a public-interest mission: to make world-class language models accessible to everyone.\nOur goal is to empower anyone to build and advance their own large language models at a global standard.\nKey Features:\n1. A 10B-parameter Korean‚ÄìEnglish reasoning model trained entirely from scratch.\n2. 100% open resources ‚Äî including all training data, code, intermediate checkpoints, and tutorials ‚Äî allowing anyone to reproduce and extend a near-SOTA model on their own.\n3. 3 trillion tokens of training data released publicly, featuring never-before-shared, high-quality full-cycle Korean datasets (for pretraining, post-training, general, reasoning, and reinforcement learning).\n4. A collaborative effort by eight master‚Äôs students at the KAIST Graduate School of Culture Technology (MLP Lab), documented in a 45-page research paper.\nIf you‚Äôve ever used a Korean language model that performs well on benchmarks but feels strange in real use, or if fine-tuning only made it worse, you‚Äôre not alone.\nKORMo solves these problems head-on.\nBy releasing every intermediate model and post-training dataset, we give users the freedom to build on the base model with their own data, customizing and fine-tuning it in any direction they want.\nüëâ \"If you want a great Korean language model, now you can build it yourself. It even works with free Colab GPUs!\" ü§ó\nüîó Links\nüìñ Technical Report: üëâ Arxive\nü§ó Hugging Face: üëâ Model Download\nüíª GitHub Repository: üëâ Training and Inference Code\nüîâ Tutorial: üëâ Instruction Tuning over google colab üëâ Youtube Tutorial\nüìà Benchmark Performance\nüìä Quantitative Evaluation\nBenchmark\nKORMo-10B\nsmolLM3-3B\nolmo2-7B\nolmo2-13B\nkanana1.5-8B\nqwen3-8B\nllama3.1-8B\ngemma3-4B\ngemma3-12B\nüá∫üá∏ English Benchmarks\narc_challenge\n58.96\n55.55\n59.13\n61.01\n56.48\n63.82\n54.61\n53.58\n63.82\narc_easy\n85.48\n83.21\n85.06\n86.57\n82.74\n87.50\n84.01\n82.83\n87.37\nboolq\n83.46\n82.17\n84.50\n86.48\n84.53\n87.71\n81.87\n80.70\n86.61\ncopa\n93.00\n91.00\n92.00\n93.00\n88.00\n92.00\n93.00\n89.00\n95.00\ngpqa_main\n30.13\n26.79\n26.34\n29.24\n29.24\n30.13\n23.44\n30.13\n35.71\nhellaswag\n60.25\n56.78\n61.52\n65.02\n59.93\n59.54\n60.96\n57.56\n63.67\nmmlu\n67.96\n61.37\n62.81\n66.85\n63.73\n76.95\n65.03\n59.60\n73.58\nmmlu_global\n63.44\n57.52\n59.88\n63.99\n60.21\n75.05\n61.30\n57.23\n70.23\nmmlu_pro\n40.18\n34.94\n27.29\n32.50\n34.93\n56.58\n36.23\n27.79\n37.07\nmmlu_redux\n69.00\n62.95\n63.53\n68.37\n65.88\n78.19\n65.86\n60.86\n75.25\nopenbookqa\n39.00\n36.40\n39.00\n39.60\n36.80\n39.20\n39.00\n37.00\n40.20\npiqa\n81.12\n78.45\n80.79\n82.64\n80.30\n79.05\n80.90\n79.49\n82.59\nsocial_iqa\n52.81\n50.72\n55.89\n57.57\n57.01\n56.96\n53.12\n51.84\n56.45\nEnglish Avg.\n63.45\n59.83\n61.36\n64.06\n61.52\n67.90\n61.49\n59.05\n66.73\nüá∞üá∑ Korean Benchmarks\nclick\n55.29\n46.97\n37.79\n41.80\n62.76\n60.70\n49.22\n49.62\n62.21\ncsatqa\n38.00\n26.67\n19.33\n24.67\n44.67\n52.00\n28.67\n28.67\n31.33\nhaerae\n68.29\n55.82\n31.62\n37.58\n80.75\n67.19\n53.25\n60.68\n74.34\nk2_eval\n84.89\n75.23\n49.54\n63.43\n84.72\n84.72\n76.62\n76.39\n85.42\nkobest\n75.05\n69.13\n57.27\n59.02\n81.93\n80.05\n70.55\n69.33\n77.70\nkobalt\n22.86\n15.86\n11.43\n13.14\n26.29\n26.57\n17.43\n15.57\n23.86\nkmmlu\n46.48\n38.52\n33.05\n31.24\n48.86\n56.93\n40.75\n39.84\n51.60\nmmlu_global (ko)\n55.16\n44.15\n34.00\n36.95\n52.65\n61.95\n46.34\n46.33\n59.68\nkr_clinical_qa\n77.32\n53.97\n48.33\n46.22\n65.84\n80.00\n63.54\n60.00\n77.22\nKorean Avg.\n58.15\n47.37\n35.82\n39.34\n60.94\n63.35\n49.60\n49.60\n60.37\nüìù Qualitative Evaluation (LLM-as-a-Judge)\nBenchmark\nKORMo-10B\nsmolLM3-3B\nolmo2-7B\nolmo2-13B\nkanana1.5-8B\nqwen3-8B\nllama3.1-8B\nexaone3.5-8B\ngemma3-12B\nMT-Bench (EN)\n8.32\n7.15\n7.32\n7.64\n8.45\n8.70\n6.32\n8.15\n8.70\nKO-MT-Bench (KO)\n8.54\n-\n-\n-\n8.02\n8.16\n4.27\n8.13\n8.51\nLogicKor (KO)\n8.96\n-\n-\n-\n8.94\n8.63\n6.45\n9.20\n8.46\nAverage\n8.61\n-\n-\n-\n8.47\n8.50\n5.68\n8.49\n8.56\nüì¶ Installation\ngit clone https://github.com/MLP-Lab/KORMo-tutorial.git\ncd KORMo-tutorial\nbash setup/create_uv_venv.sh\nsource .venv_kormo/bin/activate\nüöÄ Inference Example\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nmodel_name = \"KORMo-Team/KORMo-10B-sft\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\ntrust_remote_code=True\n)\nmessages = [\n{\"role\": \"user\", \"content\": \"What happens inside a black hole?\"}\n]\nchat_prompt = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=False\n)\ninputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(model.device)\nwith torch.inference_mode():\noutput_ids = model.generate(\n**inputs,\nmax_new_tokens=1024,\n)\nresponse = tokenizer.decode(output_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\nprint(\"Assistant:\", response)\nüß† Enabling Thinking Mode\nIf you want to enable the thinking mode, simply set enable_thinking=True:\nchat_prompt = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True,\nenable_thinking=True\n)\nContact\nKyungTae Lim, Professor at KAIST. ktlim@kaist.ac.kr\nAcknowledgments\nThis work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (RS-2025-02653113, High-Performance Research AI Computing Infrastructure Support at the 2 PFLOPS Scale)\nCitation\n@misc{KORMo,\nauthor = {Minjun Kim, Hyeonseok Lim, Hangyeol Yoo, Inho Won, Seungwoo Song, Minkyung Cho, Junghun Yuk, Changsu Choi, Dongjae Shin, Huije Lee, Hoyun Song, Alice Oh and KyungTae Lim},\ntitle = {KORMo: Korean Open Reasoning Model for Everyone},\nyear = {2025},\npublisher = {GitHub},\njournal = {Technical Report},\npaperLink = {\\url{https://arxiv.org/abs/2510.09426}},\n},\n}",
    "SciMaker/Journal_Club": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.",
    "NexaAI/Qwen3-VL-8B-Instruct-GGUF": "Qwen3-VL-8B-Instruct\nQuickstart\nModel Description\nFeatures\nUse Cases\nInputs and Outputs\nLicense\nQwen3-VL-8B-Instruct\nNote currently only NexaSDK supports this model's GGUF.\nRun Qwen3-VL-8B-Instruct optimized for CPU/GPU with NexaSDK.\nQuickstart\nInstall NexaSDK\nRun the model locally with one line of code:\nnexa infer NexaAI/Qwen3-VL-8B-Instruct-GGUF\nModel Description\nQwen3-VL-8B-Instruct is an 8-billion-parameter instruction-tuned multimodal large language model developed by the Qwen team at Alibaba Cloud.It belongs to the Qwen3-VL series, designed for seamless understanding and reasoning across text, image, and video. This version combines the visual intelligence of Qwen3-VL with the instruction-following capabilities of Qwen3-LM, enabling natural, grounded conversations around complex visual content.\nCompared to the 4B variant, the 8B model delivers stronger reasoning, richer context retention, and improved performance on visual and multilingual benchmarks while maintaining efficiency for deployment.\nFeatures\nEnhanced Visual Understanding: Handles complex scenes, documents, and multi-image inputs.\nInstruction-Tuned Dialogue: Produces coherent and context-aware responses aligned with user intent.\nMultilingual Support: Capable of understanding and generating in multiple languages.\nExtended Context Window: Supports longer text and multimodal contexts for better reasoning continuity.\nOptimized Performance: Balances large-scale reasoning capability with deployability for high-end edge or server environments.\nUse Cases\nVisual chatbots and multimodal assistants\nDocument and chart interpretation\nImage-grounded content generation and summarization\nVideo frame reasoning and analysis\nMultilingual multimodal tutoring or knowledge assistants\nInputs and Outputs\nInput:\nText, images, or combined multimodal prompts\nOptional video frames or sequential image sets\nOutput:\nNatural-language answers, summaries, captions, or structured reasoning outputs\nCan provide visual explanations or reasoning narratives when prompted\nLicense\nSee the official Qwen license for details on usage and redistribution.",
    "inclusionAI/Ring-mini-sparse-2.0-exp": "Ring-mini-sparse-2.0-exp\nIntroduction\nEvaluation\nHighly Sparse, High-Speed Generation\nQuickstart\nü§ó Hugging Face Transformers\nüöÄ SGLang\nRing-mini-sparse-2.0-exp\nü§ó Hugging Face¬†¬† | ¬†¬†ü§ñ ModelScope\nIntroduction\nWe are excited to annouce the official release of Ring-mini-sparse-2.0-exp. This model employs a Mixture of Block Attention (MoBA) architecture, delivering highly efficient inference without compromising performance.  This model inherts from Ling-mini-base-2.0, continually trained on an additional 100B tokens. The performance of the MoBA-based model is on par with the standard attention models of the same size (e.g., Ring-mini-v2). Furthermore, by applying YaRN-based 4√ó window extrapolation, we extend the context length to 128K tokens, delivering superior inference speed on tasks that involve long inputs and outputs.\nFigure 1: The Model Architecture of Ring-mini-sparse-2.0-exp\nEvaluation\nTo comprehensively assess the reasoning capability of our model, we conducted evaluations on five challenging benchmarks spanning mathematics, coding, and science, comparing it with Ring-mini-2.0, Qwen3-8B-Thinking, and GPT-OSS-20B-Medium. The MoBA architecture demonstrates comparable performance to full softmax attention models.\nFigure 2: Model Performance Comparison\nHighly Sparse, High-Speed Generation\nRing-mini-sparse-2.0-exp achieves high inference efficiency through highly sparse attention and a Mixture-of-Experts (MoE) architecture. Unlike MoBA used in Kimi, our approach shares the same KV block selection across all heads within a GQA group, reducing the total number of KV tokens each query head retrieves from the KV cache during decoding. During 64K-context decoding, only 8,192 key-value (KV) tokens are activated per query‚Äîreducing KV cache retrieval overhead by 87.5% compared to full attention and delivering up to 3√ó inference speedup over Ring-mini-2.0. This design significantly lowers computational costs for high-concurrency scenarios involving reasoning-intensive models while maintaining competitive performance. Additionally, with YaRN extrapolation, the model extends context capacity to 128K tokens, achieving up to 2√ó relative speedup in long-input scenarios compared to Ring-mini-2.0 (full softmax attention).\nFigure 4: Inference speedup ratios of Ring-mini-sparse-2.0-exp compared to Ring-mini-2.0.\nQuickstart\nü§ó Hugging Face Transformers\nInstallation requirements:\npip install flash-attn==2.6.3\npip install transformers==4.56.1\nHere is a code snippet to show you how to use the chat model with transformers:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"inclusionAI/Ring-mini-sparse-2.0-exp\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ndtype=\"auto\",\ndevice_map=\"auto\",\ntrust_remote_code=True,\nattn_implementation=\"flash_attention_2\",\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompts = [\n\"Give me a short introduction to large language models.\"\n]\ninput_texts = []\nfor prompt in prompts:\nmessages = [\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ninput_texts.append(text)\nprint(input_texts)\nmodel_inputs = tokenizer(input_texts, return_tensors=\"pt\", return_token_type_ids=False, padding=True, padding_side='left').to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=8192,\ndo_sample=False,\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\nprint(\"*\" * 30)\nprint(responses)\nprint(\"*\" * 30)\nüöÄ SGLang\nEnvironment Preparation\nWe have submitted our PR to SGLang official release and it will be merged later, for now we can prepare the environment following steps, firstly install the community version SGLang and required packages:\npip install sglang==0.5.3 sgl-kernel==0.3.15 torch==2.8.0 torchvision==0.23.0 torchao\nThen you should install our sglang wheel package:\ngit clone https://github.com/inclusionAI/Ring-V2.git\npip install Ring-V2/moba/whls/sglang-0.5.3.post1-py3-none-any.whl --no-deps --force-reinstall\nRun Inference\nOur model is supported by SGLang now. You can launch the sever with the command in the following:\nStart server:\npython -m sglang.launch_server \\\n--model-path <model_path> \\\n--trust-remote-code \\\n--tp-size 4 \\\n--disable-radix-cache \\\n--chunked-prefill-size 0 \\\n--attention-backend moba\nClient:\ncurl -s http://localhost:${PORT}/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\"model\": \"auto\", \"temperature\": 0.6, \"messages\": [{\"role\": \"user\", \"content\": \"Give me a short introduction to large language models.\"}]}'\nMore usage can be found here",
    "nanochat-students/nanochat-d20": "NanoChat SFT\nUsage\nvLLM Integration:\nChat SFT Training Metrics\nChat evaluation sft\nNanoChat SFT\nThis is the the checkpoint from Andrej Karpathy's fullstack llm project to build an LLM, nanochat.\nUsage\nInstall transformers from this specific branch:\npip install git+https://github.com/huggingface/transformers.git@nanochat-implementation\nThen, you can run this inference snippet:\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id=\"nanochat-students/d20-chat-transformers\"\nmax_new_tokens=64\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=False)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=False, dtype=torch.bfloat16).to(device)\nmodel.eval()\nconversation = [\n{\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n]\ninputs = tokenizer.apply_chat_template(\nconversation,\nadd_generation_prompt=True,\ntokenize=True,\nreturn_tensors=\"pt\"\n).to(device)\nwith torch.no_grad():\noutputs = model.generate(\n**inputs,\nmax_new_tokens=max_new_tokens,\n)\n# Decode only the generated tokens (excluding the input prompt)\ngenerated_tokens = outputs[0, inputs.input_ids.shape[1]:]\nprint(tokenizer.decode(generated_tokens, skip_special_tokens=True))\nvLLM Integration:\nYou can also run the model in vLLM, using the above branch install:\nvllm serve nanochat-students/nanochat-d20 --enforce-eager\nAnd then you can call the model like so:\nurl http://localhost:8000/v1/completions \\\n>   -H \"Content-Type: application/json\" \\\n>   -d '{\"model\": \"nanochat-students/nanochat-d20\", \"prompt\": \"What is the capital of France?, \"max_tokens\": 7, \"temperature\": 0}'\nChat SFT Training Metrics\ntimestamp: 2025-10-14 20:17:42\nrun:\nsource: mid\ndtype: bfloat16\ndevice_batch_size: 4\nnum_epochs: 1\nmax_iterations: -1\ntarget_examples_per_step: 32\nunembedding_lr: 0.0040\nembedding_lr: 0.2000\nmatrix_lr: 0.0200\nweight_decay: 0.0000\ninit_lr_frac: 0.0200\neval_every: 100\neval_steps: 100\neval_metrics_every: 200\nTraining rows: 20,843\nNumber of iterations: 651\nTraining loss: 1.1904\nValidation loss: 1.0664\nChat evaluation sft\ntimestamp: 2025-10-14 20:29:59\nsource: sft\ntask_name: None\ndtype: bfloat16\ntemperature: 0.0000\nmax_new_tokens: 512\nnum_samples: 1\ntop_k: 50\nbatch_size: 8\nmodel_tag: None\nstep: None\nmax_problems: None\nARC-Easy: 0.4259\nARC-Challenge: 0.2961\nMMLU: 0.3250\nGSM8K: 0.0432\nHumanEval: 0.0549\nChatCORE metric: 0.0988\nLogs from training can be found here: https://huggingface.co/spaces/nanochat-students/trackio",
    "nvidia/NV-Reason-CXR-3B": "NV-Reason-CXR-3B Overview\nDescription:\nQuick start\nFramework versions\nLicense/Terms of Use:\nDeployment Geography:\nUse Case:\nRelease Date:\nModel Architecture:\nInput:\nInput Specifications:\nOutput:\nSoftware Integration:\nModel Version(s):\nTraining, Testing, and Evaluation Datasets:\nDataset Overview:\nTraining Dataset:\nInference:\nEthical Considerations:\nNV-Reason-CXR-3B Overview\nDescription:\nNV-Reason-CXR-3B is a specialized vision-language model designed for medical reasoning and interpretation of chest X-ray images, with detailed explanations. The model combines visual understanding with medical reasoning capabilities, enabling healthcare professionals to access comprehensive analyses and engage in follow-up discussions about radiological findings. NV-Reason-CXR-3B provides step-by-step reasoning that mirrors clinical thinking patterns, making it valuable for educational and research applications in medical imaging.\nThis model is for research and development only.\nüíª [Github code]ü©ª [Web Demo]\nQuick start\nimport torch\nfrom transformers import AutoModelForImageTextToText, AutoProcessor\nfrom PIL import Image\n# Load the model\nmodel_name = \"nvidia/NV-Reason-CXR-3B\"\nmodel = AutoModelForImageTextToText.from_pretrained(\nmodel_name,\ntorch_dtype=torch.float16,\n).eval().to(\"cuda\")\nprocessor = AutoProcessor.from_pretrained(model_name)\n# Load chest x-ray image\nimage = Image.open(\"chest_xray.png\")\n# Prepare input with clinical context\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"image\",\n\"image\": image,\n},\n{\n\"type\": \"text\",\n\"text\": \"Find abnormalities and support devices.\"\n}\n]\n}\n]\n# Create prompt using chat template\ntext = processor.apply_chat_template(messages, add_generation_prompt=True)\n# Process inputs\ninputs = processor(text=text, images=[image], return_tensors=\"pt\")\ninputs = inputs.to(model.device)\n# Generate\ngenerated_ids = model.generate(**inputs,  max_new_tokens=2048)\n# Trim and decode\ntrimmed_generated_ids = [\nout_ids[len(in_ids):]\nfor in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\ngenerated_text = processor.batch_decode(\ntrimmed_generated_ids,\nskip_special_tokens=True,\nclean_up_tokenization_spaces=False\n)[0]\nprint(\"Output:\")\nprint(generated_text)\nFramework versions\nTransformers: 4.56.1\nPytorch: 2.7.1\nTokenizers: 0.22.0\nLicense/Terms of Use:\nNVIDIA OneWay Non-Commercial License for academic research purposes\nDeployment Geography:\nGlobal\nUse Case:\nRadiologists, medical students, and medical researchers would be expected to use this system for chest X-ray interpretation with detailed reasoning, educational training with AI-generated explanations, and research applications requiring explainable medical AI analyses.\nImportant Medical AI Considerations:\nThis model is designed for research and educational purposes only and should not be used for clinical diagnosis or treatment decisions. All outputs should be reviewed by qualified medical professionals. The model's reasoning capabilities are intended to support medical education and research, not replace clinical judgment.\nRelease Date:\nHuggingface: 10/27/2025 via https://huggingface.co/NVIDIA\nModel Architecture:\nArchitecture Type: Transformer\nNetwork Architecture: Vision-Language Model based on Qwen2.5-VL architecture with medical reasoning capabilities\nThis model was developed by fine-tuning Qwen2.5-VL-3B using Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) for enhanced medical reasoning.\nNumber of model parameters: 3B\nInput:\nInput Type(s): Image, Text\nInput Format(s): Medical images (JPEG, PNG), Text prompts (string)\nInput Parameters: Two-Dimensional (2D) images with accompanying text queries (1D)\nOther Properties Related to Input: Supports frontal chest X-ray images with flexible scaling. Accepts natural language prompts for medical queries, follow-up questions, and reasoning requests. Input images are automatically processed without specific size constraints.\nInput Specifications:\nMedical Images: Chest X-ray images in standard medical imaging formats\nText Prompts: Natural language queries about radiological findings, diagnostic questions, or requests for detailed explanations\nInteractive Dialogue: Support for follow-up questions and clarification requests\nOutput:\nOutput Type(s): Text\nOutput Format: Structured reasoning with XML-like tags\nOutput Parameters: One-Dimensional (1D) Natural language reasoning and analysis\nOther Properties Related to Output: Outputs contain structured thinking processes enclosed in <thinking> tags showing step-by-step medical reasoning, followed by concise answers in <answer> tags. This format enables transparency in the model's diagnostic reasoning process and supports educational use cases.\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (GPU cores) and software frameworks (CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\nSoftware Integration:\nRuntime Engine(s):\nPyTorch\nTransformers library\nSupported Hardware Microarchitecture Compatibility:\nNVIDIA Ampere\nNVIDIA Hopper\nNVIDIA Lovelace\nSupported Operating System(s):\nLinux\nThe integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment.\nModel Version(s):\n0.1 - Initial release version for chest X-ray reasoning and interpretation with structured thinking output\nTraining, Testing, and Evaluation Datasets:\nDataset Overview:\nLarge-scale chest X-ray datasets including MIMIC-CXR, ChestXRay14, and CheXpert.\nTraining Dataset:\nData Modality:\nImage\nText\nInference:\nAcceleration Engine: PyTorch, Transformers\nTest Hardware:\nA100\nH100\nL40S\nEthical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please make sure you have proper rights and permissions for all input image and video content; if image or video includes people, personal health information, or intellectual property, the image or video generated will not blur or maintain proportions of image subjects included.\nPlease report model quality, risk, security vulnerabilities or concerns here."
}