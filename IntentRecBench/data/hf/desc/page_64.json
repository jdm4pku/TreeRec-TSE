{
    "pyannote/speaker-diarization-3.0": "",
    "mychen76/invoice-and-receipts_donut_v1": "",
    "23CH3COOH/rvc_models": "",
    "NOVA-vision-language/GlorIA-1.3B": "",
    "jinaai/jina-embeddings-v2-small-en": "",
    "TheBloke/leo-hessianai-13B-chat-bilingual-GGUF": "",
    "TheBloke/leo-hessianai-13B-chat-GGUF": "",
    "TheBloke/leo-hessianai-13B-chat-AWQ": "",
    "guetLzy/Bert-Vits2-PreWeight": "",
    "openaccess-ai-collective/tiny-mistral": "",
    "QuixiAI/samantha-mistral-7b": "",
    "openerotica/CodeLlama-13b-GPTQ-ERP": "",
    "microsoft/kosmos-2-patch14-224": "",
    "yungplin/Lollypop": "No model card",
    "ChocoWu/nextgpt_7b_tiva_v0": "This model contains the weights of NExT-GPT covering text-image-video-audio (tiva), which is built upon\nVicuna-7B with version 0\nImageBind\nStable Diffusion with version v1-5.\nAudioLDM with version l-full.\nZeroScope with version v2_576w.\nFor more details about the usage of the model, please refer to our code repository.",
    "Intelligent-Instruments-Lab/rave-models": "RAVE Models\nMusical Instruments\nguitar_iil_b2048_r48000_z16.ts\nsax_soprano_franziskaschroeder_b2048_r48000_z20.ts\norgan_archive_b2048_r48000_z16.ts\norgan_bach_b2048_sr48000_z16.ts\nmrp_strengjavera_b2048_r44100_z16.ts\nVoice\nvoice_vocalset_b2048_r48000_z16.ts\nvoice_hifitts_b2048_r48000_z16.ts\nvoice_jvs_b2048_r44100_z16.ts\nvoice_vctk_b2048_r44100_z22.ts\nvoice_multivoice_b2048_r48000_z11.ts\nBirds\nbirds_motherbird_b2048_r48000_z16.ts\nbirds_pluma_b2048_r48000_z12.ts\nPond Brain Marine Sounds\nwater_pondbrain_b2048_r48000_z16.ts\nhumpbacks_pondbrain_b2048_r48000_z20.ts\nmarinemammals_pondbrain_b2048_r48000_z20.ts\nThales magnets_b2048_r48000_z8.ts\nCrozzoli's Music crozzoli_bigensemblesmusic_18d.ts\nAulus-les-Bains Dawn Chorus @ CAMP birds_dawnchorus_b2048_r48000_z8.ts\nRAVE Models\nThis is a collection of RAVE models trained by the Intelligent Instruments Lab for various projects.\nFor a full description see our blog post at: https://iil.is/news/ravemodels, and for more about RAVE, see the original paper from IRCAM.\nMost of these models are encoder-decoder only, no prior, and all use the --causal mode and are exported for streaming inference with nn~, NN.ar or rave-supercollider.\nIn the checkpoints/ directory are some complete checkpoints which can be used with our fork of RAVE to speed up training by transfer learning.\nCitation:\n@misc {intelligent_instruments_lab_2023,\nauthor       = { {Intelligent Instruments Lab} },\ntitle        = { rave-models (Revision ad15daf) },\nyear         = 2023,\nurl          = { https://huggingface.co/Intelligent-Instruments-Lab/rave-models },\ndoi          = { 10.57967/hf/1235 },\npublisher    = { Hugging Face }\n}\nMusical Instruments\nguitar_iil_b2048_r48000_z16.ts\nDataset: IILGuitarTimbre, a timbre-oriented collection of plucking, strumming, striking, scraping and more recorded dry from an electric guitar.\nModel: modified RAVE v1, 48kHz, block size 2048, 16 latent dimensions.\nsax_soprano_franziskaschroeder_b2048_r48000_z20.ts\nDataset: Soprano sax improvisation by Franziska Schroeder.\nModel: modified RAVE v1, 48kHz, block size 2048, 20 latent dimensions.\norgan_archive_b2048_r48000_z16.ts\nDataset: various recordings of organ music sourced from archive.org. Small amounts of voice and other instruments were included, and vinyl record noises are prominent.\nModel: modified RAVE v1, 48kHz, block size 2048, 16 latent dimensions.\norgan_bach_b2048_sr48000_z16.ts\nDataset: various recordings of J.S. Bach music for church organ.\nModel: modified RAVE v1, 48kHz, block size 2048, 16 latent dimensions.\nmrp_strengjavera_b2048_r44100_z16.ts\nDataset: magnetic resonator piano controlled by artificial life, as part of generative installation Strengjavera by Jack Armitage premiered at AIMC 2023. See paper and Zenodo for citation.\nModel: RAVE v3, 44.1kHz, block size 2048, 16 latent dimensions.\nVoice\nvoice_vocalset_b2048_r48000_z16.ts\nDataset: VocalSet singing voice dataset.\nModel: modified RAVE v1, 48kHz, block size 2048, 16 latent dimensions.\nvoice_hifitts_b2048_r48000_z16.ts\nDataset: Hi-Fi TTS audiobooks dataset.\nModel: modified RAVE v1, 48kHz, block size 2048, 16 latent dimensions.\nvoice_jvs_b2048_r44100_z16.ts\nDataset: Hi-Fi TTS speaker 9017 (John Van Stan).\nModel: RAVE v3, 44.1kHz, block size 2048, 16 latent dimensions.\nvoice_vctk_b2048_r44100_z22.ts\nDataset: CSTR VCTK Corpus multispeaker read speech dataset.\nModel: RAVE v3, 44.1kHz, block size 2048, 22 latent dimensions.\nvoice_multivoice_b2048_r48000_z11.ts\nDataset: combination of speaking and singing voice datasets: CSTR VCTK Corpus, VocalSet, Children's Song Dataset, NUS-48E, attHACK.\nModel: RAVE v3 with spectral discriminator, 48kHz, block size 2048, 11 latent dimensions.\nBirds\nbirds_motherbird_b2048_r48000_z16.ts\nThis model of bird sounds was curated by Manuel Cherep, Jessica Shand and Jack Armitage for their piece Motherbird, performed at TENOR 2023 in Boston, May 2023.\nDataset: bird sounds.\nModel: RAVE v1, 48kHz, block size 2048, 16 latent dimensions.\nbirds_pluma_b2048_r48000_z12.ts\nThis model of bird sounds was curated by Giacomo Lepri for his instrument Pluma\nDataset: bird sounds.\nModel: modified RAVE v1, 48kHz, block size 2048, 12 latent dimensions.\nPond Brain Marine Sounds\nThese models of marine sounds were trained for Jenna Sutela's Pond Brain installations at Copenhagen Contemporary and the Helsinki Biennial\nCaution: these decoders sometimes produce a loud chirp on first initialization.\nwater_pondbrain_b2048_r48000_z16.ts\nDataset: water recordings from freesound.org.\nlist of freesound users\ninspectorj, inchadney, aesqe, vonfleisch, javetakami, atomediadesign, kolezan, zabuhailo, zaziesound, repdac3, al_sub, lgarrett, uzbazur, lydmakeren, frenkfurth, edo333, boredtoinsanity, owl, kaydinhamby, tliedes, ilmari_freesound, manoslindos, l3ardoc, alexbuk, s-light\nModel: modified RAVE v1, 48kHz, block size 2048, 16 latent dimensions.\nhumpbacks_pondbrain_b2048_r48000_z20.ts\nDataset: humpback whale recordings from the Watkins database, MBARI, and BBC.\nModel: modified RAVE v1, 48kHz, block size 2048, 20 latent dimensions.\nmarinemammals_pondbrain_b2048_r48000_z20.ts\nDataset: various marine mammal sounds from NOAA, the Watkins database, freesound users felixblume and geraldfiebig, and sound effects databases.\nModel: modified RAVE v1, 48kHz, block size 2048, 20 latent dimensions.\nThales magnets_b2048_r48000_z8.ts\nDataset: One hour recording of magnets of different dimensions hitting each other or scratching wooden and metallic surfaces. Used for Thales, a musical instrument based on magnets\nModel: RAVE v1, 48Khz, block size 2048, 8 latent dimensions.\nCrozzoli's Music crozzoli_bigensemblesmusic_18d.ts\nDataset: Six recordings of long contemporary compositions for electronic and acoustic big ensembles.\nModel: RAVE v3, 48Khz, block size 2048, 18 latent dimensions.\nAulus-les-Bains Dawn Chorus @ CAMP birds_dawnchorus_b2048_r48000_z8.ts\nDataset: ~230 minutes of dawn chorus recorded by Gregory White at Aulus-les-Bains as part of a residency at CAMPfr.com.\nModel: RAVE v3, 48Khz, block size 2048, 8 latent dimensions.",
    "nvidia/stt_en_fastconformer_hybrid_large_streaming_multi": "NVIDIA Streaming Conformer-Hybrid Large (en-US)\nModel Architecture\nTraining\nDatasets\nPerformance\nTransducer Decoder\nCTC Decoder\nHow to Use this Model\nSimulate Streaming ASR\nTranscribing using Python\nTranscribing many audio files\nInput\nOutput\nLimitations\nNVIDIA Riva: Deployment\nReferences\nNVIDIA Streaming Conformer-Hybrid Large (en-US)\n|\n|\n|\nThis collection contains large-size versions of cache-aware FastConformer-Hybrid (around 114M parameters) with multiple look-ahead support, trained on a large scale english speech.\nThese models are trained for streaming ASR, which be used for streaming applications with a variety of latencies (0ms, 80ms, 480s, 1040ms).\nThese are the worst latency and average latency of the model for each case would be half of these numbers. You may find more detail and evalution results here [5].\nModel Architecture\nThese models are cache-aware versions of Hybrid FastConfomer which are trained for streaming ASR. You may find more info on cache-aware models here: Cache-aware Streaming Conformer [5].\nThe models are trained with multiple look-aheads which makes the model to be able to support different latencies.\nTo learn on how to switch between different look-ahead, you may read the documentation on the cache-aware models.\nFastConformer [4] is an optimized version of the Conformer model [1], and\nyou may find more information on the details of FastConformer here: Fast-Conformer Model.\nThe model is trained in a multitask setup with joint Transducer and CTC decoder loss [5]. You can find more about Hybrid Transducer-CTC training here: Hybrid Transducer-CTC.\nYou may also find more on how to switch between the Transducer and CTC decoders in the documentation.\nTraining\nThe NeMo toolkit [3] was used for training the models for over several hundred epochs. These model are trained with this example script and this base config. The SentencePiece tokenizers [2] for these models were built using the text transcripts of the train set with this script.\nDatasets\nAll the models in this collection are trained on a composite dataset (NeMo ASRSET) comprising of several thousand hours of English speech:\nLibrispeech 960 hours of English speech\nFisher Corpus\nSwitchboard-1 Dataset\nWSJ-0 and WSJ-1\nNational Speech Corpus (Part 1, Part 6)\nVCTK\nVoxPopuli (EN)\nEuroparl-ASR (EN)\nMultilingual Librispeech (MLS EN) - 2,000 hours subset\nMozilla Common Voice (v7.0)\nPeople's Speech - 12,000 hrs subset\nNote: older versions of the model may have trained on smaller set of datasets.\nPerformance\nThe list of the available models in this collection is shown in the following tables for both CTC and Transducer decoders.\nPerformances of the ASR models are reported in terms of Word Error Rate (WER%) with greedy decoding.\nTransducer Decoder\natt_context_sizes\nLS test-other ([70,13]-1040ms)\nLS test-other ([70,6]-480ms)\nLS test-other ([70,1]-80ms)\nLS test-other ([70,0]-0s)\nTrain Dataset\n[[70,13],[70,6],[70,1],[70,0]]\n5.4\n5.7\n6.4\n7.0\nNeMo ASRSET 3.0\nCTC Decoder\natt_context_sizes\nLS test-other ([70,13]-1040ms)\nLS test-other ([70,6]-480ms)\nLS test-other ([70,1]-80ms)\nLS test-other ([70,0]-0s)\nTrain Dataset\n[[70,13],[70,6],[70,1],[70,0]]\n6.2\n6.7\n7.8\n8.4\nNeMo ASRSET 3.0\nHow to Use this Model\nThe model is available for use in the NeMo toolkit [3], and can be used as a pre-trained checkpoint for streaming or for fine-tuning on another dataset.\nYou will need to install NVIDIA NeMo. We recommend you install it after you've installed latest Pytorch version.\npip install nemo_toolkit['all']\nSimulate Streaming ASR\nYou may use this script to simulate streaming ASR with these models: cache-aware streaming simulation.\nYou may use --att_context_size to set the context size otherwise, the default, which is the first context size in the list (1040ms), is going to be used.\nTranscribing using Python\nCache-aware models are designed in a way that the model's predictions are the same in both offline and streaming mode.\nSo you may use the regular transcribe function to get the transcriptions. First, let's get a sample:\nwget https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav\nThen simply do:\nimport nemo.collections.asr as nemo_asr\nasr_model = nemo_asr.models.EncDecHybridRNNTCTCBPEModel.from_pretrained(model_name=\"nvidia/stt_en_fastconformer_hybrid_large_streaming_multi\")\n# Optional: change the default latency. Default latency is 1040ms. Supported latencies: {0: 0ms, 1: 80ms, 16: 480ms, 33: 1040ms}.\n# Note: These are the worst latency and average latency would be half of these numbers.\nasr_model.encoder.set_default_att_context_size([70,13])\n#Optional: change the default decoder. Default decoder is Transducer (RNNT). Supported decoders: {ctc, rnnt}.\nasr_model.change_decoding_strategy(decoder_type='rnnt')\noutput = asr_model.transcribe(['2086-149220-0033.wav'])\nprint(output[0].text)\nTranscribing many audio files\nUsing Transducer mode inference:\npython [NEMO_GIT_FOLDER]/examples/asr/transcribe_speech.py \\\npretrained_name=\"stt_en_fastconformer_hybrid_large_streaming_multi\" \\\naudio_dir=\"<DIRECTORY CONTAINING AUDIO FILES>\"\nUsing CTC mode inference:\npython [NEMO_GIT_FOLDER]/examples/asr/transcribe_speech.py \\\npretrained_name=\"stt_en_fastconformer_hybrid_large_streaming_multi\" \\\naudio_dir=\"<DIRECTORY CONTAINING AUDIO FILES>\" \\\ndecoder_type=\"ctc\"\nTo change between different look-aheads you may set att_context_size of the script transcribe_speech.py as the following:\npython [NEMO_GIT_FOLDER]/examples/asr/transcribe_speech.py \\\npretrained_name=\"stt_en_fastconformer_hybrid_large_streaming_multi\" \\\naudio_dir=\"<DIRECTORY CONTAINING AUDIO FILES>\" \\\natt_context_size=[70,0]\nSupported values for att_context_size: {[70,0]: 0ms, [70,1]: 80ms, [70,16]: 480ms, [70,33]: 1040ms}.\nInput\nThis model accepts 16000 KHz Mono-channel Audio (wav files) as input.\nOutput\nThis model provides transcribed speech as a string for a given audio sample.\nLimitations\nSince this model was trained on publicly available speech datasets, the performance of this model might degrade for speech which includes technical terms, or vernacular that the model has not been trained on. The model might also perform worse for accented speech.\nNVIDIA Riva: Deployment\nNVIDIA Riva, is an accelerated speech AI SDK deployable on-prem, in all clouds, multi-cloud, hybrid, on edge, and embedded.\nAdditionally, Riva provides:\nWorld-class out-of-the-box accuracy for the most common languages with model checkpoints trained on proprietary data with hundreds of thousands of GPU-compute hours\nBest in class accuracy with run-time word boosting (e.g., brand and product names) and customization of acoustic model, language model, and inverse text normalization\nStreaming speech recognition, Kubernetes compatible scaling, and enterprise-grade support\nAlthough this model isn‚Äôt supported yet by Riva, the list of supported models is here.Check out Riva live demo.\nReferences\n[1] Conformer: Convolution-augmented Transformer for Speech Recognition\n[2] Google Sentencepiece Tokenizer\n[3] NVIDIA NeMo Toolkit\n[4] Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition\n[5] Stateful Conformer with Cache-based Inference for Streaming Automatic Speech Recognition",
    "davidscripka/openwakeword": "OpenWakeWord Models\nOpenWakeWord Models\nThis repo contains the official pre-trained models for the openWakeWord wake word/phrase detection library.\nAll of these pre-trained models are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license due to the inclusion of datasets with unknown or restrictive licensing as part of the training data.",
    "teknium/Mistral-Trismegistus-7B": "Model Description:\nSpecial Features:\nAcknowledgements:\nDataset:\nUsage:\nBenchmarks:\nLicensing:\nMistral Trismegistus 7B\nModel Description:\nTranscendence is All You Need! Mistral Trismegistus is a model made for people interested in the esoteric, occult, and spiritual.\nHere are some outputs:\nAnswer questions about occult artifacts:\nPlay the role of a hypnotist:\nSpecial Features:\nThe First Powerful Occult Expert Model: ~10,000 high quality, deep, rich, instructions on the occult, esoteric, and spiritual.\nFast: Trained on Mistral, a state of the art 7B parameter model, you can run this model FAST on even a cpu.\nNot a positivity-nazi: This model was trained on all forms of esoteric tasks and knowledge, and is not burdened by the flowery nature of many other models, who chose positivity over creativity.\nAcknowledgements:\nSpecial thanks to @a16z.\nDataset:\nThis model was trained on a 100% synthetic, gpt-4 generated dataset, about ~10,000 examples, on a wide and diverse set of both tasks and knowledge about the esoteric, occult, and spiritual.\nThe dataset will be released soon!\nUsage:\nPrompt Format:\nUSER: <prompt>\nASSISTANT:\nOR\n<system message>\nUSER: <prompt>\nASSISTANT:\nBenchmarks:\nNo benchmark can capture the nature and essense of the quality of spirituality and esoteric knowledge and tasks. You will have to try testing it yourself!\nTraining run on wandb here: https://wandb.ai/teknium1/occult-expert-mistral-7b/runs/coccult-expert-mistral-6/overview\nLicensing:\nApache 2.0",
    "TaylorAI/bge-micro": "bge-micro\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nEvaluation Results\nFull Model Architecture\nCiting & Authors\nbge-micro\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\nIt is distilled from bge-small-en-v1.5, with 1/4 the non-embedding parameters.\nIt has 1/2 the parameters of the smallest commonly-used embedding model, all-MiniLM-L6-v2, with similar performance.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('{MODEL_NAME}')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('{MODEL_NAME}')\nmodel = AutoModel.from_pretrained('{MODEL_NAME}')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nEvaluation Results\nFor an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel\n(1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\nCiting & Authors",
    "IkariDev/Athena-v4": "Description\nRatings:\nModels+loras used and recipe\nPrompt template: Alpaca\nExperimental Athena v4 model. Use Alpaca format. Suitable for RP, ERP and general stuff.\nI should state here that this is a HIGHLY experimental model!\nDescription\nThis repo contains fp16 files of Athena-V4.\nGGUF - By TheBloke\nGPTQ - By TheBloke\nexl2 - by waldie\nAWQ - By TheBloke\nfp16 - by IkariDev+Undi95\nOLD(GGUF - by IkariDev+Undi95)\nRatings:\nNote: I have permission of all users to upload their ratings, i DONT screenshot random reviews without asking if i can put them here!\nIf you want your rating to be here, send me a message over on DC and ill put up a screenshot of it here. DC name is \"ikaridev\".\nModels+loras used and recipe\nAthena-v3\nXwin-LM/Xwin-LM-13B-V0.1\nUndi95/PsyMedRP-v1-13B\ncgato/Thespis-13b-v0.2\njondurbin/airoboros-l2-13b-3.0\nAthena-v4-tmp1 = [ Athena-v3(0.85)+Xwin-LM/Xwin-LM-13B-V0.1(0.15) ]\nAthena-v4-tmp2 = [ Undi95/PsyMedRP-v1-13B(0.55)+cgato/Thespis-13b-v0.2(0.45) ]\nAthena-v4-tmp3 = Athena-v4-tmp1(0.55) + Athena-v4-tmp2(0.35)\nAthena-v4 = Athena-v4-tmp3 + jondurbin/airoboros-l2-13b-3.0(0.1)\nPrompt template: Alpaca\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{prompt}\n### Response:\nThanks to Undi95 for providing the machine for Athena v2 and Athena v3, and giving me infos about how things work. Going forward i will use a merging server provided by a friend.",
    "HuggingFaceH4/zephyr-7b-alpha": "Model Card for Zephyr 7B Alpha\nModel description\nModel Sources\nIntended uses & limitations\nBias, Risks, and Limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nCitation\nModel Card for Zephyr 7B Alpha\nZephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-Œ± is the first model in the series, and is a fine-tuned version of mistralai/Mistral-7B-v0.1 that was trained on on a mix of publicly available, synthetic datasets using Direct Preference Optimization (DPO). We found that removing the in-built alignment of these datasets boosted performance on MT Bench and made the model more helpful. However, this means that model is likely to generate problematic text when prompted to do so.\nModel description\nModel type: A 7B parameter GPT-like model fine-tuned on a mix of publicly available, synthetic datasets.\nLanguage(s) (NLP): Primarily English\nLicense: MIT\nFinetuned from model: mistralai/Mistral-7B-v0.1\nModel Sources\nRepository: https://github.com/huggingface/alignment-handbook\nDemo: https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat\nIntended uses & limitations\nThe model was initially fine-tuned on a variant of the UltraChat dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. We then further aligned the model with ü§ó TRL's DPOTrainer on the openbmb/UltraFeedback dataset, which contain 64k prompts and model completions that are ranked by GPT-4. As a result, the model can be used for chat and you can check out our demo to test its capabilities.\nHere's how you can run the model using the pipeline() function from ü§ó Transformers:\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\nimport torch\nfrom transformers import pipeline\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-alpha\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n{\n\"role\": \"system\",\n\"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n},\n{\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\nBias, Risks, and Limitations\nZephyr-7B-Œ± has not been aligned to human preferences with techniques like RLHF or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so).\nIt is also unknown what the size and composition of the corpus was used to train the base model (mistralai/Mistral-7B-v0.1), however it is likely to have included a mix of Web data and technical sources like books and code. See the Falcon 180B model card for an example of this.\nTraining and evaluation data\nZephyr 7B Alpha achieves the following results on the evaluation set:\nLoss: 0.4605\nRewards/chosen: -0.5053\nRewards/rejected: -1.8752\nRewards/accuracies: 0.7812\nRewards/margins: 1.3699\nLogps/rejected: -327.4286\nLogps/chosen: -297.1040\nLogits/rejected: -2.7153\nLogits/chosen: -2.7447\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 5e-07\ntrain_batch_size: 2\neval_batch_size: 4\nseed: 42\ndistributed_type: multi-GPU\nnum_devices: 16\ntotal_train_batch_size: 32\ntotal_eval_batch_size: 64\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nlr_scheduler_warmup_ratio: 0.1\nnum_epochs: 1\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nRewards/chosen\nRewards/rejected\nRewards/accuracies\nRewards/margins\nLogps/rejected\nLogps/chosen\nLogits/rejected\nLogits/chosen\n0.5602\n0.05\n100\n0.5589\n-0.3359\n-0.8168\n0.7188\n0.4809\n-306.2607\n-293.7161\n-2.6554\n-2.6797\n0.4852\n0.1\n200\n0.5136\n-0.5310\n-1.4994\n0.8125\n0.9684\n-319.9124\n-297.6181\n-2.5762\n-2.5957\n0.5212\n0.15\n300\n0.5168\n-0.1686\n-1.1760\n0.7812\n1.0074\n-313.4444\n-290.3699\n-2.6865\n-2.7125\n0.5496\n0.21\n400\n0.4835\n-0.1617\n-1.7170\n0.8281\n1.5552\n-324.2635\n-290.2326\n-2.7947\n-2.8218\n0.5209\n0.26\n500\n0.5054\n-0.4778\n-1.6604\n0.7344\n1.1826\n-323.1325\n-296.5546\n-2.8388\n-2.8667\n0.4617\n0.31\n600\n0.4910\n-0.3738\n-1.5180\n0.7656\n1.1442\n-320.2848\n-294.4741\n-2.8234\n-2.8521\n0.4452\n0.36\n700\n0.4838\n-0.4591\n-1.6576\n0.7031\n1.1986\n-323.0770\n-296.1796\n-2.7401\n-2.7653\n0.4674\n0.41\n800\n0.5077\n-0.5692\n-1.8659\n0.7656\n1.2967\n-327.2416\n-298.3818\n-2.6740\n-2.6945\n0.4656\n0.46\n900\n0.4927\n-0.5279\n-1.6614\n0.7656\n1.1335\n-323.1518\n-297.5553\n-2.7817\n-2.8015\n0.4102\n0.52\n1000\n0.4772\n-0.5767\n-2.0667\n0.7656\n1.4900\n-331.2578\n-298.5311\n-2.7160\n-2.7455\n0.4663\n0.57\n1100\n0.4740\n-0.8038\n-2.1018\n0.7656\n1.2980\n-331.9604\n-303.0741\n-2.6994\n-2.7257\n0.4737\n0.62\n1200\n0.4716\n-0.3783\n-1.7015\n0.7969\n1.3232\n-323.9545\n-294.5634\n-2.6842\n-2.7135\n0.4259\n0.67\n1300\n0.4866\n-0.6239\n-1.9703\n0.7812\n1.3464\n-329.3312\n-299.4761\n-2.7046\n-2.7356\n0.4935\n0.72\n1400\n0.4747\n-0.5626\n-1.7600\n0.7812\n1.1974\n-325.1243\n-298.2491\n-2.7153\n-2.7444\n0.4211\n0.77\n1500\n0.4645\n-0.6099\n-1.9993\n0.7656\n1.3894\n-329.9109\n-299.1959\n-2.6944\n-2.7236\n0.4931\n0.83\n1600\n0.4684\n-0.6798\n-2.1082\n0.7656\n1.4285\n-332.0890\n-300.5934\n-2.7006\n-2.7305\n0.5029\n0.88\n1700\n0.4595\n-0.5063\n-1.8951\n0.7812\n1.3889\n-327.8267\n-297.1233\n-2.7108\n-2.7403\n0.4965\n0.93\n1800\n0.4613\n-0.5561\n-1.9079\n0.7812\n1.3518\n-328.0831\n-298.1203\n-2.7226\n-2.7523\n0.4337\n0.98\n1900\n0.4608\n-0.5066\n-1.8718\n0.7656\n1.3652\n-327.3599\n-297.1296\n-2.7175\n-2.7469\nFramework versions\nTransformers 4.34.0\nPytorch 2.0.1+cu118\nDatasets 2.12.0\nTokenizers 0.14.0\nCitation\nIf you find Zephyr-7B-Œ± is useful in your work, please cite it with:\n@misc{tunstall2023zephyr,\ntitle={Zephyr: Direct Distillation of LM Alignment},\nauthor={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Cl√©mentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},\nyear={2023},\neprint={2310.16944},\narchivePrefix={arXiv},\nprimaryClass={cs.LG}\n}\nIf you use the UltraChat or UltraFeedback datasets, please cite the original works:\n@misc{ding2023enhancing,\ntitle={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},\nauthor={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},\nyear={2023},\neprint={2305.14233},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\n@misc{cui2023ultrafeedback,\ntitle={UltraFeedback: Boosting Language Models with High-quality Feedback},\nauthor={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun},\nyear={2023},\neprint={2310.01377},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "BAAI/llm-embedder": "Model List\nFrequently asked questions\nUsage\nUsage for Embedding Model\nUsing FlagEmbedding\nUsing Sentence-Transformers\nUsing Langchain\nUsing HuggingFace Transformers\nUsage for Reranker\nUsing FlagEmbedding\nUsing Huggingface transformers\nEvaluation\nTrain\nBAAI Embedding\nBGE Reranker\nOur Contributors:\nContact\nCitation\nLicense\nFlagEmbedding\nModel List |\nFAQ |\nUsage  |\nEvaluation |\nTrain |\nContact |\nCitation |\nLicense\nMore details please refer to our Github: FlagEmbedding.\nEnglish | ‰∏≠Êñá\nHiring: We're seeking experienced NLP researchers and intern students focusing on dense retrieval and retrieval-augmented LLMs. If you're interested, please feel free to reach out to us via email at zhengliu1026@gmail.com.\nFlagEmbedding can map any text to a low-dimensional dense vector, which can be used for tasks like retrieval, classification,  clustering, and semantic search.\nAnd it can also be used in vector databases for LLMs.\n************* üåüUpdatesüåü *************\n10/12/2023: Release LLM-Embedder, a unified embedding model to support diverse retrieval augmentation needs for LLMs. Paper :fire:\n09/15/2023: The technical report of BGE has been released\n09/15/2023: The massive training data of BGE has been released\n09/12/2023: New models:\nNew reranker model: release cross-encoder models BAAI/bge-reranker-base and BAAI/bge-reranker-large, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models.\nupdate embedding model: release bge-*-v1.5 embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\nMore\n09/07/2023: Update fine-tune code: Add script to mine hard negatives and support adding instruction during fine-tuning.\n08/09/2023: BGE Models are integrated into Langchain, you can use it like this; C-MTEB leaderboard is available.\n08/05/2023: Release base-scale and small-scale models, best performance among the models of the same size ü§ó\n08/02/2023: Release bge-large-*(short for BAAI General Embedding) Models, rank 1st on MTEB and C-MTEB benchmark! :tada: :tada:\n08/01/2023: We release the Chinese Massive Text Embedding Benchmark (C-MTEB), consisting of 31 test dataset.\nModel List\nbge is short for BAAI general embedding.\nModel\nLanguage\nDescription\nquery instruction for retrieval [1]\nBAAI/llm-embedder\nEnglish\nInference Fine-tune\na unified embedding model to support diverse retrieval augmentation needs for LLMs\nSee README\nBAAI/bge-reranker-large\nChinese and English\nInference Fine-tune\na cross-encoder model which is more accurate but less efficient [2]\nBAAI/bge-reranker-base\nChinese and English\nInference Fine-tune\na cross-encoder model which is more accurate but less efficient [2]\nBAAI/bge-large-en-v1.5\nEnglish\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\nRepresent this sentence for searching relevant passages:\nBAAI/bge-base-en-v1.5\nEnglish\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\nRepresent this sentence for searching relevant passages:\nBAAI/bge-small-en-v1.5\nEnglish\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\nRepresent this sentence for searching relevant passages:\nBAAI/bge-large-zh-v1.5\nChinese\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\nBAAI/bge-base-zh-v1.5\nChinese\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\nBAAI/bge-small-zh-v1.5\nChinese\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\nBAAI/bge-large-en\nEnglish\nInference Fine-tune\n:trophy: rank 1st in MTEB leaderboard\nRepresent this sentence for searching relevant passages:\nBAAI/bge-base-en\nEnglish\nInference Fine-tune\na base-scale model but with similar ability to bge-large-en\nRepresent this sentence for searching relevant passages:\nBAAI/bge-small-en\nEnglish\nInference Fine-tune\na small-scale model but with competitive performance\nRepresent this sentence for searching relevant passages:\nBAAI/bge-large-zh\nChinese\nInference Fine-tune\n:trophy: rank 1st in C-MTEB benchmark\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\nBAAI/bge-base-zh\nChinese\nInference Fine-tune\na base-scale model but with similar ability to bge-large-zh\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\nBAAI/bge-small-zh\nChinese\nInference Fine-tune\na small-scale model but with competitive performance\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\n[1]: If you need to search the relevant passages in a query, we suggest to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, no instruction needs to be added to passages.\n[2]: Different from the embedding model, reranker uses question and document as input and directly output similarity instead of embedding. To balance the accuracy and time cost, cross-encoder is widely used to re-rank top-k documents retrieved by other simple models.\nFor example, use bge embedding model to retrieve top 100 relevant documents, and then use bge reranker to re-rank the top 100 documents to get the final top-3 results.\nAll models have been uploaded to Huggingface Hub, and you can see them at https://huggingface.co/BAAI.\nIf you cannot open the Huggingface Hub, you can also download the models at https://model.baai.ac.cn/models .\nFrequently asked questions\n1. How to fine-tune bge embedding model?\nFollowing this example to prepare data and fine-tune your model.\nSome suggestions:\nMine hard negatives following this example, which can improve the retrieval performance.\nIn general, larger hyper-parameter per_device_train_batch_size brings better performance. You can expand it by enabling --fp16, --deepspeed df_config.json (df_config.json can refer to ds_config.json, --gradient_checkpointing, etc.\nIf you pre-train bge on your data, the pre-trained model cannot be directly used to calculate similarity, and it must be fine-tuned with contrastive learning before computing similarity.\nIf the accuracy of the fine-tuned model is still not high, it is recommended to use/fine-tune the cross-encoder model (bge-reranker) to re-rank top-k results. Hard negatives also are needed to fine-tune reranker.\n2. The similarity score between two dissimilar sentences is higher than 0.5\nSuggest to use bge v1.5, which alleviates the issue of the similarity distribution.\nSince we finetune the models by contrastive learning with a temperature of 0.01,\nthe similarity distribution of the current BGE model is about in the interval [0.6, 1].\nSo a similarity score greater than 0.5 does not indicate that the two sentences are similar.\nFor downstream tasks, such as passage retrieval or semantic similarity,\nwhat matters is the relative order of the scores, not the absolute value.\nIf you need to filter similar sentences based on a similarity threshold,\nplease select an appropriate similarity threshold based on the similarity distribution on your data (such as 0.8, 0.85, or even 0.9).\n3. When does the query instruction need to be used\nFor the bge-*-v1.5, we improve its retrieval ability when not using instruction.\nNo instruction only has a slight degradation in retrieval performance compared with using instruction.\nSo you can generate embedding without instruction in all cases for convenience.\nFor a retrieval task that uses short queries to find long related documents,\nit is recommended to add instructions for these short queries.\nThe best method to decide whether to add instructions for queries is choosing the setting that achieves better performance on your task.\nIn all cases, the documents/passages do not need to add the instruction.\nUsage\nUsage for Embedding Model\nHere are some examples of using bge models with\nFlagEmbedding, Sentence-Transformers, Langchain, or Huggingface Transformers.\nUsing FlagEmbedding\npip install -U FlagEmbedding\nIf it doesn't work for you, you can see FlagEmbedding for more methods to install FlagEmbedding.\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"Ê†∑‰æãÊï∞ÊçÆ-1\", \"Ê†∑‰æãÊï∞ÊçÆ-2\"]\nsentences_2 = [\"Ê†∑‰æãÊï∞ÊçÆ-3\", \"Ê†∑‰æãÊï∞ÊçÆ-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5',\nquery_instruction_for_retrieval=\"‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\",\nuse_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\nqueries = ['query_1', 'query_2']\npassages = [\"Ê†∑‰æãÊñáÊ°£-1\", \"Ê†∑‰æãÊñáÊ°£-2\"]\nq_embeddings = model.encode_queries(queries)\np_embeddings = model.encode(passages)\nscores = q_embeddings @ p_embeddings.T\nFor the value of the argument query_instruction_for_retrieval, see Model List.\nBy default, FlagModel will use all available GPUs when encoding. Please set os.environ[\"CUDA_VISIBLE_DEVICES\"] to select specific GPUs.\nYou also can set os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\" to make all GPUs unavailable.\nUsing Sentence-Transformers\nYou can also use the bge models with sentence-transformers:\npip install -U sentence-transformers\nfrom sentence_transformers import SentenceTransformer\nsentences_1 = [\"Ê†∑‰æãÊï∞ÊçÆ-1\", \"Ê†∑‰æãÊï∞ÊçÆ-2\"]\nsentences_2 = [\"Ê†∑‰æãÊï∞ÊçÆ-3\", \"Ê†∑‰æãÊï∞ÊçÆ-4\"]\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nembeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\nembeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\nFor s2p(short query to long passage) retrieval task,\neach short query should start with an instruction (instructions see Model List).\nBut the instruction is not needed for passages.\nfrom sentence_transformers import SentenceTransformer\nqueries = ['query_1', 'query_2']\npassages = [\"Ê†∑‰æãÊñáÊ°£-1\", \"Ê†∑‰æãÊñáÊ°£-2\"]\ninstruction = \"‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\"\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nq_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)\np_embeddings = model.encode(passages, normalize_embeddings=True)\nscores = q_embeddings @ p_embeddings.T\nUsing Langchain\nYou can use bge in langchain like this:\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\nmodel = HuggingFaceBgeEmbeddings(\nmodel_name=model_name,\nmodel_kwargs=model_kwargs,\nencode_kwargs=encode_kwargs,\nquery_instruction=\"‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\"\n)\nmodel.query_instruction = \"‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\"\nUsing HuggingFace Transformers\nWith the transformers package, you can use the model like this: First, you pass your input through the transformer model, then you select the last hidden state of the first token (i.e., [CLS]) as the sentence embedding.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Sentences we want sentence embeddings for\nsentences = [\"Ê†∑‰æãÊï∞ÊçÆ-1\", \"Ê†∑‰æãÊï∞ÊçÆ-2\"]\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel.eval()\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, cls pooling.\nsentence_embeddings = model_output[0][:, 0]\n# normalize embeddings\nsentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\", sentence_embeddings)\nUsage for Reranker\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding.\nYou can get a relevance score by inputting query and passage to the reranker.\nThe reranker is optimized based cross-entropy loss, so the relevance score is not bounded to a specific range.\nUsing FlagEmbedding\npip install -U FlagEmbedding\nGet relevance scores (higher scores indicate more relevance):\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\nUsing Huggingface transformers\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\nmodel.eval()\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\ninputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\nscores = model(**inputs, return_dict=True).logits.view(-1, ).float()\nprint(scores)\nEvaluation\nbaai-general-embedding models achieve state-of-the-art performance on both MTEB and C-MTEB leaderboard!\nFor more details and evaluation tools see our scripts.\nMTEB:\nModel Name\nDimension\nSequence Length\nAverage (56)\nRetrieval (15)\nClustering (11)\nPair Classification (3)\nReranking (4)\nSTS (10)\nSummarization (1)\nClassification (12)\nBAAI/bge-large-en-v1.5\n1024\n512\n64.23\n54.29\n46.08\n87.12\n60.03\n83.11\n31.61\n75.97\nBAAI/bge-base-en-v1.5\n768\n512\n63.55\n53.25\n45.77\n86.55\n58.86\n82.4\n31.07\n75.53\nBAAI/bge-small-en-v1.5\n384\n512\n62.17\n51.68\n43.82\n84.92\n58.36\n81.59\n30.12\n74.14\nbge-large-en\n1024\n512\n63.98\n53.9\n46.98\n85.8\n59.48\n81.56\n32.06\n76.21\nbge-base-en\n768\n512\n63.36\n53.0\n46.32\n85.86\n58.7\n81.84\n29.27\n75.27\ngte-large\n1024\n512\n63.13\n52.22\n46.84\n85.00\n59.13\n83.35\n31.66\n73.33\ngte-base\n768\n512\n62.39\n51.14\n46.2\n84.57\n58.61\n82.3\n31.17\n73.01\ne5-large-v2\n1024\n512\n62.25\n50.56\n44.49\n86.03\n56.61\n82.05\n30.19\n75.24\nbge-small-en\n384\n512\n62.11\n51.82\n44.31\n83.78\n57.97\n80.72\n30.53\n74.37\ninstructor-xl\n768\n512\n61.79\n49.26\n44.74\n86.62\n57.29\n83.06\n32.32\n61.79\ne5-base-v2\n768\n512\n61.5\n50.29\n43.80\n85.73\n55.91\n81.05\n30.28\n73.84\ngte-small\n384\n512\n61.36\n49.46\n44.89\n83.54\n57.7\n82.07\n30.42\n72.31\ntext-embedding-ada-002\n1536\n8192\n60.99\n49.25\n45.9\n84.89\n56.32\n80.97\n30.8\n70.93\ne5-small-v2\n384\n512\n59.93\n49.04\n39.92\n84.67\n54.32\n80.39\n31.16\n72.94\nsentence-t5-xxl\n768\n512\n59.51\n42.24\n43.72\n85.06\n56.42\n82.63\n30.08\n73.42\nall-mpnet-base-v2\n768\n514\n57.78\n43.81\n43.69\n83.04\n59.36\n80.28\n27.49\n65.07\nsgpt-bloom-7b1-msmarco\n4096\n2048\n57.59\n48.22\n38.93\n81.9\n55.65\n77.74\n33.6\n66.19\nC-MTEB:We create the benchmark C-MTEB for Chinese text embedding which consists of 31 datasets from 6 tasks.\nPlease refer to C_MTEB for a detailed introduction.\nModel\nEmbedding dimension\nAvg\nRetrieval\nSTS\nPairClassification\nClassification\nReranking\nClustering\nBAAI/bge-large-zh-v1.5\n1024\n64.53\n70.46\n56.25\n81.6\n69.13\n65.84\n48.99\nBAAI/bge-base-zh-v1.5\n768\n63.13\n69.49\n53.72\n79.75\n68.07\n65.39\n47.53\nBAAI/bge-small-zh-v1.5\n512\n57.82\n61.77\n49.11\n70.41\n63.96\n60.92\n44.18\nBAAI/bge-large-zh\n1024\n64.20\n71.53\n54.98\n78.94\n68.32\n65.11\n48.39\nbge-large-zh-noinstruct\n1024\n63.53\n70.55\n53\n76.77\n68.58\n64.91\n50.01\nBAAI/bge-base-zh\n768\n62.96\n69.53\n54.12\n77.5\n67.07\n64.91\n47.63\nmultilingual-e5-large\n1024\n58.79\n63.66\n48.44\n69.89\n67.34\n56.00\n48.23\nBAAI/bge-small-zh\n512\n58.27\n63.07\n49.45\n70.35\n63.64\n61.48\n45.09\nm3e-base\n768\n57.10\n56.91\n50.47\n63.99\n67.52\n59.34\n47.68\nm3e-large\n1024\n57.05\n54.75\n50.42\n64.3\n68.2\n59.66\n48.88\nmultilingual-e5-base\n768\n55.48\n61.63\n46.49\n67.07\n65.35\n54.35\n40.68\nmultilingual-e5-small\n384\n55.38\n59.95\n45.27\n66.45\n65.85\n53.86\n45.26\ntext-embedding-ada-002(OpenAI)\n1536\n53.02\n52.0\n43.35\n69.56\n64.31\n54.28\n45.68\nluotuo\n1024\n49.37\n44.4\n42.78\n66.62\n61\n49.25\n44.39\ntext2vec-base\n768\n47.63\n38.79\n43.41\n67.41\n62.19\n49.45\n37.66\ntext2vec-large\n1024\n47.36\n41.94\n44.97\n70.86\n60.66\n49.16\n30.02\nReranking:\nSee C_MTEB for evaluation script.\nModel\nT2Reranking\nT2RerankingZh2En*\nT2RerankingEn2Zh*\nMMarcoReranking\nCMedQAv1\nCMedQAv2\nAvg\ntext2vec-base-multilingual\n64.66\n62.94\n62.51\n14.37\n48.46\n48.6\n50.26\nmultilingual-e5-small\n65.62\n60.94\n56.41\n29.91\n67.26\n66.54\n57.78\nmultilingual-e5-large\n64.55\n61.61\n54.28\n28.6\n67.42\n67.92\n57.4\nmultilingual-e5-base\n64.21\n62.13\n54.68\n29.5\n66.23\n66.98\n57.29\nm3e-base\n66.03\n62.74\n56.07\n17.51\n77.05\n76.76\n59.36\nm3e-large\n66.13\n62.72\n56.1\n16.46\n77.76\n78.27\n59.57\nbge-base-zh-v1.5\n66.49\n63.25\n57.02\n29.74\n80.47\n84.88\n63.64\nbge-large-zh-v1.5\n65.74\n63.39\n57.03\n28.74\n83.45\n85.44\n63.97\nBAAI/bge-reranker-base\n67.28\n63.95\n60.45\n35.46\n81.26\n84.1\n65.42\nBAAI/bge-reranker-large\n67.6\n64.03\n61.44\n37.16\n82.15\n84.18\n66.09\n* : T2RerankingZh2En and T2RerankingEn2Zh are cross-language retrieval tasks\nTrain\nBAAI Embedding\nWe pre-train the models using retromae and train them on large-scale pair data using contrastive learning.\nYou can fine-tune the embedding model on your data following our examples.\nWe also provide a pre-train example.\nNote that the goal of pre-training is to reconstruct the text, and the pre-trained model cannot be used for similarity calculation directly, it needs to be fine-tuned.\nFor more training details for bge see baai_general_embedding.\nBGE Reranker\nCross-encoder will perform full-attention over the input pair,\nwhich is more accurate than embedding model (i.e., bi-encoder) but more time-consuming than embedding model.\nTherefore, it can be used to re-rank the top-k documents returned by embedding model.\nWe train the cross-encoder on a multilingual pair data,\nThe data format is the same as embedding model, so you can fine-tune it easily following our example.\nFor more details please refer to ./FlagEmbedding/reranker/README.md\nOur Contributors:\nContact\nIf you have any question or suggestion related to this project, feel free to open an issue or pull request.\nYou also can email Shitao Xiao(stxiao@baai.ac.cn) and Zheng Liu(liuzheng@baai.ac.cn).\nCitation\nIf you find this repository useful, please consider giving a star :star: and citation\n@misc{bge_embedding,\ntitle={C-Pack: Packaged Resources To Advance General Chinese Embedding},\nauthor={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\nyear={2023},\neprint={2309.07597},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\n@misc{llm_embedder,\ntitle={Retrieve Anything To Augment Large Language Models},\nauthor={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\nyear={2023},\neprint={2310.07554},\narchivePrefix={arXiv},\nprimaryClass={cs.IR}\n}\nLicense\nFlagEmbedding is licensed under the MIT License. The released models can be used for commercial purposes free of charge.",
    "TaylorAI/bge-micro-v2": "bge-micro-v2\nUsage (Sentence-Transformers)\nUsage (HuggingFace Transformers)\nEvaluation Results\nFull Model Architecture\nCiting & Authors\nbge-micro-v2\nThis is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\nDistilled in a 2-step training process (bge-micro was step 1) from BAAI/bge-small-en-v1.5.\nUsage (Sentence-Transformers)\nUsing this model becomes easy when you have sentence-transformers installed:\npip install -U sentence-transformers\nThen you can use the model like this:\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\nmodel = SentenceTransformer('{MODEL_NAME}')\nembeddings = model.encode(sentences)\nprint(embeddings)\nUsage (HuggingFace Transformers)\nWithout sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('{MODEL_NAME}')\nmodel = AutoModel.from_pretrained('{MODEL_NAME}')\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\nEvaluation Results\nFor an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net\nFull Model Architecture\nSentenceTransformer(\n(0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel\n(1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\nCiting & Authors",
    "aa444rt/RVC_V2_models_5_japanese_womens": "5 japanese girl  RVC ÔºàvoicechangerÔºâmodel\nÊó•Êú¨Ë™û\n5 japanese girl  RVC ÔºàvoicechangerÔºâmodel\nÊó•Êú¨Ë™û\nlicence MITÁöÑÊéàÊ¨ä‰∏ãÔºåÈÄôÊòØ‰∏ÄÂÄãÂèØ‰æõ‰∫îÂÄãÂ∞ëÂ•≥ËÅ≤Èü≥ËΩâÊèõÁöÑRVCÊ®°Âûã„ÄÇ\nÊú¨Ê®°ÂûãÁî±ÂçÉÊó©Á•ûÁ§æÊèê‰æõ„ÄÇÂÆòÊñπÂèëÂ∏ÉÈ°µÈù¢ÈìæÊé•Ôºöhttps://chihaya369.booth.pm/items/4701666\nDownload https://huggingface.co/aa444rt/RVC_models_5_japanese_womens/tree/main\nhttps://opensource.org/license/mit/",
    "google/owlv2-base-patch16-ensemble": "Model Card: OWLv2\nModel Details\nModel Date\nModel Type\nDocuments\nUse with Transformers\nModel Use\nIntended Use\nData\nBibTeX entry and citation info\nModel Card: OWLv2\nModel Details\nThe OWLv2 model (short for Open-World Localization) was proposed in Scaling Open-Vocabulary Object Detection by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. OWLv2, like OWL-ViT, is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries.\nThe model uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection.\nModel Date\nJune 2023\nModel Type\nThe model uses a CLIP backbone with a ViT-B/16 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. The CLIP backbone is trained from scratch and fine-tuned together with the box and class prediction heads with an object detection objective.\nDocuments\nOWLv2 Paper\nUse with Transformers\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import Owlv2Processor, Owlv2ForObjectDetection\nprocessor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\nmodel = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [[\"a photo of a cat\", \"a photo of a dog\"]]\ninputs = processor(text=texts, images=image, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model(**inputs)\n# Target image sizes (height, width) to rescale box predictions [batch_size, 2]\ntarget_sizes = torch.Tensor([image.size[::-1]])\n# Convert outputs (bounding boxes and class logits) to Pascal VOC Format (xmin, ymin, xmax, ymax)\nresults = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.1)\ni = 0  # Retrieve predictions for the first image for the corresponding text queries\ntext = texts[i]\nboxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\nfor box, score, label in zip(boxes, scores, labels):\nbox = [round(i, 2) for i in box.tolist()]\nprint(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\nModel Use\nIntended Use\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection. We also hope it can be used for interdisciplinary studies of the potential impact of such models, especially in areas that commonly require identifying objects whose label is unavailable during training.\nPrimary intended uses\nThe primary intended users of these models are AI researchers.\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\nData\nThe CLIP backbone of the model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as YFCC100M. A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet. The prediction heads of OWL-ViT, along with the CLIP backbone, are fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\n(to be updated for v2)\nBibTeX entry and citation info\n@misc{minderer2023scaling,\ntitle={Scaling Open-Vocabulary Object Detection},\nauthor={Matthias Minderer and Alexey Gritsenko and Neil Houlsby},\nyear={2023},\neprint={2306.09683},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "google/owlv2-large-patch14": "Model Card: OWLv2\nModel Details\nModel Date\nModel Type\nDocuments\nUse with Transformers\nModel Use\nIntended Use\nData\nBibTeX entry and citation info\nModel Card: OWLv2\nModel Details\nThe OWLv2 model (short for Open-World Localization) was proposed in Scaling Open-Vocabulary Object Detection by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. OWLv2, like OWL-ViT, is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries.\nThe model uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection.\nModel Date\nJune 2023\nModel Type\nThe model uses a CLIP backbone with a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. The CLIP backbone is trained from scratch and fine-tuned together with the box and class prediction heads with an object detection objective.\nDocuments\nOWLv2 Paper\nUse with Transformers\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import Owlv2Processor, Owlv2ForObjectDetection\nprocessor = Owlv2Processor.from_pretrained(\"google/owlv2-large-patch14\")\nmodel = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-large-patch14\")\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [[\"a photo of a cat\", \"a photo of a dog\"]]\ninputs = processor(text=texts, images=image, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model(**inputs)\n# Target image sizes (height, width) to rescale box predictions [batch_size, 2]\ntarget_sizes = torch.Tensor([image.size[::-1]])\n# Convert outputs (bounding boxes and class logits) to Pascal VOC Format (xmin, ymin, xmax, ymax)\nresults = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.1)\ni = 0  # Retrieve predictions for the first image for the corresponding text queries\ntext = texts[i]\nboxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\nfor box, score, label in zip(boxes, scores, labels):\nbox = [round(i, 2) for i in box.tolist()]\nprint(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\nModel Use\nIntended Use\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection. We also hope it can be used for interdisciplinary studies of the potential impact of such models, especially in areas that commonly require identifying objects whose label is unavailable during training.\nPrimary intended uses\nThe primary intended users of these models are AI researchers.\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\nData\nThe CLIP backbone of the model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as YFCC100M. A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet. The prediction heads of OWL-ViT, along with the CLIP backbone, are fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\n(to be updated for v2)\nBibTeX entry and citation info\n@misc{minderer2023scaling,\ntitle={Scaling Open-Vocabulary Object Detection},\nauthor={Matthias Minderer and Alexey Gritsenko and Neil Houlsby},\nyear={2023},\neprint={2306.09683},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "google/owlv2-large-patch14-ensemble": "Model Card: OWLv2\nModel Details\nModel Date\nModel Type\nDocuments\nUse with Transformers\nModel Use\nIntended Use\nData\nBibTeX entry and citation info\nModel Card: OWLv2\nModel Details\nThe OWLv2 model (short for Open-World Localization) was proposed in Scaling Open-Vocabulary Object Detection by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. OWLv2, like OWL-ViT, is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries.\nThe model uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection.\nModel Date\nJune 2023\nModel Type\nThe model uses a CLIP backbone with a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. The CLIP backbone is trained from scratch and fine-tuned together with the box and class prediction heads with an object detection objective.\nDocuments\nOWLv2 Paper\nUse with Transformers\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import Owlv2Processor, Owlv2ForObjectDetection\nprocessor = Owlv2Processor.from_pretrained(\"google/owlv2-large-patch14-ensemble\")\nmodel = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-large-patch14-ensemble\")\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntexts = [[\"a photo of a cat\", \"a photo of a dog\"]]\ninputs = processor(text=texts, images=image, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model(**inputs)\n# Target image sizes (height, width) to rescale box predictions [batch_size, 2]\ntarget_sizes = torch.Tensor([image.size[::-1]])\n# Convert outputs (bounding boxes and class logits) to Pascal VOC Format (xmin, ymin, xmax, ymax)\nresults = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.1)\ni = 0  # Retrieve predictions for the first image for the corresponding text queries\ntext = texts[i]\nboxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\nfor box, score, label in zip(boxes, scores, labels):\nbox = [round(i, 2) for i in box.tolist()]\nprint(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\nModel Use\nIntended Use\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection. We also hope it can be used for interdisciplinary studies of the potential impact of such models, especially in areas that commonly require identifying objects whose label is unavailable during training.\nPrimary intended uses\nThe primary intended users of these models are AI researchers.\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\nData\nThe CLIP backbone of the model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as YFCC100M. A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet. The prediction heads of OWL-ViT, along with the CLIP backbone, are fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\n(to be updated for v2)\nBibTeX entry and citation info\n@misc{minderer2023scaling,\ntitle={Scaling Open-Vocabulary Object Detection},\nauthor={Matthias Minderer and Alexey Gritsenko and Neil Houlsby},\nyear={2023},\neprint={2306.09683},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "SimianLuo/LCM_Dreamshaper_v7": "Latent Consistency Models\nTry our Hugging Face demos:\nModel Descriptions:\nGeneration Results:\nUsage\nUsage (Deprecated)\nBibTeX\nLatent Consistency Models\nOfficial Repository of the paper: Latent Consistency Models.\nProject Page: https://latent-consistency-models.github.io\nTry our Hugging Face demos:\nModel Descriptions:\nDistilled from Dreamshaper v7 fine-tune of Stable-Diffusion v1-5 with only 4,000 training iterations (~32 A100 GPU Hours).\nGeneration Results:\nBy distilling classifier-free guidance into the model's input, LCM can generate high-quality images in very short inference time. We compare the inference time at the setting of 768 x 768 resolution, CFG scale w=8, batchsize=4, using a A800 GPU.\nUsage\nYou can try out Latency Consistency Models directly on:\nTo run the model yourself, you can leverage the üß® Diffusers library:\nInstall the library:\npip install --upgrade diffusers  # make sure to use at least diffusers >= 0.22\npip install transformers accelerate\nRun the model:\nfrom diffusers import DiffusionPipeline\nimport torch\npipe = DiffusionPipeline.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\")\n# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\npipe.to(torch_device=\"cuda\", torch_dtype=torch.float32)\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n# Can be set to 1~50 steps. LCM support fast inference even <= 4 steps. Recommend: 1~8 steps.\nnum_inference_steps = 4\nimages = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images\nFor more information, please have a look at the official docs:\nüëâ https://huggingface.co/docs/diffusers/api/pipelines/latent_consistency_models#latent-consistency-models\nUsage (Deprecated)\nInstall the library:\npip install diffusers transformers accelerate\nRun the model:\nfrom diffusers import DiffusionPipeline\nimport torch\npipe = DiffusionPipeline.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\", custom_pipeline=\"latent_consistency_txt2img\", custom_revision=\"main\", revision=\"fb9c5d\")\n# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\npipe.to(torch_device=\"cuda\", torch_dtype=torch.float32)\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\n# Can be set to 1~50 steps. LCM support fast inference even <= 4 steps. Recommend: 1~8 steps.\nnum_inference_steps = 4\nimages = pipe(prompt=prompt, num_inference_steps=num_inference_steps, guidance_scale=8.0, output_type=\"pil\").images\nBibTeX\n@misc{luo2023latent,\ntitle={Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference},\nauthor={Simian Luo and Yiqin Tan and Longbo Huang and Jian Li and Hang Zhao},\nyear={2023},\neprint={2310.04378},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "dima806/deepfake_vs_real_image_detection": "Checks whether an image is real or fake (AI-generated).\nNote to users who want to use this model in production\nBeware that this model is trained on a dataset collected about 3 years ago.\nSince then, there is a remarkable progress in generating deepfake images with common AI tools, resulting in a significant concept drift.\nTo mitigate that, I urge you to retrain the model using the latest available labeled data.\nAs a quick-fix approach, simple reducing the threshold (say from default 0.5 to 0.1 or even 0.01) of labelling image as a fake may suffice.\nHowever, you will do that at your own risk, and retraining the model is the better way of handling the concept drift.\nSee https://www.kaggle.com/code/dima806/deepfake-vs-real-faces-detection-vit for more details.\nClassification report:\nprecision    recall  f1-score   support\nReal     0.9921    0.9933    0.9927     38080\nFake     0.9933    0.9921    0.9927     38081\naccuracy                         0.9927     76161\nmacro avg     0.9927    0.9927    0.9927     76161\nweighted avg     0.9927    0.9927    0.9927     76161"
}