{
    "google/gemma-scope": "Gemma Scope:\nKey links:\nFull weight set:\nWhich SAE is in the Neuronpedia demo?\nCitation\nGemma Scope:\nThis is a landing page for Gemma Scope, a comprehensive, open suite of sparse autoencoders for Gemma 2 9B and 2B. Sparse Autoencoders are a \"microscope\" of sorts that can help us break down a model‚Äôs internal activations into the underlying concepts, just as biologists use microscopes to study the individual cells of plants and animals.\nThere are no model weights in this repo. If you are looking for them, please visit one of our repos:\nhttps://huggingface.co/google/gemma-scope-2b-pt-res\nhttps://huggingface.co/google/gemma-scope-2b-pt-mlp\nhttps://huggingface.co/google/gemma-scope-2b-pt-att\nhttps://huggingface.co/google/gemma-scope-9b-pt-res\nhttps://huggingface.co/google/gemma-scope-9b-pt-mlp\nhttps://huggingface.co/google/gemma-scope-9b-pt-att\nhttps://huggingface.co/google/gemma-scope-27b-pt-res\nhttps://huggingface.co/google/gemma-scope-9b-it-res\nhttps://huggingface.co/google/gemma-scope-2b-pt-transcoders\nThis tutorial has instructions on how to load the SAEs, and this tutorial explains and implements JumpReLU SAE training in PyTorch and JAX.\nKey links:\nCheck out the interactive Gemma Scope demo made by Neuronpedia.\n(NEW!) We have a colab notebook tutorial for JumpReLU SAE training in JAX and PyTorch here.\nLearn more about Gemma Scope in our Google DeepMind blog post.\nCheck out our Google Colab notebook tutorial for how to use Gemma Scope.\nRead the Gemma Scope technical report.\nCheck out Mishax, a GDM internal tool that we used in this project to expose the internal activations inside Gemma 2 models.\nFull weight set:\nThe full list of SAEs we trained at which sites and layers are linked from the following table, adapted from Figure 1 of our technical report:\nGemma 2 Model\nSAE Width\nAttention\nMLP\nResidual\nTokens\n2.6B PT(26 layers)\n2^14 ‚âà 16.4K\nAll\nAll\nAll+\n4B\n2^15\n{12}\n8B\n2^16\nAll\nAll\nAll\n8B\n2^17\n{12}\n8B\n2^18\n{12}\n8B\n2^19\n{12}\n8B\n2^20 ‚âà 1M\n{5, 12, 19}\n16B\n9B PT(42 layers)\n2^14\nAll\nAll\nAll\n4B\n2^15\n{20}\n8B\n2^16\n{20}\n8B\n2^17\nAll\nAll\nAll\n8B\n2^18\n{20}\n8B\n2^19\n{20}\n8B\n2^20\n{9, 20, 31}\n16B\n27B PT(46 layers)\n2^17\n{10, 22, 34}\n8B\n9B IT(42 layers)\n2^14\n{9, 20, 31}\n4B\n2^17\n{9, 20, 31}\n8B\nWhich SAE is in the Neuronpedia demo?\nhttps://huggingface.co/google/gemma-scope-2b-pt-res/tree/main/layer_20/width_16k/average_l0_71\nCitation\n@misc{lieberum2024gemmascopeopensparse,\ntitle={Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2},\nauthor={Tom Lieberum and Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Nicolas Sonnerat and Vikrant Varma and J√°nos Kram√°r and Anca Dragan and Rohin Shah and Neel Nanda},\nyear={2024},\neprint={2408.05147},\narchivePrefix={arXiv},\nprimaryClass={cs.LG},\nurl={https://arxiv.org/abs/2408.05147},\n}\nPaper link: https://arxiv.org/abs/2408.05147",
    "unsloth/gemma-2-2b-it-bnb-4bit": "Finetune Gemma 2, Llama 3.1, Mistral 2-5x faster with 70% less memory via Unsloth!\n‚ú® Finetune for Free\nReminder to use the dev version Transformers:\npip install git+https://github.com/huggingface/transformers.git\nFinetune Gemma 2, Llama 3.1, Mistral 2-5x faster with 70% less memory via Unsloth!\nDirectly quantized 4bit model with bitsandbytes.\nWe have a Google Colab Tesla T4 notebook for Gemma 2 (2B) here: https://colab.research.google.com/drive/1weTpKOjBZxZJ5PQ-Ql8i6ptAY2x-FWVA?usp=sharing\nWe have a Google Colab Tesla T4 notebook for Gemma 2 (9B) here: https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama 3 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n63% less\nMistral (9B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nPhi 3 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n63% less\nTinyLlama\n‚ñ∂Ô∏è Start on Colab\n3.9x faster\n74% less\nCodeLlama (34B) A100\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n27% less\nMistral (7B) 1xT4\n‚ñ∂Ô∏è Start on Kaggle\n5x faster*\n62% less\nDPO - Zephyr\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.",
    "unsloth/gemma-2-2b": "Finetune Gemma 2, Llama 3.1, Mistral 2-5x faster with 70% less memory via Unsloth!\n‚ú® Finetune for Free\nReminder to use the dev version Transformers:\npip install git+https://github.com/huggingface/transformers.git\nFinetune Gemma 2, Llama 3.1, Mistral 2-5x faster with 70% less memory via Unsloth!\nDirectly quantized 4bit model with bitsandbytes.\nWe have a Google Colab Tesla T4 notebook for Gemma 2 (2B) here: https://colab.research.google.com/drive/1weTpKOjBZxZJ5PQ-Ql8i6ptAY2x-FWVA?usp=sharing\nWe have a Google Colab Tesla T4 notebook for Gemma 2 (9B) here: https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing\n‚ú® Finetune for Free\nAll notebooks are beginner friendly! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\nUnsloth supports\nFree Notebooks\nPerformance\nMemory use\nLlama 3 (8B)\n‚ñ∂Ô∏è Start on Colab\n2.4x faster\n58% less\nGemma 2 (9B)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n63% less\nMistral (9B)\n‚ñ∂Ô∏è Start on Colab\n2.2x faster\n62% less\nPhi 3 (mini)\n‚ñ∂Ô∏è Start on Colab\n2x faster\n63% less\nTinyLlama\n‚ñ∂Ô∏è Start on Colab\n3.9x faster\n74% less\nCodeLlama (34B) A100\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n27% less\nMistral (7B) 1xT4\n‚ñ∂Ô∏è Start on Kaggle\n5x faster*\n62% less\nDPO - Zephyr\n‚ñ∂Ô∏è Start on Colab\n1.9x faster\n19% less\nThis conversational notebook is useful for ShareGPT ChatML / Vicuna templates.\nThis text completion notebook is for raw text. This DPO notebook replicates Zephyr.\n* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.",
    "Qwen/Qwen2-Audio-7B-Instruct": "Qwen2-Audio-7B-Instruct\nIntroduction\nRequirements\nQuickstart\nVoice Chat Inference\nAudio Analysis Inference\nBatch Inference\nCitation\nQwen2-Audio-7B-Instruct\nIntroduction\nQwen2-Audio is the new series of Qwen large audio-language models. Qwen2-Audio is capable of accepting various audio signal inputs and performing audio analysis or direct textual responses with regard to speech instructions. We introduce two distinct audio interaction modes:\nvoice chat: users can freely engage in voice interactions with Qwen2-Audio without text input;\naudio analysis: users could provide audio and text instructions for analysis during the interaction;\nWe release Qwen2-Audio-7B and Qwen2-Audio-7B-Instruct, which are pretrained model and chat model respectively.\nFor more details, please refer to our Blog, GitHub, and Report.\nRequirements\nThe code of Qwen2-Audio has been in the latest Hugging face transformers and we advise you to build from source with command pip install git+https://github.com/huggingface/transformers, or you might encounter the following error:\nKeyError: 'qwen2-audio'\nQuickstart\nIn the following, we demonstrate how to use Qwen2-Audio-7B-Instruct for the inference, supporting both voice chat and audio analysis modes. Note that we have used the ChatML format for dialog, in this demo we show how to leverage apply_chat_template for this purpose.\nVoice Chat Inference\nIn the voice chat mode, users can freely engage in voice interactions with Qwen2-Audio without text input:\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\nconversation = [\n{\"role\": \"user\", \"content\": [\n{\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"},\n]},\n{\"role\": \"assistant\", \"content\": \"Yes, the speaker is female and in her twenties.\"},\n{\"role\": \"user\", \"content\": [\n{\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav\"},\n]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\nif isinstance(message[\"content\"], list):\nfor ele in message[\"content\"]:\nif ele[\"type\"] == \"audio\":\naudios.append(librosa.load(\nBytesIO(urlopen(ele['audio_url']).read()),\nsr=processor.feature_extractor.sampling_rate)[0]\n)\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\nAudio Analysis Inference\nIn the audio analysis, users could provide both audio and text instructions for analysis:\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\nconversation = [\n{'role': 'system', 'content': 'You are a helpful assistant.'},\n{\"role\": \"user\", \"content\": [\n{\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n{\"type\": \"text\", \"text\": \"What's that sound?\"},\n]},\n{\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n{\"role\": \"user\", \"content\": [\n{\"type\": \"text\", \"text\": \"What can you do when you hear that?\"},\n]},\n{\"role\": \"assistant\", \"content\": \"Stay alert and cautious, and check if anyone is hurt or if there is any damage to property.\"},\n{\"role\": \"user\", \"content\": [\n{\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n{\"type\": \"text\", \"text\": \"What does the person say?\"},\n]},\n]\ntext = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\naudios = []\nfor message in conversation:\nif isinstance(message[\"content\"], list):\nfor ele in message[\"content\"]:\nif ele[\"type\"] == \"audio\":\naudios.append(\nlibrosa.load(\nBytesIO(urlopen(ele['audio_url']).read()),\nsr=processor.feature_extractor.sampling_rate)[0]\n)\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\nBatch Inference\nWe also support batch inference:\nfrom io import BytesIO\nfrom urllib.request import urlopen\nimport librosa\nfrom transformers import Qwen2AudioForConditionalGeneration, AutoProcessor\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\nmodel = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"auto\")\nconversation1 = [\n{\"role\": \"user\", \"content\": [\n{\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"},\n{\"type\": \"text\", \"text\": \"What's that sound?\"},\n]},\n{\"role\": \"assistant\", \"content\": \"It is the sound of glass shattering.\"},\n{\"role\": \"user\", \"content\": [\n{\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"},\n{\"type\": \"text\", \"text\": \"What can you hear?\"},\n]}\n]\nconversation2 = [\n{\"role\": \"user\", \"content\": [\n{\"type\": \"audio\", \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/1272-128104-0000.flac\"},\n{\"type\": \"text\", \"text\": \"What does the person say?\"},\n]},\n]\nconversations = [conversation1, conversation2]\ntext = [processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False) for conversation in conversations]\naudios = []\nfor conversation in conversations:\nfor message in conversation:\nif isinstance(message[\"content\"], list):\nfor ele in message[\"content\"]:\nif ele[\"type\"] == \"audio\":\naudios.append(\nlibrosa.load(\nBytesIO(urlopen(ele['audio_url']).read()),\nsr=processor.feature_extractor.sampling_rate)[0]\n)\ninputs = processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\ninputs['input_ids'] = inputs['input_ids'].to(\"cuda\")\ninputs.input_ids = inputs.input_ids.to(\"cuda\")\ngenerate_ids = model.generate(**inputs, max_length=256)\ngenerate_ids = generate_ids[:, inputs.input_ids.size(1):]\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{Qwen2-Audio,\ntitle={Qwen2-Audio Technical Report},\nauthor={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo,  Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2407.10759},\nyear={2024}\n}\n@article{Qwen-Audio,\ntitle={Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models},\nauthor={Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie  and Zhou, Chang and Zhou, Jingren},\njournal={arXiv preprint arXiv:2311.07919},\nyear={2023}\n}",
    "bartowski/gemma-2-2b-it-GGUF": "Llamacpp imatrix Quantizations of gemma-2-2b-it\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nCredits\nDownloading using huggingface-cli\nWhich file should I choose?\nLlamacpp imatrix Quantizations of gemma-2-2b-it\nUsing llama.cpp release b3496 for quantization.\nOriginal model: https://huggingface.co/google/gemma-2-2b-it\nAll quants made using imatrix option with dataset from here\nRun them in LM Studio\nPrompt format\n<bos><start_of_turn>user\n{prompt}<end_of_turn>\n<start_of_turn>model\n<end_of_turn>\n<start_of_turn>model\nNote that this model does not support a System prompt.\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\ngemma-2-2b-it-f32.gguf\nf32\n10.46GB\nfalse\nFull F32 weights.\ngemma-2-2b-it-Q8_0.gguf\nQ8_0\n2.78GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\ngemma-2-2b-it-Q6_K_L.gguf\nQ6_K_L\n2.29GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\ngemma-2-2b-it-Q6_K.gguf\nQ6_K\n2.15GB\nfalse\nVery high quality, near perfect, recommended.\ngemma-2-2b-it-Q5_K_M.gguf\nQ5_K_M\n1.92GB\nfalse\nHigh quality, recommended.\ngemma-2-2b-it-Q5_K_S.gguf\nQ5_K_S\n1.88GB\nfalse\nHigh quality, recommended.\ngemma-2-2b-it-Q4_K_M.gguf\nQ4_K_M\n1.71GB\nfalse\nGood quality, default size for must use cases, recommended.\ngemma-2-2b-it-Q4_K_S.gguf\nQ4_K_S\n1.64GB\nfalse\nSlightly lower quality with more space savings, recommended.\ngemma-2-2b-it-IQ4_XS.gguf\nIQ4_XS\n1.57GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\ngemma-2-2b-it-Q3_K_L.gguf\nQ3_K_L\n1.55GB\nfalse\nLower quality but usable, good for low RAM availability.\ngemma-2-2b-it-IQ3_M.gguf\nIQ3_M\n1.39GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nSome say that this improves the quality, others don't notice any difference. If you use these models PLEASE COMMENT with your findings. I would like feedback that these are actually used and useful so I don't keep uploading quants no one is using.\nThanks!\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset\nThank you ZeroWw for the inspiration to experiment with embed/output\nDownloading using huggingface-cli\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/gemma-2-2b-it-GGUF --include \"gemma-2-2b-it-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/gemma-2-2b-it-GGUF --include \"gemma-2-2b-it-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (gemma-2-2b-it-Q8_0) or download them all in place (./)\nWhich file should I choose?\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "bartowski/gemma-2-2b-it-abliterated-GGUF": "Llamacpp imatrix Quantizations of gemma-2-2b-it-abliterated\nPrompt format\nDownload a file (not the whole branch) from below:\nEmbed/output weights\nCredits\nDownloading using huggingface-cli\nWhich file should I choose?\nLlamacpp imatrix Quantizations of gemma-2-2b-it-abliterated\nUsing llama.cpp release b3496 for quantization.\nOriginal model: https://huggingface.co/IlyaGusev/gemma-2-2b-it-abliterated\nAll quants made using imatrix option with dataset from here\nRun them in LM Studio\nPrompt format\n<bos><start_of_turn>user\n{prompt}<end_of_turn>\n<start_of_turn>model\n<end_of_turn>\n<start_of_turn>model\nNote that this model does not support a System prompt.\nDownload a file (not the whole branch) from below:\nFilename\nQuant type\nFile Size\nSplit\nDescription\ngemma-2-2b-it-abliterated-f32.gguf\nf32\n10.46GB\nfalse\nFull F32 weights.\ngemma-2-2b-it-abliterated-Q8_0.gguf\nQ8_0\n2.78GB\nfalse\nExtremely high quality, generally unneeded but max available quant.\ngemma-2-2b-it-abliterated-Q6_K_L.gguf\nQ6_K_L\n2.29GB\nfalse\nUses Q8_0 for embed and output weights. Very high quality, near perfect, recommended.\ngemma-2-2b-it-abliterated-Q6_K.gguf\nQ6_K\n2.15GB\nfalse\nVery high quality, near perfect, recommended.\ngemma-2-2b-it-abliterated-Q5_K_L.gguf\nQ5_K_L\n2.07GB\nfalse\nUses Q8_0 for embed and output weights. High quality, recommended.\ngemma-2-2b-it-abliterated-Q5_K_M.gguf\nQ5_K_M\n1.92GB\nfalse\nHigh quality, recommended.\ngemma-2-2b-it-abliterated-Q5_K_S.gguf\nQ5_K_S\n1.88GB\nfalse\nHigh quality, recommended.\ngemma-2-2b-it-abliterated-Q4_K_L.gguf\nQ4_K_L\n1.85GB\nfalse\nUses Q8_0 for embed and output weights. Good quality, recommended.\ngemma-2-2b-it-abliterated-Q4_K_M.gguf\nQ4_K_M\n1.71GB\nfalse\nGood quality, default size for must use cases, recommended.\ngemma-2-2b-it-abliterated-Q3_K_XL.gguf\nQ3_K_XL\n1.69GB\nfalse\nUses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability.\ngemma-2-2b-it-abliterated-Q4_K_S.gguf\nQ4_K_S\n1.64GB\nfalse\nSlightly lower quality with more space savings, recommended.\ngemma-2-2b-it-abliterated-IQ4_XS.gguf\nIQ4_XS\n1.57GB\nfalse\nDecent quality, smaller than Q4_K_S with similar performance, recommended.\ngemma-2-2b-it-abliterated-Q3_K_L.gguf\nQ3_K_L\n1.55GB\nfalse\nLower quality but usable, good for low RAM availability.\ngemma-2-2b-it-abliterated-IQ3_M.gguf\nIQ3_M\n1.39GB\nfalse\nMedium-low quality, new method with decent performance comparable to Q3_K_M.\ngemma-2-2b-it-abliterated-Q2_K_L.gguf\nQ2_K_L\n1.37GB\nfalse\nUses Q8_0 for embed and output weights. Very low quality but surprisingly usable.\nEmbed/output weights\nSome of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.\nSome say that this improves the quality, others don't notice any difference. If you use these models PLEASE COMMENT with your findings. I would like feedback that these are actually used and useful so I don't keep uploading quants no one is using.\nThanks!\nCredits\nThank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset\nThank you ZeroWw for the inspiration to experiment with embed/output\nDownloading using huggingface-cli\nFirst, make sure you have hugginface-cli installed:\npip install -U \"huggingface_hub[cli]\"\nThen, you can target the specific file you want:\nhuggingface-cli download bartowski/gemma-2-2b-it-abliterated-GGUF --include \"gemma-2-2b-it-abliterated-Q4_K_M.gguf\" --local-dir ./\nIf the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:\nhuggingface-cli download bartowski/gemma-2-2b-it-abliterated-GGUF --include \"gemma-2-2b-it-abliterated-Q8_0/*\" --local-dir ./\nYou can either specify a new local-dir (gemma-2-2b-it-abliterated-Q8_0) or download them all in place (./)\nWhich file should I choose?\nA great write up with charts showing various performances is provided by Artefact2 here\nThe first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.\nIf you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.\nIf you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.\nNext, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.\nIf you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.\nIf you want to get more into the weeds, you can check out this extremely useful feature chart:\nllama.cpp feature matrix\nBut basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.\nThese I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.\nThe I-quants are not compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.\nWant to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",
    "second-state/gemma-2-2b-it-GGUF": "Gemma-2-2b-it-GGUF\nOriginal Model\nRun with LlamaEdge\nQuantized GGUF Models\nGemma-2-2b-it-GGUF\nOriginal Model\ngoogle/gemma-2-2b-it\nRun with LlamaEdge\nLlamaEdge version: v0.13.0 and above\nPrompt template\nPrompt type: gemma-instruct\nPrompt string\n<bos><start_of_turn>user\n{user_message}<end_of_turn>\n<start_of_turn>model\n{model_message}<end_of_turn>model\nContext size: 2048\nRun as LlamaEdge service\nwasmedge --dir .:. --nn-preload default:GGML:AUTO:gemma-2-2b-it-Q5_K_M.gguf \\\nllama-api-server.wasm \\\n--prompt-template gemma-instruct \\\n--ctx-size 2048 \\\n--model-name gemma-2-2b\nRun as LlamaEdge command app\nwasmedge --dir .:. \\\n--nn-preload default:GGML:AUTO:gemma-2-2b-it-Q5_K_M.gguf \\\nllama-chat.wasm \\\n--prompt-template gemma-instruct \\\n--ctx-size 2048\nQuantized GGUF Models\nName\nQuant method\nBits\nSize\nUse case\ngemma-2-2b-it-Q2_K.gguf\nQ2_K\n2\n1.23 GB\nsmallest, significant quality loss - not recommended for most purposes\ngemma-2-2b-it-Q3_K_L.gguf\nQ3_K_L\n3\n1.55 GB\nsmall, substantial quality loss\ngemma-2-2b-it-Q3_K_M.gguf\nQ3_K_M\n3\n1.46 GB\nvery small, high quality loss\ngemma-2-2b-it-Q3_K_S.gguf\nQ3_K_S\n3\n1.36 GB\nvery small, high quality loss\ngemma-2-2b-it-Q4_0.gguf\nQ4_0\n4\n1.63 GB\nlegacy; small, very high quality loss - prefer using Q3_K_M\ngemma-2-2b-it-Q4_K_M.gguf\nQ4_K_M\n4\n1.71 GB\nmedium, balanced quality - recommended\ngemma-2-2b-it-Q4_K_S.gguf\nQ4_K_S\n4\n1.64 GB\nsmall, greater quality loss\ngemma-2-2b-it-Q5_0.gguf\nQ5_0\n5\n1.88 GB\nlegacy; medium, balanced quality - prefer using Q4_K_M\ngemma-2-2b-it-Q5_K_M.gguf\nQ5_K_M\n5\n1.92 GB\nlarge, very low quality loss - recommended\ngemma-2-2b-it-Q5_K_S.gguf\nQ5_K_S\n5\n1.88 GB\nlarge, low quality loss - recommended\ngemma-2-2b-it-Q6_K.gguf\nQ6_K\n6\n2.15 GB\nvery large, extremely low quality loss\ngemma-2-2b-it-Q8_0.gguf\nQ8_0\n8\n2.78 GB\nvery large, extremely low quality loss - not recommended\ngemma-2-2b-it-f16.gguf\nf16\n16\n5.24 GB\nQuantized with llama.cpp b3499",
    "Blane187/all_public_uvr_models": "Ultimate Vocal Remover All Public Models\nUltimate Vocal Remover All Public Models\nWelcome to the Ultimate Vocal Remover (UVR) models repository. This repository includes various models aimed at removing vocals from audio tracks using deep learning techniques.\nvist github page herevist docs 'Audio isolating with UVR5' here",
    "mlabonne/Llama-3.1-70B-Instruct-lorablated": "ü¶ô Llama-3.1-70B-Instruct-lorablated\nüîç Applications\n‚ö°Ô∏è Quantization\nüß© Configuration\nü¶ô Llama-3.1-70B-Instruct-lorablated\nü¶ô Llama 3.1 8B Instruct abliterated\nThis is an uncensored version of Llama 3.1 70B Instruct created with abliteration (see this article to know more about it) using @grimjim's recipe.\nMore precisely, this is a LoRA-abliterated (lorablated) model:\nExtraction: We extract a LoRA adapter by comparing two models: a censored Llama 3 and an abliterated Llama 3\nMerge: We merge this new LoRA adapter using task arithmetic to a censored Llama 3.1 to abliterate it.\nI adapted this recipe to Llama 3.1 70B using failspy/Meta-Llama-3-70B-Instruct-abliterated-v3.5 and optimized the LoRA rank.\nThe model is fully uncensored in my tests and maintains a high level of quality. A more rigorous evaluation is still needed to measure the impact of this process on benchmarks.\nSpecial thanks to @grimjim for this technique (see his 8B model) and @FailSpy for his 70B abliterated model. Please follow them if you're interested in abliterated models.\nIn addition, thanks to brev.dev for providing me with compute!\nüîç Applications\nGeneral-purpose, role-play (see feedback from McUH). Use the Llama 3 chat template.\n‚ö°Ô∏è Quantization\nGGUF: https://huggingface.co/mlabonne/Llama-3.1-70B-Instruct-lorablated-GGUF\nBartowski: https://huggingface.co/bartowski/Llama-3.1-70B-Instruct-lorablated-GGUF (with IQ quants)\nüß© Configuration\nThis model was merged using the task arithmetic merge method using ./meta-llama/Meta-Llama-3.1-70B-Instruct + Llama-3-70B-Instruct-abliterated-LORA as a base.\nThe following YAML configuration was used to produce this model:\nbase_model: meta-llama/Meta-Llama-3.1-70B-Instruct+Llama-3-70B-Instruct-abliterated-LORA\ndtype: bfloat16\nmerge_method: task_arithmetic\nparameters:\nnormalize: false\nslices:\n- sources:\n- layer_range: [0, 80]\nmodel: meta-llama/Meta-Llama-3.1-70B-Instruct+Llama-3-70B-Instruct-abliterated-LORA\nparameters:\nweight: 1.0\nYou can reproduce this model using the following commands:\n# Setup\ngit clone https://github.com/arcee-ai/mergekit.git\ncd mergekit && pip install -e .\npip install bitsandbytes\n# Extraction\nmergekit-extract-lora failspy/Meta-Llama-3-70B-Instruct-abliterated-v3.5 meta-llama/Meta-Llama-3-70B-Instruct Llama-3-70B-Instruct-abliterated-LORA --rank=64\n# Merge using previous config\nmergekit-yaml config.yaml Llama-3.1-70B-Instruct-lorablated --allow-crimes --lora-merge-cache=./cache",
    "angusleung100/bad-anatomy-realism-classifier": "Model Card for Bad-Anatomy-Realism-Classifier\nModel Detail\nModel Description\nUses\nOut-of-Scope Use\nBias, Risks, and Limitations\nRecommendations\nHow to Get Started with the Model\nFinetuning\nUsing The Model For Classification\nTraining Details\nTraining and Testing Data\nDataset Image Label Criteria\nBad / Good Anatomy\nRealistic vs. Unrealistic\nCompatible Images For Dataset\nDataset Stats\nEvaluation\nResults\nModel Examination\nModel Card Contact\nModel Card for Bad-Anatomy-Realism-Classifier\nA finetuned Vision Transformer model for classifying AI-generated pictures for bad anatomy and realism.\nThis model is currently a support model for my Youtube series. Feel free to build on top of this.\nModel Detail\nDetecting Bad Anatomy in Realistic AI-Generated Images - Not all Image Generation models generate images with good anatomy. Some might generate the typical \"bad hands\" where the hand might have more than 5 fingers. This model's goaal is to detect such anatomy issues in AI-generated images.\nDetermining True Realism Versus AI Realism - AI-generated images tend to have an issue when attempting to achieve realism, which is the skin and generation style. Compared to a normal post on social media, a High-Definition upscaled AI-generated image can be easily identified by, characteristic such as shiny skin or very bright lighting. Below are some examples of such:\nModel Description\nThis was fine-tuned on the google/vit-base-patch16-224-in21k Vision Transformer (ViT).\nUses\nDetecting whether an image is actually real or is a very well AI-generated image\nDetecting bad anatomy in AI-generated images to trigger a regeneration\nOut-of-Scope Use\nRacism\nIllegal activities where doing illegal things is a crime\nBias, Risks, and Limitations\nThis initial model was trained on images generated on Stable Diffusion v1.5 on the Beautiful Realistic Asians v6 checkpoint by pleasebankai.\nThe dataset for this model was only 134 images, with only 6 being Unrealistic Bad Anatomy. (Additions of dataset details will be placed in the model card in later updates to documentation)\nRecommendations\nRecommendation is to build on the dataset and continue training with more variety of characters to raise performance for images that do not conform to the characteristics of images used in training.\nHow to Get Started with the Model\nFinetuning\nPlease refer to the initial finetune script for this model in the supporting Github Repository here: https://github.com/angusleung100/barc-finetuning-gh\nUsing The Model For Classification\nPlease refer to the Hugging Face documentation example here for Image Classification: https://huggingface.co/docs/transformers/en/tasks/image_classification#inference\nTraining Details\nTraining and Testing Data\nDataset Image Label Criteria\nBad / Good Anatomy\nAny deformed body parts or extra limbs for the character\nBackground does not overly matte (As it can always be removed or changed in post-processing with professional editing software)\nRealistic vs. Unrealistic\nThe criteria is more interesting for determining realism. Since a lot of people like to use filters now, it's actually quite hard to determine what is a good standard for realism. Here is what I narrowed it down to for this model:\nFirst glance reaction - Do I take a closer look and feel skeptical? Or do I know instantly it isn't real.\nLighting - It is easier to sort amateur style images since I can move onto the next criteria first. Some professional images do look AI-generated but are actually heavily edited. But we can definitely base it also off of unnatural lighting\nSkin and hair - If the skin and hair are too shiny (Like the images at the start of the Model Card) or there is not enough detail on an upscaled image. Or there is TOO much detail on an upscaled image.\nPhotography style - This could lead to false positives or false negatives, but if the shot looks like the focal point is weird or just very airbrushed, it could be unrealistic\nOverall it is based on \"gut feeling\" for the sorting. The model also has a goal to be able to replicate \"gut feeling\" and just your underlying feel for the image.\nCompatible Images For Dataset\nSince the default data collator is used and images are primarily from SD 1.5, I am not entirely certain whether images and sizes from different models will break the training, even if the testing pipeline didn't have any problems for the 3 images we used later on.\nHere are a list of models where default image sizes should work:\nStable Diffusion 1.5\nOpenDalle v1.1\nFlux 1\nDall-E 3 on Copilot\nDataset Stats\nNumber Images Per Label\n=======================\nRealistic Bad Anatomy: 6 (4.48%)\nRealistic Good Anatomy: 15 (11.19%)\nUnrealistic Bad Anatomy: 81 (60.45%)\nUnrealistic Good Anatomy: 32 (23.88%)\nTotal Number of Images:  134\nEvaluation\nResults\n***** train metrics *****\nepoch                    =        3.0\ntotal_flos               = 20135801GF\ntrain_loss               =     0.8453\ntrain_runtime            = 0:00:42.83\ntrain_samples_per_second =      6.514\ntrain_steps_per_second   =      0.841\n***** eval metrics *****\nepoch                   =        3.0\neval_accuracy           =     0.6341\neval_f1                 =      0.513\neval_loss               =     0.8219\neval_precision          =      0.464\neval_recall             =     0.6341\neval_runtime            = 0:00:06.95\neval_samples_per_second =      5.893\neval_steps_per_second   =      0.862\nSummary\nThe initial dataset and finetune resulted in a 64.41% accuracy and 51.3% F1 score, which is low but expected for a small amateur dataset.\nHopefully I will have time to further build on the dataset and improve the model's performance in the future.\nThe next steps would be:\nHave more variety of characters and poses\nMore variety of clothing styles and lighting\nDifferent camera styles\nDifferent model generations from different models -> Currently dominated by the SD1.5 BRAV6 and BRAV7 checkpoints\nModel Examination\nYou can view example pipeline inferences and their results on the Initial Finetune notebook\nThe examples are at the bottom of the notebook. You can do ctr+f and search for Test Model With Custom Inputs to reach it faster.\nModel Card Contact\nFeel free to contact me if you have any questions or find me on Github\nTwitter\nGithub",
    "openbmb/MiniCPM-V-2_6": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nNews\nMiniCPM-V 2.6\nEvaluation\nSingle image results on OpenCompass, MME, MMVet, OCRBench, MMMU, MathVista, MMB, AI2D, TextVQA, DocVQA, HallusionBench, Object HalBench:\nMulti-image results on Mantis Eval, BLINK Val, Mathverse mv, Sciverse mv, MIRB:\nVideo results on Video-MME and Video-ChatGPT:\nExamples\nDemo\nUsage\nChat with multiple images\nIn-context few-shot learning\nChat with video\nModel License\nStatement\nInference with llama.cpp\nModel License\nStatement\nInt4 quantized version\nModel License\nStatement\nLicense\nModel License\nStatement\nKey Techniques and Other Multimodal Projects\nCitation\nA GPT-4V Level MLLM for Single Image, Multi Image and Video on Your Phone\nGitHub | Demo\nNews\n[2025.01.14] üî•üî• We open source MiniCPM-o 2.6, with significant performance improvement over MiniCPM-V 2.6, and support real-time speech-to-speech conversation and multimodal live streaming. Try it now.\nMiniCPM-V 2.6\nMiniCPM-V 2.6 is the latest and most capable model in the MiniCPM-V series. The model is built on SigLip-400M and Qwen2-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-Llama3-V 2.5, and introduces new features for multi-image and video understanding. Notable features of MiniCPM-V 2.6 include:\nüî• Leading Performance.\nMiniCPM-V 2.6 achieves an average score of 65.2 on the latest version of OpenCompass, a comprehensive evaluation over 8 popular benchmarks. With only 8B parameters, it surpasses widely used proprietary models like GPT-4o mini, GPT-4V, Gemini 1.5 Pro, and Claude 3.5 Sonnet for single image understanding.\nüñºÔ∏è Multi Image Understanding and In-context Learning. MiniCPM-V 2.6 can also perform conversation and reasoning over multiple images. It achieves state-of-the-art performance on popular multi-image benchmarks such as Mantis-Eval, BLINK, Mathverse mv and Sciverse mv, and also shows promising in-context learning capability.\nüé¨ Video Understanding. MiniCPM-V 2.6 can also accept video inputs, performing conversation and providing dense captions for spatial-temporal information. It outperforms GPT-4V, Claude 3.5 Sonnet and LLaVA-NeXT-Video-34B on Video-MME with/without subtitles.\nüí™ Strong OCR Capability and Others.\nMiniCPM-V 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves state-of-the-art performance on OCRBench, surpassing proprietary models such as GPT-4o, GPT-4V, and Gemini 1.5 Pro.\nBased on the the latest RLAIF-V and VisCPM techniques, it features trustworthy behaviors, with significantly lower hallucination rates than GPT-4o and GPT-4V on Object HalBench, and supports multilingual capabilities on English, Chinese, German, French, Italian, Korean, etc.\nüöÄ Superior Efficiency.\nIn addition to its friendly size, MiniCPM-V 2.6 also shows state-of-the-art token density (i.e., number of pixels encoded into each visual token). It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models. This directly improves the inference speed, first-token latency, memory usage, and power consumption. As a result, MiniCPM-V 2.6 can efficiently support real-time video understanding on end-side devices such as iPad.\nüí´ Easy Usage.\nMiniCPM-V 2.6 can be easily used in various ways: (1) llama.cpp and ollama support for efficient CPU inference on local devices, (2) int4 and GGUF format quantized models in 16 sizes, (3) vLLM support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks, (5) quick local WebUI demo setup with Gradio and (6) online web demo.\nEvaluation\nSingle image results on OpenCompass, MME, MMVet, OCRBench, MMMU, MathVista, MMB, AI2D, TextVQA, DocVQA, HallusionBench, Object HalBench:\n* We evaluate this benchmark using chain-of-thought prompting.\n+ Token Density: number of pixels encoded into each visual token at maximum resolution, i.e., # pixels at maximum resolution / # visual tokens.\nNote: For proprietary models, we calculate token density based on the image encoding charging strategy defined in the official API documentation, which provides an upper-bound estimation.\nMulti-image results on Mantis Eval, BLINK Val, Mathverse mv, Sciverse mv, MIRB:\n* We evaluate the officially released checkpoint by ourselves.\nVideo results on Video-MME and Video-ChatGPT:\nClick to view few-shot results on TextVQA, VizWiz, VQAv2, OK-VQA.\n* denotes zero image shot and two additional text shots following Flamingo.\n+ We evaluate the pretraining ckpt without SFT.\nExamples\nClick to view more cases.\nWe deploy MiniCPM-V 2.6 on end devices. The demo video is the raw screen recording on a iPad Pro without edition.\nDemo\nClick here to try the Demo of MiniCPM-V 2.6.\nUsage\nInference using Huggingface transformers on NVIDIA GPUs. Requirements tested on python 3.10Ôºö\nPillow==10.1.0\ntorch==2.1.2\ntorchvision==0.16.2\ntransformers==4.40.0\nsentencepiece==0.1.99\ndecord\n# test.py\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\nattn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\nimage = Image.open('xx.jpg').convert('RGB')\nquestion = 'What is in the image?'\nmsgs = [{'role': 'user', 'content': [image, question]}]\nres = model.chat(\nimage=None,\nmsgs=msgs,\ntokenizer=tokenizer\n)\nprint(res)\n## if you want to use streaming, please make sure sampling=True and stream=True\n## the model.chat will return a generator\nres = model.chat(\nimage=None,\nmsgs=msgs,\ntokenizer=tokenizer,\nsampling=True,\nstream=True\n)\ngenerated_text = \"\"\nfor new_text in res:\ngenerated_text += new_text\nprint(new_text, flush=True, end='')\nChat with multiple images\nClick to show Python code running MiniCPM-V 2.6 with multiple images input.\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\nattn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\nimage1 = Image.open('image1.jpg').convert('RGB')\nimage2 = Image.open('image2.jpg').convert('RGB')\nquestion = 'Compare image 1 and image 2, tell me about the differences between image 1 and image 2.'\nmsgs = [{'role': 'user', 'content': [image1, image2, question]}]\nanswer = model.chat(\nimage=None,\nmsgs=msgs,\ntokenizer=tokenizer\n)\nprint(answer)\nIn-context few-shot learning\nClick to view Python code running MiniCPM-V 2.6 with few-shot input.\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\nattn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\nquestion = \"production date\"\nimage1 = Image.open('example1.jpg').convert('RGB')\nanswer1 = \"2023.08.04\"\nimage2 = Image.open('example2.jpg').convert('RGB')\nanswer2 = \"2007.04.24\"\nimage_test = Image.open('test.jpg').convert('RGB')\nmsgs = [\n{'role': 'user', 'content': [image1, question]}, {'role': 'assistant', 'content': [answer1]},\n{'role': 'user', 'content': [image2, question]}, {'role': 'assistant', 'content': [answer2]},\n{'role': 'user', 'content': [image_test, question]}\n]\nanswer = model.chat(\nimage=None,\nmsgs=msgs,\ntokenizer=tokenizer\n)\nprint(answer)\nChat with video\nClick to view Python code running MiniCPM-V 2.6 with video input.\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nfrom decord import VideoReader, cpu    # pip install decord\nmodel = AutoModel.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True,\nattn_implementation='sdpa', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6', trust_remote_code=True)\nMAX_NUM_FRAMES=64 # if cuda OOM set a smaller number\ndef encode_video(video_path):\ndef uniform_sample(l, n):\ngap = len(l) / n\nidxs = [int(i * gap + gap / 2) for i in range(n)]\nreturn [l[i] for i in idxs]\nvr = VideoReader(video_path, ctx=cpu(0))\nsample_fps = round(vr.get_avg_fps() / 1)  # FPS\nframe_idx = [i for i in range(0, len(vr), sample_fps)]\nif len(frame_idx) > MAX_NUM_FRAMES:\nframe_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES)\nframes = vr.get_batch(frame_idx).asnumpy()\nframes = [Image.fromarray(v.astype('uint8')) for v in frames]\nprint('num frames:', len(frames))\nreturn frames\nvideo_path =\"video_test.mp4\"\nframes = encode_video(video_path)\nquestion = \"Describe the video\"\nmsgs = [\n{'role': 'user', 'content': frames + [question]},\n]\n# Set decode params for video\nparams={}\nparams[\"use_image_id\"] = False\nparams[\"max_slice_nums\"] = 2 # use 1 if cuda OOM and video resolution >  448*448\nanswer = model.chat(\nimage=None,\nmsgs=msgs,\ntokenizer=tokenizer,\n**params\n)\nprint(answer)\nPlease look at GitHub for more detail about usage.\nInference with llama.cpp\nMiniCPM-V 2.6 can run with llama.cpp. See our fork of llama.cpp for more detail.\nInt4 quantized version\nDownload the int4 quantized version for lower GPU memory (7GB) usage:  MiniCPM-V-2_6-int4.\nLicense\nModel License\nThe code in this repo is released under the Apache-2.0 License.\nThe usage of MiniCPM-V series model weights must strictly follow MiniCPM Model License.md.\nThe models and weights of MiniCPM are completely free for academic research. After filling out a \"questionnaire\" for registration, MiniCPM-V 2.6 weights are also available for free commercial use.\nStatement\nAs an LMM, MiniCPM-V 2.6 generates contents by learning a large mount of multimodal corpora, but it cannot comprehend, express personal opinions or make value judgement. Anything generated by MiniCPM-V 2.6 does not represent the views and positions of the model developers\nWe will not be liable for any problems arising from the use of the MinCPM-V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination or misuse of the model.\nKey Techniques and Other Multimodal Projects\nüëè Welcome to explore key techniques of MiniCPM-V 2.6 and other multimodal projects of our team:\nVisCPM | RLHF-V | LLaVA-UHD  | RLAIF-V\nCitation\nIf you find our work helpful, please consider citing our papers üìù and liking this project ‚ù§Ô∏èÔºÅ\n@article{yao2024minicpm,\ntitle={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},\nauthor={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},\njournal={arXiv preprint arXiv:2408.01800},\nyear={2024}\n}",
    "StableDiffusionVN/Flux": "Youtube:\nColab:\nColab Training Lora:\nComfy Workflow:\n=> Text-img\n=> Img-img\n=> Inpaint\n=> Schnell Text-img\n=> Schnell Text-img-upscale\n=> Lora Tester\n=> Lora Text to img\n=> Lora Text to img Upscale\n=> Schnell Lora Text to img\n=> Union Controlnet\n=> Hyper 8Steps Lora Upscale\nComfy Node:\nComfy NF4 Support: https://github.com/comfyanonymous/ComfyUI_bitsandbytes_NF4\nComfy GGUF Support: https://github.com/city96/ComfyUI-GGUF\nComfy Instant Union Controlnet Support: https://github.com/EeroHeikkinen/ComfyUI-eesahesNodes",
    "nanotron/llama3-8b-infini-attention": "No model card",
    "zai-org/CogVideoX-2b": "CogVideoX-2B\nDemo Show\nModel Introduction\nQuick Start ü§ó\nQuantized Inference\nExplore the Model\nModel License\nCitation\nCogVideoX-2B\nüìÑ ‰∏≠ÊñáÈòÖËØª |\nü§ó Huggingface Space |\nüåê Github  |\nüìú arxiv\nüìç Visit QingYing and API Platform to experience commercial video generation models.\nDemo Show\nVideo Gallery with Captions\nA detailed wooden toy ship with intricately carved masts and sails is seen gliding smoothly over a plush, blue carpet that mimics the waves of the sea. The ship's hull is painted a rich brown, with tiny windows. The carpet, soft and textured, provides a perfect backdrop, resembling an oceanic expanse. Surrounding the ship are various other toys and children's items, hinting at a playful environment. The scene captures the innocence and imagination of childhood, with the toy ship's journey symbolizing endless adventures in a whimsical, indoor setting.\nThe camera follows behind a white vintage SUV with a black roof rack as it speeds up a steep dirt road surrounded by pine trees on a steep mountain slope, dust kicks up from it‚Äôs tires, the sunlight shines on the SUV as it speeds along the dirt road, casting a warm glow over the scene. The dirt road curves gently into the distance, with no other cars or vehicles in sight. The trees on either side of the road are redwoods, with patches of greenery scattered throughout. The car is seen from the rear following the curve with ease, making it seem as if it is on a rugged drive through the rugged terrain. The dirt road itself is surrounded by steep hills and mountains, with a clear blue sky above with wispy clouds.\nA street artist, clad in a worn-out denim jacket and a colorful bandana, stands before a vast concrete wall in the heart, holding a can of spray paint, spray-painting a colorful bird on a mottled wall.\nIn the haunting backdrop of a war-torn city, where ruins and crumbled walls tell a story of devastation, a poignant close-up frames a young girl. Her face is smudged with ash, a silent testament to the chaos around her. Her eyes glistening with a mix of sorrow and resilience, capturing the raw emotion of a world that has lost its innocence to the ravages of conflict.\nModel Introduction\nCogVideoX is an open-source version of the video generation model originating\nfrom QingYing. The table below displays the list of video generation\nmodels we currently offer, along with their foundational information.\nModel Name\nCogVideoX-2B (This Repository)\nCogVideoX-5B\nModel Description\nEntry-level model, balancing compatibility. Low cost for running and secondary development.\nLarger model with higher video generation quality and better visual effects.\nInference Precision\nFP16* (Recommended), BF16, FP32, FP8*, INT8, no support for INT4\nBF16 (Recommended), FP16, FP32, FP8*, INT8, no support for INT4\nSingle GPU VRAM Consumption\nSAT FP16: 18GB diffusers FP16: starting from 4GB*diffusers INT8(torchao): starting from 3.6GB*\nSAT BF16: 26GB diffusers BF16: starting from 5GB*diffusers INT8(torchao): starting from 4.4GB*\nMulti-GPU Inference VRAM Consumption\nFP16: 10GB* using diffusers\nBF16: 15GB* using diffusers\nInference Speed(Step = 50, FP/BF16)\nSingle A100: ~90 secondsSingle H100: ~45 seconds\nSingle A100: ~180 secondsSingle H100: ~90 seconds\nFine-tuning Precision\nFP16\nBF16\nFine-tuning VRAM Consumption (per GPU)\n47 GB (bs=1, LORA) 61 GB (bs=2, LORA) 62GB (bs=1, SFT)\n63 GB (bs=1, LORA) 80 GB (bs=2, LORA) 75GB (bs=1, SFT)\nPrompt Language\nEnglish*\nPrompt Length Limit\n226 Tokens\nVideo Length\n6 Seconds\nFrame Rate\n8 Frames per Second\nVideo Resolution\n720 x 480, no support for other resolutions (including fine-tuning)\nPositional Encoding\n3d_sincos_pos_embed\n3d_rope_pos_embed\nData Explanation\nWhen testing using the diffusers library, all optimizations provided by the diffusers library were enabled. This\nsolution has not been tested for actual VRAM/memory usage on devices other than NVIDIA A100 / H100. Generally,\nthis solution can be adapted to all devices with NVIDIA Ampere architecture and above. If the optimizations are\ndisabled, VRAM usage will increase significantly, with peak VRAM usage being about 3 times higher than the table\nshows. However, speed will increase by 3-4 times. You can selectively disable some optimizations, including:\npipe.enable_model_cpu_offload()\npipe.enable_sequential_cpu_offload()\npipe.vae.enable_slicing()\npipe.vae.enable_tiling()\nWhen performing multi-GPU inference, the enable_model_cpu_offload() optimization needs to be disabled.\nUsing INT8 models will reduce inference speed. This is to ensure that GPUs with lower VRAM can perform inference\nnormally while maintaining minimal video quality loss, though inference speed will decrease significantly.\nThe 2B model is trained with FP16 precision, and the 5B model is trained with BF16 precision. We recommend using\nthe precision the model was trained with for inference.\nPytorchAO and Optimum-quanto can be\nused to quantize the text encoder, Transformer, and VAE modules to reduce CogVideoX's memory requirements. This makes\nit possible to run the model on a free T4 Colab or GPUs with smaller VRAM! It is also worth noting that TorchAO\nquantization is fully compatible with torch.compile, which can significantly improve inference speed. FP8\nprecision must be used on devices with NVIDIA H100 or above, which requires installing\nthe torch, torchao, diffusers, and accelerate Python packages from source. CUDA 12.4 is recommended.\nThe inference speed test also used the above VRAM optimization scheme. Without VRAM optimization, inference speed\nincreases by about 10%. Only the diffusers version of the model supports quantization.\nThe model only supports English input; other languages can be translated into English during refinement by a large\nmodel.\nNote\nUsing SAT  for inference and fine-tuning of SAT version\nmodels. Feel free to visit our GitHub for more information.\nQuick Start ü§ó\nThis model supports deployment using the huggingface diffusers library. You can deploy it by following these steps.\nWe recommend that you visit our GitHub and check out the relevant prompt\noptimizations and conversions to get a better experience.\nInstall the required dependencies\n# diffusers>=0.30.1\n# transformers>=0.44.0\n# accelerate>=0.33.0 (suggest install from source)\n# imageio-ffmpeg>=0.5.1\npip install --upgrade transformers accelerate diffusers imageio-ffmpeg\nRun the code\nimport torch\nfrom diffusers import CogVideoXPipeline\nfrom diffusers.utils import export_to_video\nprompt = \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance.\"\npipe = CogVideoXPipeline.from_pretrained(\n\"THUDM/CogVideoX-2b\",\ntorch_dtype=torch.float16\n)\npipe.enable_model_cpu_offload()\npipe.enable_sequential_cpu_offload()\npipe.vae.enable_slicing()\npipe.vae.enable_tiling()\nvideo = pipe(\nprompt=prompt,\nnum_videos_per_prompt=1,\nnum_inference_steps=50,\nnum_frames=49,\nguidance_scale=6,\ngenerator=torch.Generator(device=\"cuda\").manual_seed(42),\n).frames[0]\nexport_to_video(video, \"output.mp4\", fps=8)\nQuantized Inference\nPytorchAO and Optimum-quanto can be\nused to quantize the Text Encoder, Transformer and VAE modules to lower the memory requirement of CogVideoX. This makes\nit possible to run the model on free-tier T4 Colab or smaller VRAM GPUs as well! It is also worth noting that TorchAO\nquantization is fully compatible with torch.compile, which allows for much faster inference speed.\n# To get started, PytorchAO needs to be installed from the GitHub source and PyTorch Nightly.\n# Source and nightly installation is only required until next release.\nimport torch\nfrom diffusers import AutoencoderKLCogVideoX, CogVideoXTransformer3DModel, CogVideoXPipeline\nfrom diffusers.utils import export_to_video\n+ from transformers import T5EncoderModel\n+ from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n+ quantization = int8_weight_only\n+ text_encoder = T5EncoderModel.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"text_encoder\", torch_dtype=torch.bfloat16)\n+ quantize_(text_encoder, quantization())\n+ transformer = CogVideoXTransformer3DModel.from_pretrained(\"THUDM/CogVideoX-5b\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n+ quantize_(transformer, quantization())\n+ vae = AutoencoderKLCogVideoX.from_pretrained(\"THUDM/CogVideoX-2b\", subfolder=\"vae\", torch_dtype=torch.bfloat16)\n+ quantize_(vae, quantization())\n# Create pipeline and run inference\npipe = CogVideoXPipeline.from_pretrained(\n\"THUDM/CogVideoX-2b\",\n+    text_encoder=text_encoder,\n+    transformer=transformer,\n+    vae=vae,\ntorch_dtype=torch.bfloat16,\n)\npipe.enable_model_cpu_offload()\npipe.vae.enable_tiling()\nprompt = \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance.\"\nvideo = pipe(\nprompt=prompt,\nnum_videos_per_prompt=1,\nnum_inference_steps=50,\nnum_frames=49,\nguidance_scale=6,\ngenerator=torch.Generator(device=\"cuda\").manual_seed(42),\n).frames[0]\nexport_to_video(video, \"output.mp4\", fps=8)\nAdditionally, the models can be serialized and stored in a quantized datatype to save disk space when using PytorchAO.\nFind examples and benchmarks at these links:\ntorchao\nquanto\nExplore the Model\nWelcome to our github, where you will find:\nMore detailed technical details and code explanation.\nOptimization and conversion of prompt words.\nReasoning and fine-tuning of SAT version models, and even pre-release.\nProject update log dynamics, more interactive opportunities.\nCogVideoX toolchain to help you better use the model.\nINT8 model inference code support.\nModel License\nThe CogVideoX-2B model (including its corresponding Transformers module and VAE module) is released under\nthe Apache 2.0 License.\nThe CogVideoX-5B model (Transformers module) is released under\nthe CogVideoX LICENSE.\nCitation\n@article{yang2024cogvideox,\ntitle={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer},\nauthor={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},\njournal={arXiv preprint arXiv:2408.06072},\nyear={2024}\n}",
    "paige-ai/Virchow2": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nThis model and associated code are released under the CC-BY-NC-ND 4.0 license and may only be used for non-commercial, academic research purposes with proper attribution. Any commercial use, sale, or other monetization of the Virchow2 Model and its derivatives, which include models trained on outputs from the Virchow2 Model or datasets created from the Virchow2 Model, is prohibited and requires prior approval. Please note that the primary email used to sign up for your Hugging Face account must match your institutional email to receive approval. By downloading the Virchow2 Model, you attest that all information (affiliation, research use) is correct and up-to-date. Downloading the Virchow2 Model requires prior registration on Hugging Face and agreeing to the terms of use. By downloading the Virchow2 model, you agree not to distribute, publish or reproduce a copy of the Virchow2 Model. If another user within your organization wishes to use the Virchow2 Model, they must register as an individual user and agree to comply with the terms of use. For commercial entity, we would reject by default without receiving purpose statement. Requests from commercial entities would be rejected by default if no statement of use received separately. If you are a commercial entity, please contact the corresponding author of this paper.\nFurther, by downloading the Virchow2 model, you agree you will only use the Virchow2 model for academic research purposes and will not use, or allow others to use, the Virchow2 model to:\nDiagnose, cure, mitigate, treat, or prevent disease or any other conditions, including for Investigational Use Only (‚ÄúIUO‚Äù), Research Use Only (‚ÄúRUO‚Äù), commercial, clinical or other similar use, and including as a substitute for professional medical advice, a healthcare opinion, a diagnosis, treatment, or the clinical judgment of a healthcare professional, as no license or right is granted for any such purposes.\nRe-identify the deidentified data used to develop the Virchow2 Model;\nViolate the law or others‚Äô rights, including to:\na. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content;\nb. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals;\nc. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services;\nd. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices;\ne. Collect, process, disclose, generate, or infer the identity of individuals or the health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws;\nf. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Virchow2 Model or any related materials; and\ng. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system.\nEngage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including the use of the Virchow2 Model as a medical device, clinical support, diagnostic tool, or other technology intended to be used in the diagnosis, cure, mitigation, treatment, or prevention of disease or other conditions, including for Investigational Use Only (‚ÄúIUO‚Äù), Research Use Only (‚ÄúRUO‚Äù), commercial, clinical or similar use; and\nIntentionally deceive or mislead others, including representing that the use of the Virchow2 Model or its outputs is human-generated.\nFurther, you agree that you will appropriately disclose to end users any known dangers of your AI system.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel card for Virchow2\nModel Details\nModel Usage\nRequirements\nLogin\nImage Embeddings\nUse\nDirect use\nDownstream use\nTerms of use\nCitation\nDisclaimer\nModel card for Virchow2\nVirchow2 is a self-supervised vision transformer pretrained using 3.1M whole slide histopathology images. The model can be used as a tile-level feature extractor (frozen or finetuned) to achieve state-of-the-art results for a wide variety of downstream computational pathology use cases.\nModel Details\nDeveloped by: Paige, NYC, USA and Microsoft Research, Cambridge, MA USA\nModel Type: Image feature backbone\nModel Stats:\nParams (M): 632\nImage size: 224 x 224\nModel Architecture:\nArchitecture: ViT-H/14\nPatch size: 14\nLayers: 32\nEmbedding dimension: 1280\nActivation function: SwiGLU\nAttention heads: 16\nLayerScale: true\nRegister tokens: 4\nTraining Details:\nPrecision: Mixed precision (fp16)\nObjective: Modified DINOv2 (https://doi.org/10.48550/arXiv.2304.07193)\nKoLeo regularizer replaced with kernel density estimator\nCrop-and-resize augmentation replaced with extended context translation\nPaper:\nVirchow2: Scaling Self-Supervised Mixed Magnification Models in Pathology https://arxiv.org/pdf/2408.00738\nPretraining Dataset: Internal dataset of 3.1 million whole slide images from Memorial Sloan Kettering Cancer Center, tiles sampled at 2.0, 1.0, 0.5 and 0.25 microns per pixel resolution (5x, 10x, 20x, and 40x magnification).\nLicense: CC-BY-NC-ND-4.0\nModel Usage\nRequirements\nPyTorch (2.0+ recommended)\ntimm (>= 0.9.11 required)\nhuggingface_hub\nLogin\nAfter gaining access to the model here, you will need to login to HuggingFace in the environment you wish to use the model. This can be done from the command line:\nhuggingface-cli login\nor in your Python code:\nfrom huggingface_hub import login\nlogin()\nPlease refer to official HuggingFace documentation for more details.\nImage Embeddings\nimport timm\nimport torch\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\nfrom timm.layers import SwiGLUPacked\nfrom PIL import Image\n# need to specify MLP layer and activation function for proper init\nmodel = timm.create_model(\"hf-hub:paige-ai/Virchow2\", pretrained=True, mlp_layer=SwiGLUPacked, act_layer=torch.nn.SiLU)\nmodel = model.eval()\ntransforms = create_transform(**resolve_data_config(model.pretrained_cfg, model=model))\nimage = Image.open(\"/path/to/your/image.png\")\nimage = transforms(image).unsqueeze(0)  # size: 1 x 3 x 224 x 224\noutput = model(image)  # size: 1 x 261 x 1280\nclass_token = output[:, 0]    # size: 1 x 1280\npatch_tokens = output[:, 5:]  # size: 1 x 256 x 1280, tokens 1-4 are register tokens so we ignore those\n# concatenate class token and average pool of patch tokens\nembedding = torch.cat([class_token, patch_tokens.mean(1)], dim=-1)  # size: 1 x 2560\nWe concatenate the class token and the mean patch token to create the final tile embedding. In more resource constrained settings, one can experiment with using just class token or the mean patch token. For downstream tasks with dense outputs (i.e. segmentation), the 256 x 1280 tensor of patch tokens can be used.\nWe highly recommend running the model on a GPU in mixed precision (fp16) using torch.autocast:\nmodel = model.to(\"cuda\")\nimage = image.to(\"cuda\")\nwith torch.inference_mode(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\noutput = model(image)\nclass_token = output[:, 0]\npatch_tokens = output[:, 5:]\nembedding = torch.cat([class_token, patch_tokens.mean(1)], dim=-1)\n# the model output will be fp32 because the final operation is a LayerNorm that is ran in mixed precision\n# optionally, you can convert the embedding to fp16 for efficiency in downstream use\nembedding = embedding.to(torch.float16)\nUse\nDirect use\nVirchow2 intended to be used as a frozen feature extractor as the foundation for tile-level and whole slide-level classifiers.\nDownstream use\nVirchow2 can be finetuned to adapt to specific tasks and/or datasets.\nTerms of use\nThis model and associated code are released under the CC-BY-NC-ND 4.0 license and may only be used for non-commercial, academic research purposes with proper attribution. Any commercial use, sale, or other monetization of the Virchow2 Model and its derivatives, which include models trained on outputs from the Virchow2 Model or datasets created from the Virchow2 Model, is prohibited and requires prior approval. Please note that the primary email used to sign up for your Hugging Face account must match your institutional email to receive approval. By downloading the Virchow2 Model, you attest that all information (affiliation, research use) is correct and up-to-date. Downloading the Virchow2 Model requires prior registration on Hugging Face and agreeing to the terms of use. By downloading the Virchow2 model, you agree not to distribute, publish or reproduce a copy of the Virchow2 Model. If another user within your organization wishes to use the Virchow2 Model, they must register as an individual user and agree to comply with the terms of use. If you are a commercial entity, please contact the corresponding author.\nFurther, by downloading the Virchow2 model, you agree you will only use the Virchow2 model for academic research purposes and will not use, or allow others to use, the Virchow2 model to:\nDiagnose, cure, mitigate, treat, or prevent disease or any other conditions, including for Investigational Use Only (‚ÄúIUO‚Äù), Research Use Only (‚ÄúRUO‚Äù), commercial, clinical or other similar use, and including as a substitute for professional medical advice, a healthcare opinion, a diagnosis, treatment, or the clinical judgment of a healthcare professional, as no license or right is granted for any such purposes.\nRe-identify the deidentified data used to develop the Virchow2 Model;\nViolate the law or others‚Äô rights, including to:\na. Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content;\nb. Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals;\nc. Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services;\nd. Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices;\ne. Collect, process, disclose, generate, or infer the identity of individuals or the health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws;\nf. Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Virchow2 Model or any related materials; and\ng. Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system.\nEngage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including the use of the Virchow2 Model as a medical device, clinical support, diagnostic tool, or other technology intended to be used in the diagnosis, cure, mitigation, treatment, or prevention of disease or other conditions, including for Investigational Use Only (‚ÄúIUO‚Äù), Research Use Only (‚ÄúRUO‚Äù), commercial, clinical or similar use; and\nIntentionally deceive or mislead others, including representing that the use of the Virchow2 Model or its outputs is human-generated.\nFurther, you agree that you will appropriately disclose to end users any known dangers of your AI system.\nCitation\nPlease cite the following work if you used this model in your research.\nZimmermann, E., Vorontsov, E., Viret, J. et al. Virchow2: Scaling Self-Supervised Mixed Magnification Models in Pathology. arXiv preprint arXiv:2408.00738 (2024).\n@article{zimmermann2024virchow2,\ntitle={Virchow2: Scaling Self-Supervised Mixed Magnification Models in Pathology},\nauthor={Eric Zimmermann and Eugene Vorontsov and Julian Viret and Adam Casson and Michal Zelechowski and George Shaikovski and Neil Tenenholtz and James Hall and Thomas Fuchs and Nicolo Fusi and Siqi Liu and Kristen Severson},\njournal={arXiv preprint arXiv:2408.00738},\nyear={2024},\n}\nDisclaimer\nVirchow2 has been developed for research purposes and is not intended for diagnosis of real patients or projection/prediction of future disease possibilities.\nFairness evaluation cannot be completed due to limitations in the metadata. Underlying biases of the training datasets may not be well characterized and may not be representative of all demographics.",
    "Alibaba-NLP/gte-multilingual-mlm-base": "gte-multilingual-mlm-base\nModel list\nTraining Details\nTraining Data\nTraining Procedure\nEvaluation\nCitation\ngte-multilingual-mlm-base\nWe introduce mGTE series, new generalized text encoder, embedding and reranking models that support 75 languages and the context length of up to 8192.\nThe models are built upon the transformer++ encoder backbone (BERT + RoPE + GLU, code refer to Alibaba-NLP/new-impl)\nas well as the vocabulary of XLM-R.\nThis text encoder (mGTE-MLM-8192 in our paper) outperforms the same-sized previous state-of-the-art XLM-R-base\nin both GLUE and XTREME-R.\nDeveloped by: Institute for Intelligent Computing, Alibaba Group\nModel type: Text Encoder\nPaper: mGTE: Generalized Long-Context Text Representation and Reranking\nModels for Multilingual Text Retrieval.\nModel list\nModels\nLanguage\nModel Size\nMax Seq. Length\nGLUE\nXTREME-R\ngte-multilingual-mlm-base\nMultiple\n306M\n8192\n83.47\n64.44\ngte-en-mlm-base\nEnglish\n-\n8192\n85.61\n-\ngte-en-mlm-large\nEnglish\n-\n8192\n87.58\n-\nTraining Details\nTraining Data\nMasked language modeling (MLM): c4-en, mc4, skypile, Wikipedia, CulturaX, etc (refer to paper appendix A.1)\nTraining Procedure\nTo enable the backbone model to support a context length of 8192, we adopted a multi-stage training strategy.\nThe model first undergoes preliminary MLM pre-training on shorter lengths.\nAnd then, we resample the data, reducing the proportion of short texts, and continue the MLM pre-training.\nThe entire training process is as follows:\nMLM-2048: lr 2e-4, mlm_probability 0.3, batch_size 8192, num_steps 250k, rope_base 10000\nMLM-8192: lr 5e-5, mlm_probability 0.3, batch_size 2048, num_steps 30k, rope_base 160000\nEvaluation\nModels\nLanguage\nModel Size\nMax Seq. Length\nGLUE\nXTREME-R\ngte-multilingual-mlm-base\nMultiple\n306M\n8192\n83.47\n64.44\ngte-en-mlm-base\nEnglish\n-\n8192\n85.61\n-\ngte-en-mlm-large\nEnglish\n-\n8192\n87.58\n-\nMosaicBERT-base\nEnglish\n137M\n128\n85.4\n-\nMosaicBERT-base-2048\nEnglish\n137M\n2048\n85\n-\nJinaBERT-base\nEnglish\n137M\n512\n85\n-\nnomic-bert-2048\nEnglish\n137M\n2048\n84\n-\nMosaicBERT-large\nEnglish\n434M\n128\n86.1\n-\nJinaBERT-large\nEnglish\n434M\n512\n83.7\n-\nXLM-R-base\nMultiple\n279M\n512\n80.44\n62.02\nRoBERTa-base\nEnglish\n125M\n512\n86.4\n-\nRoBERTa-large\nEnglish\n355M\n512\n88.9\n-\nCitation\nIf you find our paper or models helpful, please consider citing them as follows:\n@misc{zhang2024mgtegeneralizedlongcontexttext,\ntitle={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},\nauthor={Xin Zhang and Yanzhao Zhang and Dingkun Long and Wen Xie and Ziqi Dai and Jialong Tang and Huan Lin and Baosong Yang and Pengjun Xie and Fei Huang and Meishan Zhang and Wenjie Li and Min Zhang},\nyear={2024},\neprint={2407.19669},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2407.19669},\n}",
    "utter-project/EuroLLM-1.7B": "Model Card for EuroLLM-1.7B\nModel Details\nModel Description\nRun the model\nResults\nMachine Translation\nGeneral Benchmarks\nBias, Risks, and Limitations\nPaper\nModel updated on September 24\nModel Card for EuroLLM-1.7B\nThis is the model card for the first pre-trained model of the EuroLLM series: EuroLLM-1.7B. You can also check the instruction tuned version: EuroLLM-1.7B-Instruct.\nDeveloped by: Unbabel, Instituto Superior T√©cnico, Instituto de Telecomunica√ß√µes, University of Edinburgh, Aveni, University of Paris-Saclay, University of Amsterdam, Naver Labs, Sorbonne Universit√©.\nFunded by: European Union.\nModel type: A 1.7B parameter multilingual transfomer LLM.\nLanguage(s) (NLP): Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Greek, Hungarian, Irish, Italian, Latvian, Lithuanian, Maltese, Polish, Portuguese, Romanian, Slovak, Slovenian, Spanish, Swedish, Arabic, Catalan, Chinese, Galician, Hindi, Japanese, Korean, Norwegian, Russian, Turkish, and Ukrainian.\nLicense: Apache License 2.0.\nModel Details\nThe EuroLLM project has the goal of creating a suite of LLMs capable of understanding and generating text in all European Union languages as well as some additional relevant languages.\nEuroLLM-1.7B is a 1.7B parameter model trained on 4 trillion tokens divided across the considered languages and several data sources: Web data, parallel data (en-xx and xx-en), and high-quality datasets.\nEuroLLM-1.7B-Instruct was further instruction tuned on EuroBlocks, an instruction tuning dataset with focus on general instruction-following and machine translation.\nModel Description\nEuroLLM uses a standard, dense Transformer architecture:\nWe use grouped query attention (GQA) with 8 key-value heads, since it has been shown to increase speed at inference time while maintaining downstream performance.\nWe perform pre-layer normalization, since it improves the training stability, and use the RMSNorm, which is faster.\nWe use the SwiGLU activation function, since it has been shown to lead to good results on downstream tasks.\nWe use rotary positional embeddings (RoPE) in every layer, since these have been shown to lead to good performances while allowing the extension of the context length.\nFor pre-training, we use 256 Nvidia H100 GPUs of the Marenostrum 5 supercomputer, training the model with a constant batch size of 3,072 sequences, which corresponds to approximately 12 million tokens, using the Adam optimizer, and BF16 precision.\nHere is a summary of the model hyper-parameters:\nSequence Length\n4,096\nNumber of Layers\n24\nEmbedding Size\n2,048\nFFN Hidden Size\n5,632\nNumber of Heads\n16\nNumber of KV Heads (GQA)\n8\nActivation Function\nSwiGLU\nPosition Encodings\nRoPE (\\Theta=10,000)\nLayer Norm\nRMSNorm\nTied Embeddings\nNo\nEmbedding Parameters\n0.262B\nLM Head Parameters\n0.262B\nNon-embedding Parameters\n1.133B\nTotal Parameters\n1.657B\nRun the model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"utter-project/EuroLLM-1.7B\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\ntext = \"English: My name is EuroLLM. Portuguese:\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nResults\nMachine Translation\nWe evaluate EuroLLM-1.7B-Instruct on several machine translation benchmarks: FLORES-200, WMT-23, and WMT-24 comparing it with Gemma-2B and Gemma-7B (also instruction tuned on EuroBlocks).\nThe results show that EuroLLM-1.7B is substantially better than Gemma-2B in Machine Translation and competitive with Gemma-7B.\nFlores-200\nModel\nAVG\nAVG en-xx\nAVG xx-en\nen-ar\nen-bg\nen-ca\nen-cs\nen-da\nen-de\nen-el\nen-es-latam\nen-et\nen-fi\nen-fr\nen-ga\nen-gl\nen-hi\nen-hr\nen-hu\nen-it\nen-ja\nen-ko\nen-lt\nen-lv\nen-mt\nen-nl\nen-no\nen-pl\nen-pt-br\nen-ro\nen-ru\nen-sk\nen-sl\nen-sv\nen-tr\nen-uk\nen-zh-cn\nar-en\nbg-en\nca-en\ncs-en\nda-en\nde-en\nel-en\nes-latam-en\net-en\nfi-en\nfr-en\nga-en\ngl-en\nhi-en\nhr-en\nhu-en\nit-en\nja-en\nko-en\nlt-en\nlv-en\nmt-en\nnl-en\nno-en\npl-en\npt-br-en\nro-en\nru-en\nsk-en\nsl-en\nsv-en\ntr-en\nuk-en\nzh-cn-en\nEuroLLM-1.7B-Instruct\n86.89\n86.53\n87.25\n85.17\n89.42\n84.72\n89.13\n89.47\n86.90\n87.60\n86.29\n88.95\n89.40\n87.69\n74.89\n86.41\n76.92\n84.79\n86.78\n88.17\n89.76\n87.70\n87.27\n87.62\n67.84\n87.10\n90.00\n88.18\n89.29\n89.49\n88.32\n88.18\n86.85\n90.00\n87.31\n87.89\n86.60\n86.34\n87.45\n87.57\n87.95\n89.72\n88.80\n87.00\n86.77\n88.34\n89.09\n88.95\n82.69\n87.80\n88.37\n86.71\n87.20\n87.81\n86.79\n86.79\n85.62\n86.48\n81.10\n86.97\n90.25\n85.75\n89.20\n88.88\n86.00\n87.38\n86.76\n89.61\n87.94\nGemma-2B-EuroBlocks\n81.59\n78.97\n84.21\n76.68\n82.73\n83.14\n81.63\n84.63\n83.15\n79.42\n84.05\n72.58\n79.73\n84.97\n40.50\n82.13\n67.79\n80.53\n78.36\n84.90\n87.43\n82.98\n72.29\n68.68\n58.55\n83.13\n86.15\n82.78\n86.79\n83.14\n84.61\n78.18\n75.37\n80.89\n78.38\n84.38\n84.35\n83.88\n85.77\n86.85\n86.31\n88.24\n88.12\n84.79\n84.90\n82.51\n86.32\n88.29\n54.78\n86.53\n85.83\n85.41\n85.18\n86.77\n85.78\n84.99\n81.65\n81.78\n67.27\n85.92\n89.07\n84.14\n88.07\n87.17\n85.23\n85.09\n83.95\n87.57\n84.77\nGemma-7B-EuroBlocks\n85.27\n83.90\n86.64\n86.38\n87.87\n85.74\n84.25\n85.69\n81.49\n85.52\n86.93\n62.83\n84.96\n75.34\n84.93\n83.91\n86.92\n88.19\n86.11\n81.73\n80.55\n66.85\n85.31\n89.36\n85.87\n88.62\n88.06\n86.67\n84.79\n82.71\n86.45\n85.19\n86.67\n85.77\n86.36\n87.21\n88.09\n87.17\n89.40\n88.26\n86.74\n86.73\n87.25\n88.87\n88.81\n72.45\n87.62\n87.86\n87.08\n87.01\n87.58\n86.92\n86.70\n85.10\n85.74\n77.81\n86.83\n90.40\n85.41\n89.04\n88.77\n86.13\n86.67\n86.32\n89.27\n87.92\nWMT-23\nModel\nAVG\nAVG en-xx\nAVG xx-en\nAVG xx-xx\nen-de\nen-cs\nen-uk\nen-ru\nen-zh-cn\nde-en\nuk-en\nru-en\nzh-cn-en\ncs-uk\nEuroLLM-1.7B-Instruct\n82.91\n83.20\n81.77\n86.82\n81.56\n85.23\n81.30\n82.47\n83.61\n85.03\n84.06\n85.25\n81.31\n78.83\nGemma-2B-EuroBlocks\n79.96\n79.01\n80.86\n81.15\n76.82\n76.05\n77.92\n78.98\n81.58\n82.73\n82.71\n83.99\n80.35\n78.27\nGemma-7B-EuroBlocks\n82.76\n82.26\n82.70\n85.98\n81.37\n82.42\n81.54\n82.18\n82.90\n83.17\n84.29\n85.70\n82.46\n79.73\nWMT-24\nModel\nAVG\nAVG en-xx\nAVG xx-xx\nen-de\nen-es-latam\nen-cs\nen-ru\nen-uk\nen-ja\nen-zh-cn\nen-hi\ncs-uk\nja-zh-cn\nEuroLLM-1.7B-Instruct\n79.32\n79.32\n79.34\n79.42\n80.67\n80.55\n78.65\n80.12\n82.96\n80.60\n71.59\n83.48\n75.20\nGemma-2B-EuroBlocks\n74.72\n74.41\n75.97\n74.93\n78.81\n70.54\n74.90\n75.84\n79.48\n78.06\n62.70\n79.87\n72.07\nGemma-7B-EuroBlocks\n78.67\n78.34\n80.00\n78.88\n80.47\n78.55\n78.55\n80.12\n80.55\n78.90\n70.71\n84.33\n75.66\nGeneral Benchmarks\nWe also compare EuroLLM-1.7B with TinyLlama-v1.1 and Gemma-2B on 3 general benchmarks: Arc Challenge and Hellaswag.\nFor the non-english languages we use the Okapi datasets.\nResults show that EuroLLM-1.7B is superior to TinyLlama-v1.1 and similar to Gemma-2B on Hellaswag but worse on Arc Challenge. This can be due to the lower number of parameters of EuroLLM-1.7B (1.133B non-embedding parameters against 1.981B).\nArc Challenge\nModel\nAverage\nEnglish\nGerman\nSpanish\nFrench\nItalian\nPortuguese\nChinese\nRussian\nDutch\nArabic\nSwedish\nHindi\nHungarian\nRomanian\nUkrainian\nDanish\nCatalan\nEuroLLM-1.7B\n0.3496\n0.4061\n0.3464\n0.3684\n0.3627\n0.3738\n0.3855\n0.3521\n0.3208\n0.3507\n0.3045\n0.3605\n0.2928\n0.3271\n0.3488\n0.3516\n0.3513\n0.3396\nTinyLlama-v1.1\n0.2650\n0.3712\n0.2524\n0.2795\n0.2883\n0.2652\n0.2906\n0.2410\n0.2669\n0.2404\n0.2310\n0.2687\n0.2354\n0.2449\n0.2476\n0.2524\n0.2494\n0.2796\nGemma-2B\n0.3617\n0.4846\n0.3755\n0.3940\n0.4080\n0.3687\n0.3872\n0.3726\n0.3456\n0.3328\n0.3122\n0.3519\n0.2851\n0.3039\n0.3590\n0.3601\n0.3565\n0.3516\nHellaswag\nModel\nAverage\nEnglish\nGerman\nSpanish\nFrench\nItalian\nPortuguese\nRussian\nDutch\nArabic\nSwedish\nHindi\nHungarian\nRomanian\nUkrainian\nDanish\nCatalan\nEuroLLM-1.7B\n0.4744\n0.4760\n0.6057\n0.4793\n0.5337\n0.5298\n0.5085\n0.5224\n0.4654\n0.4949\n0.4104\n0.4800\n0.3655\n0.4097\n0.4606\n0.436\n0.4702\nTinyLlama-v1.1\n0.3674\n0.6248\n0.3650\n0.4137\n0.4010\n0.3780\n0.3892\n0.3494\n0.3588\n0.2880\n0.3561\n0.2841\n0.3073\n0.3267\n0.3349\n0.3408\n0.3613\nGemma-2B\n0.4666\n0.7165\n0.4756\n0.5414\n0.5180\n0.4841\n0.5081\n0.4664\n0.4655\n0.3868\n0.4383\n0.3413\n0.3710\n0.4316\n0.4291\n0.4471\n0.4448\nBias, Risks, and Limitations\nEuroLLM-1.7B has not been aligned to human preferences, so the model may generate problematic outputs (e.g., hallucinations, harmful content, or false statements).\nPaper\nPaper: EuroLLM: Multilingual Language Models for Europe",
    "utter-project/EuroLLM-1.7B-Instruct": "Model Card for EuroLLM-1.7B-Instruct\nModel Details\nModel Description\nRun the model\nResults\nMachine Translation\nGeneral Benchmarks\nBias, Risks, and Limitations\nPaper\nModel updated on September 24\nModel Card for EuroLLM-1.7B-Instruct\nThis is the model card for the first instruction tuned model of the EuroLLM series: EuroLLM-1.7B-Instruct. You can also check the pre-trained version: EuroLLM-1.7B.\nDeveloped by: Unbabel, Instituto Superior T√©cnico, Instituto de Telecomunica√ß√µes, University of Edinburgh, Aveni, University of Paris-Saclay, University of Amsterdam, Naver Labs, Sorbonne Universit√©.\nFunded by: European Union.\nModel type: A 1.7B parameter instruction tuned multilingual transfomer LLM.\nLanguage(s) (NLP): Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Greek, Hungarian, Irish, Italian, Latvian, Lithuanian, Maltese, Polish, Portuguese, Romanian, Slovak, Slovenian, Spanish, Swedish, Arabic, Catalan, Chinese, Galician, Hindi, Japanese, Korean, Norwegian, Russian, Turkish, and Ukrainian.\nLicense: Apache License 2.0.\nModel Details\nThe EuroLLM project has the goal of creating a suite of LLMs capable of understanding and generating text in all European Union languages as well as some additional relevant languages.\nEuroLLM-1.7B is a 1.7B parameter model trained on 4 trillion tokens divided across the considered languages and several data sources: Web data, parallel data (en-xx and xx-en), and high-quality datasets.\nEuroLLM-1.7B-Instruct was further instruction tuned on EuroBlocks, an instruction tuning dataset with focus on general instruction-following and machine translation.\nModel Description\nEuroLLM uses a standard, dense Transformer architecture:\nWe use grouped query attention (GQA) with 8 key-value heads, since it has been shown to increase speed at inference time while maintaining downstream performance.\nWe perform pre-layer normalization, since it improves the training stability, and use the RMSNorm, which is faster.\nWe use the SwiGLU activation function, since it has been shown to lead to good results on downstream tasks.\nWe use rotary positional embeddings (RoPE) in every layer, since these have been shown to lead to good performances while allowing the extension of the context length.\nFor pre-training, we use 256 Nvidia H100 GPUs of the Marenostrum 5 supercomputer, training the model with a constant batch size of 3,072 sequences, which corresponds to approximately 12 million tokens, using the Adam optimizer, and BF16 precision.\nHere is a summary of the model hyper-parameters:\nSequence Length\n4,096\nNumber of Layers\n24\nEmbedding Size\n2,048\nFFN Hidden Size\n5,632\nNumber of Heads\n16\nNumber of KV Heads (GQA)\n8\nActivation Function\nSwiGLU\nPosition Encodings\nRoPE (\\Theta=10,000)\nLayer Norm\nRMSNorm\nTied Embeddings\nNo\nEmbedding Parameters\n0.262B\nLM Head Parameters\n0.262B\nNon-embedding Parameters\n1.133B\nTotal Parameters\n1.657B\nRun the model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_id = \"utter-project/EuroLLM-1.7B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\ntext = '<|im_start|>system\\n<|im_end|>\\n<|im_start|>user\\nTranslate the following English source text to Portuguese:\\nEnglish: I am a language model for european languages. \\nPortuguese: <|im_end|>\\n<|im_start|>assistant\\n'\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nResults\nMachine Translation\nWe evaluate EuroLLM-1.7B-Instruct on several machine translation benchmarks: FLORES-200, WMT-23, and WMT-24 comparing it with Gemma-2B and Gemma-7B (also instruction tuned on EuroBlocks).\nThe results show that EuroLLM-1.7B is substantially better than Gemma-2B in Machine Translation and competitive with Gemma-7B.\nFlores-200\nModel\nAVG\nAVG en-xx\nAVG xx-en\nen-ar\nen-bg\nen-ca\nen-cs\nen-da\nen-de\nen-el\nen-es-latam\nen-et\nen-fi\nen-fr\nen-ga\nen-gl\nen-hi\nen-hr\nen-hu\nen-it\nen-ja\nen-ko\nen-lt\nen-lv\nen-mt\nen-nl\nen-no\nen-pl\nen-pt-br\nen-ro\nen-ru\nen-sk\nen-sl\nen-sv\nen-tr\nen-uk\nen-zh-cn\nar-en\nbg-en\nca-en\ncs-en\nda-en\nde-en\nel-en\nes-latam-en\net-en\nfi-en\nfr-en\nga-en\ngl-en\nhi-en\nhr-en\nhu-en\nit-en\nja-en\nko-en\nlt-en\nlv-en\nmt-en\nnl-en\nno-en\npl-en\npt-br-en\nro-en\nru-en\nsk-en\nsl-en\nsv-en\ntr-en\nuk-en\nzh-cn-en\nEuroLLM-1.7B-Instruct\n86.89\n86.53\n87.25\n85.17\n89.42\n84.72\n89.13\n89.47\n86.90\n87.60\n86.29\n88.95\n89.40\n87.69\n74.89\n86.41\n76.92\n84.79\n86.78\n88.17\n89.76\n87.70\n87.27\n87.62\n67.84\n87.10\n90.00\n88.18\n89.29\n89.49\n88.32\n88.18\n86.85\n90.00\n87.31\n87.89\n86.60\n86.34\n87.45\n87.57\n87.95\n89.72\n88.80\n87.00\n86.77\n88.34\n89.09\n88.95\n82.69\n87.80\n88.37\n86.71\n87.20\n87.81\n86.79\n86.79\n85.62\n86.48\n81.10\n86.97\n90.25\n85.75\n89.20\n88.88\n86.00\n87.38\n86.76\n89.61\n87.94\nGemma-2B-EuroBlocks\n81.59\n78.97\n84.21\n76.68\n82.73\n83.14\n81.63\n84.63\n83.15\n79.42\n84.05\n72.58\n79.73\n84.97\n40.50\n82.13\n67.79\n80.53\n78.36\n84.90\n87.43\n82.98\n72.29\n68.68\n58.55\n83.13\n86.15\n82.78\n86.79\n83.14\n84.61\n78.18\n75.37\n80.89\n78.38\n84.38\n84.35\n83.88\n85.77\n86.85\n86.31\n88.24\n88.12\n84.79\n84.90\n82.51\n86.32\n88.29\n54.78\n86.53\n85.83\n85.41\n85.18\n86.77\n85.78\n84.99\n81.65\n81.78\n67.27\n85.92\n89.07\n84.14\n88.07\n87.17\n85.23\n85.09\n83.95\n87.57\n84.77\nGemma-7B-EuroBlocks\n85.27\n83.90\n86.64\n86.38\n87.87\n85.74\n84.25\n85.69\n81.49\n85.52\n86.93\n62.83\n84.96\n75.34\n84.93\n83.91\n86.92\n88.19\n86.11\n81.73\n80.55\n66.85\n85.31\n89.36\n85.87\n88.62\n88.06\n86.67\n84.79\n82.71\n86.45\n85.19\n86.67\n85.77\n86.36\n87.21\n88.09\n87.17\n89.40\n88.26\n86.74\n86.73\n87.25\n88.87\n88.81\n72.45\n87.62\n87.86\n87.08\n87.01\n87.58\n86.92\n86.70\n85.10\n85.74\n77.81\n86.83\n90.40\n85.41\n89.04\n88.77\n86.13\n86.67\n86.32\n89.27\n87.92\nWMT-23\nModel\nAVG\nAVG en-xx\nAVG xx-en\nAVG xx-xx\nen-de\nen-cs\nen-uk\nen-ru\nen-zh-cn\nde-en\nuk-en\nru-en\nzh-cn-en\ncs-uk\nEuroLLM-1.7B-Instruct\n82.91\n83.20\n81.77\n86.82\n81.56\n85.23\n81.30\n82.47\n83.61\n85.03\n84.06\n85.25\n81.31\n78.83\nGemma-2B-EuroBlocks\n79.96\n79.01\n80.86\n81.15\n76.82\n76.05\n77.92\n78.98\n81.58\n82.73\n82.71\n83.99\n80.35\n78.27\nGemma-7B-EuroBlocks\n82.76\n82.26\n82.70\n85.98\n81.37\n82.42\n81.54\n82.18\n82.90\n83.17\n84.29\n85.70\n82.46\n79.73\nWMT-24\nModel\nAVG\nAVG en-xx\nAVG xx-xx\nen-de\nen-es-latam\nen-cs\nen-ru\nen-uk\nen-ja\nen-zh-cn\nen-hi\ncs-uk\nja-zh-cn\nEuroLLM-1.7B-Instruct\n79.32\n79.32\n79.34\n79.42\n80.67\n80.55\n78.65\n80.12\n82.96\n80.60\n71.59\n83.48\n75.20\nGemma-2B-EuroBlocks\n74.72\n74.41\n75.97\n74.93\n78.81\n70.54\n74.90\n75.84\n79.48\n78.06\n62.70\n79.87\n72.07\nGemma-7B-EuroBlocks\n78.67\n78.34\n80.00\n78.88\n80.47\n78.55\n78.55\n80.12\n80.55\n78.90\n70.71\n84.33\n75.66\nGeneral Benchmarks\nWe also compare EuroLLM-1.7B with TinyLlama-v1.1 and Gemma-2B on 3 general benchmarks: Arc Challenge and Hellaswag.\nFor the non-english languages we use the Okapi datasets.\nResults show that EuroLLM-1.7B is superior to TinyLlama-v1.1 and similar to Gemma-2B on Hellaswag but worse on Arc Challenge. This can be due to the lower number of parameters of EuroLLM-1.7B (1.133B non-embedding parameters against 1.981B).\nArc Challenge\nModel\nAverage\nEnglish\nGerman\nSpanish\nFrench\nItalian\nPortuguese\nChinese\nRussian\nDutch\nArabic\nSwedish\nHindi\nHungarian\nRomanian\nUkrainian\nDanish\nCatalan\nEuroLLM-1.7B\n0.3496\n0.4061\n0.3464\n0.3684\n0.3627\n0.3738\n0.3855\n0.3521\n0.3208\n0.3507\n0.3045\n0.3605\n0.2928\n0.3271\n0.3488\n0.3516\n0.3513\n0.3396\nTinyLlama-v1.1\n0.2650\n0.3712\n0.2524\n0.2795\n0.2883\n0.2652\n0.2906\n0.2410\n0.2669\n0.2404\n0.2310\n0.2687\n0.2354\n0.2449\n0.2476\n0.2524\n0.2494\n0.2796\nGemma-2B\n0.3617\n0.4846\n0.3755\n0.3940\n0.4080\n0.3687\n0.3872\n0.3726\n0.3456\n0.3328\n0.3122\n0.3519\n0.2851\n0.3039\n0.3590\n0.3601\n0.3565\n0.3516\nHellaswag\nModel\nAverage\nEnglish\nGerman\nSpanish\nFrench\nItalian\nPortuguese\nRussian\nDutch\nArabic\nSwedish\nHindi\nHungarian\nRomanian\nUkrainian\nDanish\nCatalan\nEuroLLM-1.7B\n0.4744\n0.4760\n0.6057\n0.4793\n0.5337\n0.5298\n0.5085\n0.5224\n0.4654\n0.4949\n0.4104\n0.4800\n0.3655\n0.4097\n0.4606\n0.436\n0.4702\nTinyLlama-v1.1\n0.3674\n0.6248\n0.3650\n0.4137\n0.4010\n0.3780\n0.3892\n0.3494\n0.3588\n0.2880\n0.3561\n0.2841\n0.3073\n0.3267\n0.3349\n0.3408\n0.3613\nGemma-2B\n0.4666\n0.7165\n0.4756\n0.5414\n0.5180\n0.4841\n0.5081\n0.4664\n0.4655\n0.3868\n0.4383\n0.3413\n0.3710\n0.4316\n0.4291\n0.4471\n0.4448\nBias, Risks, and Limitations\nEuroLLM-1.7B-Instruct has not been aligned to human preferences, so the model may generate problematic outputs (e.g., hallucinations, harmful content, or false statements).\nPaper\nPaper: EuroLLM: Multilingual Language Models for Europe",
    "pints-ai/1.5-Pints-16K-v0.1": "1.5-Pints -- A model pretrained in 9 days by using high quality data\nHow to use\nDescription\nResults\nTechnical Specifications\nUses\nTraining Data\nTraining Procedure\nTraining Hyperparameters\nCitation\nLegal Warning\n1.5-Pints -- A model pretrained in 9 days by using high quality data\nJoin us at Discord: https://discord.com/invite/RSHk22Z29j\nHow to use\nInstall dependencies\npip install transformers\n# Omit `flash-attn` if not supported by your hardware\npip install flash-attn --no-build-isolation\nUsage\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = \"cuda\" # the device to load the model onto\n# INITIALIZE the model and the tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\n\"pints-ai/1.5-Pints-2k-v0.1\",\ndevice_map=device,\nattn_implementation=\"flash_attention_2\" # can be omitted if not supported\n)\ntokenizer = AutoTokenizer.from_pretrained(\"pints-ai/1.5-Pints-2k-v0.1\")\n# PREPARE and tokenize the prompt\nprompt = \"Predict what life will be like 100 years from now.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are an AI assistant that follows instruction extremely well. Help as much as you can.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\ninput = tokenizer([text], return_tensors=\"pt\").to(device)\n# GENERATE the response\ngenerated_ids = model.generate(\ninput.input_ids,\nmax_new_tokens=512\n)\n# DECODE the response\ninput_length = len(input.input_ids[0])\n# Remove the input and decode only the output\nresponse = tokenizer.decode(generated_ids[0][input_length:])\nprint(response)\nCompute Infrastructure\nThis model can be served with a GPU containing at least 8GB of VRAM.\nDescription\n1.5 Pints is a Large Language Model that significantly advances the efficiency of LLM training by emphasizing data quality over quantity. Our pre-training corpus is a meticulously curated dataset of 57 billion tokens, thus making pre-training more accessible and environmentally-friendly.\nResults\nMTBench\nMTBench is a popular evaluation harness that uses strong LLMs like GPT-4 to act as judges and assess the quality of the models' responses.\nModel\nScore\nParameter Size\nPretrain Tokens\nmeta-llama/Llama-2-7b-chat-hf\n6.27\n7B\n2T\nmicrosoft/phi-2\n5.83\n2.7B\n1.4T\ngoogle/gemma-2b-it\n5.44\n2B\n3T\nstabilityai/stablelm-2-1_6b-chat\n4.7\n1.6B\n2T\n1.5-Pints-2K\n3.73\n1.57B\n0.115T\nTinyLlama/TinyLlama-1.1B-Chat-v1.0\n3.72\n1.1B\n3T\n1.5-Pints-16K\n3.40\n1.57B\n0.115T\napple/OpenELM-1_1B-Instruct\n3.34\n1B\n1.8T\nmicrosoft/phi-1_5\n3.33\n1.3B\n0.15T\ndatabricks/dolly-v2-3b\n2.33\n3B\n0.3T\nEleutherAI/pythia-2.8b\n1.81\n2.8B\n0.3T\ntiiuae/falcon-rw-1b\n1.18\n1B\n0.35T\nThe 2K context window version of 1.5-Pints can be found here.\nTechnical Specifications\nArchitecture\nLlama 2 Autoregressive Model with 16K Context Window and Mistral tokenizer. The model uses Float32 precision.\nParameters\nVocab Size\nEmbedding Size\nContext Length\nLayers\nHeads\nQuery Groups\nIntermediate Hidden Size\n1,565,886,464\n32,064\n2,048\n16,384\n24\n32\n4\n8,192\nContext Lengths\n1.5-Pints comes in 2 context lengths - 16k (16,384) and 2k (2,048).\nPrompt template\nThis model has been finetuned and preference-optimized using the ChatML template.\n<|im_start|>system\n{SYSTEM_PROMPT}<|im_end|>\n<|im_start|>user\n{PROMPT}<|im_end|>\n<|im_start|>assistant\nUses\nDirect Use\nThis model is meant to be an efficient and fine-tunable helpful assistant. It is designed to excel in user assistance and reasoning, and rely less on internal knowledge and factuals. Thus, for knowledge retrieval purposes, it should be used with Retrieval Augmented Generation.\nDownstream Use\nGiven the size of this model, it is possible to launch multiple instances of it for use in agentic context without breaking the compute bank.\nRecommendations\nIt is recommended to finetune this model for domain adaption, and use it for a specialized tasks.\nTo reap full performance, use a repetition penalty of 1.3 rather than 1.\nTraining Data\nPre-Train Data\nDataset: pints-ai/Expository-Prose-V1\nFine-Tune Data\nCorpora:\nHuggingFaceH4/ultrachat\nOpen-Orca/SlimOrca-Dedup\nmeta-math/MetaMathQA\nHuggingFaceH4/deita-10k-v0-sft\nWizardLM/WizardLM_evol_instruct_V2_196k\ntogethercomputer/llama-instruct\nLDJnr/Capybara\nDPO Data\nDataset: HuggingFaceH4/ultrafeedback_binarized\nTraining Procedure\nBoth Pre-Train and Finetuning used our fork of the LitGPT Framework. For DPO, we used the methods set out in The Alignment Handbook. More details can be found in our paper.\nTraining Hyperparameters\nPre-Train\nHyperparameter\nValue\nOptimizer\nAdamW(Beta1=0.9, Beta2=0.95)\nLearning Rate Scheduler\nCosine\nMax Learning Rate\n4.0x10-4\nMin Learning Rate\n4.0x10-5\nWarmup Steps\n2,000\nBatch Size\n2,097,152\nWeight Decay\n0.1\nGradient Clipping Threshold\n1.0\nSFT\nHyperparameter\nValue\nOptimizer\nAdamW(Beta1=0.9, Beta2=0.95)\nWarmup steps\n1,126 (10%)\nPeak learning rate\n2e-5\nLearning rate scheduler\nCosine\nWeight Decay\n0.1\nDPO\nDPO parameters used are the exact same as those specified in The Alignment Handbook.\nCitation\nAttribution\nDeveloped by: calvintwr, lemousehunter\nFunded by PintsAI\nReleased by: PintsAI\nModel type: Large Language Model\nLanguage(s) (NLP): English\nLicense: MIT License\nBibTeX:\n@misc{tan202415pintstechnicalreportpretraining,\ntitle={1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your Language Model Thrives on Quality Data},\nauthor={Calvin Tan and Jerome Wang},\nyear={2024},\neprint={2408.03506},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2408.03506},\n}\nAPA\nTan, C., & Wang, J. (2024). 1.5-Pints Technical Report: Pretraining in days, not months -- Your language model thrives on quality data. arXiv. https://arxiv.org/abs/2408.03506\nLegal Warning\nThough best efforts has been made to ensure, as much as possible, that all texts in the training corpora are royalty free, this does not constitute a legal guarantee that such is the case. By using any of the models, corpora or part thereof, the user agrees to bear full responsibility to do the necessary due diligence to ensure that he / she is in compliance with their local copyright laws.\nAdditionally, the user agrees to bear any damages arising as a direct cause (or otherwise) of using any artifacts released by the pints research team, as well as full responsibility for the consequences of his / her usage (or implementation) of any such released artifacts. The user also indemnifies Pints Research Team (and any of its members or agents) of any damage, related or unrelated, to the release or subsequent usage of any findings, artifacts or code by the team.\nFor the avoidance of doubt, any artifacts released by the Pints Research team are done so in accordance with the \"fair use\" clause of Copyright Law, in hopes that this will aid the research community in bringing LLMs to the next frontier.",
    "robert-moyai/unsloth-llama-3-1-8b-reddit-finetuning-merged-16bit": "Uploaded  model\nUploaded  model\nDeveloped by: Hommes\nLicense: apache-2.0\nFinetuned from model : unsloth/Meta-Llama-3.1-8B-bnb-4bit\nThis llama model was trained 2x faster with Unsloth and Huggingface's TRL library.",
    "histai/hibou-L": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nHibou-L - a Foundational Vision Transformer for digital pathology pretrained on a 1.2B image private dataset using DINOv2 framework.\nFor more information and examples of usage visit https://github.com/HistAI/hibou and read the paper.\nHibou-B is also available at https://huggingface.co/histai/hibou-b\nBasic usage:\nfrom transformers import AutoImageProcessor, AutoModel\nprocessor = AutoImageProcessor.from_pretrained(\"histai/hibou-L\", trust_remote_code=True)\nmodel = AutoModel.from_pretrained(\"histai/hibou-L\", trust_remote_code=True)\nWe use a customized implementation of the DINOv2 architecture from the transformers library to add support for registers, which requires the trust_remote_code=True flag.",
    "parler-tts/parler-tts-large-v1": "Parler-TTS Large v1\nüìñ Quick Index\nüõ†Ô∏è Usage\nüë®‚Äçüíª Installation\nüé≤ Random voice\nüéØ Using a specific speaker\nMotivation\nCitation\nLicense\nParler-TTS Large v1\nParler-TTS Large v1 is a 2.2B-parameters text-to-speech (TTS) model, trained on 45K hours of audio data, that can generate high-quality, natural sounding speech with features that can be controlled using a simple text prompt (e.g. gender, background noise, speaking rate, pitch and reverberation).\nWith Parler-TTS Mini v1, this is the second set of models published as part of the Parler-TTS project, which aims to provide the community with TTS training resources and dataset pre-processing code.\nüìñ Quick Index\nüë®‚Äçüíª Installation\nüé≤ Using a random voice\nüéØ Using a specific speaker\nMotivation\nOptimizing inference\nüõ†Ô∏è Usage\nüë®‚Äçüíª Installation\nUsing Parler-TTS is as simple as \"bonjour\". Simply install the library once:\npip install git+https://github.com/huggingface/parler-tts.git\nüé≤ Random voice\nParler-TTS has been trained to generate speech with features that can be controlled with a simple text prompt, for example:\nimport torch\nfrom parler_tts import ParlerTTSForConditionalGeneration\nfrom transformers import AutoTokenizer\nimport soundfile as sf\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-large-v1\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-large-v1\")\nprompt = \"Hey, how are you doing today?\"\ndescription = \"A female speaker delivers a slightly expressive and animated speech with a moderate speed and pitch. The recording is of very high quality, with the speaker's voice sounding clear and very close up.\"\ninput_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\nprompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\ngeneration = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\naudio_arr = generation.cpu().numpy().squeeze()\nsf.write(\"parler_tts_out.wav\", audio_arr, model.config.sampling_rate)\nüéØ Using a specific speaker\nTo ensure speaker consistency across generations, this checkpoint was also trained on 34 speakers, characterized by name (e.g. Jon, Lea, Gary, Jenna, Mike, Laura).\nTo take advantage of this, simply adapt your text description to specify which speaker to use: Jon's voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise.\nimport torch\nfrom parler_tts import ParlerTTSForConditionalGeneration\nfrom transformers import AutoTokenizer\nimport soundfile as sf\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-large-v1\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-large-v1\")\nprompt = \"Hey, how are you doing today?\"\ndescription = \"Jon's voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise.\"\ninput_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\nprompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\ngeneration = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\naudio_arr = generation.cpu().numpy().squeeze()\nsf.write(\"parler_tts_out.wav\", audio_arr, model.config.sampling_rate)\nTips:\nWe've set up an inference guide to make generation faster. Think SDPA, torch.compile, batching and streaming!\nInclude the term \"very clear audio\" to generate the highest quality audio, and \"very noisy audio\" for high levels of background noise\nPunctuation can be used to control the prosody of the generations, e.g. use commas to add small breaks in speech\nThe remaining speech features (gender, speaking rate, pitch and reverberation) can be controlled directly through the prompt\nMotivation\nParler-TTS is a reproduction of work from the paper Natural language guidance of high-fidelity text-to-speech with synthetic annotations by Dan Lyth and Simon King, from Stability AI and Edinburgh University respectively.\nContrarily to other TTS models, Parler-TTS is a fully open-source release. All of the datasets, pre-processing, training code and weights are released publicly under permissive license, enabling the community to build on our work and develop their own powerful TTS models.\nParler-TTS was released alongside:\nThe Parler-TTS repository - you can train and fine-tuned your own version of the model.\nThe Data-Speech repository - a suite of utility scripts designed to annotate speech datasets.\nThe Parler-TTS organization - where you can find the annotated datasets as well as the future checkpoints.\nCitation\nIf you found this repository useful, please consider citing this work and also the original Stability AI paper:\n@misc{lacombe-etal-2024-parler-tts,\nauthor = {Yoach Lacombe and Vaibhav Srivastav and Sanchit Gandhi},\ntitle = {Parler-TTS},\nyear = {2024},\npublisher = {GitHub},\njournal = {GitHub repository},\nhowpublished = {\\url{https://github.com/huggingface/parler-tts}}\n}\n@misc{lyth2024natural,\ntitle={Natural language guidance of high-fidelity text-to-speech with synthetic annotations},\nauthor={Dan Lyth and Simon King},\nyear={2024},\neprint={2402.01912},\narchivePrefix={arXiv},\nprimaryClass={cs.SD}\n}\nLicense\nThis model is permissively licensed under the Apache 2.0 license.",
    "bespokelabs/Bespoke-MiniCheck-7B": "Llama-3.1-Bespoke-MiniCheck-7B\nModel Variants\nModel Performance\nBelow is a simple use case\nThroughput\nAutomatic Prefix Caching\nTest on our LLM-AggreFact Benchmark\nModel Usage\nBelow is a simple use case\nThroughput\nAutomatic Prefix Caching\nTest on our LLM-AggreFact Benchmark\nLicense\nCitation\nAcknowledgements\nLlama-3.1-Bespoke-MiniCheck-7B\nThis is a fact-checking model developed by Bespoke Labs and maintained by Liyan Tang and Bespoke Labs.\nThe model is an improvement of the MiniCheck model proposed in the following paper:\nüìÉ MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents, EMNLP 2024\nGitHub Repo\nThe model takes as input a document and a sentence and determines whether the sentence is supported by the document: MiniCheck-Model(document, claim) -> {0, 1}\nIn order to fact-check a multi-sentence claim, the claim should first be broken up into sentences. The document does not need to be chunked unless it exceeds 32K tokens. Depending on use cases, adjusting chunk size may yield better performance.\nLlama-3.1-Bespoke-MiniCheck-7B is finetuned from internlm/internlm2_5-7b-chat (Cai et al., 2024)\non the combination of 35K data points only:\n21K ANLI examples (Nie et al., 2020)\n14K synthetically-generated examples following the scheme in the MiniCheck paper, but with additional proprietary data curation techniques (sampling, selecting additional high quality data sources, etc.) from Bespoke Labs. Specifically, we generate 7K \"claim-to-document\" (C2D) and 7K \"doc-to-claim\" (D2C) examples. The following steps were taken to avoid benchmark contamination: the error types of the model in the benchmark data were not used, and the data sources were curated independent of the benchmark.\nAll synthetic data is generated by meta-llama/Meta-Llama-3.1-405B-Instruct, thus the name Llama-3.1-Bespoke-MiniCheck-7B.\nWhile scaling up the model (compared to what is in MiniCheck) helped, many improvements come from high-quality curation, thus establishing the superiority of Bespoke Labs's curation technology.\nModel Variants\nWe also have other three MiniCheck model variants:\nlytang/MiniCheck-Flan-T5-Large (Model Size: 0.8B)\nlytang/MiniCheck-RoBERTa-Large (Model Size: 0.4B)\nlytang/MiniCheck-DeBERTa-v3-Large (Model Size: 0.4B)\nModel Performance\nThe performance of these models is evaluated on our new collected benchmark (unseen by our models during training), LLM-AggreFact,\nfrom 11 recent human annotated datasets on fact-checking and grounding LLM generations. Llama-3.1-Bespoke-MiniCheck-7B is the SOTA\nfact-checking model despite its small size.\nModel Usage\nPlease run the following command to install the MiniCheck package and all necessary dependencies.\npip install \"minicheck[llm] @ git+https://github.com/Liyan06/MiniCheck.git@main\"\nBelow is a simple use case\nfrom minicheck.minicheck import MiniCheck\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\ndoc = \"A group of students gather in the school library to study for their upcoming final exams.\"\nclaim_1 = \"The students are preparing for an examination.\"\nclaim_2 = \"The students are on vacation.\"\n# model_name can be one of:\n# ['roberta-large', 'deberta-v3-large', 'flan-t5-large', 'Bespoke-MiniCheck-7B']\nscorer = MiniCheck(model_name='Bespoke-MiniCheck-7B', enable_prefix_caching=False, cache_dir='./ckpts')\npred_label, raw_prob, _, _ = scorer.score(docs=[doc, doc], claims=[claim_1, claim_2]) # can set `chunk_size=your-specified-value` here, default to 32K chunk size.\nprint(pred_label) # [1, 0]\nprint(raw_prob)   # [0.9840446675150499, 0.010986349594852094]\nThroughput\nWe speed up Llama-3.1-Bespoke-MiniCheck-7B inference with vLLM. Based on our test on\na single A6000 (48 VRAM), Llama-3.1-Bespoke-MiniCheck-7B with vLLM and MiniCheck-Flan-T5-Large have throughputs > 500 docs/min.\nAutomatic Prefix Caching\nAutomatic Prefix Caching (APC in short) caches the KV cache of existing queries, so that a new query can directly reuse the KV\ncache if it shares the same prefix with one of the existing queries, allowing the new query to skip the computation of the shared part.\nTo enable automatic prefix caching for Bespoke-MiniCheck-7B, simply set enable_prefix_caching=True when initializing the\nMiniCheck model (no other changes are needed):\nscorer = MiniCheck(model_name='Bespoke-MiniCheck-7B', enable_prefix_caching=True, cache_dir='./ckpts')\nHow automatic prefix caching affects the throughput and model performance can be found in the GitHub Repo.\nTest on our LLM-AggreFact Benchmark\nimport pandas as pd\nfrom datasets import load_dataset\nfrom minicheck.minicheck import MiniCheck\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n# load 29K test data\ndf = pd.DataFrame(load_dataset(\"lytang/LLM-AggreFact\")['test'])\ndocs = df.doc.values\nclaims = df.claim.values\nscorer = MiniCheck(model_name='Bespoke-MiniCheck-7B', enable_prefix_caching=False, cache_dir='./ckpts')\npred_label, raw_prob, _, _ = scorer.score(docs=docs, claims=claims)  # ~ 500 docs/min, depending on hardware\nTo evaluate the result on the benchmark\nfrom sklearn.metrics import balanced_accuracy_score\ndf['preds'] = pred_label\nresult_df = pd.DataFrame(columns=['Dataset', 'BAcc'])\nfor dataset in df.dataset.unique():\nsub_df = df[df.dataset == dataset]\nbacc = balanced_accuracy_score(sub_df.label, sub_df.preds) * 100\nresult_df.loc[len(result_df)] = [dataset, bacc]\nresult_df.loc[len(result_df)] = ['Average', result_df.BAcc.mean()]\nresult_df.round(1)\nLicense\nThis work is licensed under CC BY-NC 4.0.\nFor commercial licensing, please contact company@bespokelabs.ai.\nCitation\n@InProceedings{tang-etal-2024-minicheck,\ntitle = {MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents},\nauthor = {Liyan Tang and Philippe Laban and Greg Durrett},\nbooktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},\nyear = {2024},\npublisher = {Association for Computational Linguistics},\nurl = {https://arxiv.org/pdf/2404.10774}\n}\n@misc{tang2024bespokeminicheck,\ntitle={Bespoke-Minicheck-7B},\nauthor={Bespoke Labs},\nyear={2024},\nurl={https://huggingface.co/bespokelabs/Bespoke-MiniCheck-7B},\n}\nAcknowledgements\nModel perfected at Bespoke Labs.\nTeam:\nLiyan Tang\nNegin Raoof\nTrung Vu\nGreg Durrett\nAlex Dimakis\nMahesh Sathiamoorthy\nWe also thank Giannis Daras for feedback and Sarthak Malhotra for market research.",
    "InstantX/FLUX.1-dev-Controlnet-Union": "FLUX.1-dev-Controlnet-Union\nRelease\nCheckpoint\nControl Mode\nInference\nMulti-Controls Inference\nResources\nAcknowledgements\nFLUX.1-dev-Controlnet-Union\nRelease\n[2024/08/26] üî• Release FLUX.1-dev-ControlNet-Union-Pro. Please install from the source before the next release. We have supported CN-Union and Multi-ControlNets via this PR.\n[2024/08/20] Release the beta version.\n[2024/08/14] Release the alpha version.\nCheckpoint\nThe training of union controlnet requires a significant amount of computational power.\nThe current release is the first beta version checkpoint that maybe not been fully trained.\nThe fully trainedbeta version is in the training process.\nWe have conducted ablation studies that have demonstrated the validity of the code.\nThe open-source release of the first beta version is solely to facilitate the rapid growth of the open-source community and the Flux ecosystem;\nit is common to encounter bad cases (please accept my apologies).\nIt is worth noting that we have found that even a fully trained Union model may not perform as well as specialized models, such as pose control.\nHowever, as training progresses, the performance of the Union model will continue to approach that of specialized models.\nControl Mode\nControl Mode\nDescription\nCurrent Model Validity\n0\ncanny\nüü¢high\n1\ntile\nüü¢high\n2\ndepth\nüü¢high\n3\nblur\nüü¢high\n4\npose\nüü¢high\n5\ngray\nüî¥low\n6\nlq\nüü¢high\nInference\nimport torch\nfrom diffusers.utils import load_image\nfrom diffusers import FluxControlNetPipeline, FluxControlNetModel\nbase_model = 'black-forest-labs/FLUX.1-dev'\ncontrolnet_model = 'InstantX/FLUX.1-dev-Controlnet-Union'\ncontrolnet = FluxControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.bfloat16)\npipe = FluxControlNetPipeline.from_pretrained(base_model, controlnet=controlnet, torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\ncontrol_image_canny = load_image(\"https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Union-alpha/resolve/main/images/canny.jpg\")\ncontrolnet_conditioning_scale = 0.5\ncontrol_mode = 0\nwidth, height = control_image.size\nprompt = 'A bohemian-style female travel blogger with sun-kissed skin and messy beach waves.'\nimage = pipe(\nprompt,\ncontrol_image=control_image,\ncontrol_mode=control_mode,\nwidth=width,\nheight=height,\ncontrolnet_conditioning_scale=controlnet_conditioning_scale,\nnum_inference_steps=24,\nguidance_scale=3.5,\n).images[0]\nimage.save(\"image.jpg\")\nMulti-Controls Inference\nimport torch\nfrom diffusers.utils import load_image\nfrom diffusers import FluxControlNetPipeline, FluxControlNetModel, FluxMultiControlNetModel\nbase_model = 'black-forest-labs/FLUX.1-dev'\ncontrolnet_model_union = 'InstantX/FLUX.1-dev-Controlnet-Union'\ncontrolnet_union = FluxControlNetModel.from_pretrained(controlnet_model_union, torch_dtype=torch.bfloat16)\ncontrolnet = FluxMultiControlNetModel([controlnet_union]) # we always recommend loading via FluxMultiControlNetModel\npipe = FluxControlNetPipeline.from_pretrained(base_model, controlnet=controlnet, torch_dtype=torch.bfloat16)\npipe.to(\"cuda\")\nprompt = 'A bohemian-style female travel blogger with sun-kissed skin and messy beach waves.'\ncontrol_image_depth = load_image(\"https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Union/resolve/main/images/depth.jpg\")\ncontrol_mode_depth = 2\ncontrol_image_canny = load_image(\"https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Union/resolve/main/images/canny.jpg\")\ncontrol_mode_canny = 0\nwidth, height = control_image.size\nimage = pipe(\nprompt,\ncontrol_image=[control_image_depth, control_image_canny],\ncontrol_mode=[control_mode_depth, control_mode_canny],\nwidth=width,\nheight=height,\ncontrolnet_conditioning_scale=[0.2, 0.4],\nnum_inference_steps=24,\nguidance_scale=3.5,\ngenerator=torch.manual_seed(42),\n).images[0]\nResources\nInstantX/FLUX.1-dev-Controlnet-Canny\nInstantX/FLUX.1-dev-Controlnet-Union\nShakker-Labs/FLUX.1-dev-ControlNet-Depth\nShakker-Labs/FLUX.1-dev-ControlNet-Union-Pro\nAcknowledgements\nThanks zzzzzero for help us pointing out some bugs in the training.",
    "madebyollin/taef1": "üç∞ Tiny AutoEncoder for FLUX.1\nUsing in üß® diffusers\nüç∞ Tiny AutoEncoder for FLUX.1\nTAEF1 is very tiny autoencoder which uses the same \"latent API\" as FLUX.1's VAE.\nFLUX.1 is useful for real-time previewing of the FLUX.1 generation process.\nThis repo contains .safetensors versions of the TAEF1 weights.\nUsing in üß® diffusers\nimport torch\nfrom diffusers import FluxPipeline, AutoencoderTiny\npipe = FluxPipeline.from_pretrained(\n\"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16\n)\npipe.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taef1\", torch_dtype=torch.bfloat16)\npipe.enable_sequential_cpu_offload()\nprompt = \"slice of delicious New York-style berry cheesecake\"\nimage = pipe(\nprompt,\nguidance_scale=0.0,\nnum_inference_steps=4,\nmax_sequence_length=256,\n).images[0]\nimage.save(\"cheesecake.png\")",
    "neifuisan/Gemma2-9b-Neuro-sama": "Im boring ok?",
    "mlc-ai/snowflake-arctic-embed-m-q0f32-MLC": "snowflake-arctic-embed-m-q0f32-MLC\nDocumentation\nsnowflake-arctic-embed-m-q0f32-MLC\nThis is the snowflake-arctic-embed-m model in MLC format q0f32.\nThe model can be used for projects MLC-LLM and WebLLM.\nDocumentation\nFor more information on MLC LLM project, please visit our documentation and GitHub repo.",
    "SonyCSLParis/music2latent": "Music2Latent\nInstallation\nHow to use\nLicense\nMusic2Latent\nEncode and decode audio samples to/from compressed representations! Useful for efficient generative modelling applications and for other downstream tasks.\nRead the ISMIR 2024 paper here.\nListen to audio samples here.\nUnder the hood, Music2Latent uses a Consistency Autoencoder model to efficiently encode and decode audio samples.\n44.1 kHz audio is encoded into a sequence of ~10 Hz, and each of the latents has 64 channels.\n48 kHz audio can also be encoded, which results in a sequence of ~12 Hz.\nA generative model can then be trained on these embeddings, or they can be used for other downstream tasks.\nMusic2Latent was trained on music and on speech. Refer to the paper for more details.\nInstallation\npip install music2latent\nThe model weights will be downloaded automatically the first time the code is run.\nHow to use\nTo encode and decode audio samples to/from latent embeddings:\naudio_path = librosa.example('trumpet')\nwv, sr = librosa.load(audio_path, sr=44100)\nfrom music2latent import EncoderDecoder\nencdec = EncoderDecoder()\nlatent = encdec.encode(wv)\n# latent has shape (batch_size/audio_channels, dim (64), sequence_length)\nwv_rec = encdec.decode(latent)\nTo extract encoder features to use in downstream tasks:\nfeatures = encoder.encode(wv, extract_features=True)\nThese features are extracted before the encoder bottleneck, and thus have more channels (contain more information) than the latents used for reconstruction. It will not be possible to directly decode these features back to audio.\nmusic2latent supports more advanced usage, including GPU memory management controls. Please refer to tutorial.ipynb.\nLicense\nThis library is released under the CC BY-NC 4.0 license. Please refer to the LICENSE file for more details.\nThis work was conducted by Marco Pasini during his PhD at Queen Mary University of London, in partnership with Sony Computer Science Laboratories Paris.\nThis work was supervised by Stefan Lattner and George Fazekas.",
    "mirla/ONNX-otimizado-financialBERT-Sentiment-Analysis": "FinancialBERT para An√°lise de Sentimentos - Vers√£o Otimizado\nIntrodu√ß√£o\nM√©tricas de Avalia√ß√£o\nConclus√£o\nFinancialBERT para An√°lise de Sentimentos - Vers√£o Otimizado\nIntrodu√ß√£o\nEste reposit√≥rio cont√©m uma vers√£o otimizada do modelo FinancialBERT para an√°lise de sentimentos, desenvolvido por Ahmed Rachid Hazourli. A otimiza√ß√£o foi realizada utilizando a biblioteca Optimum da Hugging Face com ONNX para melhorar o desempenho do modelo sem comprometer a precis√£o.\nM√©tricas de Avalia√ß√£o\nOs modelos foram testados utilizando o conjunto de teste da base de dados nickmuchi/financial-classification.\nPrecis√£o:\nA precis√£o do modelo permaneceu a mesma ap√≥s a otimiza√ß√£o.\nTempo Total em Segundos:\nModelo Original: 74.01 segundos\nModelo Otimizado: 64.65 segundos\nAn√°lise: Redu√ß√£o de 12.66% no tempo total de execu√ß√£o.\nAmostras por Segundo:\nModelo Original: 6.84 amostras/segundo\nModelo Otimizado: 7.83 amostras/segundo\nAn√°lise: Aumento na efici√™ncia de processamento.\nLat√™ncia em Segundos:\nModelo Original: 0.1463 segundos\nModelo Otimizado: 0.1278 segundos\nAn√°lise: Melhoria de 12.66% na lat√™ncia.\nConclus√£o\nO modelo FinancialBERT otimizado apresenta m√©tricas de desempenho aprimoradas, mantendo o mesmo n√≠vel de precis√£o. A redu√ß√£o na lat√™ncia e no tempo total de processamento o torna uma excelente escolha para uso em aplica√ß√µes de an√°lise de sentimentos no setor financeiro.",
    "ab-ai/PII-Model-Phi3-Mini": "PII Detection Model - Phi3 Mini Fine-Tuned\nModel Overview\nModel Architecture\nDetected PII Entities\nPrompt Format\nUsage\nInstallation\nRun Inference\nPII Detection Model - Phi3 Mini Fine-Tuned\nThis repository contains a fine-tuned version of the Phi3 Mini model for detecting personally identifiable information (PII). The model has been specifically trained to recognize various PII entities in text, making it a powerful tool for tasks such as data redaction, privacy protection, and compliance with data protection regulations.\nModel Overview\nModel Architecture\nBase Model: Phi3 Mini\nFine-Tuned For: PII detection\nFramework: Hugging Face Transformers\nDetected PII Entities\nThe model is capable of detecting the following PII entities:\nPersonal Information:\nfirstname\nmiddlename\nlastname\nsex\ndob (Date of Birth)\nage\ngender\nheight\neyecolor\nContact Information:\nemail\nphonenumber\nurl\nusername\nuseragent\nAddress Information:\nstreet\ncity\nstate\ncounty\nzipcode\ncountry\nsecondaryaddress\nbuildingnumber\nordinaldirection\nGeographical Information:\nnearbygpscoordinate\nOrganizational Information:\ncompanyname\njobtitle\njobarea\njobtype\nFinancial Information:\naccountname\naccountnumber\ncreditcardnumber\ncreditcardcvv\ncreditcardissuer\niban\nbic\ncurrency\ncurrencyname\ncurrencysymbol\ncurrencycode\namount\nUnique Identifiers:\npin\nssn\nimei (Phone IMEI)\nmac (MAC Address)\nvehiclevin (Vehicle VIN)\nvehiclevrm (Vehicle VRM)\nCryptocurrency Information:\nbitcoinaddress\nlitecoinaddress\nethereumaddress\nOther Information:\nip (IP Address)\nipv4\nipv6\nmaskednumber\npassword\ntime\nordinaldirection\nprefix\nPrompt Format\n### Instruction:\nIdentify and extract the following PII entities from the text, if present: companyname, pin, currencyname, email, phoneimei, litecoinaddress, currency, eyecolor, street, mac, state, time, vehiclevin, jobarea, date, bic, currencysymbol, currencycode, age, nearbygpscoordinate, amount, ssn, ethereumaddress, zipcode, buildingnumber, dob, firstname, middlename, ordinaldirection, jobtitle, bitcoinaddress, jobtype, phonenumber, height, password, ip, useragent, accountname, city, gender, secondaryaddress, iban, sex, prefix, ipv4, maskednumber, url, username, lastname, creditcardcvv, county, vehiclevrm, ipv6, creditcardissuer, accountnumber, creditcardnumber. Return the output in JSON format.\n### Input:\nGreetings, Mason! Let's celebrate another year of wellness on 14/01/1977. Don't miss the event at 176,Apt. 388.\n### Output:\nUsage\nInstallation\nTo use this model, you'll need to have the transformers library installed:\npip install transformers\nRun Inference\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"ab-ai/PII-Model-Phi3-Mini\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"ab-ai/PII-Model-Phi3-Mini\")\ninput_text = \"Hi Abner, just a reminder that your next primary care appointment is on 23/03/1926. Please confirm by replying to this email Nathen15@hotmail.com.\"\nmodel_prompt = f\"\"\"### Instruction:\nIdentify and extract the following PII entities from the text, if present: companyname, pin, currencyname, email, phoneimei, litecoinaddress, currency, eyecolor, street, mac, state, time, vehiclevin, jobarea, date, bic, currencysymbol, currencycode, age, nearbygpscoordinate, amount, ssn, ethereumaddress, zipcode, buildingnumber, dob, firstname, middlename, ordinaldirection, jobtitle, bitcoinaddress, jobtype, phonenumber, height, password, ip, useragent, accountname, city, gender, secondaryaddress, iban, sex, prefix, ipv4, maskednumber, url, username, lastname, creditcardcvv, county, vehiclevrm, ipv6, creditcardissuer, accountnumber, creditcardnumber. Return the output in JSON format.\n### Input:\n{input_text}\n### Output: \"\"\"\ninputs = tokenizer(model_prompt, return_tensors=\"pt\").to(device)\n# adjust max_new_tokens according to your need\noutputs = model.generate(**inputs, do_sample=True, max_new_tokens=120)\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response) #{'middlename': ['Abner'], 'dob': ['23/03/1926'], 'email': ['Nathen15@hotmail.com']}"
}