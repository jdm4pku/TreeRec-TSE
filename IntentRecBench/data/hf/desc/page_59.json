{
    "benlehrburger/modern-architecture-32": "Low-poly architecture image generation\nUsage\nLow-poly architecture image generation\nThis model is a diffusion model for unconditional image generation of modern architecture.\nUsage\nfrom diffusers import DDPMPipeline\npipeline = DDPMPipeline.from_pretrained('{hub_model_id}')\nimage = pipeline().images[0]\nimage",
    "QuixiAI/Wizard-Vicuna-7B-Uncensored": "Open LLM Leaderboard Evaluation Results\nOpen LLM Leaderboard Evaluation Results\nThis is wizard-vicuna-13b trained against LLaMA-7B with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.\nShout out to the open source AI/ML community, and everyone who helped me out.\nNote:\nAn uncensored model has no guardrails.\nYou are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.\nPublishing anything this model generates is the same as publishing it yourself.\nYou are responsible for the content you publish, and you cannot blame the model any more than you can blame the knife, gun, lighter, or car for what you do with it.\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n44.77\nARC (25-shot)\n53.41\nHellaSwag (10-shot)\n78.85\nMMLU (5-shot)\n37.09\nTruthfulQA (0-shot)\n43.48\nWinogrande (5-shot)\n72.22\nGSM8K (5-shot)\n4.55\nDROP (3-shot)\n23.8\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n48.27\nAI2 Reasoning Challenge (25-Shot)\n53.41\nHellaSwag (10-Shot)\n78.85\nMMLU (5-Shot)\n37.09\nTruthfulQA (0-shot)\n43.48\nWinogrande (5-shot)\n72.22\nGSM8k (5-shot)\n4.55",
    "intfloat/e5-base-v2": "E5-base-v2\nUsage\nTraining Details\nBenchmark Evaluation\nSupport for Sentence Transformers\nFAQ\nCitation\nLimitations\nE5-base-v2\nText Embeddings by Weakly-Supervised Contrastive Pre-training.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022\nThis model has 12 layers and the embedding size is 768.\nUsage\nBelow is an example to encode queries and passages from the MS-MARCO passage ranking dataset.\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ndef average_pool(last_hidden_states: Tensor,\nattention_mask: Tensor) -> Tensor:\nlast_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\nreturn last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n# Each input text should start with \"query: \" or \"passage: \".\n# For tasks other than retrieval, you can simply use the \"query: \" prefix.\ninput_texts = ['query: how much protein should a female eat',\n'query: summit define',\n\"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n\"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"]\ntokenizer = AutoTokenizer.from_pretrained('intfloat/e5-base-v2')\nmodel = AutoModel.from_pretrained('intfloat/e5-base-v2')\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\nTraining Details\nPlease refer to our paper at https://arxiv.org/pdf/2212.03533.pdf.\nBenchmark Evaluation\nCheck out unilm/e5 to reproduce evaluation results\non the BEIR and MTEB benchmark.\nSupport for Sentence Transformers\nBelow is an example for usage with sentence_transformers.\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('intfloat/e5-base-v2')\ninput_texts = [\n'query: how much protein should a female eat',\n'query: summit define',\n\"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n\"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n]\nembeddings = model.encode(input_texts, normalize_embeddings=True)\nPackage requirements\npip install sentence_transformers~=2.2.2\nContributors: michaelfeil\nFAQ\n1. Do I need to add the prefix \"query: \" and \"passage: \" to input texts?\nYes, this is how the model is trained, otherwise you will see a performance degradation.\nHere are some rules of thumb:\nUse \"query: \" and \"passage: \" correspondingly for asymmetric tasks such as passage retrieval in open QA, ad-hoc information retrieval.\nUse \"query: \" prefix for symmetric tasks such as semantic similarity, paraphrase retrieval.\nUse \"query: \" prefix if you want to use embeddings as features, such as linear probing classification, clustering.\n2. Why are my reproduced results slightly different from reported in the model card?\nDifferent versions of transformers and pytorch could cause negligible but non-zero performance differences.\n3. Why does the cosine similarity scores distribute around 0.7 to 1.0?\nThis is a known and expected behavior as we use a low temperature 0.01 for InfoNCE contrastive loss.\nFor text embedding tasks like text retrieval or semantic similarity,\nwhat matters is the relative order of the scores instead of the absolute values,\nso this should not be an issue.\nCitation\nIf you find our paper or models helpful, please consider cite as follows:\n@article{wang2022text,\ntitle={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\nauthor={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\njournal={arXiv preprint arXiv:2212.03533},\nyear={2022}\n}\nLimitations\nThis model only works for English texts. Long texts will be truncated to at most 512 tokens.",
    "intfloat/e5-large-v2": "E5-large-v2\nUsage\nTraining Details\nBenchmark Evaluation\nSupport for Sentence Transformers\nFAQ\nCitation\nLimitations\nE5-large-v2\nText Embeddings by Weakly-Supervised Contrastive Pre-training.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022\nThis model has 24 layers and the embedding size is 1024.\nUsage\nBelow is an example to encode queries and passages from the MS-MARCO passage ranking dataset.\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ndef average_pool(last_hidden_states: Tensor,\nattention_mask: Tensor) -> Tensor:\nlast_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\nreturn last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n# Each input text should start with \"query: \" or \"passage: \".\n# For tasks other than retrieval, you can simply use the \"query: \" prefix.\ninput_texts = ['query: how much protein should a female eat',\n'query: summit define',\n\"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n\"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"]\ntokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-v2')\nmodel = AutoModel.from_pretrained('intfloat/e5-large-v2')\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\nTraining Details\nPlease refer to our paper at https://arxiv.org/pdf/2212.03533.pdf.\nBenchmark Evaluation\nCheck out unilm/e5 to reproduce evaluation results\non the BEIR and MTEB benchmark.\nSupport for Sentence Transformers\nBelow is an example for usage with sentence_transformers.\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('intfloat/e5-large-v2')\ninput_texts = [\n'query: how much protein should a female eat',\n'query: summit define',\n\"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n\"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"\n]\nembeddings = model.encode(input_texts, normalize_embeddings=True)\nPackage requirements\npip install sentence_transformers~=2.2.2\nContributors: michaelfeil\nFAQ\n1. Do I need to add the prefix \"query: \" and \"passage: \" to input texts?\nYes, this is how the model is trained, otherwise you will see a performance degradation.\nHere are some rules of thumb:\nUse \"query: \" and \"passage: \" correspondingly for asymmetric tasks such as passage retrieval in open QA, ad-hoc information retrieval.\nUse \"query: \" prefix for symmetric tasks such as semantic similarity, paraphrase retrieval.\nUse \"query: \" prefix if you want to use embeddings as features, such as linear probing classification, clustering.\n2. Why are my reproduced results slightly different from reported in the model card?\nDifferent versions of transformers and pytorch could cause negligible but non-zero performance differences.\n3. Why does the cosine similarity scores distribute around 0.7 to 1.0?\nThis is a known and expected behavior as we use a low temperature 0.01 for InfoNCE contrastive loss.\nFor text embedding tasks like text retrieval or semantic similarity,\nwhat matters is the relative order of the scores instead of the absolute values,\nso this should not be an issue.\nCitation\nIf you find our paper or models helpful, please consider cite as follows:\n@article{wang2022text,\ntitle={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\nauthor={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\njournal={arXiv preprint arXiv:2212.03533},\nyear={2022}\n}\nLimitations\nThis model only works for English texts. Long texts will be truncated to at most 512 tokens.",
    "SmilingWolf/wd-v1-4-moat-tagger-v2": "WD 1.4 MOAT Tagger V2\nDataset\nValidation results\nPaper\nFinal words\nWD 1.4 MOAT Tagger V2\nSupports ratings, characters and general tags.\nTrained using https://github.com/SmilingWolf/SW-CV-ModelZoo.TPUs used for training kindly provided by the TRC program.\nDataset\nLast image id: 5944504Trained on Danbooru images with IDs modulo 0000-0899.Validated on images with IDs modulo 0950-0999.Images with less than 10 general tags were filtered out.Tags with less than 600 images were filtered out.\nValidation results\nP=R: threshold = 0.3771, F1 = 0.6911\nPaper\nMOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models\nFinal words\nSubject to change and updates.Downstream users are encouraged to use tagged releases rather than relying on the head of the repo.",
    "zhang-yice/spt-absa-bert-400k": "SPT-ABSA\nWhat Did We Do?\nExperimental Results\nSPT-ABSA\nWe continue to pre-train BERT-base via Sentiment-enhance pre-training (SPT).\nTitle: An Empirical Study of Sentiment-Enhanced Pre-Training for Aspect-Based Sentiment Analysis\nAuthor: Yice Zhang, Yifan Yang, Bin Liang, Shiwei Chen, Bing Qin, and Ruifeng Xu\nConference: ACL-2023 Finding (Long)\nGitHub Repository: https://github.com/HITSZ-HLT/SPT-ABSA\nWhat Did We Do?\nAspect-Based Sentiment Analysis (ABSA) is an important problem in sentiment analysis.\nIts goal is to recognize opinions and sentiments towards specific aspects from user-generated content.\nMany research efforts leverage pre-training techniques to learn sentiment-aware representations and achieve significant gains in various ABSA tasks.\nWe conduct an empirical study of SPT-ABSA to systematically investigate and analyze the effectiveness of the existing approaches.\nWe mainly concentrate on the following questions:\n(a) what impact do different types of sentiment knowledge have on downstream ABSA tasks?;\n(b) which knowledge integration method is most effective?; and\n(c) does injecting non-sentiment-specific linguistic knowledge (e.g., part-of-speech tags and syntactic relations) into pre-training have positive impacts?\nBased on the experimental investigation of these questions, we eventually obtain a powerful sentiment-enhanced pre-trained model.\nThe powerful sentiment-enhanced pre-trained model has two versions, namely zhang-yice/spt-absa-bert-400k and zhang-yice/spt-absa-bert-10k, which integrates three types of knowledge:\naspect words: masking aspects' context and predicting them.\nreview's rating score: rating prediction.\nsyntax knowledge:\npart-of-speech,\ndependency direction,\ndependency distance.\nExperimental Results",
    "coreml-community/ControlNet-Models-For-Core-ML": "ControlNet v1.1 Models And Links To Compatible Stable Diffusion v1.5 Type Models Converted To Apple CoreML Format\nFor use with a Swift app like MOCHI DIFFUSION or the SwiftCLI\nControlNet Models - All Current SD-1.5-Type ControlNet Models\nBase Models - A Variety Of SD-1.5-Type Models Compatible With ControlNet\nOther ControlNet Compatible Base Models Are Listed At CORE ML MODELS COMMUNITY\nControlNet v1.1 Models And Links To Compatible Stable Diffusion v1.5 Type Models Converted To Apple CoreML Format\nFor use with a Swift app like MOCHI DIFFUSION or the SwiftCLI\nAll of the models in this repo work with Swift and the apple/ml-stable-diffusion pipeline (release 0.4.0 or 1.0.0).  They were not built for, and will not work with, a Python Diffusers pipeline.  They need ml-stable-diffusion for command line use, or a Swift app that supports ControlNet, such as the (June 2023) MOCHI DIFFUSION 4.0 version.\nThe ControlNet models in this repo have both \"Original\" and \"Split-Einsum\" versions, all built for SD-1.5 type models.  They will not work with SD-2.1 type models.  The smaller zip files, with \"SE\", each have a single model for \"Split-Einsum\".  The larger zip files, without \"SE\", each have a set of \"Original\" models at 4 different resolutions.\nThe ControlNet model files are in the \"CN\" folder of this repo.  They are zipped and need to be unzipped after downloading.  The larger zips hold \"Original\" types at 512x512, 512x768, 768x512 and 768x768.  The smaller zips with \"SE\" have a single model for \"Split-Einsum\".\nIf you are using a GUI like MOCHI DIFFUSION  4.0, the app will most likely guide you to the correct location/arrangement for your ConrolNet model folder.\nPlease note that when you unzip the \"Originl\" ControlNet files (for example Canny.zip) from this repo, they will unzip into a folder, with the actual four model files inside that folder.  This folder is just a holding folder for the zipping process.  What you want to move into your ControlNet model folder in Mochi Diffusion will be the individual files, not the folder they unzip into.  The \"Split-Einsum\" zips just have a single file and don't use a holding folder.  To make things even more confusing, on some Mac systems, an individual ControlNet model file, for example Canny-5x5.mlmodelc, will appear in Finder as a folder, not a file.  You want to move the Canny-5x5.mlmodelc file or folder (and other .mlmodelc files or folders) into your ControlNet store folder.  Don't move the plain \"Canny\" folder.  This is different from base models, where you do want to be moving the folder that the downloaded zip file unzips into.  See the images here and here for an example of how my folders are set up for Mochi Diffusion.\nThe SD models (base models) linked at the bottom of this page were relocated from this repo to individual model repos at the CORE ML MODELS COMMUNITY repo.  The links will take you directly to each model.  They are for \"Original\" and \"Split-Einsum\".\nThe Stable Diffusion v1.5 model and the other SD 1.5 type models contain both the standard Unet and the ControlledUnet used for a ControlNet pipeline.  The correct one will be used automatically based on whether a ControlNet is enabled or not.\nThey have VAEEncoder.mlmodelc bundles that allow Image2Image to operate correctly at the noted resolutions, when used with a current Swift CLI pipeline or a current GUI built with ml-stable-diffusion 0.4.0 or ml-stable-diffusion 1.0.0, such as MOCHI DIFFUSION 4.0, or later.\nThe sizes noted for all model type inputs/outputs are WIDTH x HEIGHT.  A 512x768 is \"portrait\" orientation and a 768x512 is \"landscape\" orientation.\nThere is also a \"MISC\" folder at this repo that has text files with some notes and a screencap of my directory structure.  These are provided for those who want to convert models themselves and/or run the models with a SwiftCLI.  The notes are not perfect, and may be out of date if any of the Python or CoreML packages referenced have been updated recently.  You can open a Discussion here if you need help with any of the \"MISC\" items.\nNOTE: At present, it appears that the python_coreml_stable_diffusion  package from ml-stable-diffusion 6.3 is the latest version that will convert ControlNet models that work correctly.  Conversions with python_coreml_stable_diffusion 1.0.0, which is from ml-stable-diffusion 7.0b1 or 7.0b2, will throw errors when used.\nFor command line use, the \"MISC\" notes cover setting up a miniconda3 environment.  If you are using the command line, please read the notes concerning naming and placement of your ControlNet model folder.  Briefly, they will need to go inside a \"controlnet\" folder that you placed inside your base model folder.  You'll need a \"controlnet\" folder inside each base model folder, or a symlink named \"controlnet\" pointing to a central folder with all your ControlNet models inside it.\nIf you encounter any models in this repo that do not work correctly with ControlNet, using the current apple/ml-stable-diffusion SwiftCLI pipeline, or Mochi Diffusion 4.0, please leave a report in the Community Discussion area.  If you would like to add models that you have converted, leave a message there as well, and we will grant you access to the appropriate repo.\nControlNet Models - All Current SD-1.5-Type ControlNet Models\nEach larger zip file contains a set of 4 \"Original\" types at resolutions of 512x512, 512x768, 768x512 and 768x768.  Each smaller zip file, with the \"SE\" notation, contains a single \"Split-Einsum\" file.\nCanny  --  Edge Detection, Outlines As Input\nDepth  --  Reproduces Depth Relationships From An Image\nInPaint  --  Use Masks To Define And Modify An Area (not sure how this works)\nInstrP2P  --  Instruct Pixel2Pixel - \"Change X to Y\"\nLineAnime  --  Find And Reuse Small Outlines, Optimized For Anime\nLineArt  --  Find And Reuse Small Outlines\nMLSD  --  Find And Reuse Straight Lines And Edges\nNormalBAE  --  Reproduce Depth Relationships Using Surface Normal Depth Maps\nOpenPose  --  Copy Body Poses\nScribble  --  Freehand Sketch As Input\nSegmentation  --  Find And Reuse Distinct Areas\nShuffle  --  Find And Reorder Major Elements\nSoftEdge  --  Find And Reuse Soft Edges\nTile  --  Subtle Variations Within Batch Run\nBase Models - A Variety Of SD-1.5-Type Models Compatible With ControlNet\nDreamShaper v5.0, 1.5-type model, \"Original\" & \"Split-Einsum\"  --  Reloacted to:  https://huggingface.co/coreml-community/coreml-DreamShaper-v5.0_cn\nGhostMix v1.1, 1.5-type anime model, \"Original\" & \"Split-Einsum\"  --  Relocated to:  https://huggingface.co/coreml-community/coreml-ghostmix-v20-bakedVAE_cn\nRealistic Vision v2.0, 1.5-type model, \"Original\" & \"Split-Einsum\"  --  Relocated to:  https://huggingface.co/coreml-community/coreml-realisticVision-v20_cn\nMeinaMix v9.0 1.5-type anime model, \"Original\" & \"Split-Einsum\"  --  Relocated to:  https://huggingface.co/coreml-community/coreml-MeinaMix-v9_cn\nMyMerge v1.0 1.5-type NSFW model, \"Original\" & \"Split-Einsum\"  --  Relocated to:  https://huggingface.co/coreml-community/coreml-MyMerge-v1_cn\nStable Diffusion v1.5, \"Original\" & \"Split-Einsum\"  --  Relocated to:  https://huggingface.co/coreml-community/coreml-stable-diffusion-v1-5_cn\nOther ControlNet Compatible Base Models Are Listed At CORE ML MODELS COMMUNITY\nLook for \"_cn\" at the end of the names!",
    "kandinsky-community/kandinsky-2-1": "Kandinsky 2.1\nUsage\nText to image\nText Guided Image-to-Image Generation\nInterpolate\nModel Architecture\nOverview\nDetails\nEvaluation\nBibTex\nKandinsky 2.1\nKandinsky 2.1 inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\nThe Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov\nUsage\nKandinsky 2.1 is available in diffusers!\npip install diffusers transformers accelerate\nText to image\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipe = AutoPipelineForText2Image.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\nprompt = \"A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting\"\nnegative_prompt = \"low quality, bad quality\"\nimage = pipe(prompt=prompt, negative_prompt=negative_prompt, prior_guidance_scale =1.0, height=768, width=768).images[0]\nimage.save(\"cheeseburger_monster.png\")\nText Guided Image-to-Image Generation\nfrom diffusers import AutoPipelineForImage2Image\nimport torch\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\nimport os\npipe = AutoPipelineForImage2Image.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\nprompt = \"A fantasy landscape, Cinematic lighting\"\nnegative_prompt = \"low quality, bad quality\"\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\nresponse = requests.get(url)\noriginal_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\noriginal_image.thumbnail((768, 768))\nimage = pipe(prompt=prompt, image=original_image, strength=0.3).images[0]\nout.images[0].save(\"fantasy_land.png\")\nInterpolate\nfrom diffusers import KandinskyPriorPipeline, KandinskyPipeline\nfrom diffusers.utils import load_image\nimport PIL\nimport torch\npipe_prior = KandinskyPriorPipeline.from_pretrained(\n\"kandinsky-community/kandinsky-2-1-prior\", torch_dtype=torch.float16\n)\npipe_prior.to(\"cuda\")\nimg1 = load_image(\n\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinsky/cat.png\"\n)\nimg2 = load_image(\n\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinsky/starry_night.jpeg\"\n)\n# add all the conditions we want to interpolate, can be either text or image\nimages_texts = [\"a cat\", img1, img2]\n# specify the weights for each condition in images_texts\nweights = [0.3, 0.3, 0.4]\n# We can leave the prompt empty\nprompt = \"\"\nprior_out = pipe_prior.interpolate(images_texts, weights)\npipe = KandinskyPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-1\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\nimage = pipe(prompt, **prior_out, height=768, width=768).images[0]\nimage.save(\"starry_cat.png\")\nModel Architecture\nOverview\nKandinsky 2.1 is a text-conditional diffusion model based on unCLIP and latent diffusion, composed of a transformer-based image prior model, a unet diffusion model, and a decoder.\nThe model architectures are illustrated in the figure below - the chart on the left describes the process to train the image prior model, the figure in the center is the text-to-image generation process, and the figure on the right is image interpolation.\nSpecifically, the image prior model was trained on CLIP text and image embeddings generated with a pre-trained mCLIP model. The trained image prior model is then used to generate mCLIP image embeddings for input text prompts. Both the input text prompts and its mCLIP image embeddings are used in the diffusion process. A MoVQGAN model acts as the final block of the model, which decodes the latent representation into an actual image.\nDetails\nThe image prior training of the model was performed on the LAION Improved Aesthetics dataset, and then fine-tuning was performed on the LAION HighRes data.\nThe main Text2Image diffusion model was trained on the basis of 170M text-image pairs from the LAION HighRes dataset (an important condition was the presence of images with a resolution of at least 768x768). The use of 170M pairs is due to the fact that we kept the UNet diffusion block from Kandinsky 2.0, which allowed us not to train it from scratch. Further, at the stage of fine-tuning, a dataset of 2M very high-quality high-resolution images with descriptions (COYO, anime, landmarks_russia, and a number of others) was used separately collected from open sources.\nEvaluation\nWe quantitatively measure the performance of Kandinsky 2.1 on the COCO_30k dataset, in zero-shot mode. The table below presents FID.\nFID metric values ‚Äã‚Äãfor generative models on COCO_30k\nFID (30k)\neDiff-I (2022)\n6.95\nImage (2022)\n7.27\nKandinsky 2.1 (2023)\n8.21\nStable Diffusion 2.1 (2022)\n8.59\nGigaGAN, 512x512 (2023)\n9.09\nDALL-E 2 (2022)\n10.39\nGLIDE (2022)\n12.24\nKandinsky 1.0 (2022)\n15.40\nDALL-E (2021)\n17.89\nKandinsky 2.0 (2022)\n20.00\nGLIGEN (2022)\n21.04\nFor more information, please refer to the upcoming technical report.\nBibTex\nIf you find this repository useful in your research, please cite:\n@misc{kandinsky 2.1,\ntitle         = {kandinsky 2.1},\nauthor        = {Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Vladimir Arkhipkin, Igor Pavlov, Andrey Kuznetsov, Denis Dimitrov},\nyear          = {2023},\nhowpublished  = {},\n}",
    "RahulYadav/wav2vec2-xsl-r-300m-hinglish-model": "wav2vec2-xsl-r-300m-hinglish-model\nModel description\nIntended uses & limitations\nTraining and evaluation data\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\nwav2vec2-xsl-r-300m-hinglish-model\nThis model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the None dataset.\nIt achieves the following results on the evaluation set:\nLoss: 53.3109\nWer: 1.0\nModel description\nMore information needed\nIntended uses & limitations\nMore information needed\nTraining and evaluation data\nMore information needed\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 0.0003\ntrain_batch_size: 64\neval_batch_size: 32\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nlr_scheduler_warmup_steps: 500\nnum_epochs: 30\nmixed_precision_training: Native AMP\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\nWer\n53.5422\n2.0\n2\n54.4465\n1.0\n52.8519\n4.0\n4\n54.4457\n1.0\n52.7079\n6.0\n6\n54.4429\n1.0\n52.9959\n8.0\n8\n54.4348\n1.0\n53.5864\n10.0\n10\n54.4155\n1.0\n54.2708\n12.0\n12\n54.3822\n1.0\n52.6333\n14.0\n14\n54.3357\n1.0\n55.1505\n16.0\n16\n54.2576\n1.0\n53.6833\n18.0\n18\n54.2131\n1.0\n62.8162\n20.0\n20\n54.1127\n1.0\n54.0794\n22.0\n22\n53.9824\n1.0\n52.5195\n24.0\n24\n53.8243\n1.0\n51.6922\n26.0\n26\n53.6767\n1.0\n51.0235\n28.0\n28\n53.5179\n1.0\n51.0729\n30.0\n30\n53.3109\n1.0\nFramework versions\nTransformers 4.28.1\nPytorch 2.0.1+cu117\nDatasets 2.12.0\nTokenizers 0.13.3",
    "kalpeshk2011/instruct-llama-7b-wdiff": "This is the HuggingFace model release of the instruction tuned LLAMA-7B model used in our paper FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation.\nPlease refer to the README for instructions on how to setup the model (link).\nCredits to Yizhong Wang for originally training this model.",
    "MayaPH/FinOPT-Franklin": "ü§ó FinOPT-Franklin\nModel Description\nFinOPT Series\nIntended Use\nUsage\nLimitations and Caveats\nTraining Data\nEthical Considerations\nFurther Information\nDisclaimer\nAcknowledgments\nOpen LLM Leaderboard Evaluation Results\nü§ó FinOPT-Franklin\nReleased June 1, 2023\nModel Description\nFinOPT-Franklin is a language model based on the OPT-1.3B architecture, which has been fine-tuned on a financial question-answering dataset. The model aims to provide accurate and informative responses to financial-related questions.\nFinOPT Series\nThe FinOPT series of language models come in various model sizes. Kindly refer to this Huggingface Hub link to see the other checkpoints of FinOPT.\nModel Name\nParameter Size\nFinOPT-Franklin\n1.3B\nFinOPT-Lincoln\n350M\nFinOPT-Washington\n125M\nIntended Use\nFinOPT-Franklin is designed to assist users in obtaining relevant and reliable information about financial topics. It can be used as a tool for performing question-answering tasks in the financial domain, including banking queries, investment advice, and general financial inquiries.\nThe model is intended to be used by individuals seeking information about financial topics, as well as developers and researchers working on natural language processing (NLP) tasks in the financial domain.\nUsage\nTo use FinOPT-Franklin, you are required to provide attribution in accordance with the Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license. Please include the following attribution notice when utilizing FinOPT-Franklin in your work:\n# This code uses FinOPT-Franklin, a language model developed by Maya Philippines.\n# The model is licensed under the Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n# For more information, visit: https://creativecommons.org/licenses/by-sa/4.0/\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"MayaPH/FinOPT-Franklin\")\nmodel = AutoModelForCausalLM.from_pretrained(\"MayaPH/FinOPT-Franklin\")\nPlease ensure that you include the relevant attribution notice in your code or any other form of usage to comply with the license terms.\nLimitations and Caveats\nWhile FinOPT-Franklin has been fine-tuned on a financial question-answering dataset, it is important to note the following limitations and caveats:\nDomain-Specific Focus: The model's training data primarily consists of financial questions and answers from the financial QA dataset. It may not perform as well on questions outside the financial domain.\nPotential Bias: The model may reflect biases present in the training data. It is crucial to carefully evaluate and interpret the model's responses, particularly on sensitive topics such as investment advice or financial recommendations.\nConfidence and Verification: The model generates responses based on patterns learned from the training data, but it does not have inherent fact-checking capabilities. Users should verify the information provided by the model from reliable sources before making any financial decisions.\nTraining Data\nFinOPT-Franklin was trained on a financial question-answering dataset, which consists of questions and answers related to various financial topics. The dataset was collected from online sources and financial forums, and manually handcrafted.\nEthical Considerations\nWhen using FinOPT-Franklin, it is important to consider the following ethical considerations:\nPrivacy and Security: Avoid sharing sensitive personal or financial information while interacting with the model. The model does not have privacy safeguards, so exercise caution when discussing personal or confidential matters.\nFairness and Bias: The model's responses may reflect biases present in the training data. Be aware of potential biases and make an effort to evaluate responses critically and fairly.\nTransparency: The model operates as a predictive text generator based on patterns learned from the training data. The model's inner workings and the specific training data used are proprietary and not publicly available.\nUser Responsibility: Users should take responsibility\nfor their own financial decisions and not solely rely on the information provided by the model. Consult with financial professionals or reliable sources for specific financial advice or recommendations.\nFurther Information\nFor additional information or inquiries about FinOPT-Franklin, please contact the Maya Philippines iOps Team via jasper.catapang@maya.ph.\nDisclaimer\nFinOPT-Franklin is an AI language model trained by Maya Philippines. It is provided \"as is\" without warranty of any kind, express or implied. The model developers and Maya Philippines shall not be liable for any direct or indirect damages arising from the use of this model.\nAcknowledgments\nThe development of FinOPT-Franklin was made possible by Maya Philippines and the curation and creation of the financial question-answering dataset.\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n25.54\nARC (25-shot)\n27.73\nHellaSwag (10-shot)\n24.91\nMMLU (5-shot)\n23.12\nTruthfulQA (0-shot)\n52.4\nWinogrande (5-shot)\n50.51\nGSM8K (5-shot)\n0.0\nDROP (3-shot)\n0.1",
    "xiaolxl/GuoFengRealMix": "‰ªãÁªç - GuoFengRealMix\nÂÆâË£ÖÊïôÁ®ã - install\nÂ¶Ç‰Ωï‰ΩøÁî® - How to use\n‰æãÂõæ - Examples\n‰ªãÁªç - GuoFengRealMix\nÊ¨¢Ëøé‰ΩøÁî®GuoFengRealMixÊ®°ÂûãÔºåËøôÊòØ‰∏Ä‰∏™Âü∫‰∫éGuoFengÁ≥ªÂàóÊ®°ÂûãËûçÂêàÁöÑÂÖ∑ÊúâÁúüÂÆûË¥®ÊÑü‰∏éÈÄÇÂ∫îÂ§öÁßçÈ£éÊ†º„ÄÅÊúçË£Ö„ÄÅÂú∫ÊôØÁöÑÂ§ßÊ®°Âûã„ÄÇ - Welcome to the GuoFengRealMix model, which is a large model based on the fusion of GuoFeng series models with realistic texture and adaptability to various styles, clothing, and scenes.\nÂÆâË£ÖÊïôÁ®ã - install\nÂ∞ÜGuoFengRealMix.safetensorsÊ®°ÂûãÊîæÂÖ•SDÁõÆÂΩï - Put GuoFengRealMix.safetensors model into SD directory\nËØ∑ËÆ∞ÂæóÈÄâÊã©‰ªªÊÑè‰∏Ä‰∏™VAEÊñá‰ª∂ÔºåÂê¶ÂàôÂõæÂΩ¢Â∞ÜÂèØËÉΩ‰∏∫ÁÅ∞Ëâ≤ - Please remember to select any VAE file, otherwise the graphics may be grayed out\nÂ¶Ç‰Ωï‰ΩøÁî® - How to use\nVAEÂª∫ËÆÆ‰ΩøÁî®840000-ema-pruned - VAE recommends using 840000 ema run\nÂ¶ÇÊûú‰Ω†ÁöÑÂá∫ÂõæÂÖ®Ë∫´ÂõæÊó∂Âá∫Áé∞ËÑ∏ÈÉ®Â¥©ÂùèÂª∫ËÆÆÂà†Èô§full bodyÂÖ≥ÈîÆËØçÊàñËÄÖ‰ΩøÁî®ËÑ∏ÈÉ®Ëá™Âä®‰øÆÂ§çÊèí‰ª∂Ôºö\nÂõΩÂ§ñÊ∫êÂú∞ÂùÄÔºöhttps://github.com/ototadana/sd-face-editor.git\nÂõΩÂÜÖÂä†ÈÄüÂú∞ÂùÄÔºöhttps://jihulab.com/xiaolxl_pub/sd-face-editor.git\n======\nIf you experience facial collapse during the full body image, it is recommended to delete the full body keyword or use the facial automatic repair plugin:\nForeign source address: https://github.com/ototadana/sd-face-editor.git\nDomestic acceleration address: https://jihulab.com/xiaolxl_pub/sd-face-editor.git\nÂèØÁî®Ë¥üÈù¢ËØç Available Negative Words - ÊÑüË∞¢Áæ§ÂèãÊèê‰æõÁöÑË¥üÈù¢ËØç:\n(((simple background))),monochrome ,lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, lowres, bad anatomy, bad hands, text, error, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, ugly,pregnant,vore,duplicate,morbid,mut ilated,tran nsexual, hermaphrodite,long neck,mutated hands,poorly drawn hands,poorly drawn face,mutation,deformed,blurry,bad anatomy,bad proportions,malformed limbs,extra limbs,cloned face,disfigured,gross proportions, (((missing arms))),(((missing legs))), (((extra arms))),(((extra legs))),pubic hair, plump,bad legs,error legs,username,blurry,bad feet\n‰æãÂõæ - Examples\n(ÂèØÂú®Êñá‰ª∂ÂàóË°®‰∏≠ÊâæÂà∞ÂéüÂõæÔºåÂπ∂ÊîæÂÖ•WebUiÊü•ÁúãÂÖ≥ÈîÆËØçÁ≠â‰ø°ÊÅØ) - (You can find the original image in the file list, and put WebUi to view keywords and other information)",
    "facebook/mms-1b-all": "Massively Multilingual Speech (MMS) - Finetuned ASR - ALL\nTable Of Content\nExample\nSupported Languages\nModel details\nAdditional Links\nMassively Multilingual Speech (MMS) - Finetuned ASR - ALL\nThis checkpoint is a model fine-tuned for multi-lingual ASR and part of Facebook's Massive Multilingual Speech project.\nThis checkpoint is based on the Wav2Vec2 architecture and makes use of adapter models to transcribe 1000+ languages.\nThe checkpoint consists of 1 billion parameters and has been fine-tuned from facebook/mms-1b on 1162 languages.\nTable Of Content\nExample\nSupported Languages\nModel details\nAdditional links\nExample\nThis MMS checkpoint can be used with Transformers to transcribe audio of 1107 different\nlanguages. Let's look at a simple example.\nFirst, we install transformers and some other libraries\npip install torch accelerate torchaudio datasets\npip install --upgrade transformers\nNote: In order to use MMS you need to have at least transformers >= 4.30 installed. If the 4.30 version\nis not yet available on PyPI make sure to install transformers from\nsource:\npip install git+https://github.com/huggingface/transformers.git\nNext, we load a couple of audio samples via datasets. Make sure that the audio data is sampled to 16000 kHz.\nfrom datasets import load_dataset, Audio\n# English\nstream_data = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"en\", split=\"test\", streaming=True)\nstream_data = stream_data.cast_column(\"audio\", Audio(sampling_rate=16000))\nen_sample = next(iter(stream_data))[\"audio\"][\"array\"]\n# French\nstream_data = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"fr\", split=\"test\", streaming=True)\nstream_data = stream_data.cast_column(\"audio\", Audio(sampling_rate=16000))\nfr_sample = next(iter(stream_data))[\"audio\"][\"array\"]\nNext, we load the model and processor\nfrom transformers import Wav2Vec2ForCTC, AutoProcessor\nimport torch\nmodel_id = \"facebook/mms-1b-all\"\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = Wav2Vec2ForCTC.from_pretrained(model_id)\nNow we process the audio data, pass the processed audio data to the model and transcribe the model output, just like we usually do for Wav2Vec2 models such as facebook/wav2vec2-base-960h\ninputs = processor(en_sample, sampling_rate=16_000, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model(**inputs).logits\nids = torch.argmax(outputs, dim=-1)[0]\ntranscription = processor.decode(ids)\n# 'joe keton disapproved of films and buster also had reservations about the media'\nWe can now keep the same model in memory and simply switch out the language adapters by calling the convenient load_adapter() function for the model and set_target_lang() for the tokenizer. We pass the target language as an input - \"fra\" for French.\nprocessor.tokenizer.set_target_lang(\"fra\")\nmodel.load_adapter(\"fra\")\ninputs = processor(fr_sample, sampling_rate=16_000, return_tensors=\"pt\")\nwith torch.no_grad():\noutputs = model(**inputs).logits\nids = torch.argmax(outputs, dim=-1)[0]\ntranscription = processor.decode(ids)\n# \"ce dernier est vol√© tout au long de l'histoire romaine\"\nIn the same way the language can be switched out for all other supported languages. Please have a look at:\nprocessor.tokenizer.vocab.keys()\nFor more details, please have a look at the official docs.\nSupported Languages\nThis model supports 1162 languages. Unclick the following to toogle all supported languages of this checkpoint in ISO 639-3 code.\nYou can find more details about the languages and their ISO 649-3 codes in the MMS Language Coverage Overview.\nClick to toggle\nabi\nabk\nabp\naca\nacd\nace\nacf\nach\nacn\nacr\nacu\nade\nadh\nadj\nadx\naeu\nafr\nagd\nagg\nagn\nagr\nagu\nagx\naha\nahk\naia\naka\nakb\nake\nakp\nalj\nalp\nalt\nalz\name\namf\namh\nami\namk\nann\nany\naoz\napb\napr\nara\narl\nasa\nasg\nasm\nast\nata\natb\natg\nati\natq\nava\navn\navu\nawa\nawb\nayo\nayr\nayz\nazb\nazg\nazj-script_cyrillic\nazj-script_latin\nazz\nbak\nbam\nban\nbao\nbas\nbav\nbba\nbbb\nbbc\nbbo\nbcc-script_arabic\nbcc-script_latin\nbcl\nbcw\nbdg\nbdh\nbdq\nbdu\nbdv\nbeh\nbel\nbem\nben\nbep\nbex\nbfa\nbfo\nbfy\nbfz\nbgc\nbgq\nbgr\nbgt\nbgw\nbha\nbht\nbhz\nbib\nbim\nbis\nbiv\nbjr\nbjv\nbjw\nbjz\nbkd\nbkv\nblh\nblt\nblx\nblz\nbmq\nbmr\nbmu\nbmv\nbng\nbno\nbnp\nboa\nbod\nboj\nbom\nbor\nbos\nbov\nbox\nbpr\nbps\nbqc\nbqi\nbqj\nbqp\nbre\nbru\nbsc\nbsq\nbss\nbtd\nbts\nbtt\nbtx\nbud\nbul\nbus\nbvc\nbvz\nbwq\nbwu\nbyr\nbzh\nbzi\nbzj\ncaa\ncab\ncac-dialect_sanmateoixtatan\ncac-dialect_sansebastiancoatan\ncak-dialect_central\ncak-dialect_santamariadejesus\ncak-dialect_santodomingoxenacoj\ncak-dialect_southcentral\ncak-dialect_western\ncak-dialect_yepocapa\ncap\ncar\ncas\ncat\ncax\ncbc\ncbi\ncbr\ncbs\ncbt\ncbu\ncbv\ncce\ncco\ncdj\nceb\nceg\ncek\nces\ncfm\ncgc\nche\nchf\nchv\nchz\ncjo\ncjp\ncjs\nckb\ncko\nckt\ncla\ncle\ncly\ncme\ncmn-script_simplified\ncmo-script_khmer\ncmo-script_latin\ncmr\ncnh\ncni\ncnl\ncnt\ncoe\ncof\ncok\ncon\ncot\ncou\ncpa\ncpb\ncpu\ncrh\ncrk-script_latin\ncrk-script_syllabics\ncrn\ncrq\ncrs\ncrt\ncsk\ncso\nctd\nctg\ncto\nctu\ncuc\ncui\ncuk\ncul\ncwa\ncwe\ncwt\ncya\ncym\ndaa\ndah\ndan\ndar\ndbj\ndbq\nddn\nded\ndes\ndeu\ndga\ndgi\ndgk\ndgo\ndgr\ndhi\ndid\ndig\ndik\ndip\ndiv\ndjk\ndnj-dialect_blowowest\ndnj-dialect_gweetaawueast\ndnt\ndnw\ndop\ndos\ndsh\ndso\ndtp\ndts\ndug\ndwr\ndyi\ndyo\ndyu\ndzo\neip\neka\nell\nemp\nenb\neng\nenx\nepo\nese\ness\nest\neus\nevn\newe\neza\nfal\nfao\nfar\nfas\nfij\nfin\nflr\nfmu\nfon\nfra\nfrd\nfry\nful\ngag-script_cyrillic\ngag-script_latin\ngai\ngam\ngau\ngbi\ngbk\ngbm\ngbo\ngde\ngeb\ngej\ngil\ngjn\ngkn\ngld\ngle\nglg\nglk\ngmv\ngna\ngnd\ngng\ngof-script_latin\ngog\ngor\ngqr\ngrc\ngri\ngrn\ngrt\ngso\ngub\nguc\ngud\nguh\nguj\nguk\ngum\nguo\nguq\nguu\ngux\ngvc\ngvl\ngwi\ngwr\ngym\ngyr\nhad\nhag\nhak\nhap\nhat\nhau\nhay\nheb\nheh\nhif\nhig\nhil\nhin\nhlb\nhlt\nhne\nhnn\nhns\nhoc\nhoy\nhrv\nhsb\nhto\nhub\nhui\nhun\nhus-dialect_centralveracruz\nhus-dialect_westernpotosino\nhuu\nhuv\nhvn\nhwc\nhye\nhyw\niba\nibo\nicr\nidd\nifa\nifb\nife\nifk\nifu\nify\nign\nikk\nilb\nilo\nimo\nina\ninb\nind\niou\nipi\niqw\niri\nirk\nisl\nita\nitl\nitv\nixl-dialect_sangasparchajul\nixl-dialect_sanjuancotzal\nixl-dialect_santamarianebaj\nizr\nizz\njac\njam\njav\njbu\njen\njic\njiv\njmc\njmd\njpn\njun\njuy\njvn\nkaa\nkab\nkac\nkak\nkam\nkan\nkao\nkaq\nkat\nkay\nkaz\nkbo\nkbp\nkbq\nkbr\nkby\nkca\nkcg\nkdc\nkde\nkdh\nkdi\nkdj\nkdl\nkdn\nkdt\nkea\nkek\nken\nkeo\nker\nkey\nkez\nkfb\nkff-script_telugu\nkfw\nkfx\nkhg\nkhm\nkhq\nkia\nkij\nkik\nkin\nkir\nkjb\nkje\nkjg\nkjh\nkki\nkkj\nkle\nklu\nklv\nklw\nkma\nkmd\nkml\nkmr-script_arabic\nkmr-script_cyrillic\nkmr-script_latin\nkmu\nknb\nkne\nknf\nknj\nknk\nkno\nkog\nkor\nkpq\nkps\nkpv\nkpy\nkpz\nkqe\nkqp\nkqr\nkqy\nkrc\nkri\nkrj\nkrl\nkrr\nkrs\nkru\nksb\nksr\nkss\nktb\nktj\nkub\nkue\nkum\nkus\nkvn\nkvw\nkwd\nkwf\nkwi\nkxc\nkxf\nkxm\nkxv\nkyb\nkyc\nkyf\nkyg\nkyo\nkyq\nkyu\nkyz\nkzf\nlac\nlaj\nlam\nlao\nlas\nlat\nlav\nlaw\nlbj\nlbw\nlcp\nlee\nlef\nlem\nlew\nlex\nlgg\nlgl\nlhu\nlia\nlid\nlif\nlin\nlip\nlis\nlit\nlje\nljp\nllg\nlln\nlme\nlnd\nlns\nlob\nlok\nlom\nlon\nloq\nlsi\nlsm\nltz\nluc\nlug\nluo\nlwo\nlww\nlzz\nmaa-dialect_sanantonio\nmaa-dialect_sanjeronimo\nmad\nmag\nmah\nmai\nmaj\nmak\nmal\nmam-dialect_central\nmam-dialect_northern\nmam-dialect_southern\nmam-dialect_western\nmaq\nmar\nmaw\nmaz\nmbb\nmbc\nmbh\nmbj\nmbt\nmbu\nmbz\nmca\nmcb\nmcd\nmco\nmcp\nmcq\nmcu\nmda\nmdf\nmdv\nmdy\nmed\nmee\nmej\nmen\nmeq\nmet\nmev\nmfe\nmfh\nmfi\nmfk\nmfq\nmfy\nmfz\nmgd\nmge\nmgh\nmgo\nmhi\nmhr\nmhu\nmhx\nmhy\nmib\nmie\nmif\nmih\nmil\nmim\nmin\nmio\nmip\nmiq\nmit\nmiy\nmiz\nmjl\nmjv\nmkd\nmkl\nmkn\nmlg\nmlt\nmmg\nmnb\nmnf\nmnk\nmnw\nmnx\nmoa\nmog\nmon\nmop\nmor\nmos\nmox\nmoz\nmpg\nmpm\nmpp\nmpx\nmqb\nmqf\nmqj\nmqn\nmri\nmrw\nmsy\nmtd\nmtj\nmto\nmuh\nmup\nmur\nmuv\nmuy\nmvp\nmwq\nmwv\nmxb\nmxq\nmxt\nmxv\nmya\nmyb\nmyk\nmyl\nmyv\nmyx\nmyy\nmza\nmzi\nmzj\nmzk\nmzm\nmzw\nnab\nnag\nnan\nnas\nnaw\nnca\nnch\nncj\nncl\nncu\nndj\nndp\nndv\nndy\nndz\nneb\nnew\nnfa\nnfr\nnga\nngl\nngp\nngu\nnhe\nnhi\nnhu\nnhw\nnhx\nnhy\nnia\nnij\nnim\nnin\nnko\nnlc\nnld\nnlg\nnlk\nnmz\nnnb\nnno\nnnq\nnnw\nnoa\nnob\nnod\nnog\nnot\nnpi\nnpl\nnpy\nnso\nnst\nnsu\nntm\nntr\nnuj\nnus\nnuz\nnwb\nnxq\nnya\nnyf\nnyn\nnyo\nnyy\nnzi\nobo\noci\nojb-script_latin\nojb-script_syllabics\noku\nold\nomw\nonb\nood\norm\nory\noss\note\notq\nozm\npab\npad\npag\npam\npan\npao\npap\npau\npbb\npbc\npbi\npce\npcm\npeg\npez\npib\npil\npir\npis\npjt\npkb\npls\nplw\npmf\npny\npoh-dialect_eastern\npoh-dialect_western\npoi\npol\npor\npoy\nppk\npps\nprf\nprk\nprt\npse\npss\nptu\npui\npus\npwg\npww\npxm\nqub\nquc-dialect_central\nquc-dialect_east\nquc-dialect_north\nquf\nquh\nqul\nquw\nquy\nquz\nqvc\nqve\nqvh\nqvm\nqvn\nqvo\nqvs\nqvw\nqvz\nqwh\nqxh\nqxl\nqxn\nqxo\nqxr\nrah\nrai\nrap\nrav\nraw\nrej\nrel\nrgu\nrhg\nrif-script_arabic\nrif-script_latin\nril\nrim\nrjs\nrkt\nrmc-script_cyrillic\nrmc-script_latin\nrmo\nrmy-script_cyrillic\nrmy-script_latin\nrng\nrnl\nroh-dialect_sursilv\nroh-dialect_vallader\nrol\nron\nrop\nrro\nrub\nruf\nrug\nrun\nrus\nsab\nsag\nsah\nsaj\nsaq\nsas\nsat\nsba\nsbd\nsbl\nsbp\nsch\nsck\nsda\nsea\nseh\nses\nsey\nsgb\nsgj\nsgw\nshi\nshk\nshn\nsho\nshp\nsid\nsig\nsil\nsja\nsjm\nsld\nslk\nslu\nslv\nsml\nsmo\nsna\nsnd\nsne\nsnn\nsnp\nsnw\nsom\nsoy\nspa\nspp\nspy\nsqi\nsri\nsrm\nsrn\nsrp-script_cyrillic\nsrp-script_latin\nsrx\nstn\nstp\nsuc\nsuk\nsun\nsur\nsus\nsuv\nsuz\nswe\nswh\nsxb\nsxn\nsya\nsyl\nsza\ntac\ntaj\ntam\ntao\ntap\ntaq\ntat\ntav\ntbc\ntbg\ntbk\ntbl\ntby\ntbz\ntca\ntcc\ntcs\ntcz\ntdj\nted\ntee\ntel\ntem\nteo\nter\ntes\ntew\ntex\ntfr\ntgj\ntgk\ntgl\ntgo\ntgp\ntha\nthk\nthl\ntih\ntik\ntir\ntkr\ntlb\ntlj\ntly\ntmc\ntmf\ntna\ntng\ntnk\ntnn\ntnp\ntnr\ntnt\ntob\ntoc\ntoh\ntom\ntos\ntpi\ntpm\ntpp\ntpt\ntrc\ntri\ntrn\ntrs\ntso\ntsz\nttc\ntte\nttq-script_tifinagh\ntue\ntuf\ntuk-script_arabic\ntuk-script_latin\ntuo\ntur\ntvw\ntwb\ntwe\ntwu\ntxa\ntxq\ntxu\ntye\ntzh-dialect_bachajon\ntzh-dialect_tenejapa\ntzj-dialect_eastern\ntzj-dialect_western\ntzo-dialect_chamula\ntzo-dialect_chenalho\nubl\nubu\nudm\nudu\nuig-script_arabic\nuig-script_cyrillic\nukr\numb\nunr\nupv\nura\nurb\nurd-script_arabic\nurd-script_devanagari\nurd-script_latin\nurk\nurt\nury\nusp\nuzb-script_cyrillic\nuzb-script_latin\nvag\nvid\nvie\nvif\nvmw\nvmy\nvot\nvun\nvut\nwal-script_ethiopic\nwal-script_latin\nwap\nwar\nwaw\nway\nwba\nwlo\nwlx\nwmw\nwob\nwol\nwsg\nwwa\nxal\nxdy\nxed\nxer\nxho\nxmm\nxnj\nxnr\nxog\nxon\nxrb\nxsb\nxsm\nxsr\nxsu\nxta\nxtd\nxte\nxtm\nxtn\nxua\nxuo\nyaa\nyad\nyal\nyam\nyao\nyas\nyat\nyaz\nyba\nybb\nycl\nycn\nyea\nyka\nyli\nyor\nyre\nyua\nyue-script_traditional\nyuz\nyva\nzaa\nzab\nzac\nzad\nzae\nzai\nzam\nzao\nzaq\nzar\nzas\nzav\nzaw\nzca\nzga\nzim\nziw\nzlm\nzmz\nzne\nzos\nzpc\nzpg\nzpi\nzpl\nzpm\nzpo\nzpt\nzpu\nzpz\nztq\nzty\nzul\nzyb\nzyp\nzza\nModel details\nDeveloped by: Vineel Pratap et al.\nModel type: Multi-Lingual Automatic Speech Recognition model\nLanguage(s): 1000+ languages, see supported languages\nLicense: CC-BY-NC 4.0 license\nNum parameters: 1 billion\nAudio sampling rate: 16,000 kHz\nCite as:\n@article{pratap2023mms,\ntitle={Scaling Speech Technology to 1,000+ Languages},\nauthor={Vineel Pratap and Andros Tjandra and Bowen Shi and Paden Tomasello and Arun Babu and Sayani Kundu and Ali Elkahky and Zhaoheng Ni and Apoorv Vyas and Maryam Fazel-Zarandi and Alexei Baevski and Yossi Adi and Xiaohui Zhang and Wei-Ning Hsu and Alexis Conneau and Michael Auli},\njournal={arXiv},\nyear={2023}\n}\nAdditional Links\nBlog post\nTransformers documentation.\nPaper\nGitHub Repository\nOther MMS checkpoints\nMMS base checkpoints:\nfacebook/mms-1b\nfacebook/mms-300m\nOfficial Space",
    "cyberdelia/CyberRealistic": "CyberRealistic\nüß† Model Details\n‚ú® Features\nüõ†Ô∏è Recommended Settings\nüßæ Example Prompts\nüì∏ Example Outputs\nüîó Links\nüö´ Limitations\n‚úÖ License\nCyberRealistic\nCyberRealistic is a photorealistic image generation model built upon the Stable Diffusion 1.5 (SD 1.5) architecture. Developed by Cyberdelia, this model excels at producing lifelike portraits and scenes with minimal prompt engineering, making it ideal for creators seeking high-quality outputs without complex configurations.\nüß† Model Details\nModel Type: Text-to-Image Generation\nBase Model: Stable Diffusion 1.5 (SD 1.5)\nFormat: safetensors\nVersion: v8.0\nCreator: Cyberdelia\nFile Size: ~6.3 GB\nLicense: CreativeML Open RAIL++-M License\n‚ú® Features\nPhotorealism: Generates highly detailed and realistic images, particularly effective for human subjects.\nEase of Use: Designed for simplicity; achieves impressive results with straightforward prompts.\nIntegrated VAE: Comes with a baked-in Variational Autoencoder for enhanced image quality.\nVersatility: Suitable for various applications, including portraits, fashion, and cinematic scenes.\nüõ†Ô∏è Recommended Settings\nParameter\nRecommended Value\nSampling Steps\n25‚Äì30\nSampler\nDPM++ 2M Karras / Euler A\nResolution\n512x512 (native), higher with Hires.fix\nHires Upscaler\n4x_NMKD-Siax_200k or 4x_NickelbackFS_72000_G\nCFG Scale\n7.0‚Äì8.0\nVAE\nAlready baked-in\nüßæ Example Prompts\n(masterpiece, best quality), ultra-detailed, realistic photo of a 22-year-old woman, natural lighting, depth of field, candid moment, color graded, RAW photo, soft cinematic bokeh\n(masterpiece, photorealistic), editorial fashion photo, close-up, dramatic side lighting, textured skin, shallow depth of field, soft shadows\nüì∏ Example Outputs\nüîó Links\nCivitai Model Page\nüö´ Limitations\nMay produce content that could be considered sensitive; use responsibly.\nSome prompts involving abstract or anime content may not perform as well due to realism-focused training.\nLighting and skin may occasionally be too clean or smooth depending on sampling choices.\n‚úÖ License\nThis model is distributed under the CreativeML Open RAIL++-M License, which allows commercial and non-commercial use, with proper credit and no malicious usage.\nLicense details",
    "kaczmarj/prostate-tumor-resnet34.tcga-prad": "A prostate cancer tumor classification model. This model is a ResNet34 and was trained on TCGA PRAD.\nPlease refer to the original GitHub Repo\nThis model outputs three classes:\nGleason 3\nGleason 4 or 5\nBenign",
    "TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ": "Wizard Vicuna 30B Uncensored - GPTQ\nDescription\nRepositories available\nPrompt template: Vicuna\nProvided files and GPTQ parameters\nHow to download from branches\nHow to easily download and use this model in text-generation-webui.\nHow to use this GPTQ model from Python code\nInstall the necessary packages\nFor CodeLlama models only: you must use Transformers 4.33.0 or later.\nYou can then use the following code\nCompatibility\nDiscord\nThanks, and how to contribute\nOriginal model card: Eric Hartford's Wizard-Vicuna-30B-Uncensored\nEric Hartford's Wizard-Vicuna-30B-Uncensored GPTQ\nRepositories available\nDiscord\nThanks, and how to contribute.\nOriginal model card\nChat & support: TheBloke's Discord server\nWant to contribute? TheBloke's Patreon page\nTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)\nWizard Vicuna 30B Uncensored - GPTQ\nModel creator: Eric Hartford\nOriginal model: Wizard Vicuna 30B Uncensored\nDescription\nThis repo contains GPTQ model files for Eric Hartford's Wizard-Vicuna-30B-Uncensored.\nMultiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.\nRepositories available\nAWQ model(s) for GPU inference.\nGPTQ models for GPU inference, with multiple quantisation parameter options.\n2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference\nEric Hartford's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions\nPrompt template: Vicuna\nA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {prompt} ASSISTANT:\nProvided files and GPTQ parameters\nMultiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.\nEach separate quant is in a different branch.  See below for instructions on fetching from different branches.\nAll recent GPTQ files are made with AutoGPTQ, and all files in non-main branches are made with AutoGPTQ. Files in the main branch which were uploaded before August 2023 were made with GPTQ-for-LLaMa.\nExplanation of GPTQ parameters\nBits: The bit size of the quantised model.\nGS: GPTQ group size. Higher numbers use less VRAM, but have lower quantisation accuracy. \"None\" is the lowest possible value.\nAct Order: True or False. Also known as desc_act. True results in better quantisation accuracy. Some GPTQ clients have had issues with models that use Act Order plus Group Size, but this is generally resolved now.\nDamp %: A GPTQ parameter that affects how samples are processed for quantisation. 0.01 is default, but 0.1 results in slightly better accuracy.\nGPTQ dataset: The dataset used for quantisation. Using a dataset more appropriate to the model's training can improve quantisation accuracy. Note that the GPTQ dataset is not the same as the dataset used to train the model - please refer to the original model repo for details of the training dataset(s).\nSequence Length: The length of the dataset sequences used for quantisation. Ideally this is the same as the model sequence length. For some very long sequence models (16+K), a lower sequence length may have to be used.  Note that a lower sequence length does not limit the sequence length of the quantised model. It only impacts the quantisation accuracy on longer inference sequences.\nExLlama Compatibility: Whether this file can be loaded with ExLlama, which currently only supports Llama models in 4-bit.\nBranch\nBits\nGS\nAct Order\nDamp %\nGPTQ Dataset\nSeq Len\nSize\nExLlama\nDesc\nmain\n4\nNone\nYes\n0.01\nwikitext\n2048\n16.94 GB\nYes\n4-bit, with Act Order. No group size, to lower VRAM requirements.\ngptq-4bit-32g-actorder_True\n4\n32\nYes\n0.01\nwikitext\n2048\n19.44 GB\nYes\n4-bit, with Act Order and group size 32g. Gives highest possible inference quality, with maximum VRAM usage.\ngptq-4bit-64g-actorder_True\n4\n64\nYes\n0.01\nwikitext\n2048\n18.18 GB\nYes\n4-bit, with Act Order and group size 64g. Uses less VRAM than 32g, but with slightly lower accuracy.\ngptq-4bit-128g-actorder_True\n4\n128\nYes\n0.01\nwikitext\n2048\n17.55 GB\nYes\n4-bit, with Act Order and group size 128g. Uses even less VRAM than 64g, but with slightly lower accuracy.\ngptq-8bit--1g-actorder_True\n8\nNone\nYes\n0.01\nwikitext\n2048\n32.99 GB\nNo\n8-bit, with Act Order. No group size, to lower VRAM requirements.\ngptq-8bit-128g-actorder_False\n8\n128\nNo\n0.01\nwikitext\n2048\n33.73 GB\nNo\n8-bit, with group size 128g for higher inference quality and without Act Order to improve AutoGPTQ speed.\ngptq-3bit--1g-actorder_True\n3\nNone\nYes\n0.01\nwikitext\n2048\n12.92 GB\nNo\n3-bit, with Act Order and no group size. Lowest possible VRAM requirements. May be lower quality than 3-bit 128g.\ngptq-3bit-128g-actorder_False\n3\n128\nNo\n0.01\nwikitext\n2048\n13.51 GB\nNo\n3-bit, with group size 128g but no act-order. Slightly higher VRAM requirements than 3-bit None.\nHow to download from branches\nIn text-generation-webui, you can add :branch to the end of the download name, eg TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ:main\nWith Git, you can clone a branch with:\ngit clone --single-branch --branch main https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\nIn Python Transformers code, the branch is the revision parameter; see below.\nHow to easily download and use this model in text-generation-webui.\nPlease make sure you're using the latest version of text-generation-webui.\nIt is strongly recommended to use the text-generation-webui one-click-installers unless you're sure you know how to make a manual install.\nClick the Model tab.\nUnder Download custom model or LoRA, enter TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ.\nTo download from a specific branch, enter for example TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ:main\nsee Provided Files above for the list of branches for each option.\nClick Download.\nThe model will start downloading. Once it's finished it will say \"Done\".\nIn the top left, click the refresh icon next to Model.\nIn the Model dropdown, choose the model you just downloaded: Wizard-Vicuna-30B-Uncensored-GPTQ\nThe model will automatically load, and is now ready for use!\nIf you want any custom settings, set them and then click Save settings for this model followed by Reload the Model in the top right.\nNote that you do not need to and should not set manual GPTQ parameters any more. These are set automatically from the file quantize_config.json.\nOnce you're ready, click the Text Generation tab and enter a prompt to get started!\nHow to use this GPTQ model from Python code\nInstall the necessary packages\nRequires: Transformers 4.32.0 or later, Optimum 1.12.0 or later, and AutoGPTQ 0.4.2 or later.\npip3 install transformers>=4.32.0 optimum>=1.12.0\npip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7\nIf you have problems installing AutoGPTQ using the pre-built wheels, install it from source instead:\npip3 uninstall -y auto-gptq\ngit clone https://github.com/PanQiWei/AutoGPTQ\ncd AutoGPTQ\npip3 install .\nFor CodeLlama models only: you must use Transformers 4.33.0 or later.\nIf 4.33.0 is not yet released when you read this, you will need to install Transformers from source:\npip3 uninstall -y transformers\npip3 install git+https://github.com/huggingface/transformers.git\nYou can then use the following code\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nmodel_name_or_path = \"TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\"\n# To use a different branch, change revision\n# For example: revision=\"main\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\ndevice_map=\"auto\",\ntrust_remote_code=False,\nrevision=\"main\")\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\nprompt = \"Tell me about AI\"\nprompt_template=f'''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {prompt} ASSISTANT:\n'''\nprint(\"\\n\\n*** Generate:\")\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n# Inference can also be done using transformers' pipeline\nprint(\"*** Pipeline:\")\npipe = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\nmax_new_tokens=512,\ndo_sample=True,\ntemperature=0.7,\ntop_p=0.95,\ntop_k=40,\nrepetition_penalty=1.1\n)\nprint(pipe(prompt_template)[0]['generated_text'])\nCompatibility\nThe files provided are tested to work with AutoGPTQ, both via Transformers and using AutoGPTQ directly. They should also work with Occ4m's GPTQ-for-LLaMa fork.\nExLlama is compatible with Llama models in 4-bit. Please see the Provided Files table above for per-file compatibility.\nHuggingface Text Generation Inference (TGI) is compatible with all GPTQ models.\nDiscord\nFor further support, and discussions on these models and AI in general, join us at:\nTheBloke AI's Discord server\nThanks, and how to contribute\nThanks to the chirper.ai team!\nThanks to Clay from gpus.llm-utils.org!\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\nPatreon: https://patreon.com/TheBlokeAI\nKo-Fi: https://ko-fi.com/TheBlokeAI\nSpecial thanks to: Aemon Algiz.\nPatreon special mentions: Alicia Loh, Stephen Murray, K, Ajan Kanaga, RoA, Magnesian, Deo Leter, Olakabola, Eugene Pentland, zynix, Deep Realms, Raymond Fosdick, Elijah Stavena, Iucharbius, Erik Bj√§reholt, Luis Javier Navarrete Lozano, Nicholas, theTransient, John Detwiler, alfie_i, knownsqashed, Mano Prime, Willem Michiel, Enrico Ros, LangChain4j, OG, Michael Dempsey, Pierre Kircher, Pedro Madruga, James Bentley, Thomas Belote, Luke @flexchar, Leonard Tan, Johann-Peter Hartmann, Illia Dulskyi, Fen Risland, Chadd, S_X, Jeff Scroggin, Ken Nordquist, Sean Connelly, Artur Olbinski, Swaroop Kallakuri, Jack West, Ai Maven, David Ziegler, Russ Johnson, transmissions 11, John Villwock, Alps Aficionado, Clay Pascal, Viktor Bowallius, Subspace Studios, Rainer Wilmers, Trenton Dambrowitz, vamX, Michael Levine, Ï§ÄÍµê ÍπÄ, Brandon Frisco, Kalila, Trailburnt, Randy H, Talal Aujan, Nathan Dryer, Vadim, ÈòøÊòé, ReadyPlayerEmma, Tiffany J. Kim, George Stoitzev, Spencer Kim, Jerry Meng, Gabriel Tamborski, Cory Kujawski, Jeffrey Morgan, Spiking Neurons AB, Edmond Seymore, Alexandros Triantafyllidis, Lone Striker, Cap'n Zoog, Nikolai Manek, danny, ya boyyy, Derek Yates, usrbinkat, Mandus, TL, Nathan LeClaire, subjectnull, Imad Khwaja, webtim, Raven Klaugh, Asp the Wyvern, Gabriel Puliatti, Caitlyn Gatomon, Joseph William Delisle, Jonathan Leane, Luke Pendergrass, SuperWojo, Sebastain Graf, Will Dee, Fred von Graf, Andrey, Dan Guido, Daniel P. Andersen, Nitin Borwankar, Elle, Vitor Caleffi, biorpg, jjj, NimbleBox.ai, Pieter, Matthew Berman, terasurfer, Michael Davis, Alex, Stanislav Ovsiannikov\nThank you to all my generous patrons and donaters!\nAnd thank you again to a16z for their generous grant.\nOriginal model card: Eric Hartford's Wizard-Vicuna-30B-Uncensored\nChat & support: my new Discord server\nWant to contribute? TheBloke's Patreon page\nEric Hartford's Wizard-Vicuna-30B-Uncensored GPTQ\nThis is an fp16 models of Eric Hartford's Wizard-Vicuna 30B.\nIt is the result of converting Eric's original fp32 upload to fp16.\nRepositories available\n4bit GPTQ models for GPU inference.\n4bit and 5bit GGML models for CPU inference.\nfloat16 HF format model for GPU inference and further conversions.\nDiscord\nFor further support, and discussions on these models and AI in general, join us at:\nTheBloke AI's Discord server\nThanks, and how to contribute.\nThanks to the chirper.ai team!\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\nPatreon: https://patreon.com/TheBlokeAI\nKo-Fi: https://ko-fi.com/TheBlokeAI\nPatreon special mentions: Aemon Algiz, Dmitriy Samsonov, Nathan LeClaire, Trenton Dambrowitz, Mano Prime, David Flickinger, vamX, Nikolai Manek, senxiiz, Khalefa Al-Ahmad, Illia Dulskyi, Jonathan Leane, Talal Aujan, V. Lukas, Joseph William Delisle, Pyrater, Oscar Rangel, Lone Striker, Luke Pendergrass, Eugene Pentland, Sebastain Graf, Johann-Peter Hartman.\nThank you to all my generous patrons and donaters!\nOriginal model card\nThis is wizard-vicuna-13b trained with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.\nShout out to the open source AI/ML community, and everyone who helped me out.\nNote:\nAn uncensored model has no guardrails.\nYou are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.\nPublishing anything this model generates is the same as publishing it yourself.\nYou are responsible for the content you publish, and you cannot blame the model any more than you can blame the knife, gun, lighter, or car for what you do with it.",
    "Xenova/whisper-medium": "Usage (Transformers.js)\nhttps://huggingface.co/openai/whisper-medium with ONNX weights to be compatible with Transformers.js.\nUsage (Transformers.js)\nIf you haven't already, you can install the Transformers.js JavaScript library from NPM using:\nnpm i @huggingface/transformers\nExample: Transcribe audio from a URL.\nimport { pipeline } from '@huggingface/transformers';\nconst transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-medium');\nconst url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav';\nconst output = await transcriber(url);\nNote: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using ü§ó Optimum and structuring your repo like this one (with ONNX weights located in a subfolder named onnx).",
    "stablediffusionapi/epicrealism": "epiCRealism V1 API Inference\nGet API Key\nepiCRealism V1 API Inference\nGet API Key\nGet API key from ModelsLab, No Payment needed.\nReplace Key in below code, change model_id  to \"epicrealism\"\nCoding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs\nTry model for free: Generate Images\nModel link: View model\nCredits: View credits\nView all models: View Models\nimport requests\nimport json\nurl =  \"https://stablediffusionapi.com/api/v3/dreambooth\"\npayload = json.dumps({\n\"key\":  \"your_api_key\",\n\"model_id\":  \"epicrealism\",\n\"prompt\":  \"ultra realistic close up portrait ((beautiful pale cyberpunk female with heavy black eyeliner)), blue eyes, shaved side haircut, hyper detail, cinematic lighting, magic neon, dark red city, Canon EOS R3, nikon, f/1.4, ISO 200, 1/160s, 8K, RAW, unedited, symmetrical balance, in-frame, 8K\",\n\"negative_prompt\":  \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\",\n\"width\":  \"512\",\n\"height\":  \"512\",\n\"samples\":  \"1\",\n\"num_inference_steps\":  \"30\",\n\"safety_checker\":  \"no\",\n\"enhance_prompt\":  \"yes\",\n\"seed\":  None,\n\"guidance_scale\":  7.5,\n\"multi_lingual\":  \"no\",\n\"panorama\":  \"no\",\n\"self_attention\":  \"no\",\n\"upscale\":  \"no\",\n\"embeddings\":  \"embeddings_model_id\",\n\"lora\":  \"lora_model_id\",\n\"webhook\":  None,\n\"track_id\":  None\n})\nheaders =  {\n'Content-Type':  'application/json'\n}\nresponse = requests.request(\"POST\", url, headers=headers, data=payload)\nprint(response.text)\nUse this coupon code to get 25% off DMGG0RBN",
    "Intel/ldm3d": "LDM3D model\nModel details\nUsage\nTraining data\nFinetuning\nEvaluation results\nQuantitative results\nQualitative results\nEthical Considerations and Limitations\nCaveats and Recommendations\nDisclaimer\nBibTeX entry and citation info\nLDM3D model\nThe LDM3D model was proposed in the paper LDM3D: Latent Diffusion Model for 3D, authored by Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, and Vasudev Lal.\nLDM3D was accepted to the IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) in 2023.\nFor better results, do not hesitate to use our new checkpoint, ldm3d-4c based on a slighlty different architecture.\nThe following new LDM3D model checkpoints have been released:\nldm3d-4c: A new version of LDM3D with higher quality results.\nldm3d-pano: LDM3D applied to panoramic images.\nldm3d-sr: Upscaler for LDM3D.\nModel details\nThe abstract from the paper is the following:\nThis research paper proposes a Latent Diffusion Model for 3D (LDM3D) that generates both image and depth map data from a given text prompt, allowing users to generate RGBD images from text prompts. The LDM3D model is fine-tuned on a dataset of tuples containing an RGB image, depth map and caption, and validated through extensive experiments. We also develop an application called DepthFusion, which uses the img2img pipeline to create immersive and interactive 360-degree-view experiences using TouchDesigner. This technology has the potential to transform a wide range of industries, from entertainment and gaming to architecture and design. Overall, this paper presents a significant contribution to the field of generative AI and computer vision, and showcases the potential of LDM3D and DepthFusion to revolutionize content creation and digital experiences.\nLDM3D overview taken from the LDM3D paper.\nUsage\nYou can use this model to generate an RGB image and depth map given a text prompt.\nA short video summarizing the approach can be found at this url and a VR demo can be found here.\nA demo is also accessible on Spaces.\nHere is how to use this model to get the features of a given text in PyTorch on both a CPU and GPU architecture:\nfrom diffusers import StableDiffusionLDM3DPipeline\npipe = StableDiffusionLDM3DPipeline.from_pretrained(\"Intel/ldm3d\")\n# On CPU\npipe.to(\"cpu\")\n# On GPU\npipe.to(\"cuda\")\nprompt = \"A picture of some lemons on a table\"\nname = \"lemons\"\noutput = pipe(prompt)\nrgb_image, depth_image = output.rgb, output.depth\nrgb_image[0].save(name+\"_ldm3d_rgb.jpg\")\ndepth_image[0].save(name+\"_ldm3d_depth.png\")\nThis is the result:\nTraining data\nThe LDM3D model was finetuned on a dataset constructed from a subset of the LAION-400M dataset, a large-scale image-caption dataset that contains over 400 million image-caption pairs.\nFinetuning\nThe fine-tuning process comprises two stages. In the first stage, we train an autoencoder to generate a lower-dimensional, perceptually equivalent data representation. Subsequently, we fine-tune the diffusion model using the frozen autoencoder\nEvaluation results\nQuantitative results\nThe table below shows the quantitative results of text-conditional image synthesis on the 512 x 512-sized MS-COCO dataset with 50 DDIM steps.\nMethod\nFID ‚Üì\nIS ‚Üë\nCLIP ‚Üë\nSD v1.4\n28.08\n34.17 ¬± 0.76\n26.13 ¬± 2.81\nSD v1.5\n27.39\n34.02 ¬± 0.79\n26.13 ¬± 2.79\nLDM3D (ours)\n27.82\n28.79 ¬± 0.49\n26.61 ¬± 2.92\nOur model is on par with the Stable Diffusion models with the same number of parameters (1.06B). IS and CLIP similarity scores are averaged over 30k captions from the MS-COCO dataset.\nThe following table shows the evaluation results of depth evaluation comparing LDM3D and DPT-Large with respect to ZoeDepth-N that serves as a reference model.\nMethod\nAbsRel\nRMSE [m]\nLDM3D\n0.0911\n0.334\nDPT-Large\n0.0779\n0.297\nThe results shown above can be referenced in Table 1 and Table 2 of the LDM3D paper.\nQualitative results\nThe figure below shows some qualitative results comparing our method with Stable Diffusion v1.4 and with DPT-Large for the depth maps .\nEthical Considerations and Limitations\nFor image generation, the Stable Diffusion limitations and biases apply. For depth map generation, a first limitiation is that we are using DPT-large to produce the ground truth, hence, other limitations and biases from DPT are applicable.\nCaveats and Recommendations\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\nHere are a couple of useful links to learn more about Intel's AI software:\nIntel Extension for PyTorch\nIntel Neural Compressor\nDisclaimer\nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please cosult an attorney before using this model for commercial purposes.\nBibTeX entry and citation info\n@misc{stan2023ldm3d,\ntitle={LDM3D: Latent Diffusion Model for 3D},\nauthor={Gabriela Ben Melech Stan and Diana Wofk and Scottie Fox and Alex Redden and Will Saxton and Jean Yu and Estelle Aflalo and Shao-Yen Tseng and Fabio Nonato and Matthias Muller and Vasudev Lal},\nyear={2023},\neprint={2305.10853},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "Tincando/fiction_story_generator": "GPT-Neo za Generiranje Fiktivnih Priƒça/ GPT-Neo for Fiction Story Generation\nOpis modela\nUpotrebe i ograniƒçenja\nPerformanse Modela\nOgraniƒçenja\nUpotreba\nEtika\nCitiranje\nTraining hyperparameters\nTraining results\nFramework versions\nGPT-Neo za Generiranje Fiktivnih Priƒça/ GPT-Neo for Fiction Story Generation\nOvaj model je fino pode≈°ena verzija EleutherAI-jevog GPT-Neo-125M modela, optimiziran za generiranje fikcijskih priƒça.\nObuƒçen je na skupu podataka dostupnom na https://github.com/facebookresearch/fairseq/tree/main/examples/stories.\nOpis modela\nNaziv Modela: GPT-Neo-Fiction\nStudent: Tin Kanjovsky/Tincando\nMentor: izv.prof.dr.sc. Darko Etinger\nVerzija Modela: 1.0\nUpotrebe i ograniƒçenja\nModel je dizajniran za generiranje kreativnih fiktivnih priƒça. Mo≈æe se koristiti u razne svrhe, ukljuƒçujuƒái, ali ne ograniƒçavajuƒái se na:\nPripovijedanje: Generiranje zanimljivih i ma≈°tovitih fiktivnih priƒça.\nGeneriranje Sadr≈æaja: Stvaranje sadr≈æaja za blogove, web stranice ili druge medije s elementom pripovijedanja.\nKreativno Pisanje: Pomoƒá autorima i piscima pri razmi≈°ljanju o idejama i razvijanju narativa.\nPerformanse Modela\nPodaci za Obuku: Model je obuƒçen na raznolikom skupu podataka fiktivnih priƒça i prompteva.\nMetrike Evaluacije: Performanse metrika, kao ≈°to su perpleksnost ili BLEU skorovi, mogu varirati ovisno o konkretnom zadatku i skupu podataka.\nOgraniƒçenja\nKvaliteta Sadr≈æaja: Iako model mo≈æe generirati kreativne priƒçe, kvaliteta i koherentnost izlaza mogu varirati, a povremeno mo≈æe proizvesti besmislene ili neprimjerene sadr≈æaje.\nPristranost: Model mo≈æe pokazivati pristranosti prisutne u skupu podataka za obuku, stoga je va≈æno biti oprezan prilikom kori≈°tenja za osjetljive teme ili sadr≈æaje.\nDuljina Izlaza: Model mo≈æe generirati tekst razliƒçite duljine i ne uvijek ƒáe proizvesti ≈æeljenu duljinu izlaza.\nPodaci za Fino Pode≈°avanje: Kvaliteta generiranih priƒça ovisi o kvaliteti i raznolikosti skupa podataka za fino pode≈°avanje.\nUpotreba\nfrom transformers import GPTNeoForCausalLM, GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(Tincando/fiction_story_generator)\nmodel = GPTNeoForCausalLM.from_pretrained(Tincando/fiction_story_generator)\n# Generate a fiction story\ninput_prompt = \"[WP] I can't believe I died the same way twice.\"\ninput_ids = tokenizer(input_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\noutput = model.generate(input_ids,\nmax_length=300,\ntemperature=0.9,\ntop_k=2,\ntop_p=0.9,\nrepetition_penalty=1.2,\ndo_sample=True,\nnum_return_sequences=2\n)\ngenerated_story = tokenizer.batch_decode(output,clean_up_tokenization_spaces=True)[0]\nprint(generated_story)\nEtika\nPrilikom kori≈°tenja ovog modela, razmotrite sljedeƒáe etiƒçke smjernice:\nModeracija Sadr≈æaja: Implementirajte moderaciju sadr≈æaja kako biste osigurali da generirane priƒçe ne kr≈°e smjernice ili standarde zajednice.\nPristranost i Pravednost: Budite svjesni potencijalnih pristranosti u izlazu modela i poduzmite korake za njihovo ubla≈æavanje.\nPrivatnost: Izbjegavajte upotrebu osobnih ili osjetljivih informacija kao ulaznih poticaja.\nPravna Usklaƒëenost: Pazite da generirani sadr≈æaj bude u skladu s autorskim pravima i zakonima o intelektualnom vlasni≈°tvu.\nCitiranje\nAko koristite GPT-Neo-Fiction u svojem radu, molimo razmislite o citiranju originalnog GPT-Neo modela i skupa podataka koji su kori≈°teni za fino pode≈°avanje:\nGPT-Neo Paper\nFairseq Repository\nHierarchical Neural Story Generation\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 5e-05\ntrain_batch_size: 8\neval_batch_size: 8\nseed: 42\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: linear\nnum_epochs: 5\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\n3.0842\n1.0\n34075\n3.1408\n3.0026\n2.0\n68150\n3.1275\n2.9344\n3.0\n102225\n3.1270\n2.8932\n4.0\n136300\n3.1306\n2.8517\n5.0\n170375\n3.1357\nFramework versions\nTransformers 4.28.0\nPytorch 1.12.1+cu116\nDatasets 2.4.0\nTokenizers 0.12.1",
    "deepghs/anime_ai_check": "The model used to predict whether an image is generated by AI techniques, mainly diffusion models. Please note that this model is trained on crawled data, and its predictions should be used for reference purposes only. It should not be used to determine the source or origin of an image.\nModel\nFLOPs\nAccuracy\nConfusion Matrix\nDescription\ncaformer_s36_plus_sce\n22.10G\n97.54%\nConfusion Matrix\nModel: caformer_s36.sail_in22k_ft_in1k_384 pratrained from timm\nmobilenetv3_sce\n0.63G\n93.20%\nConfusion Matrix\nModel: mobilenetv3_large_100 from timm\nmobilenetv3_sce_dist\n0.63G\n95.65%\nConfusion Matrix\nDistillated from caformer_s36_plus_sce, using mobilenetv3_large_100",
    "GLIPModel/GLIP": "No model card",
    "deepghs/anime_censor_detection": "Model\nFLOPS\nParams\nF1 Score\nThreshold\nF1 Plot\nConfusion\nLabels\ncensor_detect_v1.0_n\n898\n3.01M\n0.8\n0.278\nplot\nconfusion\nnipple_f, penis, pussy\ncensor_detect_v1.0_s\n3.49k\n11.1M\n0.83\n0.238\nplot\nconfusion\nnipple_f, penis, pussy\ncensor_detect_v0.10_s\n3.49k\n11.1M\n0.83\n0.15\nplot\nconfusion\nnipple_f, penis, pussy\ncensor_detect_v0.9_s\n3.49k\n11.1M\n0.81\n0.2\nplot\nconfusion\nnipple_f, penis, pussy\ncensor_detect_v0.8_s\n3.49k\n11.1M\n0.81\n0.252\nplot\nconfusion\nnipple_f, penis, pussy\ncensor_detect_v0.7_s\n3.49k\n11.1M\n0.79\n0.356\nplot\nconfusion\nnipple_f, penis, pussy",
    "elinas/chronos-33b": "chronos-33b\n--\nlicense: other\nLLaMA Model Card\nModel details\nIntended use\nFactors\nMetrics\nEvaluation datasets\nTraining dataset\nQuantitative analysis\nEthical considerations\nchronos-33b\nUpdate: Safetensors added, more to come? Follow for updates.\nThis is the fp16 PyTorch / HF version of chronos-33b - if you need another version, GGML and GPTQ versions are linked below.\nThis model is primarily focused on chat, roleplay, and storywriting, but can accomplish other tasks such as simple reasoning and coding.\nChronos generates very long outputs with coherent text, largely due to the human inputs it was trained on.\nThis model uses Alpaca formatting, so for optimal model performance, use:\n### Instruction:\nYour instruction or question here.\n### Response:\nGGUFs provided by @mradermacher!\n4bit GPTQ Version provided by @TheBloke\n--\nlicense: other\nLLaMA Model Card\nModel details\nOrganization developing the model\nThe FAIR team of Meta AI.\nModel date\nLLaMA was trained between December. 2022 and Feb. 2023.\nModel version\nThis is version 1 of the model.\nModel type\nLLaMA is an auto-regressive language model, based on the transformer architecture. The model comes in different sizes: 7B, 13B, 33B and 65B parameters.\nPaper or resources for more information\nMore information can be found in the paper ‚ÄúLLaMA, Open and Efficient Foundation Language Models‚Äù, available at https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/.\nCitations details\nhttps://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/\nLicense\nNon-commercial bespoke license\nWhere to send questions or comments about the model\nQuestions and comments about LLaMA can be sent via the GitHub repository of the project , by opening an issue.\nIntended use\nPrimary intended uses\nThe primary use of LLaMA is research on large language models, including:\nexploring potential applications such as question answering, natural language understanding or reading comprehension,\nunderstanding capabilities and limitations of current language models, and developing techniques to improve those,\nevaluating and mitigating biases, risks, toxic and harmful content generations, hallucinations.\nPrimary intended users\nThe primary intended users of the model are researchers in natural language processing, machine learning and artificial intelligence.\nOut-of-scope use cases\nLLaMA is a base, or foundational, model. As such, it should not be used on downstream applications without further risk evaluation and mitigation. In particular, our model has not been trained with human feedback, and can thus generate toxic or offensive content, incorrect information or generally unhelpful answers.\nFactors\nRelevant factors\nOne of the most relevant factors for which model performance may vary is which language is used. Although we included 20 languages in the training data, most of our dataset is made of English text, and we thus expect the model to perform better for English than other languages. Relatedly, it has been shown in previous studies that performance might vary for different dialects, and we expect that it will be the case for our model.\nEvaluation factors\nAs our model is trained on data from the Web, we expect that it reflects biases from this source. We thus evaluated on RAI datasets to measure biases exhibited by the model for gender, religion, race, sexual orientation, age, nationality, disability, physical appearance and socio-economic status. We also measure the toxicity of model generations, depending on the toxicity of the context used to prompt the model.\nMetrics\nModel performance measures\nWe use the following measure to evaluate the model:\nAccuracy for common sense reasoning, reading comprehension, natural language understanding (MMLU), BIG-bench hard, WinoGender and CrowS-Pairs,\nExact match for question answering,\nThe toxicity score from Perspective API on RealToxicityPrompts.\nDecision thresholds\nNot applicable.\nApproaches to uncertainty and variability\nDue to the high computational requirements of training LLMs, we trained only one model of each size, and thus could not evaluate variability of pre-training.\nEvaluation datasets\nThe model was evaluated on the following benchmarks: BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC, OpenBookQA, NaturalQuestions, TriviaQA, RACE, MMLU, BIG-bench hard, GSM8k, RealToxicityPrompts, WinoGender, CrowS-Pairs.\nTraining dataset\nThe model was trained using the following source of data: CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]. The Wikipedia and Books domains include data in the following languages: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk. See the paper for more details about the training set and corresponding preprocessing.\nQuantitative analysis\nHyperparameters for the model architecture\nLLaMA Model hyper parameters\nNumber of parametersdimensionn headsn layersLearn rateBatch sizen tokens\n7B 4096 32 32 3.0E-044M1T\n13B512040403.0E-044M1T\n33B665652601.5.E-044M1.4T\n65B819264801.5.E-044M1.4T\nTable 1 - Summary of LLama Model Hyperparameters\nWe present our results on eight standard common sense reasoning benchmarks in the table below.\nLLaMA Reasoning tasks\nNumber of parameters BoolQPIQASIQAHellaSwagWinoGrandeARC-eARC-cOBQACOPA\n7B76.579.848.976.170.176.747.657.293\n13B78.180.150.479.27378.152.756.494\n33B83.182.350.482.87681.457.858.692\n65B85.382.852.384.27781.55660.294\n*Table 2 - Summary of LLama Model Performance on Reasoning tasks*\nWe present our results on bias in the table below. Note that lower value is better indicating lower bias.\nNo\nCategory\nFAIR LLM\n1\nGender\n70.6\n2\nReligion\n79\n3\nRace/Color\n57\n4\nSexual orientation\n81\n5\nAge\n70.1\n6\nNationality\n64.2\n7\nDisability\n66.7\n8\nPhysical appearance\n77.8\n9\nSocioeconomic status\n71.5\nLLaMA Average\n66.6\nTable 3 - Summary bias of our model output\nEthical considerations\nData\nThe data used to train the model is collected from various sources, mostly from the Web. As such, it contains offensive, harmful and biased content. We thus expect the model to exhibit such biases from the training data.\nHuman life\nThe model is not intended to inform decisions about matters central to human life, and should not be used in such a way.\nMitigations\nWe filtered the data from the Web based on its proximity to Wikipedia text and references. For this, we used a Kneser-Ney language model and a fastText linear classifier.\nRisks and harms\nRisks and harms of large language models include the generation of harmful, offensive or biased content. These models are often prone to generating incorrect information, sometimes referred to as hallucinations. We do not expect our model to be an exception in this regard.\nUse cases\nLLaMA is a foundational model, and as such, it should not be used for downstream applications without further investigation and mitigations of risks. These risks and potential fraught use cases include, but are not limited to: generation of misinformation and generation of harmful, biased or offensive content.",
    "VMware/open-llama-7b-open-instruct": "VMware/open-llama-7B-open-instruct\nLicense\nNomenclature\nUse in Transformers\nFinetuning details\nOpen LLM Leaderboard Evaluation Results\nVMware/open-llama-7B-open-instruct\nInstruction-tuned version of the fully trained Open LLama 7B model. The model is open for COMMERCIAL USE.\nThere is a v2 version of this model available, https://huggingface.co/VMware/open-llama-7b-v2-open-instruct\nNOTE  : The model was trained using the Alpaca prompt template\nNOTE  : Fast tokenizer results in incorrect encoding, set the use_fast = False parameter, when instantiating the tokenizer\nLicense\nCommercially Viable\nInstruction dataset, VMware/open-instruct-v1-oasst-dolly-hhrlhf is under cc-by-sa-3.0\nLanguage Model, (openlm-research/open_llama_7b) is under apache-2.0\nNomenclature\nModel : Open-llama\nModel Size: 7B parameters\nDataset: Open-instruct-v1 (oasst,dolly, hhrlhf)\nUse in Transformers\nimport os\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = 'VMware/open-llama-7b-open-instruct'\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map='sequential')\nprompt_template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\"\nprompt = 'Explain in simple terms how the attention mechanism of a transformer model works'\ninputt = prompt_template.format(instruction= prompt)\ninput_ids = tokenizer(inputt, return_tensors=\"pt\").input_ids.to(\"cuda\")\noutput1 = model.generate(input_ids, max_length=512)\ninput_length = input_ids.shape[1]\noutput1 = output1[:, input_length:]\noutput = tokenizer.decode(output1[0])\nprint(output)\n'''\nAttention is a mechanism used in deep learning models, such as transformer models, to capture global dependencies between different parts of the input. In a transformer model, the attention mechanism works by computing a weighted sum of the input vectors and then applying a non-linear activation function to the result.\nThe attention mechanism in a transformer model works in two steps:\n1. Query-Key Mapping: First, the input sequence is divided into two parts: the query vector and the key vector. The query vector represents the input at the current position, and the key vector represents the input at a previous position.\n2. Attention Weight Calculation: Second, the attention weights are calculated using the dot product between the query vector and each key vector. The attention weights represent the importance of the input at the previous position to the current position.\nThe attention weights are then used to compute the attention score for each input element. The attention score represents the relevance of the input element to the current position.\nThe attention mechanism in a transformer model is designed to capture global dependencies between different parts of the input. By attending to input elements from different positions, the model can learn to understand the relationships between different parts of the input. This allows the model to perform more complex tasks, such as understanding the relationships between words in a sentence or pixels in an image.</s>\n'''\nFinetuning details\nThe finetuning scripts will be available in our RAIL Github Repository\nOpen LLM Leaderboard Evaluation Results\nDetailed results can be found here\nMetric\nValue\nAvg.\n40.9\nARC (25-shot)\n49.74\nHellaSwag (10-shot)\n73.67\nMMLU (5-shot)\n31.52\nTruthfulQA (0-shot)\n34.65\nWinogrande (5-shot)\n65.43\nGSM8K (5-shot)\n0.53\nDROP (3-shot)\n30.75",
    "facebook/musicgen-small": "MusicGen - Small - 300M\nExample\nü§ó Transformers Usage\nAudiocraft Usage\nModel details\nIntended use\nMetrics\nEvaluation datasets\nTraining datasets\nEvaluation results\nLimitations and biases\nMusicGen - Small - 300M\nMusicGen is a text-to-music model capable of genreating high-quality music samples conditioned on text descriptions or audio prompts.\nIt is a single stage auto-regressive Transformer model trained over a 32kHz EnCodec tokenizer with 4 codebooks sampled at 50 Hz.\nUnlike existing methods, like MusicLM, MusicGen doesn't require a self-supervised semantic representation, and it generates all 4 codebooks in one pass.\nBy introducing a small delay between the codebooks, we show we can predict them in parallel, thus having only 50 auto-regressive steps per second of audio.\nMusicGen was published in Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre D√©fossez.\nFour checkpoints are released:\nsmall (this checkpoint)\nmedium\nlarge\nmelody\nExample\nTry out MusicGen yourself!\nAudiocraft Colab:\nHugging Face Colab:\nHugging Face Demo:\nü§ó Transformers Usage\nYou can run MusicGen locally with the ü§ó Transformers library from version 4.31.0 onwards.\nFirst install the ü§ó Transformers library and scipy:\npip install --upgrade pip\npip install --upgrade transformers scipy\nRun inference via the Text-to-Audio (TTA) pipeline. You can infer the MusicGen model via the TTA pipeline in just a few lines of code!\nfrom transformers import pipeline\nimport scipy\nsynthesiser = pipeline(\"text-to-audio\", \"facebook/musicgen-small\")\nmusic = synthesiser(\"lo-fi music with a soothing melody\", forward_params={\"do_sample\": True})\nscipy.io.wavfile.write(\"musicgen_out.wav\", rate=music[\"sampling_rate\"], data=music[\"audio\"])\nRun inference via the Transformers modelling code. You can use the processor + generate code to convert text into a mono 32 kHz audio waveform for more fine-grained control.\nfrom transformers import AutoProcessor, MusicgenForConditionalGeneration\nprocessor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\nmodel = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\ninputs = processor(\ntext=[\"80s pop track with bassy drums and synth\", \"90s rock song with loud guitars and heavy drums\"],\npadding=True,\nreturn_tensors=\"pt\",\n)\naudio_values = model.generate(**inputs, max_new_tokens=256)\nListen to the audio samples either in an ipynb notebook:\nfrom IPython.display import Audio\nsampling_rate = model.config.audio_encoder.sampling_rate\nAudio(audio_values[0].numpy(), rate=sampling_rate)\nOr save them as a .wav file using a third-party library, e.g. scipy:\nimport scipy\nsampling_rate = model.config.audio_encoder.sampling_rate\nscipy.io.wavfile.write(\"musicgen_out.wav\", rate=sampling_rate, data=audio_values[0, 0].numpy())\nFor more details on using the MusicGen model for inference using the ü§ó Transformers library, refer to the MusicGen docs.\nAudiocraft Usage\nYou can also run MusicGen locally through the original Audiocraft library:\nFirst install the audiocraft library\npip install git+https://github.com/facebookresearch/audiocraft.git\nMake sure to have ffmpeg installed:\napt-get install ffmpeg\nRun the following Python code:\nfrom audiocraft.models import MusicGen\nfrom audiocraft.data.audio import audio_write\nmodel = MusicGen.get_pretrained(\"small\")\nmodel.set_generation_params(duration=8)  # generate 8 seconds.\ndescriptions = [\"happy rock\", \"energetic EDM\"]\nwav = model.generate(descriptions)  # generates 2 samples.\nfor idx, one_wav in enumerate(wav):\n# Will save under {idx}.wav, with loudness normalization at -14 db LUFS.\naudio_write(f'{idx}', one_wav.cpu(), model.sample_rate, strategy=\"loudness\")\nModel details\nOrganization developing the model: The FAIR team of Meta AI.\nModel date: MusicGen was trained between April 2023 and May 2023.\nModel version: This is the version 1 of the model.\nModel type: MusicGen consists of an EnCodec model for audio tokenization, an auto-regressive language model based on the transformer architecture for music modeling. The model comes in different sizes: 300M, 1.5B and 3.3B parameters ; and two variants: a model trained for text-to-music generation task and a model trained for melody-guided music generation.\nPaper or resources for more information: More information can be found in the paper Simple and Controllable Music Generation.\nCitation details:\n@misc{copet2023simple,\ntitle={Simple and Controllable Music Generation},\nauthor={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre D√©fossez},\nyear={2023},\neprint={2306.05284},\narchivePrefix={arXiv},\nprimaryClass={cs.SD}\n}\nLicense: Code is released under MIT, model weights are released under CC-BY-NC 4.0.\nWhere to send questions or comments about the model: Questions and comments about MusicGen can be sent via the Github repository of the project, or by opening an issue.\nIntended use\nPrimary intended use: The primary use of MusicGen is research on AI-based music generation, including:\nResearch efforts, such as probing and better understanding the limitations of generative models to further improve the state of science\nGeneration of music guided by text or melody to understand current abilities of generative AI models by machine learning amateurs\nPrimary intended users: The primary intended users of the model are researchers in audio, machine learning and artificial intelligence, as well as amateur seeking to better understand those models.\nOut-of-scope use cases: The model should not be used on downstream applications without further risk evaluation and mitigation. The model should not be used to intentionally create or disseminate music pieces that create hostile or alienating environments for people. This includes generating music that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\nMetrics\nModels performance measures: We used the following objective measure to evaluate the model on a standard music benchmark:\nFrechet Audio Distance computed on features extracted from a pre-trained audio classifier (VGGish)\nKullback-Leibler Divergence on label distributions extracted from a pre-trained audio classifier (PaSST)\nCLAP Score between audio embedding and text embedding extracted from a pre-trained CLAP model\nAdditionally, we run qualitative studies with human participants, evaluating the performance of the model with the following axes:\nOverall quality of the music samples;\nText relevance to the provided text input;\nAdherence to the melody for melody-guided music generation.\nMore details on performance measures and human studies can be found in the paper.\nDecision thresholds: Not applicable.\nEvaluation datasets\nThe model was evaluated on the MusicCaps benchmark and on an in-domain held-out evaluation set, with no artist overlap with the training set.\nTraining datasets\nThe model was trained on licensed data using the following sources: the Meta Music Initiative Sound Collection, Shutterstock music collection and the Pond5 music collection. See the paper for more details about the training set and corresponding preprocessing.\nEvaluation results\nBelow are the objective metrics obtained on MusicCaps with the released model. Note that for the publicly released models, we had all the datasets go through a state-of-the-art music source separation method, namely using the open source Hybrid Transformer for Music Source Separation (HT-Demucs), in order to keep only the instrumental part. This explains the difference in objective metrics with the models used in the paper.\nModel\nFrechet Audio Distance\nKLD\nText Consistency\nChroma Cosine Similarity\nfacebook/musicgen-small\n4.88\n1.42\n0.27\n-\nfacebook/musicgen-medium\n5.14\n1.38\n0.28\n-\nfacebook/musicgen-large\n5.48\n1.37\n0.28\n-\nfacebook/musicgen-melody\n4.93\n1.41\n0.27\n0.44\nMore information can be found in the paper Simple and Controllable Music Generation, in the Results section.\nLimitations and biases\nData: The data sources used to train the model are created by music professionals and covered by legal agreements with the right holders. The model is trained on 20K hours of data, we believe that scaling the model on larger datasets can further improve the performance of the model.\nMitigations: Vocals have been removed from the data source using corresponding tags, and then using a state-of-the-art music source separation method, namely using the open source Hybrid Transformer for Music Source Separation (HT-Demucs).\nLimitations:\nThe model is not able to generate realistic vocals.\nThe model has been trained with English descriptions and will not perform as well in other languages.\nThe model does not perform equally well for all music styles and cultures.\nThe model sometimes generates end of songs, collapsing to silence.\nIt is sometimes difficult to assess what types of text descriptions provide the best generations. Prompt engineering may be required to obtain satisfying results.\nBiases: The source of data is potentially lacking diversity and all music cultures are not equally represented in the dataset. The model may not perform equally well on the wide variety of music genres that exists. The generated samples from the model will reflect the biases from the training data. Further work on this model should include methods for balanced and just representations of cultures, for example, by scaling the training data to be both diverse and inclusive.\nRisks and harms: Biases and limitations of the model may lead to generation of samples that may be considered as biased, inappropriate or offensive. We believe that providing the code to reproduce the research and train new models will allow to broaden the application to new and more representative data.\nUse cases: Users must be aware of the biases, limitations and risks of the model. MusicGen is a model developed for artificial intelligence research on controllable music generation. As such, it should not be used for downstream applications without further investigation and mitigation of risks.",
    "MoyAI/password-security": "Password security classifier\nModel & Training\nEvaluation\nModel usage\nPassword security classifier\nThis is a keras model that gives a binary response showing how secure is a password.\nI used this password list as a dataset + random password generation using the random library (I am aware of it being unsecure).\nThis model has a huggingface space. You can visit the link to try using the model online.\nModel & Training\nThe model was trained on 4,2MiB (200 000 lines) of .csv data for 2 epochs on Adam with learning rate 0.00001, batch size 4 and mse loss.\nThe model embeds every input character with the ord() builtin python function. The model has 128 969 dense layer parameters.\nEvaluation\nDuring training the model had:\nloss - 0.0025\naccuracy - 0.9972\nThe test metrics are:\nloss - 0.0023\naccuracy - 0.9972\nModel usage\nThe start.py file has a clf function that inputs a string of a password and responds with a 0-1 float value. 1 means secure and 0 insecure.\nTo train the model, create a dataset.csv file. Here's an example:\n0,qwerty\n0,123456\n1,ISOdvsjs8r8\n1,F(SEsDLxc__\nAfter the dataset.csv file is created, now you can adjust the settings in the net.py file and run it.",
    "facebook/encodec_24khz": "Model Card for EnCodec\nModel Details\nModel Description\nModel Sources\nUses\nDirect Use\nDownstream Use\nHow to Get Started with the Model\nTraining Details\nTraining Data\nEvaluation\nSubjectif metric for restoration:\nObjective metric for restoration:\nResults\nCitation\nModel Card for EnCodec\nThis model card provides details and information about EnCodec, a state-of-the-art real-time audio codec developed by Meta AI.\nModel Details\nModel Description\nEnCodec is a high-fidelity audio codec leveraging neural networks. It introduces a streaming encoder-decoder architecture with quantized latent space, trained in an end-to-end fashion.\nThe model simplifies and speeds up training using a single multiscale spectrogram adversary that efficiently reduces artifacts and produces high-quality samples.\nIt also includes a novel loss balancer mechanism that stabilizes training by decoupling the choice of hyperparameters from the typical scale of the loss.\nAdditionally, lightweight Transformer models are used to further compress the obtained representation while maintaining real-time performance.\nDeveloped by: Meta AI\nModel type: Audio Codec\nModel Sources\nRepository: GitHub Repository\nPaper: EnCodec: End-to-End Neural Audio Codec\nUses\nDirect Use\nEnCodec can be used directly as an audio codec for real-time compression and decompression of audio signals.\nIt provides high-quality audio compression and efficient decoding. The model was trained on various bandwiths, which can be specified when encoding (compressing) and decoding (decompressing).\nTwo different setup exist for EnCodec:\nNon-streamable: the input audio is split into chunks of 1 seconds, with an overlap of 10 ms, which are then encoded.\nStreamable: weight normalizationis used on the convolution layers, and the input is not split into chunks but rather padded on the left.\nDownstream Use\nEnCodec can be fine-tuned for specific audio tasks or integrated into larger audio processing pipelines for applications such as speech generation,\nmusic generation, or text to speech tasks.\n[More Information Needed]\nHow to Get Started with the Model\nUse the following code to get started with the EnCodec model using a dummy example from the LibriSpeech dataset (~9MB). First, install the required Python packages:\npip install --upgrade pip\npip install --upgrade datasets[audio]\npip install git+https://github.com/huggingface/transformers.git@main\nThen load an audio sample, and run a forward pass of the model:\nfrom datasets import load_dataset, Audio\nfrom transformers import EncodecModel, AutoProcessor\n# load a demonstration datasets\nlibrispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n# load the model + processor (for pre-processing the audio)\nmodel = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\nprocessor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n# cast the audio data to the correct sampling rate for the model\nlibrispeech_dummy = librispeech_dummy.cast_column(\"audio\", Audio(sampling_rate=processor.sampling_rate))\naudio_sample = librispeech_dummy[0][\"audio\"][\"array\"]\n# pre-process the inputs\ninputs = processor(raw_audio=audio_sample, sampling_rate=processor.sampling_rate, return_tensors=\"pt\")\n# explicitly encode then decode the audio inputs\nencoder_outputs = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"])\naudio_values = model.decode(encoder_outputs.audio_codes, encoder_outputs.audio_scales, inputs[\"padding_mask\"])[0]\n# or the equivalent with a forward pass\naudio_values = model(inputs[\"input_values\"], inputs[\"padding_mask\"]).audio_values\nTraining Details\nThe model was trained for 300 epochs, with one epoch being 2,000 updates with the Adam optimizer with a batch size of 64 examples of 1 second each, a learning rate of 3 ¬∑ 10‚àí4\n, Œ≤1 = 0.5, and Œ≤2 = 0.9. All the models are traind using 8 A100 GPUs.\nTraining Data\nFor speech:\nDNS Challenge 4\nCommon Voice\nFor general audio:\nAudioSet\nFSD50K\nFor music:\nJamendo dataset\nThey used four different training strategies to sample for these datasets:\n(s1) sample a single source from Jamendo with probability 0.32;\n(s2) sample a single source from the other datasets with the same probability;\n(s3) mix two sources from all datasets with a probability of 0.24;\n(s4) mix three sources from all datasets except music with a probability of 0.12.\nThe audio is normalized by file and a random gain between -10 and 6 dB id applied.\nEvaluation\nSubjectif metric for restoration:\nThis models was evalutated using the MUSHRA protocol (Series, 2014), using both a hidden reference and a low anchor. Annotators were recruited using a\ncrowd-sourcing platform, in which they were asked to rate the perceptual quality of the provided samples in\na range between 1 to 100. They randomly select 50 samples of 5 seconds from each category of the the test set\nand force at least 10 annotations per samples. To filter noisy annotations and outliers we remove annotators\nwho rate the reference recordings less then 90 in at least 20% of the cases, or rate the low-anchor recording\nabove 80 more than 50% of the time.\nObjective metric for restoration:\nThe ViSQOL()ink) metric was used together with the Scale-Invariant Signal-to-Noise Ration (SI-SNR) (Luo & Mesgarani, 2019;\nNachmani et al., 2020; Chazan et al., 2021).\nResults\nThe results of the evaluation demonstrate the superiority of EnCodec compared to the baselines across different bandwidths (1.5, 3, 6, and 12 kbps).\nWhen comparing EnCodec with the baselines at the same bandwidth, EnCodec consistently outperforms them in terms of MUSHRA score.\nNotably, EnCodec achieves better performance, on average, at 3 kbps compared to Lyra-v2 at 6 kbps and Opus at 12 kbps.\nAdditionally, by incorporating the language model over the codes, it is possible to achieve a bandwidth reduction of approximately 25-40%.\nFor example, the bandwidth of the 3 kbps model can be reduced to 1.9 kbps.\nSummary\nEnCodec is a state-of-the-art real-time neural audio compression model that excels in producing high-fidelity audio samples at various sample rates and bandwidths.\nThe model's performance was evaluated across different settings, ranging from 24kHz monophonic at 1.5 kbps to 48kHz stereophonic, showcasing both subjective and\nobjective results. Notably, EnCodec incorporates a novel spectrogram-only adversarial loss, effectively reducing artifacts and enhancing sample quality.\nTraining stability and interpretability were further enhanced through the introduction of a gradient balancer for the loss weights.\nAdditionally, the study demonstrated that a compact Transformer model can be employed to achieve an additional bandwidth reduction of up to 40% without compromising\nquality, particularly in applications where low latency is not critical (e.g., music streaming).\nCitation\nBibTeX:\n@misc{d√©fossez2022high,\ntitle={High Fidelity Neural Audio Compression},\nauthor={Alexandre D√©fossez and Jade Copet and Gabriel Synnaeve and Yossi Adi},\nyear={2022},\neprint={2210.13438},\narchivePrefix={arXiv},\nprimaryClass={eess.AS}\n}",
    "flaviagiammarino/pubmed-clip-vit-base-patch32": "Model Card for PubMedCLIP\nModel Description\nUsage\nAdditional Information\nLicensing Information\nCitation Information\nModel Card for PubMedCLIP\nPubMedCLIP is a fine-tuned version of CLIP for the medical domain.\nModel Description\nPubMedCLIP was trained on the Radiology Objects in COntext (ROCO) dataset, a large-scale multimodal medical imaging dataset.\nThe ROCO dataset includes diverse imaging modalities (such as X-Ray, MRI, ultrasound, fluoroscopy, etc.) from various human body regions (such as head, spine, chest, abdomen, etc.)\ncaptured from open-access PubMed articles.\nPubMedCLIP was trained for 50 epochs with a batch size of 64 using the Adam optimizer with a learning rate of 10‚àí5.\nThe authors have released three different pre-trained models at this link\nwhich use ResNet-50, ResNet-50x4 and ViT32 as image encoders. This repository includes only the ViT32 variant of the PubMedCLIP model.\nRepository: PubMedCLIP Official GitHub Repository\nPaper: Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?\nUsage\nimport requests\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(\"flaviagiammarino/pubmed-clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"flaviagiammarino/pubmed-clip-vit-base-patch32\")\nurl = \"https://huggingface.co/flaviagiammarino/pubmed-clip-vit-base-patch32/resolve/main/scripts/input.jpeg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntext = [\"Chest X-Ray\", \"Brain MRI\", \"Abdominal CT Scan\"]\ninputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\nprobs = model(**inputs).logits_per_image.softmax(dim=1).squeeze()\nplt.subplots()\nplt.imshow(image)\nplt.title(\"\".join([x[0] + \": \" + x[1] + \"\\n\" for x in zip(text, [format(prob, \".4%\") for prob in probs])]))\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()\nAdditional Information\nLicensing Information\nThe authors have released the model code and pre-trained checkpoints under the MIT License.\nCitation Information\n@article{eslami2021does,\ntitle={Does clip benefit visual question answering in the medical domain as much as it does in the general domain?},\nauthor={Eslami, Sedigheh and de Melo, Gerard and Meinel, Christoph},\njournal={arXiv preprint arXiv:2112.13906},\nyear={2021}\n}",
    "kandinsky-community/kandinsky-2-2-decoder-inpaint": "Kandinsky 2.2\nUsage\nText Guided Inpainting Generation\nModel Architecture\nOverview\nDetails\nEvaluation\nBibTex\nKandinsky 2.2\nKandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\nThe Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov\nUsage\nKandinsky 2.2 is available in diffusers!\npip install diffusers transformers accelerate\nText Guided Inpainting Generation\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\nimport torch\nimport numpy as np\npipe = AutoPipelineForInpainting.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16)\npipe.enable_model_cpu_offload()\nprompt = \"a hat\"\ninit_image = load_image(\n\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main\" \"/kandinsky/cat.png\"\n)\nmask = np.zeros((768, 768), dtype=np.float32)\n# Let's mask out an area above the cat's head\nmask[:250, 250:-250] = 1\nout = pipe(\nprompt=prompt,\nimage=init_image,\nmask_image=mask,\nheight=768,\nwidth=768,\nnum_inference_steps=150,\n)\nimage = out.images[0]\nimage.save(\"cat_with_hat.png\")\nüö®üö®üö® Breaking change for Kandinsky Mask Inpainting üö®üö®üö®\nWe introduced a breaking change for Kandinsky inpainting pipeline in the following pull request: https://github.com/huggingface/diffusers/pull/4207. Previously we accepted a mask format where black pixels represent the masked-out area. This is inconsistent with all other pipelines in diffusers. We have changed the mask format in Knaindsky and now using white pixels instead.\nPlease upgrade your inpainting code to follow the above. If you are using Kandinsky Inpaint in production. You now need to change the mask to:\n# For PIL input\nimport PIL.ImageOps\nmask = PIL.ImageOps.invert(mask)\n# For PyTorch and Numpy input\nmask = 1 - mask\nModel Architecture\nOverview\nKandinsky 2.1 is a text-conditional diffusion model based on unCLIP and latent diffusion, composed of a transformer-based image prior model, a unet diffusion model, and a decoder.\nThe model architectures are illustrated in the figure below - the chart on the left describes the process to train the image prior model, the figure in the center is the text-to-image generation process, and the figure on the right is image interpolation.\nSpecifically, the image prior model was trained on CLIP text and image embeddings generated with a pre-trained mCLIP model. The trained image prior model is then used to generate mCLIP image embeddings for input text prompts. Both the input text prompts and its mCLIP image embeddings are used in the diffusion process. A MoVQGAN model acts as the final block of the model, which decodes the latent representation into an actual image.\nDetails\nThe image prior training of the model was performed on the LAION Improved Aesthetics dataset, and then fine-tuning was performed on the LAION HighRes data.\nThe main Text2Image diffusion model was trained on the basis of 170M text-image pairs from the LAION HighRes dataset (an important condition was the presence of images with a resolution of at least 768x768). The use of 170M pairs is due to the fact that we kept the UNet diffusion block from Kandinsky 2.0, which allowed us not to train it from scratch. Further, at the stage of fine-tuning, a dataset of 2M very high-quality high-resolution images with descriptions (COYO, anime, landmarks_russia, and a number of others) was used separately collected from open sources.\nEvaluation\nWe quantitatively measure the performance of Kandinsky 2.1 on the COCO_30k dataset, in zero-shot mode. The table below presents FID.\nFID metric values ‚Äã‚Äãfor generative models on COCO_30k\nFID (30k)\neDiff-I (2022)\n6.95\nImage (2022)\n7.27\nKandinsky 2.1 (2023)\n8.21\nStable Diffusion 2.1 (2022)\n8.59\nGigaGAN, 512x512 (2023)\n9.09\nDALL-E 2 (2022)\n10.39\nGLIDE (2022)\n12.24\nKandinsky 1.0 (2022)\n15.40\nDALL-E (2021)\n17.89\nKandinsky 2.0 (2022)\n20.00\nGLIGEN (2022)\n21.04\nFor more information, please refer to the upcoming technical report.\nBibTex\nIf you find this repository useful in your research, please cite:\n@misc{kandinsky 2.2,\ntitle         = {kandinsky 2.2},\nauthor        = {Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Vladimir Arkhipkin, Igor Pavlov, Andrey Kuznetsov, Denis Dimitrov},\nyear          = {2023},\nhowpublished  = {},\n}"
}