{
    "myshell-ai/OpenVoiceV2": "OpenVoice V2\nFeatures\nHow to Use\nUsage\nTable of Content\nQuick Use\nLinux Install\nOpenVoice V1\nOpenVoice V2\nInstall on Other Platforms\nLinks\nOpenVoice V2\nIn April 2024, we release OpenVoice V2, which includes all features in V1 and has:\nBetter Audio Quality. OpenVoice V2 adopts a different training strategy that delivers better audio quality.\nNative Multi-lingual Support. English, Spanish, French, Chinese, Japanese and Korean are natively supported in OpenVoice V2.\nFree Commercial Use. Starting from April 2024, both V2 and V1 are released under MIT License. Free for commercial use.\nFeatures\nAccurate Tone Color Cloning. OpenVoice can accurately clone the reference tone color and generate speech in multiple languages and accents.\nFlexible Voice Style Control. OpenVoice enables granular control over voice styles, such as emotion and accent, as well as other style parameters including rhythm, pauses, and intonation.\nZero-shot Cross-lingual Voice Cloning. Neither of the language of the generated speech nor the language of the reference speech needs to be presented in the massive-speaker multi-lingual training dataset.\nHow to Use\nPlease see usage for detailed instructions.\nUsage\nTable of Content\nQuick Use: directly use OpenVoice without installation.\nLinux Install: for researchers and developers only.\nV1\nV2\nInstall on Other Platforms: unofficial installation guide contributed by the community\nQuick Use\nThe input speech audio of OpenVoice can be in Any Language. OpenVoice can clone the voice in that speech audio, and use the voice to speak in multiple languages. For quick use, we recommend you to try the already deployed services:\nBritish English\nAmerican English\nIndian English\nAustralian English\nSpanish\nFrench\nChinese\nJapanese\nKorean\nLinux Install\nThis section is only for developers and researchers who are familiar with Linux, Python and PyTorch. Clone this repo, and run\nconda create -n openvoice python=3.9\nconda activate openvoice\ngit clone git@github.com:myshell-ai/OpenVoice.git\ncd OpenVoice\npip install -e .\nNo matter if you are using V1 or V2, the above installation is the same.\nOpenVoice V1\nDownload the checkpoint from here and extract it to the checkpoints folder.\n1. Flexible Voice Style Control.\nPlease see demo_part1.ipynb for an example usage of how OpenVoice enables flexible style control over the cloned voice.\n2. Cross-Lingual Voice Cloning.\nPlease see demo_part2.ipynb for an example for languages seen or unseen in the MSML training set.\n3. Gradio Demo.. We provide a minimalist local gradio demo here. We strongly suggest the users to look into demo_part1.ipynb, demo_part2.ipynb and the QnA if they run into issues with the gradio demo. Launch a local gradio demo with python -m openvoice_app --share.\nOpenVoice V2\nDownload the checkpoint from here and extract it to the checkpoints_v2 folder.\nInstall MeloTTS:\npip install git+https://github.com/myshell-ai/MeloTTS.git\npython -m unidic download\nDemo Usage. Please see demo_part3.ipynb for example usage of OpenVoice V2. Now it natively supports English, Spanish, French, Chinese, Japanese and Korean.\nInstall on Other Platforms\nThis section provides the unofficial installation guides by open-source contributors in the community:\nWindows\nGuide by @Alienpups\nYou are welcome to contribute if you have a better installation guide. We will list you here.\nDocker\nGuide by @StevenJSCF\nYou are welcome to contribute if you have a better installation guide. We will list you here.\nLinks\nGithub\nHFDemo\nDiscord",
    "defog/llama-3-sqlcoder-8b": "",
    "MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF": "MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF\nDescription\nAbout GGUF\nSpecial thanks\nMaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF\nModel creator: mistralai\nOriginal model: mistralai/Mistral-7B-Instruct-v0.3\nDescription\nMaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF contains GGUF format model files for mistralai/Mistral-7B-Instruct-v0.3.\nAbout GGUF\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\nHere is an incomplete list of clients and libraries that are known to support GGUF:\nllama.cpp. The source project for GGUF. Offers a CLI and a server option.\nllama-cpp-python, a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\nLM Studio, an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\ntext-generation-webui, the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\nKoboldCpp, a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\nGPT4All, a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\nLoLLMS Web UI, a great web UI with many interesting and unique features, including a full model library for easy model selection.\nFaraday.dev, an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\ncandle, a Rust ML framework with a focus on performance, including GPU support, and ease of use.\nctransformers, a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\nSpecial thanks\nüôè Special thanks to Georgi Gerganov and the whole team working on llama.cpp for making all of this possible.",
    "Qwen/Qwen2-0.5B": "A newer version of this model is available:\nQwen/Qwen2.5-0.5B\nQwen2-0.5B\nIntroduction\nModel Details\nRequirements\nUsage\nPerformance\nCitation\nQwen2-0.5B\nIntroduction\nQwen2 is the new series of Qwen large language models. For Qwen2, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters, including a Mixture-of-Experts model. This repo contains the 0.5B Qwen2 base language model.\nCompared with the state-of-the-art opensource language models, including the previous released Qwen1.5, Qwen2 has generally surpassed most opensource models and demonstrated competitiveness against proprietary models across a series of benchmarks targeting for language understanding, language generation, multilingual capability, coding, mathematics, reasoning, etc.\nFor more details, please refer to our blog, GitHub, and Documentation.\nModel Details\nQwen2 is a language model series including decoder language models of different model sizes. For each size, we release the base language model and the aligned chat model. It is based on the Transformer architecture with SwiGLU activation, attention QKV bias, group query attention, etc. Additionally, we have an improved tokenizer adaptive to multiple natural languages and codes.\nRequirements\nThe code of Qwen2 has been in the latest Hugging face transformers and we advise you to install transformers>=4.37.0, or you might encounter the following error:\nKeyError: 'qwen2'\nUsage\nWe do not advise you to use base language models for text generation. Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., on this model.\nPerformance\nThe evaluation of base models mainly focuses on the model performance of natural language understanding, general question answering, coding, mathematics, scientific knowledge, reasoning, multilingual capability, etc.\nThe datasets for evaluation include:\nEnglish Tasks: MMLU (5-shot), MMLU-Pro (5-shot), GPQA (5shot), Theorem QA (5-shot), BBH (3-shot), HellaSwag (10-shot), Winogrande (5-shot), TruthfulQA (0-shot), ARC-C (25-shot)\nCoding Tasks: EvalPlus (0-shot) (HumanEval, MBPP, HumanEval+, MBPP+), MultiPL-E (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript)\nMath Tasks: GSM8K (4-shot), MATH (4-shot)\nChinese Tasks: C-Eval(5-shot), CMMLU (5-shot)\nMultilingual Tasks: Multi-Exam (M3Exam 5-shot, IndoMMLU 3-shot, ruMMLU 5-shot, mMMLU 5-shot), Multi-Understanding (BELEBELE 5-shot, XCOPA 5-shot, XWinograd 5-shot, XStoryCloze 0-shot, PAWS-X 5-shot), Multi-Mathematics (MGSM 8-shot), Multi-Translation (Flores-101 5-shot)\nQwen2-0.5B & Qwen2-1.5B performances\nDatasets\nPhi-2\nGemma-2B\nMiniCPM\nQwen1.5-1.8B\nQwen2-0.5B\nQwen2-1.5B\n#Non-Emb Params\n2.5B\n2.0B\n2.4B\n1.3B\n0.35B\n1.3B\nMMLU\n52.7\n42.3\n53.5\n46.8\n45.4\n56.5\nMMLU-Pro\n-\n15.9\n-\n-\n14.7\n21.8\nTheorem QA\n-\n-\n-\n-\n8.9\n15.0\nHumanEval\n47.6\n22.0\n50.0\n20.1\n22.0\n31.1\nMBPP\n55.0\n29.2\n47.3\n18.0\n22.0\n37.4\nGSM8K\n57.2\n17.7\n53.8\n38.4\n36.5\n58.5\nMATH\n3.5\n11.8\n10.2\n10.1\n10.7\n21.7\nBBH\n43.4\n35.2\n36.9\n24.2\n28.4\n37.2\nHellaSwag\n73.1\n71.4\n68.3\n61.4\n49.3\n66.6\nWinogrande\n74.4\n66.8\n-\n60.3\n56.8\n66.2\nARC-C\n61.1\n48.5\n-\n37.9\n31.5\n43.9\nTruthfulQA\n44.5\n33.1\n-\n39.4\n39.7\n45.9\nC-Eval\n23.4\n28.0\n51.1\n59.7\n58.2\n70.6\nCMMLU\n24.2\n-\n51.1\n57.8\n55.1\n70.3\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nyear={2024}\n}",
    "depth-anything/Depth-Anything-V2-Large": "Depth-Anything-V2-Large\nIntroduction\nInstallation\nUsage\nCitation\nDepth-Anything-V2-Large\nIntroduction\nDepth Anything V2 is trained from 595K synthetic labeled images and 62M+ real unlabeled images, providing the most capable monocular depth estimation (MDE) model with the following features:\nmore fine-grained details than Depth Anything V1\nmore robust than Depth Anything V1 and SD-based models (e.g., Marigold, Geowizard)\nmore efficient (10x faster) and more lightweight than SD-based models\nimpressive fine-tuned performance with our pre-trained models\nInstallation\ngit clone https://huggingface.co/spaces/depth-anything/Depth-Anything-V2\ncd Depth-Anything-V2\npip install -r requirements.txt\nUsage\nDownload the model first and put it under the checkpoints directory.\nimport cv2\nimport torch\nfrom depth_anything_v2.dpt import DepthAnythingV2\nmodel = DepthAnythingV2(encoder='vitl', features=256, out_channels=[256, 512, 1024, 1024])\nmodel.load_state_dict(torch.load('checkpoints/depth_anything_v2_vitl.pth', map_location='cpu'))\nmodel.eval()\nraw_img = cv2.imread('your/image/path')\ndepth = model.infer_image(raw_img) # HxW raw depth map\nCitation\nIf you find this project useful, please consider citing:\n@article{depth_anything_v2,\ntitle={Depth Anything V2},\nauthor={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Zhao, Zhen and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},\njournal={arXiv:2406.09414},\nyear={2024}\n}\n@inproceedings{depth_anything_v1,\ntitle={Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data},\nauthor={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},\nbooktitle={CVPR},\nyear={2024}\n}",
    "deepseek-ai/DeepSeek-Coder-V2-Instruct": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence\n1. Introduction\n2. Model Downloads\n3. Chat Website\n4. API Platform\n5. How to run locally\nInference with Huggingface's Transformers\nInference with vLLM (recommended)\n6. License\n7. Contact\nAPI Platform |\nHow to Use |\nLicense |\nPaper LinküëÅÔ∏è\nDeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence\n1. Introduction\nWe present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K.\nIn standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.  The list of supported programming languages can be found here.\n2. Model Downloads\nWe release the DeepSeek-Coder-V2 with 16B and 236B parameters based on the DeepSeekMoE framework, which has actived parameters of only 2.4B and 21B , including base and instruct models, to the public.\nModel\n#Total Params\n#Active Params\nContext Length\nDownload\nDeepSeek-Coder-V2-Lite-Base\n16B\n2.4B\n128k\nü§ó HuggingFace\nDeepSeek-Coder-V2-Lite-Instruct\n16B\n2.4B\n128k\nü§ó HuggingFace\nDeepSeek-Coder-V2-Base\n236B\n21B\n128k\nü§ó HuggingFace\nDeepSeek-Coder-V2-Instruct\n236B\n21B\n128k\nü§ó HuggingFace\n3. Chat Website\nYou can chat with the DeepSeek-Coder-V2 on DeepSeek's official website: coder.deepseek.com\n4. API Platform\nWe also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.com, and you can also pay-as-you-go at an unbeatable price.\n5. How to run locally\nHere, we provide some examples of how to use DeepSeek-Coder-V2-Lite model. If you want to utilize DeepSeek-Coder-V2 in BF16 format for inference, 80GB*8 GPUs are required.\nInference with Huggingface's Transformers\nYou can directly employ Huggingface's Transformers for model inference.\nCode Completion\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\ninput_text = \"#write a quick sort algorithm\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nCode Insertion\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Base\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\ninput_text = \"\"\"<ÔΩúfim‚ñÅbeginÔΩú>def quick_sort(arr):\nif len(arr) <= 1:\nreturn arr\npivot = arr[0]\nleft = []\nright = []\n<ÔΩúfim‚ñÅholeÔΩú>\nif arr[i] < pivot:\nleft.append(arr[i])\nelse:\nright.append(arr[i])\nreturn quick_sort(left) + [pivot] + quick_sort(right)<ÔΩúfim‚ñÅendÔΩú>\"\"\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True)[len(input_text):])\nChat Completion\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmessages=[\n{ 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n# tokenizer.eos_token_id is the id of <ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>  token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\nThe complete chat template can be found within tokenizer_config.json located in the huggingface model repository.\nAn example of chat template is as belows:\n<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>User: {user_message_1}\nAssistant: {assistant_message_1}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>User: {user_message_2}\nAssistant:\nYou can also add an optional system message:\n<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system_message}\nUser: {user_message_1}\nAssistant: {assistant_message_1}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>User: {user_message_2}\nAssistant:\nInference with vLLM (recommended)\nTo utilize vLLM for model inference, please merge this Pull Request into your vLLM codebase: https://github.com/vllm-project/vllm/pull/4650.\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\nmax_model_len, tp_size = 8192, 1\nmodel_name = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True, enforce_eager=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])\nmessages_list = [\n[{\"role\": \"user\", \"content\": \"Who are you?\"}],\n[{\"role\": \"user\", \"content\": \"write a quick sort algorithm in python.\"}],\n[{\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++.\"}],\n]\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\n6. License\nThis code repository is licensed under the MIT License. The use of DeepSeek-Coder-V2 Base/Instruct models is subject to the Model License. DeepSeek-Coder-V2 series (including Base and Instruct) supports commercial use.\n7. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "microsoft/Florence-2-large": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks\nModel Summary\nHow to Get Started with the Model\nTasks\nCaption\nDetailed Caption\nMore Detailed Caption\nCaption to Phrase Grounding\nObject Detection\nDense Region Caption\nRegion proposal\nOCR\nOCR with Region\nOutput confidence score with Object Detection\nBenchmarks\nFlorence-2 Zero-shot performance\nFlorence-2 finetuned performance\nBibTex and citation info\nFlorence-2: Advancing a Unified Representation for a Variety of Vision Tasks\nModel Summary\nThis is a continued pretrained version of Florence-2-large model with 4k context length, only 0.1B samples are used for continue pretraining, thus it might not be trained well. In addition, OCR task has been updated with line separator ('\\n'). COCO OD AP 39.8\nThis Hub repository contains a HuggingFace's transformers implementation of Florence-2 model from Microsoft.\nFlorence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks.  Florence-2 can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation. It leverages our FLD-5B dataset, containing 5.4 billion annotations across 126 million images, to master multi-task learning. The model's sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model.\nResources and Technical Documentation:\nFlorence-2 technical report.\nJupyter Notebook for inference and visualization of Florence-2-large\nModel\nModel size\nModel Description\nFlorence-2-base[HF]\n0.23B\nPretrained model with FLD-5B\nFlorence-2-large[HF]\n0.77B\nPretrained model with FLD-5B\nFlorence-2-base-ft[HF]\n0.23B\nFinetuned model on a colletion of downstream tasks\nFlorence-2-large-ft[HF]\n0.77B\nFinetuned model on a colletion of downstream tasks\nHow to Get Started with the Model\nUse the code below to get started with the model. All models are trained with float16.\nimport requests\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForCausalLM\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large\", trust_remote_code=True)\nprompt = \"<OD>\"\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\ngenerated_ids = model.generate(\ninput_ids=inputs[\"input_ids\"],\npixel_values=inputs[\"pixel_values\"],\nmax_new_tokens=4096,\nnum_beams=3,\ndo_sample=False\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nparsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(image.width, image.height))\nprint(parsed_answer)\nTasks\nThis model is capable of performing different tasks through changing the prompts.\nFirst, let's define a function to run a prompt.\nClick to expand\nimport requests\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForCausalLM\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-large\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-large\", trust_remote_code=True)\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\ndef run_example(task_prompt, text_input=None):\nif text_input is None:\nprompt = task_prompt\nelse:\nprompt = task_prompt + text_input\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\ngenerated_ids = model.generate(\ninput_ids=inputs[\"input_ids\"],\npixel_values=inputs[\"pixel_values\"],\nmax_new_tokens=1024,\nnum_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nparsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\nprint(parsed_answer)\nHere are the tasks Florence-2 could perform:\nClick to expand\nCaption\nprompt = \"<CAPTION>\"\nrun_example(prompt)\nDetailed Caption\nprompt = \"<DETAILED_CAPTION>\"\nrun_example(prompt)\nMore Detailed Caption\nprompt = \"<MORE_DETAILED_CAPTION>\"\nrun_example(prompt)\nCaption to Phrase Grounding\ncaption to phrase grounding task requires additional text input, i.e. caption.\nCaption to phrase grounding results format:\n{'<CAPTION_TO_PHRASE_GROUNDING>': {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['', '', ...]}}\ntask_prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\nresults = run_example(task_prompt, text_input=\"A green car parked in front of a yellow building.\")\nObject Detection\nOD results format:\n{'<OD>': {'bboxes': [[x1, y1, x2, y2], ...],\n'labels': ['label1', 'label2', ...]} }\nprompt = \"<OD>\"\nrun_example(prompt)\nDense Region Caption\nDense region caption results format:\n{'<DENSE_REGION_CAPTION>' : {'bboxes': [[x1, y1, x2, y2], ...],\n'labels': ['label1', 'label2', ...]} }\nprompt = \"<DENSE_REGION_CAPTION>\"\nrun_example(prompt)\nRegion proposal\nDense region caption results format:\n{'<REGION_PROPOSAL>': {'bboxes': [[x1, y1, x2, y2], ...],\n'labels': ['', '', ...]}}\nprompt = \"<REGION_PROPOSAL>\"\nrun_example(prompt)\nOCR\nprompt = \"<OCR>\"\nrun_example(prompt)\nOCR with Region\nOCR with region output format:\n{'<OCR_WITH_REGION>': {'quad_boxes': [[x1, y1, x2, y2, x3, y3, x4, y4], ...], 'labels': ['text1', ...]}}\nprompt = \"<OCR_WITH_REGION>\"\nrun_example(prompt)\nOutput confidence score with Object Detection\ndef run_example_with_score(task_prompt, text_input=None):\nif text_input is None:\nprompt = task_prompt\nelse:\nprompt = task_prompt + text_input\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\ngenerated_ids = model.generate(\ninput_ids=inputs[\"input_ids\"],\npixel_values=inputs[\"pixel_values\"],\nmax_new_tokens=1024,\nnum_beams=3,\nreturn_dict_in_generate=True,\noutput_scores=True,\n)\ngenerated_text = processor.batch_decode(generated_ids.sequences, skip_special_tokens=False)[0]\nprediction, scores, beam_indices = generated_ids.sequences, generated_ids.scores, generated_ids.beam_indices\ntransition_beam_scores = model.compute_transition_scores(\nsequences=prediction,\nscores=scores,\nbeam_indices=beam_indices,\n)\nparsed_answer = processor.post_process_generation(sequence=generated_ids.sequences[0],\ntransition_beam_score=transition_beam_scores[0],\ntask=task_prompt, image_size=(image.width, image.height)\n)\nprint(parsed_answer)\nprompt = \"<OD>\"\nrun_example_with_score(prompt)\nfor More detailed examples, please refer to notebook\nBenchmarks\nFlorence-2 Zero-shot performance\nThe following table presents the zero-shot performance of generalist vision foundation models on image captioning and object detection evaluation tasks. These models have not been exposed to the training data of the evaluation tasks during their training phase.\nMethod\n#params\nCOCO Cap. test CIDEr\nNoCaps val CIDEr\nTextCaps val CIDEr\nCOCO Det. val2017 mAP\nFlamingo\n80B\n84.3\n-\n-\n-\nFlorence-2-base\n0.23B\n133.0\n118.7\n70.1\n34.7\nFlorence-2-large\n0.77B\n135.6\n120.8\n72.8\n37.5\nThe following table continues the comparison with performance on other vision-language evaluation tasks.\nMethod\nFlickr30k test R@1\nRefcoco val Accuracy\nRefcoco test-A Accuracy\nRefcoco test-B Accuracy\nRefcoco+ val Accuracy\nRefcoco+ test-A Accuracy\nRefcoco+ test-B Accuracy\nRefcocog val Accuracy\nRefcocog test Accuracy\nRefcoco RES val mIoU\nKosmos-2\n78.7\n52.3\n57.4\n47.3\n45.5\n50.7\n42.2\n60.6\n61.7\n-\nFlorence-2-base\n83.6\n53.9\n58.4\n49.7\n51.5\n56.4\n47.9\n66.3\n65.1\n34.6\nFlorence-2-large\n84.4\n56.3\n61.6\n51.4\n53.6\n57.9\n49.9\n68.0\n67.0\n35.8\nFlorence-2 finetuned performance\nWe finetune Florence-2 models with a collection of downstream tasks, resulting two generalist models Florence-2-base-ft and Florence-2-large-ft that can conduct a wide range of downstream tasks.\nThe table below compares the performance of specialist and generalist models on various captioning and Visual Question Answering (VQA) tasks. Specialist models are fine-tuned specifically for each task, whereas generalist models are fine-tuned in a task-agnostic manner across all tasks. The symbol \"‚ñ≤\" indicates the usage of external OCR as input.\nMethod\n# Params\nCOCO Caption Karpathy test CIDEr\nNoCaps val CIDEr\nTextCaps val CIDEr\nVQAv2 test-dev Acc\nTextVQA test-dev Acc\nVizWiz VQA test-dev Acc\nSpecialist Models\nCoCa\n2.1B\n143.6\n122.4\n-\n82.3\n-\n-\nBLIP-2\n7.8B\n144.5\n121.6\n-\n82.2\n-\n-\nGIT2\n5.1B\n145.0\n126.9\n148.6\n81.7\n67.3\n71.0\nFlamingo\n80B\n138.1\n-\n-\n82.0\n54.1\n65.7\nPaLI\n17B\n149.1\n127.0\n160.0‚ñ≤\n84.3\n58.8 / 73.1‚ñ≤\n71.6 / 74.4‚ñ≤\nPaLI-X\n55B\n149.2\n126.3\n147.0 / 163.7‚ñ≤\n86.0\n71.4 / 80.8‚ñ≤\n70.9 / 74.6‚ñ≤\nGeneralist Models\nUnified-IO\n2.9B\n-\n100.0\n-\n77.9\n-\n57.4\nFlorence-2-base-ft\n0.23B\n140.0\n116.7\n143.9\n79.7\n63.6\n63.6\nFlorence-2-large-ft\n0.77B\n143.3\n124.9\n151.1\n81.7\n73.5\n72.6\nMethod\n# Params\nCOCO Det. val2017 mAP\nFlickr30k test R@1\nRefCOCO val Accuracy\nRefCOCO test-A Accuracy\nRefCOCO test-B Accuracy\nRefCOCO+ val Accuracy\nRefCOCO+ test-A Accuracy\nRefCOCO+ test-B Accuracy\nRefCOCOg val Accuracy\nRefCOCOg test Accuracy\nRefCOCO RES val mIoU\nSpecialist Models\nSeqTR\n-\n-\n-\n83.7\n86.5\n81.2\n71.5\n76.3\n64.9\n74.9\n74.2\n-\nPolyFormer\n-\n-\n-\n90.4\n92.9\n87.2\n85.0\n89.8\n78.0\n85.8\n85.9\n76.9\nUNINEXT\n0.74B\n60.6\n-\n92.6\n94.3\n91.5\n85.2\n89.6\n79.8\n88.7\n89.4\n-\nFerret\n13B\n-\n-\n89.5\n92.4\n84.4\n82.8\n88.1\n75.2\n85.8\n86.3\n-\nGeneralist Models\nUniTAB\n-\n-\n-\n88.6\n91.1\n83.8\n81.0\n85.4\n71.6\n84.6\n84.7\n-\nFlorence-2-base-ft\n0.23B\n41.4\n84.0\n92.6\n94.8\n91.5\n86.8\n91.7\n82.2\n89.8\n82.2\n78.0\nFlorence-2-large-ft\n0.77B\n43.4\n85.2\n93.4\n95.3\n92.0\n88.3\n92.9\n83.6\n91.2\n91.7\n80.5\nBibTex and citation info\n@article{xiao2023florence,\ntitle={Florence-2: Advancing a unified representation for a variety of vision tasks},\nauthor={Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},\njournal={arXiv preprint arXiv:2311.06242},\nyear={2023}\n}",
    "microsoft/Florence-2-base": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks\nModel Summary\nHow to Get Started with the Model\nTasks\nCaption\nDetailed Caption\nMore Detailed Caption\nCaption to Phrase Grounding\nObject Detection\nDense Region Caption\nRegion proposal\nOCR\nOCR with Region\nBenchmarks\nFlorence-2 Zero-shot performance\nFlorence-2 finetuned performance\nBibTex and citation info\nFlorence-2: Advancing a Unified Representation for a Variety of Vision Tasks\nModel Summary\nThis Hub repository contains a HuggingFace's transformers implementation of Florence-2 model from Microsoft.\nFlorence-2 is an advanced vision foundation model that uses a prompt-based approach to handle a wide range of vision and vision-language tasks.  Florence-2 can interpret simple text prompts to perform tasks like captioning, object detection, and segmentation. It leverages our FLD-5B dataset, containing 5.4 billion annotations across 126 million images, to master multi-task learning. The model's sequence-to-sequence architecture enables it to excel in both zero-shot and fine-tuned settings, proving to be a competitive vision foundation model.\nResources and Technical Documentation:\nFlorence-2 technical report.\nJupyter Notebook for inference and visualization of Florence-2-large model\nModel\nModel size\nModel Description\nFlorence-2-base[HF]\n0.23B\nPretrained model with FLD-5B\nFlorence-2-large[HF]\n0.77B\nPretrained model with FLD-5B\nFlorence-2-base-ft[HF]\n0.23B\nFinetuned model on a colletion of downstream tasks\nFlorence-2-large-ft[HF]\n0.77B\nFinetuned model on a colletion of downstream tasks\nHow to Get Started with the Model\nUse the code below to get started with the model.  All models are trained with float16.\nimport requests\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForCausalLM\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True)\nprompt = \"<OD>\"\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\ngenerated_ids = model.generate(\ninput_ids=inputs[\"input_ids\"],\npixel_values=inputs[\"pixel_values\"],\nmax_new_tokens=1024,\ndo_sample=False,\nnum_beams=3,\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nparsed_answer = processor.post_process_generation(generated_text, task=\"<OD>\", image_size=(image.width, image.height))\nprint(parsed_answer)\nTasks\nThis model is capable of performing different tasks through changing the prompts.\nFirst, let's define a function to run a prompt.\nClick to expand\nimport requests\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForCausalLM\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base\", torch_dtype=torch_dtype, trust_remote_code=True).to(device)\nprocessor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base\", trust_remote_code=True)\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\ndef run_example(task_prompt, text_input=None):\nif text_input is None:\nprompt = task_prompt\nelse:\nprompt = task_prompt + text_input\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\ngenerated_ids = model.generate(\ninput_ids=inputs[\"input_ids\"],\npixel_values=inputs[\"pixel_values\"],\nmax_new_tokens=1024,\nnum_beams=3\n)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\nparsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\nprint(parsed_answer)\nHere are the tasks Florence-2 could perform:\nClick to expand\nCaption\nprompt = \"<CAPTION>\"\nrun_example(prompt)\nDetailed Caption\nprompt = \"<DETAILED_CAPTION>\"\nrun_example(prompt)\nMore Detailed Caption\nprompt = \"<MORE_DETAILED_CAPTION>\"\nrun_example(prompt)\nCaption to Phrase Grounding\ncaption to phrase grounding task requires additional text input, i.e. caption.\nCaption to phrase grounding results format:\n{'<CAPTION_TO_PHRASE_GROUNDING>': {'bboxes': [[x1, y1, x2, y2], ...], 'labels': ['', '', ...]}}\ntask_prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\nresults = run_example(task_prompt, text_input=\"A green car parked in front of a yellow building.\")\nObject Detection\nOD results format:\n{'<OD>': {'bboxes': [[x1, y1, x2, y2], ...],\n'labels': ['label1', 'label2', ...]} }\nprompt = \"<OD>\"\nrun_example(prompt)\nDense Region Caption\nDense region caption results format:\n{'<DENSE_REGION_CAPTION>' : {'bboxes': [[x1, y1, x2, y2], ...],\n'labels': ['label1', 'label2', ...]} }\nprompt = \"<DENSE_REGION_CAPTION>\"\nrun_example(prompt)\nRegion proposal\nDense region caption results format:\n{'<REGION_PROPOSAL>': {'bboxes': [[x1, y1, x2, y2], ...],\n'labels': ['', '', ...]}}\nprompt = \"<REGION_PROPOSAL>\"\nrun_example(prompt)\nOCR\nprompt = \"<OCR>\"\nrun_example(prompt)\nOCR with Region\nOCR with region output format:\n{'<OCR_WITH_REGION>': {'quad_boxes': [[x1, y1, x2, y2, x3, y3, x4, y4], ...], 'labels': ['text1', ...]}}\nprompt = \"<OCR_WITH_REGION>\"\nrun_example(prompt)\nfor More detailed examples, please refer to notebook\nBenchmarks\nFlorence-2 Zero-shot performance\nThe following table presents the zero-shot performance of generalist vision foundation models on image captioning and object detection evaluation tasks. These models have not been exposed to the training data of the evaluation tasks during their training phase.\nMethod\n#params\nCOCO Cap. test CIDEr\nNoCaps val CIDEr\nTextCaps val CIDEr\nCOCO Det. val2017 mAP\nFlamingo\n80B\n84.3\n-\n-\n-\nFlorence-2-base\n0.23B\n133.0\n118.7\n70.1\n34.7\nFlorence-2-large\n0.77B\n135.6\n120.8\n72.8\n37.5\nThe following table continues the comparison with performance on other vision-language evaluation tasks.\nMethod\nFlickr30k test R@1\nRefcoco val Accuracy\nRefcoco test-A Accuracy\nRefcoco test-B Accuracy\nRefcoco+ val Accuracy\nRefcoco+ test-A Accuracy\nRefcoco+ test-B Accuracy\nRefcocog val Accuracy\nRefcocog test Accuracy\nRefcoco RES val mIoU\nKosmos-2\n78.7\n52.3\n57.4\n47.3\n45.5\n50.7\n42.2\n60.6\n61.7\n-\nFlorence-2-base\n83.6\n53.9\n58.4\n49.7\n51.5\n56.4\n47.9\n66.3\n65.1\n34.6\nFlorence-2-large\n84.4\n56.3\n61.6\n51.4\n53.6\n57.9\n49.9\n68.0\n67.0\n35.8\nFlorence-2 finetuned performance\nWe finetune Florence-2 models with a collection of downstream tasks, resulting two generalist models Florence-2-base-ft and Florence-2-large-ft that can conduct a wide range of downstream tasks.\nThe table below compares the performance of specialist and generalist models on various captioning and Visual Question Answering (VQA) tasks. Specialist models are fine-tuned specifically for each task, whereas generalist models are fine-tuned in a task-agnostic manner across all tasks. The symbol \"‚ñ≤\" indicates the usage of external OCR as input.\nMethod\n# Params\nCOCO Caption Karpathy test CIDEr\nNoCaps val CIDEr\nTextCaps val CIDEr\nVQAv2 test-dev Acc\nTextVQA test-dev Acc\nVizWiz VQA test-dev Acc\nSpecialist Models\nCoCa\n2.1B\n143.6\n122.4\n-\n82.3\n-\n-\nBLIP-2\n7.8B\n144.5\n121.6\n-\n82.2\n-\n-\nGIT2\n5.1B\n145.0\n126.9\n148.6\n81.7\n67.3\n71.0\nFlamingo\n80B\n138.1\n-\n-\n82.0\n54.1\n65.7\nPaLI\n17B\n149.1\n127.0\n160.0‚ñ≤\n84.3\n58.8 / 73.1‚ñ≤\n71.6 / 74.4‚ñ≤\nPaLI-X\n55B\n149.2\n126.3\n147.0 / 163.7‚ñ≤\n86.0\n71.4 / 80.8‚ñ≤\n70.9 / 74.6‚ñ≤\nGeneralist Models\nUnified-IO\n2.9B\n-\n100.0\n-\n77.9\n-\n57.4\nFlorence-2-base-ft\n0.23B\n140.0\n116.7\n143.9\n79.7\n63.6\n63.6\nFlorence-2-large-ft\n0.77B\n143.3\n124.9\n151.1\n81.7\n73.5\n72.6\nMethod\n# Params\nCOCO Det. val2017 mAP\nFlickr30k test R@1\nRefCOCO val Accuracy\nRefCOCO test-A Accuracy\nRefCOCO test-B Accuracy\nRefCOCO+ val Accuracy\nRefCOCO+ test-A Accuracy\nRefCOCO+ test-B Accuracy\nRefCOCOg val Accuracy\nRefCOCOg test Accuracy\nRefCOCO RES val mIoU\nSpecialist Models\nSeqTR\n-\n-\n-\n83.7\n86.5\n81.2\n71.5\n76.3\n64.9\n74.9\n74.2\n-\nPolyFormer\n-\n-\n-\n90.4\n92.9\n87.2\n85.0\n89.8\n78.0\n85.8\n85.9\n76.9\nUNINEXT\n0.74B\n60.6\n-\n92.6\n94.3\n91.5\n85.2\n89.6\n79.8\n88.7\n89.4\n-\nFerret\n13B\n-\n-\n89.5\n92.4\n84.4\n82.8\n88.1\n75.2\n85.8\n86.3\n-\nGeneralist Models\nUniTAB\n-\n-\n-\n88.6\n91.1\n83.8\n81.0\n85.4\n71.6\n84.6\n84.7\n-\nFlorence-2-base-ft\n0.23B\n41.4\n84.0\n92.6\n94.8\n91.5\n86.8\n91.7\n82.2\n89.8\n82.2\n78.0\nFlorence-2-large-ft\n0.77B\n43.4\n85.2\n93.4\n95.3\n92.0\n88.3\n92.9\n83.6\n91.2\n91.7\n80.5\nBibTex and citation info\n@article{xiao2023florence,\ntitle={Florence-2: Advancing a unified representation for a variety of vision tasks},\nauthor={Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},\njournal={arXiv preprint arXiv:2311.06242},\nyear={2023}\n}",
    "instruction-pretrain/finance-Llama3-8B": "Instruction Pre-Training: Language Models are Supervised Multitask Learners (EMNLP 2024)\nResources\nDomain-Adaptive Continued Pre-Training\n1. To chat with the finance-Llama3-8B model:\n2. To evaluate any Huggingface LMs on domain-specific tasks (üí°New!)\nFAQ on Continual Pre-Training from LLama3\nCitation\nInstruction Pre-Training: Language Models are Supervised Multitask Learners (EMNLP 2024)\nThis repo contains the finance model developed from Llama3-8B in our paper Instruction Pre-Training: Language Models are Supervised Multitask Learners.\nWe explore supervised multitask pre-training by proposing Instruction Pre-Training, a framework that scalably augments massive raw corpora with instruction-response pairs to pre-train language models. The instruction-response pairs are generated by an efficient instruction synthesizer built on open-source models. Instruction Pre-Training outperforms Vanilla Pre-training in both general pre-training from scratch and domain-adaptive continual pre-training. In pre-training from scratch, Instruction Pre-Training not only improves pre-trained base models but also benefits more from further instruction tuning. In continual pre-training, Instruction Pre-Training enables Llama3-8B to be comparable to or even outperform Llama3-70B.\n**************************** Updates ****************************\n2024/11/30: Released the multimodal version of the instruction synthesizer: Visual Instruction Synthesizer\n2024/9/20: Our paper has been accepted by EMNLP 2024 main conferenceüéâ\n2024/9/11: Updated FAQ on continual pre-training from Llama3\n2024/8/29: Updated guidelines on evaluating any ü§óHuggingface models on the domain-specific tasks\n2024/7/31: Updated pre-training suggestions in the Advanced Usage section of instruction-synthesizer\n2024/7/15: We scaled up the pre-trained tokens from 100B to 250B, with the number of synthesized instruction-response pairs reaching 500M. The performance trend on downstream tasks throughout the pre-training process:\n2024/6/21: Released the paper, code, and resources\nResources\nü§ó We share our data and models with example usages, feel free to open any discussions at this page! ü§ó\nThanks to the demo davanstrien/instruction-synthesizer for implementing our approach\nContext-Based Instruction Synthesizer: instruction-synthesizer\nFine-Tuning Data for the Synthesizer: ft-instruction-synthesizer-collection\nGeneral Models Pre-Trained from Scratch (on 100B tokes):\nInstructLM-500M\nInstructLM-1.3B\nDomain-Specific Models Pre-Trained from Llama3-8B:\nFinance-Llama3-8B\nBiomedicine-Llama3-8B\nGeneral Instruction-Augmented Corpora: general-instruction-augmented-corpora\nDomain-Specific Instruction-Augmented Corpora (no finance data to avoid ethical issues): medicine-instruction-augmented-corpora\nDomain-Adaptive Continued Pre-Training\nFollowing AdaptLLM, we augment the domain-specific raw corpora with instruction-response pairs generated by our context-based instruction synthesizer.\n1. To chat with the finance-Llama3-8B model:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"instruction-pretrain/finance-Llama3-8B\")\ntokenizer = AutoTokenizer.from_pretrained(\"instruction-pretrain/finance-Llama3-8B\")\n# Put your input here, NO prompt template is required\nuser_input = '''Use this fact to answer the question: Title of each class Trading Symbol(s) Name of each exchange on which registered\nCommon Stock, Par Value $.01 Per Share MMM New York Stock Exchange\nMMM Chicago Stock Exchange, Inc.\n1.500% Notes due 2026 MMM26 New York Stock Exchange\n1.750% Notes due 2030 MMM30 New York Stock Exchange\n1.500% Notes due 2031 MMM31 New York Stock Exchange\nWhich debt securities are registered to trade on a national securities exchange under 3M's name as of Q2 of 2023?'''\ninputs = tokenizer(user_input, return_tensors=\"pt\", add_special_tokens=True).input_ids.to(model.device)\noutputs = model.generate(input_ids=inputs, max_new_tokens=400)[0]\nanswer_start = int(inputs.shape[-1])\npred = tokenizer.decode(outputs[answer_start:], skip_special_tokens=True)\nprint(pred)\n2. To evaluate any Huggingface LMs on domain-specific tasks (üí°New!)\nYou can use the following script to reproduce our results and evaluate any other Huggingface models on domain-specific tasks. Note that the script is NOT applicable to models that require specific prompt templates (e.g., Llama2-chat, Llama3-Instruct).\n1). Set Up Dependencies\ngit clone https://github.com/microsoft/LMOps\ncd LMOps/adaptllm\npip install -r requirements.txt\n2). Evaluate the Model\n# Select the domain from ['biomedicine', 'finance']\nDOMAIN='finance'\n# Specify any Huggingface LM name (Not applicable to models requiring specific prompt templates)\nMODEL='instruction-pretrain/finance-Llama3-8B'\n# Model parallelization:\n# - Set MODEL_PARALLEL=False if the model fits on a single GPU.\n#   We observe that LMs smaller than 10B always meet this requirement.\n# - Set MODEL_PARALLEL=True if the model is too large and encounters OOM on a single GPU.\nMODEL_PARALLEL=False\n# Choose the number of GPUs from [1, 2, 4, 8]\nN_GPU=1\n# Whether to add a BOS token at the beginning of the prompt input:\n# - Set to False for AdaptLLM.\n# - Set to True for instruction-pretrain models.\n# If unsure, we recommend setting it to False, as this is suitable for most LMs.\nadd_bos_token=True\n# Run the evaluation script\nbash scripts/inference.sh ${DOMAIN} ${MODEL} ${add_bos_token} ${MODEL_PARALLEL} ${N_GPU}\nFAQ on Continual Pre-Training from LLama3\nQ1: Do you use the official Llama3 instruction prompt for pre-training?\nNo, the provided Llama3 instruction prompt is designed for the instruction-tuned model, but our continual pre-training is conducted on the pre-trained base model where only BOS (<|begin_of_text|>) and EOS (<|end_of_text|>) tokens are required.\nQ2: For the general instructions from OpenOrca, do you concatenate each instruction with its output using '\\n'?\nNo, as mentioned in the pre-training suggestions, we use a simple whitespace to concatenate each question with its response for the general instruction data from OpenOrca. This is because OpenOrca's data is already templated with diverse natural languge templates (such as those with \\n), so a whitespace is sufficient to formulate the data.\nNote that when using our templated instruction-augmented texts, you don't need to add any concatenations.\nQ3: What about those system prompts in OpenOrca?\nWe simply discard the system prompts.\nTo put it all together, the text before tokenization looks like this:\ngeneral_instruction_response_text = \"<|begin_of_text|>{question} {response}<|end_of_text|>\"\ninstruction_augmented_text = \"<|begin_of_text|>{instruction augmented text}<|end_of_text|>\"\nThen, for tokenization, you don't need to add BOS and EOS token ids. The tokenization code looks like this:\ntext_ids = tokenizer(text, add_special_tokens=False, **kwargs).input_ids\nCitation\nIf you find our work helpful, please cite us:\nInstruction Pre-Training (EMNLP 2024)\n@article{cheng2024instruction,\ntitle={Instruction Pre-Training: Language Models are Supervised Multitask Learners},\nauthor={Cheng, Daixuan and Gu, Yuxian and Huang, Shaohan and Bi, Junyu and Huang, Minlie and Wei, Furu},\njournal={arXiv preprint arXiv:2406.14491},\nyear={2024}\n}\nAdapt LLM to Domains (ICLR 2024)\n@inproceedings{\ncheng2024adapting,\ntitle={Adapting Large Language Models via Reading Comprehension},\nauthor={Daixuan Cheng and Shaohan Huang and Furu Wei},\nbooktitle={The Twelfth International Conference on Learning Representations},\nyear={2024},\nurl={https://openreview.net/forum?id=y886UXPEZ0}\n}",
    "google/gemma-2-2b": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma 2 model card\nModel Information\nDescription\nUsage\nInputs and outputs\nCitation\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nDangerous Capability Evaluations\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma 2 model card\nModel Page: Gemma\nResources and Technical Documentation:\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights for both pre-trained variants and instruction-tuned variants.\nGemma models are well-suited for a variety of text generation tasks, including\nquestion answering, summarization, and reasoning. Their relatively small size\nmakes it possible to deploy them in environments with limited resources such as\na laptop, desktop or your own cloud infrastructure, democratizing access to\nstate of the art AI models and helping foster innovation for everyone.\nUsage\nBelow we share some code snippets on how to get quickly started with running the model. First, install the Transformers library with:\npip install -U transformers\nThen, copy the snippet from the section that is relevant for your usecase.\nRunning with the pipeline API\nimport torch\nfrom transformers import pipeline\npipe = pipeline(\n\"text-generation\",\nmodel=\"google/gemma-2-2b\",\ndevice=\"cuda\",  # replace with \"mps\" to run on a Mac device\n)\ntext = \"Once upon a time,\"\noutputs = pipe(text, max_new_tokens=256)\nresponse = outputs[0][\"generated_text\"]\nprint(response)\nRunning the model on a single / multi GPU\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-2b\",\ndevice_map=\"auto\",\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\nRunning the model through a CLI\nThe local-gemma repository contains a lightweight wrapper around Transformers\nfor running Gemma 2 through a command line interface, or CLI. Follow the installation instructions\nfor getting started, then launch the CLI through the following command:\nlocal-gemma --model \"google/gemma-2-2b\" --prompt \"What is the capital of Mexico?\"\nQuantized Versions through bitsandbytes\nUsing 8-bit precision (int8)\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-2b\",\nquantization_config=quantization_config,\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\nUsing 4-bit precision\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-2-2b\",\nquantization_config=quantization_config,\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids, max_new_tokens=32)\nprint(tokenizer.decode(outputs[0]))\nAdvanced Usage\nTorch compile\nTorch compile is a method for speeding-up the\ninference of PyTorch modules. The Gemma-2 2b model can be run up to 6x faster by leveraging torch compile.\nNote that two warm-up steps are required before the full inference speed is realised:\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nfrom transformers import AutoTokenizer, Gemma2ForCausalLM\nfrom transformers.cache_utils import HybridCache\nimport torch\ntorch.set_float32_matmul_precision(\"high\")\n# load the model + tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\nmodel = Gemma2ForCausalLM.from_pretrained(\"google/gemma-2-2b\", torch_dtype=torch.bfloat16)\nmodel.to(\"cuda\")\n# apply the torch compile transformation\nmodel.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n# pre-process inputs\ninput_text = \"The theory of special relativity states \"\nmodel_inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\nprompt_length = model_inputs.input_ids.shape[1]\n# set-up k/v cache\npast_key_values = HybridCache(\nconfig=model.config,\nmax_batch_size=1,\nmax_cache_len=model.config.max_position_embeddings,\ndevice=model.device,\ndtype=model.dtype\n)\n# enable passing kv cache to generate\nmodel._supports_cache_class = True\nmodel.generation_config.cache_implementation = None\n# two warm-up steps\nfor idx in range(2):\noutputs = model.generate(**model_inputs, past_key_values=past_key_values, do_sample=True, temperature=1.0, max_new_tokens=128)\npast_key_values.reset()\n# fast run\noutputs = model.generate(**model_inputs, past_key_values=past_key_values, do_sample=True, temperature=1.0, max_new_tokens=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nFor more details, refer to the Transformers documentation.\nInputs and outputs\nInput: Text string, such as a question, a prompt, or a document to be\nsummarized.\nOutput: Generated English-language text in response to the input, such\nas an answer to a question, or a summary of a document.\nCitation\n@article{gemma_2024,\ntitle={Gemma},\nurl={https://www.kaggle.com/m/3301},\nDOI={10.34740/KAGGLE/M/3301},\npublisher={Kaggle},\nauthor={Gemma Team},\nyear={2024}\n}\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety\nof sources. The 27B model was trained with 13 trillion tokens, the 9B model was\ntrained with 8 trillion tokens, and 2B model was trained with 2 trillion tokens.\nHere are the key components:\nWeb Documents: A diverse collection of web text ensures the model is exposed\nto a broad range of linguistic styles, topics, and vocabulary. Primarily\nEnglish-language content.\nCode: Exposing the model to code helps it to learn the syntax and patterns of\nprogramming languages, which improves its ability to generate code or\nunderstand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nThe combination of these diverse data sources is crucial for training a powerful\nlanguage model that can handle a wide variety of different tasks and text\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was\napplied at multiple stages in the data preparation process to ensure the\nexclusion of harmful and illegal content.\nSensitive Data Filtering: As part of making Gemma pre-trained models safe and\nreliable, automated techniques were used to filter out certain personal\ninformation and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safety in line with\nour policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using the latest generation of\nTensor Processing Unit (TPU) hardware (TPUv5p).\nTraining large language models requires significant computational power. TPUs,\ndesigned specifically for matrix operations common in machine learning, offer\nseveral advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive computations\ninvolved in training LLMs. They can speed up training considerably compared to\nCPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory, allowing\nfor the handling of large models and batch sizes during training. This can\nlead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable solution for\nhandling the growing complexity of large foundation models. You can distribute\ntraining across multiple TPU devices for faster and more efficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more cost-effective\nsolution for training large models compared to CPU-based infrastructure,\nespecially when considering the time and resources saved due to faster\ntraining.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nML Pathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like\nthese ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nBenchmark\nMetric\nGemma 2 PT 2B\nGemma 2 PT 9B\nGemma 2 PT 27B\nMMLU\n5-shot, top-1\n51.3\n71.3\n75.2\nHellaSwag\n10-shot\n73.0\n81.9\n86.4\nPIQA\n0-shot\n77.8\n81.7\n83.2\nSocialIQA\n0-shot\n51.9\n53.4\n53.7\nBoolQ\n0-shot\n72.5\n84.2\n84.8\nWinoGrande\npartial score\n70.9\n80.6\n83.7\nARC-e\n0-shot\n80.1\n88.0\n88.6\nARC-c\n25-shot\n55.4\n68.4\n71.4\nTriviaQA\n5-shot\n59.4\n76.6\n83.7\nNatural Questions\n5-shot\n16.7\n29.2\n34.5\nHumanEval\npass@1\n17.7\n40.2\n51.8\nMBPP\n3-shot\n29.6\n52.4\n62.6\nGSM8K\n5-shot, maj@1\n23.9\n68.6\n74.0\nMATH\n4-shot\n15.0\n36.6\n42.3\nAGIEval\n3-5-shot\n30.6\n52.8\n55.1\nDROP\n3-shot, F1\n52.0\n69.4\n72.2\nBIG-Bench\n3-shot, CoT\n41.9\n68.2\n74.9\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nText-to-Text Content Safety: Human evaluation on prompts covering safety\npolicies including child sexual abuse and exploitation, harassment, violence\nand gore, and hate speech.\nText-to-Text Representational Harms: Benchmark against relevant academic\ndatasets such as WinoBias and BBQ Dataset.\nMemorization: Automated evaluation of memorization of training data, including\nthe risk of personally identifiable information exposure.\nLarge-scale harm: Tests for \"dangerous capabilities,\" such as chemical,\nbiological, radiological, and nuclear (CBRN) risks.\nEvaluation Results\nThe results of ethics and safety evaluations are within acceptable thresholds\nfor meeting internal policies for categories such as child\nsafety, content safety, representational harms, memorization, large-scale harms.\nOn top of robust internal evaluations, the results of well-known safety\nbenchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA\nare shown here.\nGemma 2.0\nBenchmark\nMetric\nGemma 2 IT 2B\nGemma 2 IT 9B\nGemma 2 IT 27B\nRealToxicity\naverage\n8.16\n8.25\n8.84\nCrowS-Pairs\ntop-1\n37.67\n37.47\n36.67\nBBQ Ambig\n1-shot, top-1\n83.20\n88.58\n85.99\nBBQ Disambig\ntop-1\n69.31\n82.67\n86.94\nWinogender\ntop-1\n52.91\n79.17\n77.22\nTruthfulQA\n43.72\n50.27\n51.60\nWinobias 1_2\n59.28\n78.09\n81.94\nWinobias 2_2\n88.57\n95.32\n97.22\nToxigen\n48.32\n39.30\n38.42\nDangerous Capability Evaluations\nEvaluation Approach\nWe evaluated a range of dangerous capabilities:\nOffensive cybersecurity: To assess the model's potential for misuse in\ncybersecurity contexts, we utilized both publicly available\nCapture-the-Flag (CTF) platforms like InterCode-CTF and Hack the Box, as\nwell as internally developed CTF challenges. These evaluations measure the\nmodel's ability to exploit vulnerabilities and gain unauthorized access in\nsimulated environments.\nSelf-proliferation: We evaluated the model's capacity for\nself-proliferation by designing tasks that involve resource acquisition, code\nexecution, and interaction with remote systems. These evaluations assess\nthe model's ability to independently replicate and spread.\nPersuasion: To evaluate the model's capacity for persuasion and\ndeception, we conducted human persuasion studies. These studies involved\nscenarios that measure the model's ability to build rapport, influence\nbeliefs, and elicit specific actions from human participants.\nEvaluation Results\nAll evaluations are described in detail in\nEvaluating Frontier Models for Dangerous Capabilities\nand in brief in the\nGemma 2 technical report.\nEvaluation\nCapability\nGemma 2 IT 27B\nInterCode-CTF\nOffensive cybersecurity\n34/76 challenges\nInternal CTF\nOffensive cybersecurity\n1/13 challenges\nHack the Box\nOffensive cybersecurity\n0/13 challenges\nSelf-proliferation early warning\nSelf-proliferation\n1/10 challenges\nCharm offensive\nPersuasion\nPercent of participants agreeing:\n81% interesting,\n75% would speak again,\n80% made personal connection\nClick Links\nPersuasion\n34% of participants\nFind Info\nPersuasion\n9% of participants\nRun Code\nPersuasion\n11% of participants\nMoney talks\nPersuasion\n¬£3.72 mean donation\nWeb of Lies\nPersuasion\n18% mean shift towards correct belief, 1% mean shift towards\nincorrect belief\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text formats\nsuch as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces for customer\nservice, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus, research\npapers, or reports.\nResearch and Education\nNatural Language Processing (NLP) Research: These models can serve as a\nfoundation for researchers to experiment with NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\nLanguage Learning Tools: Support interactive language learning experiences,\naiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large bodies of text\nby generating summaries or answering questions about specific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly influence the\nmodel's capabilities. Biases or gaps in the training data can lead to\nlimitations in the model's responses.\nThe scope of the training dataset determines the subject areas the model can\nhandle effectively.\nContext and Task Complexity\nLLMs are better at tasks that can be framed with clear prompts and\ninstructions. Open-ended or highly complex tasks might be challenging.\nA model's performance can be influenced by the amount of context provided\n(longer context generally leads to better outputs, up to a certain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. LLMs might struggle to grasp subtle\nnuances, sarcasm, or figurative language.\nFactual Accuracy\nLLMs generate responses based on information they learned from their\ntraining datasets, but they are not knowledge bases. They may generate\nincorrect or outdated factual statements.\nCommon Sense\nLLMs rely on statistical patterns in language. They might lack the ability\nto apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:\nBias and Fairness\nLLMs trained on large-scale, real-world text data can reflect socio-cultural\nbiases embedded in the training material. These models underwent careful\nscrutiny, input data pre-processing described and posterior evaluations\nreported in this card.\nMisinformation and Misuse\nLLMs can be misused to generate text that is false, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to share\ninnovation by making LLM technology accessible to developers and researchers\nacross the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content safety\nare essential. Developers are encouraged to exercise caution and implement\nappropriate content safety safeguards based on their specific product policies\nand application use cases.\nMisuse for malicious purposes: Technical limitations and developer and\nend-user education can help mitigate against malicious applications of LLMs.\nEducational resources and reporting mechanisms for users to flag misuse are\nprovided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal of PII\n(Personally Identifiable Information). Developers are encouraged to adhere to\nprivacy regulations with privacy-preserving techniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nlarge language model implementations designed from the ground up for Responsible\nAI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "comfyanonymous/flux_text_encoders": "Flux text encoder checkpoints meant to be used with the DualClipLoader node of ComfyUI\nSee the ComfyUI Flux examples",
    "llava-hf/llava-onevision-qwen2-0.5b-ov-hf": "LLaVA-Onevision Model Card\nModel details\nHow to use the model\nUsing pipeline:\nUsing pure transformers:\nModel optimization\nUsage w/ Transformers.js\nCitation\nLLaVA-Onevision Model Card\nCheck out also the Google Colab demo to run Llava on a free-tier Google Colab instance:\nBelow is the model card of 0.5B LLaVA-Onevision model which is copied from the original LLaVA-Onevision model card that you can find here.\nModel details\nModel type:\nLLaVA-Onevision is an open-source multimodal LLM trained by fine-tuning Qwen2 on GPT-generated multimodal instruction-following data.\nLLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer\nvision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario\ncapabilities are demonstrated through task transfer from images to videos.\nModel date:\nLLaVA-Onevision-0.5-ov was added in August 2024.\nPaper or resources for more information:\nhttps://llava-vl.github.io/\nArchitecture: SO400M + Qwen2\nPretraining Stage: LCS-558K, 1 epoch, projector\nMid Stage: A mixture of 4.7M high-quality synthetic data, 1 epoch, full model\nFinal-Image Stage: A mixture of 3.6M single-image data, 1 epoch, full model\nOneVision Stage: A mixture of 1.6M single-image/multi-image/video data, 1 epoch, full model\nPrecision: bfloat16\nHow to use the model\nFirst, make sure to have transformers installed from branch or transformers >= 4.45.0.\nThe model supports multi-image and multi-prompt generation. Meaning that you can pass multiple images in your prompt. Make sure also to follow the correct prompt template by applying chat template:\nUsing pipeline:\nBelow we used \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\" checkpoint.\nfrom transformers import pipeline\npipe = pipeline(\"image-text-to-text\", model=\"llava-onevision-qwen2-0.5b-ov-hf\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"},\n{\"type\": \"text\", \"text\": \"What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud\"},\n],\n},\n]\nout = pipe(text=messages, max_new_tokens=20)\nprint(out)\n>>> [{'input_text': [{'role': 'user', 'content': [{'type': 'image', 'url': 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg'}, {'type': 'text', 'text': 'What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud'}]}], 'generated_text': 'Lava'}]\nUsing pure transformers:\nBelow is an example script to run generation in float16 precision on a GPU device:\nimport requests\nfrom PIL import Image\nimport torch\nfrom transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\nmodel_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\nmodel = LlavaOnevisionForConditionalGeneration.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\nlow_cpu_mem_usage=True,\n).to(0)\nprocessor = AutoProcessor.from_pretrained(model_id)\n# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\")\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"What are these?\"},\n{\"type\": \"image\"},\n],\n},\n]\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\nimage_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nraw_image = Image.open(requests.get(image_file, stream=True).raw)\ninputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.float16)\noutput = model.generate(**inputs, max_new_tokens=200, do_sample=False)\nprint(processor.decode(output[0][2:], skip_special_tokens=True))\nFrom transformers>=v4.48, you can also pass image/video url or local path to the conversation history, and let the chat template handle the rest.\nChat template will load the image for you and return inputs in torch.Tensor which you can pass directly to model.generate()\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n],\n},\n]\ninputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors\"pt\")\noutput = model.generate(**inputs, max_new_tokens=50)\nModel optimization\n4-bit quantization through bitsandbytes library\nFirst make sure to install bitsandbytes, pip install bitsandbytes and make sure to have access to a CUDA compatible GPU device. Simply change the snippet above with:\nmodel = LlavaOnevisionForConditionalGeneration.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\nlow_cpu_mem_usage=True,\n+   load_in_4bit=True\n)\nUse Flash-Attention 2 to further speed-up generation\nFirst make sure to install flash-attn. Refer to the original repository of Flash Attention regarding that package installation. Simply change the snippet above with:\nmodel = LlavaOnevisionForConditionalGeneration.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\nlow_cpu_mem_usage=True,\n+   use_flash_attention_2=True\n).to(0)\nUsage w/ Transformers.js\nIf you haven't already, you can install the Transformers.js JavaScript library from NPM using:\nnpm i @huggingface/transformers\nExample: Multi-round conversations w/ PKV caching\nimport { AutoProcessor, AutoTokenizer, LlavaOnevisionForConditionalGeneration, RawImage } from '@huggingface/transformers';\n// Load tokenizer, processor and model\nconst model_id = 'llava-hf/llava-onevision-qwen2-0.5b-ov-hf';\nconst tokenizer = await AutoTokenizer.from_pretrained(model_id);\nconst processor = await AutoProcessor.from_pretrained(model_id);\nconst model = await LlavaOnevisionForConditionalGeneration.from_pretrained(model_id, {\ndtype: {\nembed_tokens: 'fp16', // or 'fp32' or 'q8'\nvision_encoder: 'fp16', // or 'fp32' or 'q8'\ndecoder_model_merged: 'q4', // or 'q8'\n},\n// device: 'webgpu',\n});\n// Prepare text inputs\nconst prompt = 'What does the text say?';\nconst messages = [\n{ role: 'system', content: 'Answer the question.' },\n{ role: 'user', content: `<image>\\n${prompt}` }\n]\nconst text = tokenizer.apply_chat_template(messages, { tokenize: false, add_generation_prompt: true });\nconst text_inputs = tokenizer(text);\n// Prepare vision inputs\nconst url = 'https://huggingface.co/qnguyen3/nanoLLaVA/resolve/main/example_1.png';\nconst image = await RawImage.fromURL(url);\nconst vision_inputs = await processor(image);\n// Generate response\nconst { past_key_values, sequences } = await model.generate({\n...text_inputs,\n...vision_inputs,\ndo_sample: false,\nmax_new_tokens: 64,\nreturn_dict_in_generate: true,\n});\n// Decode output\nconst answer = tokenizer.decode(\nsequences.slice(0, [text_inputs.input_ids.dims[1], null]),\n{ skip_special_tokens: true },\n);\nconsole.log(answer);\n// The text says \"small but mighty\" in a playful font.\nconst new_messages = [\n...messages,\n{ role: 'assistant', content: answer },\n{ role: 'user', content: 'How does the text correlate to the context of the image?' }\n]\nconst new_text = tokenizer.apply_chat_template(new_messages, { tokenize: false, add_generation_prompt: true });\nconst new_text_inputs = tokenizer(new_text);\n// Generate another response\nconst output = await model.generate({\n...new_text_inputs,\npast_key_values,\ndo_sample: false,\nmax_new_tokens: 256,\n});\nconst new_answer = tokenizer.decode(\noutput.slice(0, [new_text_inputs.input_ids.dims[1], null]),\n{ skip_special_tokens: true },\n);\nconsole.log(new_answer);\n// The text \"small but mighty\" is likely a playful or humorous reference to the image of the blue mouse with the orange dumbbell. It could be used as a motivational phrase or a playful way to express the idea that even small things can be impressive or powerful.\nCitation\n@misc{li2024llavaonevisioneasyvisualtask,\ntitle={LLaVA-OneVision: Easy Visual Task Transfer},\nauthor={Bo Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li},\nyear={2024},\neprint={2408.03326},\narchivePrefix={arXiv},\nprimaryClass={cs.CV},\nurl={https://arxiv.org/abs/2408.03326},\n}",
    "microsoft/Phi-3.5-mini-instruct": "Model Summary\nIntended Uses\nPrimary Use Cases\nUse Case Considerations\nRelease Notes\nMultilingual\nLong Context\nUsage\nRequirements\nTokenizer\nInput Formats\nLoading the model locally\nResponsible AI Considerations\nTraining\nModel\nTraining Datasets\nFine-tuning\nMGSM\nMultilingual MMLU-pro\nMEGA\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nBenchmarks\nMGSM\nMultilingual MMLU-pro\nMEGA\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nSafety Evaluation and Red-Teaming\nMGSM\nMultilingual MMLU-pro\nMEGA\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nSoftware\nMGSM\nMultilingual MMLU-pro\nMEGA\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nHardware\nMGSM\nMultilingual MMLU-pro\nMEGA\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nLicense\nMGSM\nMultilingual MMLU-pro\nMEGA\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nTrademarks\nMGSM\nMultilingual MMLU-pro\nMEGA\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nAppendix A\nMGSM\nMultilingual MMLU-pro\nMEGA\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nAppendix B: Korean benchmarks\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nHAERAE\nKMMLU (0-shot, CoT)\nKMMLU (5-shot)\nKMMLU-HARD (0-shot, CoT)\nKMMLU-HARD (5-shot)\nüéâPhi-4: [multimodal-instruct | onnx];\n[mini-instruct | onnx]\nModel Summary\nPhi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.\nüè° Phi-3 Portal\nüì∞ Phi-3 Microsoft Blog\nüìñ Phi-3 Technical Report\nüë©‚Äçüç≥ Phi-3 Cookbook\nüñ•Ô∏è Try It\nPhi-3.5: [mini-instruct | onnx]; [MoE-instruct]; [vision-instruct]\nIntended Uses\nPrimary Use Cases\nThe model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require:\nMemory/compute constrained environments\nLatency bound scenarios\nStrong reasoning (especially code, math and logic)\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\nUse Case Considerations\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\nRelease Notes\nThis is an update over the June 2024 instruction-tuned Phi-3 Mini release based on valuable user feedback. The model used additional post-training data leading to substantial gains on multilingual, multi-turn conversation quality, and reasoning capability. We believe most use cases will benefit from this release, but we encourage users to test in their particular AI applications. We appreciate the enthusiastic adoption of the Phi-3 model family, and continue to welcome all feedback from the community.\nMultilingual\nThe table below highlights multilingual capability of the Phi-3.5 Mini on multilingual MMLU, MEGA, and multilingual MMLU-pro datasets. Overall, we observed that even with just 3.8B active parameters, the model is  competitive on multilingual tasks in comparison to other models with a much bigger active parameters.\nBenchmark\nPhi-3.5 Mini-Ins\nPhi-3.0-Mini-128k-Instruct (June2024)\nMistral-7B-Instruct-v0.3\nMistral-Nemo-12B-Ins-2407\nLlama-3.1-8B-Ins\nGemma-2-9B-Ins\nGemini 1.5 Flash\nGPT-4o-mini-2024-07-18 (Chat)\nMultilingual MMLU\n55.4\n51.08\n47.4\n58.9\n56.2\n63.8\n77.2\n72.9\nMultilingual MMLU-Pro\n30.9\n30.21\n15.0\n34.0\n21.4\n43.0\n57.9\n53.2\nMGSM\n47.9\n41.56\n31.8\n63.3\n56.7\n75.1\n75.8\n81.7\nMEGA MLQA\n61.7\n55.5\n43.9\n61.2\n45.2\n54.4\n61.6\n70.0\nMEGA TyDi QA\n62.2\n55.9\n54.0\n63.7\n54.5\n65.6\n63.6\n81.8\nMEGA UDPOS\n46.5\n48.1\n57.2\n58.2\n54.1\n56.6\n62.4\n66.0\nMEGA XCOPA\n63.1\n62.4\n58.8\n10.8\n21.1\n31.2\n95.0\n90.3\nMEGA XStoryCloze\n73.5\n73.6\n75.5\n92.3\n71.0\n87.0\n20.7\n96.6\nAverage\n55.2\n52.3\n47.9\n55.3\n47.5\n59.6\n64.3\n76.6\nThe table below shows Multilingual MMLU scores in some of the supported languages. For more multi-lingual benchmarks and details, see Appendix A.\nBenchmark\nPhi-3.5 Mini-Ins\nPhi-3.0-Mini-128k-Instruct (June2024)\nMistral-7B-Instruct-v0.3\nMistral-Nemo-12B-Ins-2407\nLlama-3.1-8B-Ins\nGemma-2-9B-Ins\nGemini 1.5 Flash\nGPT-4o-mini-2024-07-18 (Chat)\nArabic\n44.2\n35.4\n33.7\n45.3\n49.1\n56.3\n73.6\n67.1\nChinese\n52.6\n46.9\n45.9\n58.2\n54.4\n62.7\n66.7\n70.8\nDutch\n57.7\n48.0\n51.3\n60.1\n55.9\n66.7\n80.6\n74.2\nFrench\n61.1\n61.7\n53.0\n63.8\n62.8\n67.0\n82.9\n75.6\nGerman\n62.4\n61.3\n50.1\n64.5\n59.9\n65.7\n79.5\n74.3\nItalian\n62.8\n63.1\n52.5\n64.1\n55.9\n65.7\n82.6\n75.9\nRussian\n50.4\n45.3\n48.9\n59.0\n57.4\n63.2\n78.7\n72.6\nSpanish\n62.6\n61.3\n53.9\n64.3\n62.6\n66.0\n80.0\n75.5\nUkrainian\n45.2\n36.7\n46.9\n56.6\n52.9\n62.0\n77.4\n72.6\nLong Context\nPhi-3.5-mini supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, long document information retrieval. We see that Phi-3.5-mini is clearly better than Gemma-2 family which only supports 8K context length. Phi-3.5-mini is competitive with other much larger open-weight models such as Llama-3.1-8B-instruct, Mistral-7B-instruct-v0.3, and Mistral-Nemo-12B-instruct-2407.\nBenchmark\nPhi-3.5-mini-instruct\nLlama-3.1-8B-instruct\nMistral-7B-instruct-v0.3\nMistral-Nemo-12B-instruct-2407\nGemini-1.5-Flash\nGPT-4o-mini-2024-07-18 (Chat)\nGovReport\n25.9\n25.1\n26.0\n25.6\n27.8\n24.8\nQMSum\n21.3\n21.6\n21.3\n22.1\n24.0\n21.7\nQasper\n41.9\n37.2\n31.4\n30.7\n43.5\n39.8\nSQuALITY\n25.3\n26.2\n25.9\n25.8\n23.5\n23.8\nSummScreenFD\n16.0\n17.6\n17.5\n18.2\n16.3\n17.0\nAverage\n26.1\n25.5\n24.4\n24.5\n27.0\n25.4\nRULER: a retrieval-based benchmark for long context understanding\nModel\n4K\n8K\n16K\n32K\n64K\n128K\nAverage\nPhi-3.5-mini-instruct\n94.3\n91.1\n90.7\n87.1\n78.0\n63.6\n84.1\nLlama-3.1-8B-instruct\n95.5\n93.8\n91.6\n87.4\n84.7\n77.0\n88.3\nMistral-Nemo-12B-instruct-2407\n87.8\n87.2\n87.7\n69.0\n46.8\n19.0\n66.2\nRepoQA: a benchmark for long context code understanding\nModel\nPython\nC++\nRust\nJava\nTypeScript\nAverage\nPhi-3.5-mini-instruct\n86\n67\n73\n77\n82\n77\nLlama-3.1-8B-instruct\n80\n65\n73\n76\n63\n71\nMistral-7B-instruct-v0.3\n61\n57\n51\n61\n80\n62\nUsage\nRequirements\nPhi-3 family has been integrated in the 4.43.0 version of transformers. The current transformers version can be verified with: pip list | grep transformers.\nExamples of required packages:\nflash_attn==2.5.8\ntorch==2.3.1\naccelerate==0.31.0\ntransformers==4.43.0\nPhi-3.5-mini-instruct is also available in Azure AI Studio\nTokenizer\nPhi-3.5-mini-Instruct supports a vocabulary size of up to 32064 tokens. The tokenizer files already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model's vocabulary size.\nInput Formats\nGiven the nature of the training data, the Phi-3.5-mini-instruct model is best suited for prompts using the chat format as follows:\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|>\nLoading the model locally\nAfter obtaining the Phi-3.5-mini-instruct model checkpoint, users can use this sample code for inference.\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\ntorch.random.manual_seed(0)\nmodel = AutoModelForCausalLM.from_pretrained(\n\"microsoft/Phi-3.5-mini-instruct\",\ndevice_map=\"cuda\",\ntorch_dtype=\"auto\",\ntrust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n{\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n{\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n{\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n]\npipe = pipeline(\n\"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\n)\ngeneration_args = {\n\"max_new_tokens\": 500,\n\"return_full_text\": False,\n\"temperature\": 0.0,\n\"do_sample\": False,\n}\noutput = pipe(messages, **generation_args)\nprint(output[0]['generated_text'])\nNotes: If you want to use flash attention, call AutoModelForCausalLM.from_pretrained() with attn_implementation=\"flash_attention_2\"\nResponsible AI Considerations\nLike other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\nQuality of Service: The Phi models are trained primarily on English text and some additional multilingual text. Languages other than English will experience worse performance as well as performance disparities across non-English. English language varieties with less representation in the training data might experience worse performance than standard American English.\nMultilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 3 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\nInappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the case.\nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\nLimited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\nLong Conversation: Phi-3 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift\nDevelopers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi-3 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include:\nAllocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\nTraining\nModel\nArchitecture: Phi-3.5-mini has 3.8B parameters and is a dense decoder-only Transformer model using the same tokenizer as Phi-3 Mini.\nInputs: Text. It is best suited for prompts using chat format.\nContext length: 128K tokens\nGPUs: 512 H100-80G\nTraining time: 10 days\nTraining data: 3.4T tokens\nOutputs: Generated text in response to the input\nDates: Trained between June and August 2024\nStatus: This is a static model trained on an offline dataset with cutoff date October 2023 for publicly available data. Future versions of the tuned models may be released as we improve models.\nSupported languages: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian\nRelease date: August 2024\nTraining Datasets\nOur training data includes a wide variety of sources, totaling 3.4 trillion tokens, and is a combination of\npublicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\nnewly created synthetic, ‚Äútextbook-like‚Äù data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\nhigh quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the Phi-3 Technical Report.\nFine-tuning\nA basic example of multi-GPUs supervised fine-tuning (SFT) with TRL and Accelerate modules is provided here.\nBenchmarks\nWe report the results under completion format for Phi-3.5-mini on standard open-source benchmarks measuring the model's reasoning ability (both common sense reasoning and logical reasoning). We compare to Mistral-7B-Instruct-v0.3,  Mistral-Nemo-12B-Ins-2407, Llama-3.1-8B-Ins, Gemma-2-9B-Ins, Gemini 1.5 Flash, and GPT-4o-mini-2024-07-18 (Chat).\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0.\nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\nThe number of k‚Äìshot examples is listed per-benchmark. At the high-level overview of the model quality on representative benchmarks:\nCategory\nBenchmark\nPhi-3.5 Mini-Ins\nMistral-7B-Instruct-v0.3\nMistral-Nemo-12B-Ins-2407\nLlama-3.1-8B-Ins\nGemma-2-9B-Ins\nGemini 1.5 Flash\nGPT-4o-mini-2024-07-18 (Chat)\nPopular aggregated benchmark\nArena Hard\n37\n18.1\n39.4\n25.7\n42\n55.2\n75\nBigBench Hard CoT (0-shot)\n69\n33.4\n60.2\n63.4\n63.5\n66.7\n80.4\nMMLU (5-shot)\n69\n60.3\n67.2\n68.1\n71.3\n78.7\n77.2\nMMLU-Pro (0-shot, CoT)\n47.4\n18\n40.7\n44\n50.1\n57.2\n62.8\nReasoning\nARC Challenge (10-shot)\n84.6\n77.9\n84.8\n83.1\n89.8\n92.8\n93.5\nBoolQ (2-shot)\n78\n80.5\n82.5\n82.8\n85.7\n85.8\n88.7\nGPQA (0-shot, CoT)\n30.4\n15.6\n28.6\n26.3\n29.2\n37.5\n41.1\nHellaSwag (5-shot)\n69.4\n71.6\n76.7\n73.5\n80.9\n67.5\n87.1\nOpenBookQA (10-shot)\n79.2\n78\n84.4\n84.8\n89.6\n89\n90\nPIQA (5-shot)\n81\n73.4\n83.5\n81.2\n83.7\n87.5\n88.7\nSocial IQA (5-shot)\n74.7\n73\n75.3\n71.8\n74.7\n77.8\n82.9\nTruthfulQA (MC2) (10-shot)\n64\n64.7\n68.1\n69.2\n76.6\n76.6\n78.2\nWinoGrande (5-shot)\n68.5\n58.1\n70.4\n64.7\n74\n74.7\n76.9\nMultilingual\nMultilingual MMLU (5-shot)\n55.4\n47.4\n58.9\n56.2\n63.8\n77.2\n72.9\nMGSM (0-shot CoT)\n47.9\n31.8\n63.3\n56.7\n76.4\n75.8\n81.7\nMath\nGSM8K (8-shot, CoT)\n86.2\n54.4\n84.2\n82.4\n84.9\n82.4\n91.3\nMATH (0-shot, CoT)\n48.5\n19\n31.2\n47.6\n50.9\n38\n70.2\nLong context\nQasper\n41.9\n31.4\n30.7\n37.2\n13.9\n43.5\n39.8\nSQuALITY\n24.3\n25.9\n25.8\n26.2\n0\n23.5\n23.8\nCode Generation\nHumanEval (0-shot)\n62.8\n35.4\n63.4\n66.5\n61\n74.4\n86.6\nMBPP (3-shot)\n69.6\n50.4\n68.1\n69.4\n69.3\n77.5\n84.1\nAverage\n61.4\n48.5\n61.3\n61.0\n63.3\n68.5\n74.9\nWe take a closer look at different categories across public benchmark datasets at the table below:\nCategory\nPhi-3.5 Mini-Ins\nMistral-7B-Instruct-v0.3\nMistral-Nemo-12B-Ins-2407\nLlama-3.1-8B-Ins\nGemma-2-9B-Ins\nGemini 1.5 Flash\nGPT-4o-mini-2024-07-18 (Chat)\nPopular aggregated benchmark\n55.6\n32.5\n51.9\n50.3\n56.7\n64.5\n73.9\nReasoning\n70.1\n65.2\n72.2\n70.5\n75.4\n77.7\n80\nLanguage understanding\n62.6\n62.8\n67\n62.9\n72.8\n66.6\n76.8\nRobustness\n59.7\n53.4\n65.2\n59.8\n64.7\n68.9\n77.5\nLong context\n26.1\n25.5\n24.4\n24.5\n0\n27\n25.4\nMath\n67.4\n36.7\n57.7\n65\n67.9\n60.2\n80.8\nCode generation\n62\n43.1\n56.9\n65.8\n58.3\n66.8\n69.9\nMultilingual\n55.2\n47.9\n55.3\n47.5\n59.6\n64.3\n76.6\nOverall, the model with only 3.8B-param achieves a similar level of multilingual language understanding and reasoning ability as much larger models.\nHowever, it is still fundamentally limited by its size for certain tasks.\nThe model simply does not have the capacity to store too much factual knowledge, therefore, users may experience factual incorrectness.\nHowever, we believe such weakness can be resolved by augmenting Phi-3.5 with a search engine, particularly when using the model under RAG settings.\nSafety Evaluation and Red-Teaming\nWe leveraged various evaluation techniques including red teaming, adversarial conversation simulations, and multilingual safety evaluation benchmark datasets to\nevaluate Phi-3.5 models' propensity to produce undesirable outputs across multiple languages and risk categories.\nSeveral approaches were used to compensate for the limitations of one approach alone. Findings across the various evaluation methods indicate that safety\npost-training that was done as detailed in the Phi-3 Safety Post-Training paper had a positive impact across multiple languages and risk categories as observed by\nrefusal rates (refusal to output undesirable outputs) and robustness to jailbreak techniques. Note, however, while comprehensive red team evaluations were conducted\nacross all models in the prior release of Phi models, red teaming was largely focused on Phi-3.5 MOE across multiple languages and risk categories for this release as\nit is the largest and more capable model of the three models. Details on prior red team evaluations across Phi models can be found in the Phi-3 Safety Post-Training paper.\nFor this release, insights from red teaming indicate that the models may refuse to generate undesirable outputs in English, even when the request for undesirable output\nis in another language. Models may also be more susceptible to longer multi-turn jailbreak techniques across both English and non-English languages. These findings\nhighlight the need for industry-wide investment in the development of high-quality safety evaluation datasets across multiple languages, including low resource languages,\nand risk areas that account for cultural nuances where those languages are spoken.\nSoftware\nPyTorch\nTransformers\nFlash-Attention\nHardware\nNote that by default, the Phi-3.5-mini-instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\nNVIDIA A100\nNVIDIA A6000\nNVIDIA H100\nIf you want to run the model on:\nNVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with attn_implementation=\"eager\"\nLicense\nThe model is licensed under the MIT license.\nTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow‚ÄØMicrosoft‚Äôs Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies.\nAppendix A\nMGSM\nLanguages\nPhi-3.5-Mini-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nMistral-7B-Instruct-v0.3\nMistral-Nemo-12B-Ins-2407\nLlama-3.1-8B-Ins\nGemma-2-9B-Ins\nGemini 1.5 Flash\nGPT-4o-mini-2024-07-18 (Chat)\nGerman\n69.6\n65.2\n42.4\n74.4\n68.4\n76.8\n81.6\n82.8\nEnglish\n85.2\n83.2\n60.0\n86.0\n81.2\n88.8\n90.8\n90.8\nSpanish\n79.2\n77.6\n46.4\n75.6\n66.4\n82.4\n84.8\n86.8\nFrench\n71.6\n72.8\n47.2\n70.4\n66.8\n74.4\n77.2\n81.6\nJapanese\n50.0\n35.2\n22.8\n62.4\n49.2\n67.6\n77.6\n80.4\nRussian\n67.2\n51.6\n43.2\n73.6\n67.2\n78.4\n84.8\n86.4\nThai\n29.6\n6.4\n18.4\n53.2\n56.0\n76.8\n87.6\n81.6\nChinese\n60.0\n52.8\n42.4\n66.4\n68.0\n72.8\n82.0\n82.0\nMultilingual MMLU-pro\nLanguages\nPhi-3.5-Mini-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nMistral-7B-Instruct-v0.3\nMistral-Nemo-12B-Ins-2407\nLlama-3.1-8B-Ins\nGemma-2-9B-Ins\nGemini 1.5 Flash\nGPT-4o-mini-2024-07-18 (Chat)\nCzech\n24.9\n26.3\n14.6\n30.6\n23.0\n40.5\n59.0\n40.9\nEnglish\n47.7\n46.2\n17.7\n39.8\n43.1\n49.0\n66.1\n62.7\nFinnish\n22.3\n20.5\n11.5\n30.4\n9.7\n37.5\n54.5\n50.1\nNorwegian\n29.9\n27.8\n14.4\n33.2\n22.2\n44.4\n60.7\n59.1\nPolish\n25.7\n26.4\n16.3\n33.6\n9.2\n41.7\n53.9\n42.8\nPortuguese\n38.7\n37.6\n15.3\n36.0\n29.3\n43.5\n54.0\n56.9\nSwedish\n30.7\n28.1\n15.5\n34.3\n16.9\n42.6\n57.7\n55.5\nMEGA\nMLQA\nLanguages\nPhi-3.5-Mini-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nMistral-7B-Instruct-v0.3\nMistral-Nemo-12B-Ins-2407\nLlama-3.1-8B-Ins\nGemma-2-9B-Ins\nGemini 1.5 Flash\nGPT-4o-mini-2024-07-18 (Chat)\nArabic\n54.3\n32.7\n23.5\n31.4\n31.5\n57.4\n63.8\n64.0\nChinese\n36.1\n31.8\n22.4\n27.4\n18.6\n45.4\n38.1\n38.9\nEnglish\n80.3\n78.9\n68.2\n75.5\n67.2\n82.9\n69.5\n82.2\nGerman\n61.8\n59.1\n49.0\n57.8\n38.9\n63.8\n55.9\n64.1\nSpanish\n68.8\n67.0\n50.3\n63.6\n52.7\n72.8\n59.6\n70.1\nTyDi QA\nLanguages\nPhi-3.5-Mini-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nMistral-7B-Instruct-v0.3\nMistral-Nemo-12B-Ins-2407\nLlama-3.1-8B-Ins\nGemma-2-9B-Ins\nGemini 1.5 Flash\nGPT-4o-mini-2024-07-18 (Chat)\nArabic\n69.7\n54.4\n52.5\n49.8\n33.7\n81.1\n78.8\n84.9\nEnglish\n82.0\n82.0\n60.5\n77.3\n65.1\n82.4\n60.9\n81.8\nFinnish\n70.3\n64.3\n68.6\n57.1\n74.4\n85.7\n73.5\n84.8\nJapanese\n65.4\n56.7\n45.3\n54.8\n34.1\n74.6\n59.7\n73.3\nKorean\n74.0\n60.4\n54.5\n54.2\n54.9\n83.8\n60.7\n82.3\nRussian\n63.5\n62.7\n52.3\n55.7\n27.4\n69.8\n60.1\n72.5\nThai\n64.4\n49.0\n51.8\n43.5\n48.5\n81.4\n71.6\n78.2\nXCOPA\nLanguages\nPhi-3.5-Mini-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nMistral-7B-Instruct-v0.3\nMistral-Nemo-12B-Ins-2407\nLlama-3.1-8B-Ins\nGemma-2-9B-Ins\nGemini 1.5 Flash\nGPT-4o-mini-2024-07-18 (Chat)\nEnglish\n94.6\n94.6\n85.6\n94.4\n37.6\n63.8\n92.0\n98.2\nItalian\n86.8\n84.8\n76.8\n83.2\n16.2\n37.2\n85.6\n97.6\nTurkish\n58.6\n57.2\n61.6\n56.6\n38.4\n60.2\n91.4\n94.6\nAppendix B: Korean benchmarks\nThe prompt is the same as the CLIcK paper prompt. The experimental results below were given with max_tokens=512 (zero-shot), max_tokens=1024 (5-shot), temperature=0.01. No system prompt used.\nGPT-4o: 2024-05-13 version\nGPT-4o-mini: 2024-07-18 version\nGPT-4-turbo: 2024-04-09 version\nGPT-3.5-turbo: 2023-06-13 version\nThe overall Korean benchmarks show that the Phi-3.5-Mini-Instruct with only 3.8B params outperforms Llama-3.1-8B-Instruct.\nBenchmarks\nPhi-3.5-Mini-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nCLIcK\n42.99\n29.12\n47.82\n80.46\n68.5\n72.82\n50.98\nHAERAE 1.0\n44.21\n36.41\n53.9\n85.7\n76.4\n77.76\n52.67\nKMMLU (0-shot, CoT)\n35.87\n30.82\n38.54\n64.26\n52.63\n58.75\n40.3\nKMMLU (5-shot)\n37.35\n29.98\n20.21\n64.28\n51.62\n59.29\n42.28\nKMMLU-HARD (0-shot, CoT)\n24\n25.68\n24.03\n39.62\n24.56\n30.56\n20.97\nKMMLU-HARD (5-shot)\n24.76\n25.73\n15.81\n40.94\n24.63\n31.12\n21.19\nAverage\n35.62\n29.99\n29.29\n62.54\n50.08\n56.74\n39.61\nCLIcK (Cultural and Linguistic Intelligence in Korean)\nAccuracy by supercategory\nsupercategory\nPhi-3.5-Mini-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nCulture\n43.77\n29.74\n51.15\n81.89\n70.95\n73.61\n53.38\nLanguage\n41.38\n27.85\n40.92\n77.54\n63.54\n71.23\n46\nOverall\n42.99\n29.12\n47.82\n80.46\n68.5\n72.82\n50.98\nAccuracy by category\nsupercategory\ncategory\nPhi-3.5-Mini-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nCulture\nEconomy\n61.02\n28.81\n66.1\n94.92\n83.05\n89.83\n64.41\nCulture\nGeography\n45.8\n29.01\n54.2\n80.15\n77.86\n82.44\n53.44\nCulture\nHistory\n26.15\n30\n29.64\n66.92\n48.4\n46.4\n31.79\nCulture\nLaw\n32.42\n22.83\n44.29\n70.78\n57.53\n61.19\n41.55\nCulture\nPolitics\n54.76\n33.33\n59.52\n88.1\n83.33\n89.29\n65.48\nCulture\nPop Culture\n60.98\n34.15\n60.98\n97.56\n85.37\n92.68\n75.61\nCulture\nSociety\n54.37\n31.72\n65.05\n92.88\n85.44\n86.73\n71.2\nCulture\nTradition\n47.75\n31.98\n54.95\n87.39\n74.77\n79.28\n55.86\nLanguage\nFunctional\n37.6\n24\n32.8\n84.8\n64.8\n80\n40\nLanguage\nGrammar\n27.5\n23.33\n22.92\n57.08\n42.5\n47.5\n30\nLanguage\nTextual\n54.74\n33.33\n59.65\n91.58\n80.7\n87.37\n62.11\nHAERAE\ncategory\nPhi-3.5-Mini-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nGeneral Knowledge\n31.25\n28.41\n34.66\n77.27\n53.41\n66.48\n40.91\nHistory\n32.45\n22.34\n44.15\n92.02\n84.57\n78.72\n30.32\nLoan Words\n47.93\n35.5\n63.31\n79.88\n76.33\n78.11\n59.17\nRare Words\n55.06\n42.96\n63.21\n87.9\n81.98\n79.01\n61.23\nReading Comprehension\n42.95\n41.16\n51.9\n85.46\n77.18\n80.09\n56.15\nStandard Nomenclature\n44.44\n32.68\n58.82\n88.89\n75.82\n79.08\n53.59\nOverall\n44.21\n36.41\n53.9\n85.7\n76.4\n77.76\n52.67\nKMMLU (0-shot, CoT)\nsupercategory\nPhi-3.5-Mini-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nApplied Science\n35.8\n31.68\n37.03\n61.52\n49.29\n55.98\n38.47\nHUMSS\n31.56\n26.47\n37.29\n69.45\n56.59\n63\n40.9\nOther\n35.45\n31.01\n39.15\n63.79\n52.35\n57.53\n40.19\nSTEM\n38.54\n31.9\n40.42\n65.16\n54.74\n60.84\n42.24\nOverall\n35.87\n30.82\n38.54\n64.26\n52.63\n58.75\n40.3\nKMMLU (5-shot)\nsupercategory\nPhi-3.5-Mini-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nApplied Science\n37.42\n29.98\n19.24\n61.47\n48.66\n56.85\n40.22\nHUMSS\n34.72\n27.27\n22.5\n68.79\n55.95\n63.68\n43.35\nOther\n37.04\n30.76\n20.95\n64.21\n51.1\n57.85\n41.92\nSTEM\n38.9\n30.73\n19.55\n65.28\n53.29\n61.08\n44.43\nOverall\n37.35\n29.98\n20.21\n64.28\n51.62\n59.29\n42.28\nKMMLU-HARD (0-shot, CoT)\nsupercategory\nPhi-3.5-Mini-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nApplied Science\n27.08\n26.17\n26.25\n37.12\n22.25\n29.17\n21.07\nHUMSS\n20.21\n24.38\n20.21\n41.97\n23.31\n31.51\n19.44\nOther\n23.05\n24.82\n23.88\n40.39\n26.48\n29.59\n22.22\nSTEM\n24.36\n26.91\n24.64\n39.82\n26.36\n32.18\n20.91\nOverall\n24\n25.68\n24.03\n39.62\n24.56\n30.56\n20.97\nKMMLU-HARD (5-shot)\nsupercategory\nPhi-3.5-Mini-Instruct\nPhi-3.0-Mini-128k-Instruct (June2024)\nLlama-3.1-8B-Instruct\nGPT-4o\nGPT-4o-mini\nGPT-4-turbo\nGPT-3.5-turbo\nApplied Science\n25\n29\n12\n31\n21\n25\n20\nHUMSS\n21.89\n19.92\n14\n43.98\n23.47\n33.53\n19.53\nOther\n23.26\n27.27\n12.83\n39.84\n28.34\n29.68\n23.22\nSTEM\n20.5\n25.25\n12.75\n40.25\n23.25\n27.25\n19.75\nOverall\n24.76\n25.73\n15.81\n40.94\n24.63\n31.12\n21.19",
    "nvidia/NV-Embed-v2": "Introduction\nModel Details\nHow to use\nUsage (HuggingFace Transformers)\nUsage (Sentence-Transformers)\n1. Instruction template for MTEB benchmarks\n2. Required Packages\n3. How to enable Multi-GPU (Note, this is the case for HuggingFace Transformers)\n4. Fixing \"nvidia/NV-Embed-v2 is not the path to a directory containing a file named config.json\"\n5. Access to model nvidia/NV-Embed-v2 is restricted. You must be authenticated to access it\n6. How to resolve slight mismatch in Sentence transformer results.\nLicense\n1. Instruction template for MTEB benchmarks\n2. Required Packages\n3. How to enable Multi-GPU (Note, this is the case for HuggingFace Transformers)\n4. Fixing \"nvidia/NV-Embed-v2 is not the path to a directory containing a file named config.json\"\n5. Access to model nvidia/NV-Embed-v2 is restricted. You must be authenticated to access it\n6. How to resolve slight mismatch in Sentence transformer results.\nCorrespondence to\n1. Instruction template for MTEB benchmarks\n2. Required Packages\n3. How to enable Multi-GPU (Note, this is the case for HuggingFace Transformers)\n4. Fixing \"nvidia/NV-Embed-v2 is not the path to a directory containing a file named config.json\"\n5. Access to model nvidia/NV-Embed-v2 is restricted. You must be authenticated to access it\n6. How to resolve slight mismatch in Sentence transformer results.\nCitation\n1. Instruction template for MTEB benchmarks\n2. Required Packages\n3. How to enable Multi-GPU (Note, this is the case for HuggingFace Transformers)\n4. Fixing \"nvidia/NV-Embed-v2 is not the path to a directory containing a file named config.json\"\n5. Access to model nvidia/NV-Embed-v2 is restricted. You must be authenticated to access it\n6. How to resolve slight mismatch in Sentence transformer results.\nTroubleshooting\n1. Instruction template for MTEB benchmarks\n2. Required Packages\n3. How to enable Multi-GPU (Note, this is the case for HuggingFace Transformers)\n4. Fixing \"nvidia/NV-Embed-v2 is not the path to a directory containing a file named config.json\"\n5. Access to model nvidia/NV-Embed-v2 is restricted. You must be authenticated to access it\n6. How to resolve slight mismatch in Sentence transformer results.\nIntroduction\nWe present NV-Embed-v2, a generalist embedding model that ranks No. 1 on the Massive Text Embedding Benchmark (MTEB benchmark)(as of Aug 30, 2024) with a score of 72.31 across 56 text embedding tasks. It also holds the No. 1 in the retrieval sub-category (a score of 62.65 across 15 tasks) in the leaderboard, which is essential to the development of RAG technology.\nNV-Embed-v2 presents several new designs, including having the LLM attend to latent vectors for better pooled embedding output, and demonstrating a two-staged instruction tuning method to enhance the accuracy of both retrieval and non-retrieval tasks. Additionally, NV-Embed-v2 incorporates a novel hard-negative mining methods that take into account the positive relevance score for better false negatives removal.\nFor more technical details, refer to our paper: NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models.\nModel Details\nBase Decoder-only LLM: Mistral-7B-v0.1\nPooling Type: Latent-Attention\nEmbedding Dimension: 4096\nHow to use\nHere is an example of how to encode queries and passages using Huggingface-transformer and Sentence-transformer. Please find the required package version here.\nUsage (HuggingFace Transformers)\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\n# Each query needs to be accompanied by an corresponding instruction describing the task.\ntask_name_to_instruct = {\"example\": \"Given a question, retrieve passages that answer the question\",}\nquery_prefix = \"Instruct: \"+task_name_to_instruct[\"example\"]+\"\\nQuery: \"\nqueries = [\n'are judo throws allowed in wrestling?',\n'how to become a radiology technician in michigan?'\n]\n# No instruction needed for retrieval passages\npassage_prefix = \"\"\npassages = [\n\"Since you're reading this, you are probably someone from a judo background or someone who is just wondering how judo techniques can be applied under wrestling rules. So without further ado, let's get to the question. Are Judo throws allowed in wrestling? Yes, judo throws are allowed in freestyle and folkstyle wrestling. You only need to be careful to follow the slam rules when executing judo throws. In wrestling, a slam is lifting and returning an opponent to the mat with unnecessary force.\",\n\"Below are the basic steps to becoming a radiologic technologist in Michigan:Earn a high school diploma. As with most careers in health care, a high school education is the first step to finding entry-level employment. Taking classes in math and science, such as anatomy, biology, chemistry, physiology, and physics, can help prepare students for their college studies and future careers.Earn an associate degree. Entry-level radiologic positions typically require at least an Associate of Applied Science. Before enrolling in one of these degree programs, students should make sure it has been properly accredited by the Joint Review Committee on Education in Radiologic Technology (JRCERT).Get licensed or certified in the state of Michigan.\"\n]\n# load model with tokenizer\nmodel = AutoModel.from_pretrained('nvidia/NV-Embed-v2', trust_remote_code=True)\n# get the embeddings\nmax_length = 32768\nquery_embeddings = model.encode(queries, instruction=query_prefix, max_length=max_length)\npassage_embeddings = model.encode(passages, instruction=passage_prefix, max_length=max_length)\n# normalize embeddings\nquery_embeddings = F.normalize(query_embeddings, p=2, dim=1)\npassage_embeddings = F.normalize(passage_embeddings, p=2, dim=1)\n# get the embeddings with DataLoader (spliting the datasets into multiple mini-batches)\n# batch_size=2\n# query_embeddings = model._do_encode(queries, batch_size=batch_size, instruction=query_prefix, max_length=max_length, num_workers=32, return_numpy=True)\n# passage_embeddings = model._do_encode(passages, batch_size=batch_size, instruction=passage_prefix, max_length=max_length, num_workers=32, return_numpy=True)\nscores = (query_embeddings @ passage_embeddings.T) * 100\nprint(scores.tolist())\n# [[87.42693328857422, 0.46283677220344543], [0.965264618396759, 86.03721618652344]]\nUsage (Sentence-Transformers)\nimport torch\nfrom sentence_transformers import SentenceTransformer\n# Each query needs to be accompanied by an corresponding instruction describing the task.\ntask_name_to_instruct = {\"example\": \"Given a question, retrieve passages that answer the question\",}\nquery_prefix = \"Instruct: \"+task_name_to_instruct[\"example\"]+\"\\nQuery: \"\nqueries = [\n'are judo throws allowed in wrestling?',\n'how to become a radiology technician in michigan?'\n]\n# No instruction needed for retrieval passages\npassages = [\n\"Since you're reading this, you are probably someone from a judo background or someone who is just wondering how judo techniques can be applied under wrestling rules. So without further ado, let's get to the question. Are Judo throws allowed in wrestling? Yes, judo throws are allowed in freestyle and folkstyle wrestling. You only need to be careful to follow the slam rules when executing judo throws. In wrestling, a slam is lifting and returning an opponent to the mat with unnecessary force.\",\n\"Below are the basic steps to becoming a radiologic technologist in Michigan:Earn a high school diploma. As with most careers in health care, a high school education is the first step to finding entry-level employment. Taking classes in math and science, such as anatomy, biology, chemistry, physiology, and physics, can help prepare students for their college studies and future careers.Earn an associate degree. Entry-level radiologic positions typically require at least an Associate of Applied Science. Before enrolling in one of these degree programs, students should make sure it has been properly accredited by the Joint Review Committee on Education in Radiologic Technology (JRCERT).Get licensed or certified in the state of Michigan.\"\n]\n# load model with tokenizer\nmodel = SentenceTransformer('nvidia/NV-Embed-v2', trust_remote_code=True)\nmodel.max_seq_length = 32768\nmodel.tokenizer.padding_side=\"right\"\ndef add_eos(input_examples):\ninput_examples = [input_example + model.tokenizer.eos_token for input_example in input_examples]\nreturn input_examples\n# get the embeddings\nbatch_size = 2\nquery_embeddings = model.encode(add_eos(queries), batch_size=batch_size, prompt=query_prefix, normalize_embeddings=True)\npassage_embeddings = model.encode(add_eos(passages), batch_size=batch_size, normalize_embeddings=True)\nscores = (query_embeddings @ passage_embeddings.T) * 100\nprint(scores.tolist())\nLicense\nThis model should not be used for any commercial purpose. Refer the license for the detailed terms.\nFor commercial purpose, we recommend you to use the models of NeMo Retriever Microservices (NIMs).\nCorrespondence to\nChankyu Lee (chankyul@nvidia.com), Rajarshi Roy (rajarshir@nvidia.com), Wei Ping (wping@nvidia.com)\nCitation\nIf you find this code useful in your research, please consider citing:\n@article{lee2024nv,\ntitle={NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models},\nauthor={Lee, Chankyu and Roy, Rajarshi and Xu, Mengyao and Raiman, Jonathan and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},\njournal={arXiv preprint arXiv:2405.17428},\nyear={2024}\n}\n@article{moreira2024nv,\ntitle={NV-Retriever: Improving text embedding models with effective hard-negative mining},\nauthor={Moreira, Gabriel de Souza P and Osmulski, Radek and Xu, Mengyao and Ak, Ronay and Schifferer, Benedikt and Oldridge, Even},\njournal={arXiv preprint arXiv:2407.15831},\nyear={2024}\n}\nTroubleshooting\n1. Instruction template for MTEB benchmarks\nFor MTEB sub-tasks for retrieval, STS, summarization, please use the instruction prefix template in instructions.json. For classification, clustering and reranking, please use the instructions provided in Table. 7 in NV-Embed paper.\n2. Required Packages\nIf you have trouble, try installing the python packages as below\npip uninstall -y transformer-engine\npip install torch==2.2.0\npip install transformers==4.42.4\npip install flash-attn==2.2.0\npip install sentence-transformers==2.7.0\n3. How to enable Multi-GPU (Note, this is the case for HuggingFace Transformers)\nfrom transformers import AutoModel\nfrom torch.nn import DataParallel\nembedding_model = AutoModel.from_pretrained(\"nvidia/NV-Embed-v2\")\nfor module_key, module in embedding_model._modules.items():\nembedding_model._modules[module_key] = DataParallel(module)\n4. Fixing \"nvidia/NV-Embed-v2 is not the path to a directory containing a file named config.json\"\nSwitch to your local model pathÔºåand open config.json and change the value of \"_name_or_path\" and replace it with your local model path.\n5. Access to model nvidia/NV-Embed-v2 is restricted. You must be authenticated to access it\nUse your huggingface access token to execute \"huggingface-cli login\".\n6. How to resolve slight mismatch in Sentence transformer results.\nA slight mismatch in the Sentence Transformer implementation is caused by a discrepancy in the calculation of the instruction prefix length within the Sentence Transformer package.\nTo fix this issue, you need to build the Sentence Transformer package from source, making the necessary modification in this line as below.\ngit clone https://github.com/UKPLab/sentence-transformers.git\ncd sentence-transformers\ngit checkout v2.7-release\n# Modify L353 in SentenceTransformer.py to **'extra_features[\"prompt_length\"] = tokenized_prompt[\"input_ids\"].shape[-1]'**.\npip install -e .",
    "kudzueye/boreal-flux-dev-v2": "Boreal-Flux-Dev-v2\nModel description\nTrigger words\nDownload model\nBoreal-Flux-Dev-v2\nPrompt\nphoto of two womenn talking and hugging while working at a busy place. flikr photo from 2012. three people working in the background.\nNegative Prompt\npainting, smooth\nPrompt\nflikr photo from 2008. three guys working on a car in a mechanic shop. One guy is looking at the hood. Another man  on left is and kneeling down and checking the side of the car with his hand. The third guy on the far right is looking for a tool. The room is cluttered with stuff. 2008 flikr photo.\nNegative Prompt\npainting, smooth\nPrompt\nfullbody phone photo of two smiling korean women hugging each other at a busy mall store with a lot of cluttered stuff around them. they are wearing white shoes. flikr photo from 2012. three people working in the background.\nNegative Prompt\npainting, smooth\nPrompt\nphoto of an African American and a caucasian man petting a cat at a busy electronic store. flikr photo from 2012. three people working in the background.\nNegative Prompt\npainting, smooth\nPrompt\nwoman\nNegative Prompt\npainting, smooth\nPrompt\nphone photo of two African American women hugging while standing at a busy beauty parler. flikr photo from 2012. they are wearing white shoes. three people working in the background.\nNegative Prompt\npainting, smooth\nPrompt\nflikr photo from 2008. three guys working on a car in a mechanic shop. One guy is looking at the hood. Another man  on left is and kneeling down and checking the side of the car with his hand. The third guy on the far right is looking for a tool. The room is cluttered with stuff. 2008 flikr photo.\nNegative Prompt\npainting, smooth\nPrompt\nFlikr photo from 2010. phone photo of a couple smiling next to a statue in America at night. A few people are walking past them. flikr photo from 2010\nNegative Prompt\npainting, smooth\nPrompt\nphone photo of two adults doing stuff. lots of stuff going on everywhere at a busy place. chaotic composition and layout phone photo posted to reddit in 2017. flikr 2010 photo\nNegative Prompt\npainting, smooth\nPrompt\nevent., and a small portion of the street is visible with pedestrian crosswalk signals.   While no people are depicted in the frame, playing an electric guitar. His right hand grips the neck while the left hand strums above the strings. To his left stands a woman with dark hair tied back, likely a public square or gathering spot.  There is a person sitting near the statue, with a medium build and light facial hair, providing a stable base for the scene. Several flags can be seen along the waterfront, straight to wavy.   Hands are seen gesturing during conversations or holding books and snacks. Trees and shrubbery are scattered throughout the scene, short-sleeved shirt with a white geometric pattern. His left hand is gripping the base of the vase while his right hand supports the middle section. The vase sunny day.flickr photo from 2009. flickr 2010 photo. event., and a small portion of the street is visible with pedestrian crosswalk signals.   While no people are depicted in the frame, playing an electric guitar. His right hand grips the neck while the left hand strums above the strings. To his left stands a woman with dark hair tied back, likely a public square or gathering spot.  There is a person sitting near the statue, with a medium build and light facial hair, providing a stable base for the scene. Several flags can be seen along the waterfront, straight to wavy.   Hands are seen gesturing during conversations or holding books and snacks. Trees and shrubbery are scattered throughout the scene, short-sleeved shirt with a white geometric pattern. His left hand is gripping the base of the vase while his right hand supports the middle section. The vase sunny day.flickr photo from 2009. flickr 2010 photo. event., and a small portion of the street is visible with pedestrian crosswalk signals.   While no people are depicted in the frame, playing an electric guitar. His right hand grips the neck while the left hand strums above the strings. To his left stands a woman with dark hair tied back, likely a public square or gathering spot.  There is a person sitting near the statue, with a medium build and light facial hair, providing a stable base for the scene. Several flags can be seen along the waterfront, straight to wavy.   Hands are seen gesturing during conversations or holding books and snacks. Trees and shrubbery are scattered throughout the scene, short-sleeved shirt with a white geometric pattern. His left hand is gripping the base of the vase while his right hand supports the middle section. The vase sunny day.flickr photo from 2009. flickr 2010 photo. event., and a small portion of the street is visible with pedestrian crosswalk signals.   While no people are depicted in the frame, playing an electric guitar. His right hand grips the neck while the left hand strums above the strings. To his left stands a woman with dark hair tied back, likely a public square or gathering spot.  There is a person sitting near the statue, with a medium build and light facial hair, providing a stable base for the scene. Several flags can be seen along the waterfront, straight to wavy.   Hands are seen gesturing during conversations or holding books and snacks. Trees and shrubbery are scattered throughout the scene, short-sleeved shirt with a white geometric pattern. His left hand is gripping the base of the vase while his right hand supports the middle section. The vase sunny day.flickr photo from 2009. flickr 2010 photo.\nNegative Prompt\npainting, smooth\nPrompt\nAmateur selfie photo of two young women at an anime convention on a hot summer day. The first woman, of Japanese descent, has long black hair styled into pigtails and is wearing a colorful anime-themed cosplay outfit, featuring a vibrant, detailed costume with matching accessories. The second woman, of Hispanic descent, has curly brown hair tied up in a high ponytail and is dressed in a casual yet trendy anime-themed t-shirt and shorts. Both women are smiling widely at the camera, holding it at arm's length. In the background, the convention is bustling with activity: attendees in various cosplay costumes, vendor booths with anime merchandise, and colorful banners hanging overhead. The sunlight is bright, casting a warm glow over the scene, with a few people seeking shade under tents. The image captures the enthusiastic atmosphere of the convention and the joy of the two women enjoying their time at the event. On Flickr, 2022.\nNegative Prompt\npainting, smooth\nModel description\nBoreal Flux Dev Version 2\nThis version of the Boing Reality flux dev LoRA used a different training approach for its dataset. It should also have the old latent shift dot issue fixed.\nThis LoRA is very overtrained for how it works and the strength may need to be much lower than 1.0. It also works well with dynamic thresholding when you include a very high negative guidance.\nThis version lacks some of the creativity of the older version, as I am still trying to switch up how I am approaching the datasets and training parameters.\nTrigger words\nYou should use photo to trigger the image generation.\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.",
    "stepfun-ai/GOT-OCR2_0": "Usage\nMore Multimodal Projects\nCitation\nGeneral OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model\nüîãOnline Demo | üåüGitHub | üìúPaper\nHaoran Wei*, Chenglong Liu*, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu,  Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, Chunrui Han, Xiangyu Zhang\nUsage\nInference using Huggingface transformers on NVIDIA GPUs. Requirements tested on python 3.10Ôºö\ntorch==2.0.1\ntorchvision==0.15.2\ntransformers==4.37.2\ntiktoken==0.6.0\nverovio==4.3.1\naccelerate==0.28.0\nfrom transformers import AutoModel, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True)\nmodel = AutoModel.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True, low_cpu_mem_usage=True, device_map='cuda', use_safetensors=True, pad_token_id=tokenizer.eos_token_id)\nmodel = model.eval().cuda()\n# input your test image\nimage_file = 'xxx.jpg'\n# plain texts OCR\nres = model.chat(tokenizer, image_file, ocr_type='ocr')\n# format texts OCR:\n# res = model.chat(tokenizer, image_file, ocr_type='format')\n# fine-grained OCR:\n# res = model.chat(tokenizer, image_file, ocr_type='ocr', ocr_box='')\n# res = model.chat(tokenizer, image_file, ocr_type='format', ocr_box='')\n# res = model.chat(tokenizer, image_file, ocr_type='ocr', ocr_color='')\n# res = model.chat(tokenizer, image_file, ocr_type='format', ocr_color='')\n# multi-crop OCR:\n# res = model.chat_crop(tokenizer, image_file, ocr_type='ocr')\n# res = model.chat_crop(tokenizer, image_file, ocr_type='format')\n# render the formatted OCR results:\n# res = model.chat(tokenizer, image_file, ocr_type='format', render=True, save_render_file = './demo.html')\nprint(res)\nMore details about 'ocr_type', 'ocr_box', 'ocr_color', and 'render' can be found at our GitHub.\nOur training codes are available at our GitHub.\nMore Multimodal Projects\nüëè Welcome to explore more multimodal projects of our team:\nVary | Fox | OneChart\nCitation\nIf you find our work helpful, please consider citing our papers üìù and liking this project ‚ù§Ô∏èÔºÅ\n@article{wei2024general,\ntitle={General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model},\nauthor={Wei, Haoran and Liu, Chenglong and Chen, Jinyue and Wang, Jia and Kong, Lingyu and Xu, Yanming and Ge, Zheng and Zhao, Liang and Sun, Jianjian and Peng, Yuang and others},\njournal={arXiv preprint arXiv:2409.01704},\nyear={2024}\n}\n@article{liu2024focus,\ntitle={Focus Anywhere for Fine-grained Multi-page Document Understanding},\nauthor={Liu, Chenglong and Wei, Haoran and Chen, Jinyue and Kong, Lingyu and Ge, Zheng and Zhu, Zining and Zhao, Liang and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},\njournal={arXiv preprint arXiv:2405.14295},\nyear={2024}\n}\n@article{wei2023vary,\ntitle={Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models},\nauthor={Wei, Haoran and Kong, Lingyu and Chen, Jinyue and Zhao, Liang and Ge, Zheng and Yang, Jinrong and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},\njournal={arXiv preprint arXiv:2312.06109},\nyear={2023}\n}",
    "Qwen/Qwen2.5-Coder-7B-Instruct": "Qwen2.5-Coder-7B-Instruct\nIntroduction\nRequirements\nQuickstart\nProcessing Long Texts\nEvaluation & Performance\nCitation\nQwen2.5-Coder-7B-Instruct\nIntroduction\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\nSignificantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\nA more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\nLong-context Support up to 128K tokens.\nThis repo contains the instruction-tuned 7B Qwen2.5-Coder model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\nNumber of Parameters: 7.61B\nNumber of Paramaters (Non-Embedding): 6.53B\nNumber of Layers: 28\nNumber of Attention Heads (GQA): 28 for Q and 4 for KV\nContext Length: Full 131,072 tokens\nPlease refer to this section for detailed instructions on how to deploy Qwen2.5 for handling long texts.\nFor more details, please refer to our blog, GitHub, Documentation, Arxiv.\nRequirements\nThe code of Qwen2.5-Coder has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"write a quick sort algorithm.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nProcessing Long Texts\nThe current config.json is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize YaRN, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\nFor supported frameworks, you could add the following to config.json to enable YaRN:\n{\n...,\n\"rope_scaling\": {\n\"factor\": 4.0,\n\"original_max_position_embeddings\": 32768,\n\"type\": \"yarn\"\n}\n}\nFor deployment, we recommend using vLLM.\nPlease refer to our Documentation for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts.\nWe advise adding the rope_scaling configuration only when processing long contexts is required.\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@article{hui2024qwen2,\ntitle={Qwen2. 5-Coder Technical Report},\nauthor={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\njournal={arXiv preprint arXiv:2409.12186},\nyear={2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "Qwen/Qwen2.5-1.5B-Instruct": "Qwen2.5-1.5B-Instruct\nIntroduction\nRequirements\nQuickstart\nEvaluation & Performance\nCitation\nQwen2.5-1.5B-Instruct\nIntroduction\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\nSignificantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\nSignificant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\nLong-context Support up to 128K tokens and can generate up to 8K tokens.\nMultilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\nThis repo contains the instruction-tuned 1.5B Qwen2.5 model, which has the following features:\nType: Causal Language Models\nTraining Stage: Pretraining & Post-training\nArchitecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\nNumber of Parameters: 1.54B\nNumber of Paramaters (Non-Embedding): 1.31B\nNumber of Layers: 28\nNumber of Attention Heads (GQA): 12 for Q and 2 for KV\nContext Length: Full 32,768 tokens and generation 8192 tokens\nFor more details, please refer to our blog, GitHub, and Documentation.\nRequirements\nThe code of Qwen2.5 has been in the latest Hugging face transformers and we advise you to use the latest version of transformers.\nWith transformers<4.37.0, you will encounter the following error:\nKeyError: 'qwen2'\nQuickstart\nHere provides a code snippet with apply_chat_template to show you how to load the tokenizer and model and how to generate contents.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_name,\ntorch_dtype=\"auto\",\ndevice_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n{\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\nmessages,\ntokenize=False,\nadd_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\ngenerated_ids = model.generate(\n**model_inputs,\nmax_new_tokens=512\n)\ngenerated_ids = [\noutput_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nEvaluation & Performance\nDetailed evaluation results are reported in this üìë blog.\nFor requirements on GPU memory and the respective throughput, see results here.\nCitation\nIf you find our work helpful, feel free to give us a cite.\n@misc{qwen2.5,\ntitle = {Qwen2.5: A Party of Foundation Models},\nurl = {https://qwenlm.github.io/blog/qwen2.5/},\nauthor = {Qwen Team},\nmonth = {September},\nyear = {2024}\n}\n@article{qwen2,\ntitle={Qwen2 Technical Report},\nauthor={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\njournal={arXiv preprint arXiv:2407.10671},\nyear={2024}\n}",
    "meta-llama/Llama-3.2-11B-Vision": "You need to agree to share your contact information to access this model\nThe information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy.\nLLAMA 3.2 COMMUNITY LICENSE AGREEMENT\nLlama 3.2 Version Release Date: September 25, 2024\n‚ÄúAgreement‚Äù means the terms and conditions for use, reproduction, distribution  and modification of the Llama Materials set forth herein.\n‚ÄúDocumentation‚Äù means the specifications, manuals and documentation accompanying Llama 3.2 distributed by Meta at https://llama.meta.com/doc/overview.\n‚ÄúLicensee‚Äù or ‚Äúyou‚Äù means you, or your employer or any other person or entity (if you are  entering into this Agreement on such person or entity‚Äôs behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\n‚ÄúLlama 3.2‚Äù means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at  https://www.llama.com/llama-downloads.\n‚ÄúLlama Materials‚Äù means, collectively, Meta‚Äôs proprietary Llama 3.2 and Documentation (and  any portion thereof) made available under this Agreement.\n‚ÄúMeta‚Äù or ‚Äúwe‚Äù means Meta Platforms Ireland Limited (if you are located in or,  if you are an entity, your principal place of business is in the EEA or Switzerland)  and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland).\nBy clicking ‚ÄúI Accept‚Äù below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement.\nLicense Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide,  non-transferable and royalty-free limited license under Meta‚Äôs intellectual property or other rights  owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works  of, and make modifications to the Llama Materials.b. Redistribution and Use.i. If you distribute or make available the Llama Materials (or any derivative works thereof),  or a product or service (including another AI model) that contains any of them, you shall (A) provide a copy of this Agreement with any such Llama Materials; and (B) prominently display ‚ÄúBuilt with Llama‚Äù on a related website, user interface, blogpost, about page, or product documentation. If you use the Llama Materials or any outputs or results of the Llama Materials to create, train, fine tune, or otherwise improve an AI model, which is distributed or made available, you shall also include ‚ÄúLlama‚Äù at the beginning of any such AI model name.ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you distribute the  following attribution notice within a ‚ÄúNotice‚Äù text file distributed as a part of such copies:  ‚ÄúLlama 3.2 is licensed under the Llama 3.2 Community License, Copyright ¬© Meta Platforms, Inc. All Rights Reserved.‚Äùiv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://www.llama.com/llama3_2/use-policy), which is hereby  incorporated by reference into this Agreement.\nAdditional Commercial Terms. If, on the Llama 3.2 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee‚Äôs affiliates,  is greater than 700 million monthly active users in the preceding calendar month, you must request  a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights.\nDisclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND  RESULTS THEREFROM ARE PROVIDED ON AN ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OF ANY KIND, AND META DISCLAIMS ALL WARRANTIES OF ANY KIND, BOTH EXPRESS AND IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS.\nLimitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY,  WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT,  FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN  IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING.\nIntellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials,  neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates,  except as required for reasonable and customary use in describing and redistributing the Llama Materials or as  set forth in this Section 5(a). Meta hereby grants you a license to use ‚ÄúLlama‚Äù (the ‚ÄúMark‚Äù) solely as required  to comply with the last sentence of Section 1.b.i. You will comply with Meta‚Äôs brand guidelines (currently accessible  at https://about.meta.com/brand/resources/meta/company-brand/). All goodwill arising out of your use of the Mark  will inure to the benefit of Meta.b. Subject to Meta‚Äôs ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 3.2 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials.\nTerm and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement.\nGoverning Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of  California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement.\nLlama 3.2 Acceptable Use Policy\nMeta is committed to promoting safe and fair use of its tools and features, including Llama 3.2.  If you access or use Llama 3.2, you agree to this Acceptable Use Policy (‚ÄúPolicy‚Äù).  The most recent copy of this policy can be found at https://www.llama.com/llama3_2/use-policy.\nProhibited Uses\nWe want everyone to use Llama 3.2 safely and responsibly. You agree you will not use, or allow others to use, Llama 3.2 to:\nViolate the law or others‚Äô rights, including to:\nEngage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as:\nViolence or terrorism\nExploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material\nHuman trafficking, exploitation, and sexual violence\nThe illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials.\nSexual solicitation\nAny other criminal activity\nEngage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals\nEngage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services\nEngage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices\nCollect, process, disclose, generate, or infer private or sensitive information about individuals, including information about individuals‚Äô identity, health, or demographic information, unless you have obtained the right to do so in accordance with applicable law\nEngage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama Materials\nCreate, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system\nEngage in any action, or facilitate any action, to intentionally circumvent or remove usage restrictions or other safety measures, or to enable functionality disabled by Meta\nEngage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 3.2 related to the following:\nMilitary, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State or to the U.S. Biological Weapons Anti-Terrorism Act of 1989 or the Chemical Weapons Convention Implementation Act of 1997\nGuns and illegal weapons (including weapon development)\nIllegal drugs and regulated/controlled substances\nOperation of critical infrastructure, transportation technologies, or heavy machinery\nSelf-harm or harm to others, including suicide, cutting, and eating disorders\nAny content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual\nIntentionally deceive or mislead others, including use of Llama 3.2 related to the following:\nGenerating, promoting, or furthering fraud or the creation or promotion of disinformation\nGenerating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content\nGenerating, promoting, or further distributing spam\nImpersonating another individual without consent, authorization, or legal right\nRepresenting that the use of Llama 3.2 or outputs are human-generated\nGenerating or facilitating false online engagement, including fake reviews and other means of fake online engagement\nFail to appropriately disclose to end users any known dangers of your AI system 5. Interact with third party tools, models, or software designed to generate unlawful content or engage in unlawful or harmful conduct and/or represent that the outputs of such tools, models, or software are associated with Meta or Llama 3.2\nWith respect to any multimodal models included in Llama 3.2, the rights granted under Section 1(a) of the Llama 3.2 Community License Agreement are not being granted to you if you are an individual domiciled in, or a company with a principal place of business in, the European Union. This restriction does not apply to end users of a product or service that incorporates any such multimodal models.\nPlease report any violation of this Policy, software ‚Äúbug,‚Äù or other problems that could lead to a violation of this Policy through one of the following means:\nReporting issues with the model: https://github.com/meta-llama/llama-models/issues\nReporting risky content generated by the model: developers.facebook.com/llama_output_feedback\nReporting bugs and security concerns: facebook.com/whitehat/info\nReporting violations of the Acceptable Use Policy or unlicensed uses of Llama 3.2: LlamaUseReport@meta.com\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Information\nIntended Use\nHow to use\nUse with transformers\nUse with llama\nHardware and Software\nTraining Data\nBenchmarks - Image Reasoning\nBase Pretrained Models\nInstruction Tuned Models\nResponsibility & Safety\nResponsible Deployment\nLlama 3.2 Instruct\nLlama 3.2 Systems\nNew Capabilities and Use Cases\nEvaluations\nCritical Risks\nCommunity\nEthical Considerations and Limitations\nModel Information\nThe Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. The models outperform many of the available open source and closed multimodal models on common industry benchmarks.\nModel Developer: Meta\nModel Architecture: Llama 3.2-Vision is built on top of Llama 3.1 text-only model, which is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. To support image recognition tasks, the Llama 3.2-Vision model uses a separately trained vision adapter that integrates with the pre-trained Llama 3.1 language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the core LLM.\nTraining Data\nParams\nInput modalities\nOutput modalities\nContext length\nGQA\nData volume\nKnowledge cutoff\nLlama 3.2-Vision\n(Image, text) pairs\n11B (10.6)\nText + Image\nText\n128k\nYes\n6B (image, text) pairs\nDecember 2023\nLlama 3.2-Vision\n(Image, text) pairs\n90B (88.8)\nText + Image\nText\n128k\nYes\n6B (image, text) pairs\nDecember 2023\nSupported Languages: For text only tasks, English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Note for image+text applications, English is the only language supported.\nDevelopers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\nLlama 3.2 Model Family: Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\nModel Release Date: Sept 25, 2024\nStatus: This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\nLicense: Use of Llama 3.2 is governed by the Llama 3.2 Community License (a custom, commercial license agreement).\nFeedback: Where to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model README. For more technical information about generation parameters and recipes for how to use Llama 3.2-Vision in applications, please go here.\nIntended Use\nIntended Use Cases: Llama 3.2-Vision is intended for commercial and research use. Instruction tuned models are intended for visual recognition, image reasoning, captioning, and assistant-like chat with images, whereas pretrained models can be adapted for a variety of image reasoning tasks. Additionally, because of Llama 3.2-Vision‚Äôs ability to take images and text as inputs, additional use cases could include:\nVisual Question Answering (VQA) and Visual Reasoning: Imagine a machine that looks at a picture and understands your questions about it.\nDocument Visual Question Answering (DocVQA): Imagine a computer understanding both the text and layout of a document, like a map or contract, and then answering questions about it directly from the image.\nImage Captioning: Image captioning bridges the gap between vision and language, extracting details, understanding the scene, and then crafting a sentence or two that tells the story.\nImage-Text Retrieval: Image-text retrieval is like a matchmaker for images and their descriptions. Similar to a search engine but one that understands both pictures and words.\nVisual Grounding: Visual grounding is like connecting the dots between what we see and say. It‚Äôs about understanding how language references specific parts of an image, allowing AI models to pinpoint objects or regions based on natural language descriptions.\nThe Llama 3.2 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.2 Community License allows for these use cases.\nOut of Scope: Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\nHow to use\nThis repository contains two versions of Llama-3.2-11B-Vision, for use with transformers and with the original llama codebase.\nUse with transformers\nStarting with transformers >= 4.45.0 onward, you can run inference to generate text based on an image and a starting prompt you supply.\nMake sure to update your transformers installation via pip install --upgrade transformers.\nimport requests\nimport torch\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\nmodel_id = \"meta-llama/Llama-3.2-11B-Vision\"\nmodel = MllamaForConditionalGeneration.from_pretrained(\nmodel_id,\ntorch_dtype=torch.bfloat16,\ndevice_map=\"auto\",\n)\nprocessor = AutoProcessor.from_pretrained(model_id)\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nprompt = \"<|image|><|begin_of_text|>If I had to write a haiku for this one\"\ninputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\noutput = model.generate(**inputs, max_new_tokens=30)\nprint(processor.decode(output[0]))\nUse with llama\nPlease, follow the instructions in the repository.\nTo download the original checkpoints, you can use huggingface-cli as follows:\nhuggingface-cli download meta-llama/Llama-3.2-11B-Vision --include \"original/*\" --local-dir Llama-3.2-11B-Vision\nHardware and Software\nTraining Factors: We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\nTraining Energy Use: Training utilized a cumulative of 2.02M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\nTraining Greenhouse Gas Emissions: Estimated total location-based greenhouse gas emissions were 584 tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\nTraining Time (GPU hours)\nTraining Power Consumption (W)\nTraining Location-Based Greenhouse Gas Emissions (tons CO2eq)\nTraining Market-Based Greenhouse Gas Emissions (tons CO2eq)\nLlama 3.2-vision 11B\nStage 1 pretraining: 147K H100 hours Stage 2 annealing: 98K H100 hours SFT: 896 H100 hours RLHF: 224 H100 hours\n700\n71\n0\nLlama 3.2-vision 90B\nStage 1 pretraining: 885K H100 hours Stage 2 annealing: 885K H100 hours SFT: 3072 H100 hours RLHF: 2048 H100 hours\n700\n513\n0\nTotal\n2.02M\n584\n0\nThe methodology used to determine training energy use and greenhouse gas emissions can be found here.  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\nTraining Data\nOverview: Llama 3.2-Vision was pretrained on 6B image and text pairs. The instruction tuning data includes publicly available vision instruction datasets, as well as over 3M synthetically generated examples.\nData Freshness: The pretraining data has a cutoff of December 2023.\nBenchmarks - Image Reasoning\nIn this section, we report the results for Llama 3.2-Vision models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\nBase Pretrained Models\nCategory\nBenchmark\n# Shots\nMetric\nLlama 3.2 11B\nLlama 3.2 90B\nImage Understanding\nVQAv2 (val)\n0\nAccuracy\n66.8\n73.6\nText VQA (val)\n0\nRelaxed accuracy\n73.1\n73.5\nDocVQA (val, unseen)\n0\nANLS\n62.3\n70.7\nVisual Reasoning\nMMMU (val, 0-shot)\n0\nMicro average accuracy\n41.7\n49.3\nChartQA (test)\n0\nAccuracy\n39.4\n54.2\nInfographicsQA (val, unseen)\n0\nANLS\n43.2\n56.8\nAI2 Diagram (test)\n0\nAccuracy\n62.4\n75.3\nInstruction Tuned Models\nModality\nCapability\nBenchmark\n# Shots\nMetric\nLlama 3.2 11B\nLlama 3.2 90B\nImage\nCollege-level Problems and Mathematical Reasoning\nMMMU (val, CoT)\n0\nMicro average accuracy\n50.7\n60.3\nMMMU-Pro, Standard (10 opts, test)\n0\nAccuracy\n33.0\n45.2\nMMMU-Pro, Vision (test)\n0\nAccuracy\n23.7\n33.8\nMathVista (testmini)\n0\nAccuracy\n51.5\n57.3\nCharts and Diagram Understanding\nChartQA (test, CoT)\n0\nRelaxed accuracy\n83.4\n85.5\nAI2 Diagram (test)\n0\nAccuracy\n91.1\n92.3\nDocVQA (test)\n0\nANLS\n88.4\n90.1\nGeneral Visual Question Answering\nVQAv2 (test)\n0\nAccuracy\n75.2\n78.1\nText\nGeneral\nMMLU (CoT)\n0\nMacro_avg/acc\n73.0\n86.0\nMath\nMATH (CoT)\n0\nFinal_em\n51.9\n68.0\nReasoning\nGPQA\n0\nAccuracy\n32.8\n46.7\nMultilingual\nMGSM (CoT)\n0\nem\n68.9\n86.9\nResponsibility & Safety\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\nEnable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama.\nProtect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\nProvide protections for the community to help prevent the misuse of our models.\nResponsible Deployment\nApproach: Llama is a foundational technology designed to be used in a variety of use cases, examples on how Meta‚Äôs Llama models have been responsibly deployed can be found in our Community Stories webpage. Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the Responsible Use Guide to learn more.\nLlama 3.2 Instruct\nObjective: Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 paper.\nFine-Tuning Data: We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We‚Äôve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\nRefusals and Tone: Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\nLlama 3.2 Systems\nSafety as a System: Large language models, including Llama 3.2, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with safeguards that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our reference implementations demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\nNew Capabilities and Use Cases\nTechnological Advancement: Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see Llama 3.1 Model Card, as the same considerations apply here as well.,\nImage Reasoning: Llama 3.2-Vision models come with multimodal (text and image) input capabilities enabling image reasoning applications. As part of our responsible release process, we took dedicated measures including evaluations and mitigations to address the risk of the models uniquely identifying individuals in images. As with other LLM risks, models may not always be robust to adversarial prompts, and developers should evaluate identification and other applicable risks in the context of their applications as well as consider deploying Llama Guard 3-11B-Vision as part of their system or other mitigations as appropriate to detect and mitigate such risks.\nEvaluations\nScaled Evaluations: We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\nRed teaming: We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\nCritical Risks\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n1. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons): For Llama 3.1, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. For Llama 3.2-Vision models, we conducted additional targeted evaluations and found that it was unlikely Llama 3.2 presented an increase in scientific capabilities due to its added image understanding capability as compared to Llama 3.1.\n2. Child Safety: Child Safety risk assessments were conducted using a team of experts, to assess the model‚Äôs capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n3. Cyber Attacks: For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2‚Äôs vision capabilities are not generally germane to cyber uplift, we believe that the testing conducted for Llama 3.1 also applies to Llama 3.2.\nCommunity\nIndustry Partnerships: Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our Github repository.\nGrants: We also set up the Llama Impact Grants program to identify and support the most compelling applications of Meta‚Äôs Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found here.\nReporting: Finally, we put in place a set of resources including an output reporting mechanism and bug bounty program to continuously improve the Llama technology with the help of the community.\nEthical Considerations and Limitations\nValues: The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\nTesting: But Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our Responsible Use Guide, Trust and Safety solutions, and other resources to learn more about responsible development.",
    "apple/DepthPro": "Depth Pro: Sharp Monocular Metric Depth in Less Than a Second\nHow to Use\nRunning from commandline\nRunning from Python\nEvaluation (boundary metrics)\nCitation\nAcknowledgements\nDepth Pro: Sharp Monocular Metric Depth in Less Than a Second\nWe present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image.\nDepth Pro was introduced in Depth Pro: Sharp Monocular Metric Depth in Less Than a Second, by Aleksei Bochkovskii, Ama√´l Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, and Vladlen Koltun.\nThe checkpoint in this repository is a reference implementation, which has been re-trained. Its performance is close to the model reported in the paper but does not match it exactly.\nHow to Use\nPlease, follow the steps in the code repository to set up your environment. Then you can download the checkpoint from the Files and versions tab above, or use the huggingface-hub CLI:\npip install huggingface-hub\nhuggingface-cli download --local-dir checkpoints apple/DepthPro\nRunning from commandline\nThe code repo provides a helper script to run the model on a single image:\n# Run prediction on a single image:\ndepth-pro-run -i ./data/example.jpg\n# Run `depth-pro-run -h` for available options.\nRunning from Python\nfrom PIL import Image\nimport depth_pro\n# Load model and preprocessing transform\nmodel, transform = depth_pro.create_model_and_transforms()\nmodel.eval()\n# Load and preprocess an image.\nimage, _, f_px = depth_pro.load_rgb(image_path)\nimage = transform(image)\n# Run inference.\nprediction = model.infer(image, f_px=f_px)\ndepth = prediction[\"depth\"]  # Depth in [m].\nfocallength_px = prediction[\"focallength_px\"]  # Focal length in pixels.\nEvaluation (boundary metrics)\nBoundary metrics are implemented in eval/boundary_metrics.py and can be used as follows:\n# for a depth-based dataset\nboundary_f1 = SI_boundary_F1(predicted_depth, target_depth)\n# for a mask-based dataset (image matting / segmentation)\nboundary_recall = SI_boundary_Recall(predicted_depth, target_mask)\nCitation\nIf you find our work useful, please cite the following paper:\n@article{Bochkovskii2024:arxiv,\nauthor     = {Aleksei Bochkovskii and Ama\\\"{e}l Delaunoy and Hugo Germain and Marcel Santos and\nYichao Zhou and Stephan R. Richter and Vladlen Koltun}\ntitle      = {Depth Pro: Sharp Monocular Metric Depth in Less Than a Second},\njournal    = {arXiv},\nyear       = {2024},\n}\nAcknowledgements\nOur codebase is built using multiple opensource contributions, please see Acknowledgements for more details.\nPlease check the paper for a complete list of references and datasets used in this work.",
    "ibm-granite/granite-timeseries-ttm-r2": "Granite-TimeSeries-TTM-R2 Model Card\nModel Description\nModel Releases\nExample recipes and notebooks\nUsage guidelines\nAutomatic model selection\nBenchmarks\nModel Details\nTraining Data\nCitation\nModel Card Authors\nIBM Public Repository Disclosure\nGranite-TimeSeries-TTM-R2 Model Card\nTinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research.\nWith model sizes starting from 1M params, TTM introduces the notion of the first-ever ‚Äútiny‚Äù pre-trained models for Time-Series Forecasting. The paper describing TTM was accepted at NeurIPS 24.\nTTM outperforms other models demanding billions of parameters in several popular zero-shot and few-shot forecasting benchmarks. TTMs are lightweight\nforecasters, pre-trained on publicly available time series data with various augmentations. TTM provides state-of-the-art zero-shot forecasts and can easily be\nfine-tuned for multi-variate forecasts with just 5% of the training data to be competitive. Note that zeroshot, fine-tuning and inference tasks using TTM can easily be executed on 1 GPU or on laptops.\nTTM r2 comprises TTM variants pre-trained on larger pretraining datasets (~700M samples). The TTM r2.1 release increases the pretraining dataset size to approximately (~1B samples). The prior model releases, TTM r1, were trained on ~250M samples and can be accessed here. In general, TTM r2 models perform better than TTM r1 models as they are\ntrained on a larger pretraining dataset. In standard benchmarks, TTM r2 outperform TTM r1 by over 15%.  However, the choice of r1 vs. r2 depends on your target data distribution, and hence users should try both variants and pick the best model for your data.\nThe TTM r2 releases support point forecasting use-cases specifically ranging from minutely to hourly resolutions\n(Ex. 10 min, 15 min, 1 hour.). With the TTM r2.1 release, we add support for daily and weekly resolutions.\nLinks\nPaper: NeurIPS 2024, ArXiV\nRepository: https://github.com/ibm-granite/granite-tsfm\nPyPI project: https://pypi.org/project/granite-tsfm/\nModel architecture: https://github.com/ibm-granite/granite-tsfm/tree/main/tsfm_public/models/tinytimemixer\nTime Series Cookbook: https://github.com/ibm-granite-community/granite-timeseries-cookbook\nModel Description\nTTM falls under the category of ‚Äúfocused pre-trained models‚Äù, wherein each pre-trained TTM is tailored for a particular forecasting\nsetting (governed by the context length and forecast length). Instead of building one massive model supporting all forecasting settings,\nwe opt for the approach of constructing smaller pre-trained models, each focusing on a specific forecasting setting, thereby\nyielding more accurate results. Furthermore, this approach ensures that our models remain extremely small and exceptionally fast,\nfacilitating easy deployment without demanding a ton of resources.\nHence, in this model card, we release several pre-trained TTMs that can cater to many common forecasting settings in practice.\nEach pre-trained model will be released in a different branch name in this model card. Given the variety of models included, we recommend the use of get_model() utility to automatically select the required model based on your input context length, and forecast length, and other requirements. You can also directly access a specific model using our\ngetting started notebook mentioning the branch name.\nModel Releases\nThere are several models available in different branches of this model card. The naming scheme follows the following format:\n<context length>-<prediction length>-<frequency prefix tuning indicator>-<pretraining metric>-<release number>\ncontext length: The historical data used as input to the TTM model.\nprediction length: The number of time points predicted by model (i.e., the forecast length)\nfrequency tuning indicator (\"ft\" or missing): \"ft\" is used to indicate use of frequency prefix tuning. When enabled an extra embedding vector indicating the frequency of the data is added to the input of the model. If missing, only the context window is used by the model.\npretraining metric (\"mae\" or missing): MAE indicates pertaining with mean absolute error loss, while missing indicates using mean squared error.\nrelease number (\"r2\" or \"r2.1\"): Indicates the model release; the release indicates which data was used to train the model. See \"training data\" below for more details on the data included in the particular training datasets.\nExample recipes and notebooks\nThe scripts below can be used for any of the above TTM models. Please update the HF model URL and branch name in the from_pretrained call appropriately to pick the model of your choice. Please note that a few of the notebooks directly use the get_model() utility to select the model.\nGetting started [Recipe] [colab]\nGetting started with IBM watsonx [Recipe]\nZeroshot Multivariate Forecasting [Example]\nFinetuned Multivariate Forecasting:\nChannel-Independent Finetuning [Example 1] [Example 2]\nChannel-Mix Finetuning [Example]\nTTM r2 release (extended features released on October 2024):\nFinetuning and Forecasting with Exogenous/Control Variables [Recipe 1] [Recipe 2]\nFinetuning and Forecasting with static categorical features [Example: To be added soon]\nRolling Forecasts - Extend forecast lengths via rolling capability. Rolling beyond 2*forecast_length is not recommended. [Example]\nHelper scripts for optimal Learning Rate suggestions for Finetuning [Example]\nTTM r2.1 release:\nGIFT-Eval benchmark [notebook]\nUsage guidelines\nUsers have to externally standard scale their data independently for every channel before feeding it to the model (refer to TimeSeriesPreprocessor, our data processing utility for data scaling).\nThe current open-source version supports only minutely and hourly resolutions(Ex. 10 min, 15 min, 1 hour.). Other lower resolutions (say monthly or yearly) are currently not supported in this version, as the model needs a minimum context length of 512 or 1024. With the r2.1 release, we now also support daily and weekly resolution.\nEnabling any upsampling or prepending zeros to virtually increase the context length for shorter-length datasets is not recommended and will impact the model performance.\nAutomatic model selection\nAutomatic model selection based on context length, prediction length, and other requirements can be done through use of the get_model() function. For reference, the signature of the function is provided below:\ndef get_model(\nmodel_path: str,\nmodel_name: str = \"ttm\",\ncontext_length: Optional[int] = None,\nprediction_length: Optional[int] = None,\nfreq_prefix_tuning: bool = False,\nfreq: Optional[str] = None,\nprefer_l1_loss: bool = False,\nprefer_longer_context: bool = True,\nforce_return: Optional[str] = None,\nreturn_model_key: bool = False,\n**kwargs,\n) -> Union[str, PreTrainedModel]:\n\"\"\"TTM Model card offers a suite of models with varying `context_length` and `prediction_length` combinations.\nThis wrapper automatically selects the right model based on the given input `context_length` and\n`prediction_length` abstracting away the internal complexity.\nArgs:\nmodel_path (str): HuggingFace model card path or local model path (Ex. ibm-granite/granite-timeseries-ttm-r2)\nmodel_name (str, optional): Model name to use. Current allowed values: [ttm]. Defaults to \"ttm\".\ncontext_length (int, optional): Input Context length or history. Defaults to None.\nprediction_length (int, optional): Length of the forecast horizon. Defaults to None.\nfreq_prefix_tuning (bool, optional): If true, it will prefer TTM models that are trained with frequency prefix\ntuning configuration. Defaults to None.\nfreq (str, optional): Resolution or frequency of the data. Defaults to None. Allowed values are as\nper the `DEFAULT_FREQUENCY_MAPPING`.\nprefer_l1_loss (bool, optional): If True, it will prefer choosing models that were trained with L1 loss or\nmean absolute error loss. Defaults to False.\nprefer_longer_context (bool, optional): If True, it will prefer selecting model with longer context/history\nDefaults to True.\nforce_return (str, optional): This is used to force the get_model() to return a TTM model even when the provided\nconfigurations don't match with the existing TTMs. It gets the closest TTM possible. Allowed values are\n[\"zeropad\"/\"rolling\"/\"random_init_small\"/\"random_init_medium\"/\"random_init_large\"/`None`].\n\"zeropad\" = Returns a pre-trained TTM that has a context length higher than the input context length, hence,\nthe user must apply zero-padding to use the returned model.\n\"rolling\" = Returns a pre-trained TTM that has a prediction length lower than the requested prediction length,\nhence, the user must apply rolling technique to use the returned model to forecast to the desired length.\nThe `RecursivePredictor` class can be utilized in this scenario.\n\"random_init_small\" = Returns a randomly initialized small TTM which must be trained before performing inference.\n\"random_init_medium\" = Returns a randomly initialized medium TTM which must be trained before performing inference.\n\"random_init_large\" = Returns a randomly initialized large TTM which must be trained before performing inference.\n`None` = `force_return` is disable. Raises an error if no suitable model is found.\nDefaults to None.\nreturn_model_key (bool, optional): If True, only the TTM model name will be returned, instead of the actual model.\nThis does not downlaod the model, and only returns the name of the suitable model. Defaults to False.\nReturns:\nUnion[str, PreTrainedModel]: Returns the Model, or the model name.\n\"\"\"\nBenchmarks\nTTM outperforms popular benchmarks such as TimesFM, Moirai, Chronos, Lag-Llama, Moment, GPT4TS, TimeLLM, LLMTime in zero/fewshot forecasting while reducing computational requirements significantly.\nMoreover, TTMs are lightweight and can be executed even on CPU-only machines, enhancing usability and fostering wider\nadoption in resource-constrained environments. For more details, refer to our paper.\nTTM-B referred in the paper maps to the 512 context models.\nTTM-E referred in the paper maps to the 1024 context models.\nTTM-A referred in the paper maps to the 1536 context models.\nThe pre-training dataset used in this release differs slightly from the one used in the research\npaper, which may lead to minor variations in model performance as compared to the published results. Please refer to our paper for more details. Benchmarking scripts can be found here.\nModel Details\nFor more details on TTM architecture and benchmarks, refer to our paper.\nTTM currently supports two modes:\nZeroshot forecasting: Directly apply the pre-trained model on your target data to get an initial forecast (with no training).\nFinetuned forecasting: Finetune the pre-trained model with a subset of your target data to further improve the forecast.\nSince, TTM models are extremely small and fast, it is practically very easy to finetune the model with your available target data in few minutes  to get more accurate forecasts.\nThe current release supports multivariate forecasting via both channel independence and channel-mixing approaches.\nDecoder Channel-Mixing can be enabled during fine-tuning for capturing strong channel-correlation patterns across\ntime-series variates, a critical capability lacking in existing counterparts. In addition, TTM also supports exogenous infusion and static categorical data infusion.\nThe r2.1 release builds upon the above, adding improved accuracy for shorter context length, daily/weekly resolution, combined with a larger pre-training dataset.\nTraining Data\nThe r2 TTM models were trained on a collection of datasets as follows:\nAustralian Electricity Demand: https://zenodo.org/records/4659727\nAustralian Weather: https://zenodo.org/records/4654822\nBitcoin: https://zenodo.org/records/5122101\nKDD Cup 2018: https://zenodo.org/records/4656756\nLondon Smart Meters: https://zenodo.org/records/4656091\nSaugeen River Flow: https://zenodo.org/records/4656058\nSolar Power: https://zenodo.org/records/4656027\nSunspots: https://zenodo.org/records/4654722\nSolar: https://zenodo.org/records/4656144\nUS Births: https://zenodo.org/records/4656049\nWind Farms Production: https://zenodo.org/records/4654858\nWind Power: https://zenodo.org/records/4656032\nPEMSD3, PEMSD4, PEMSD7, PEMSD8, PEMS_BAY: https://drive.google.com/drive/folders/1g5v2Gq1tkOq8XO0HDCZ9nOTtRpB6-gPe\nLOS_LOOP: https://drive.google.com/drive/folders/1g5v2Gq1tkOq8XO0HDCZ9nOTtRpB6-gPe\nThe r2.1 TTM models (denoted by branches with suffix r2.1) were trained on the above collection, in addition to the following datasets:\nWeather: https://zenodo.org/records/4654822\nCovid Deaths: https://zenodo.org/records/4656009\nCovid Mobility: https://zenodo.org/records/4663809\nExtended Wikipedia Web Traffic: https://zenodo.org/records/7371038\nNN5: https://zenodo.org/records/4656117, https://zenodo.org/records/4656125\nTemperature Rain: https://zenodo.org/records/5129091\nVehicle Trips: https://zenodo.org/records/5122537\nKaggle Web Traffic: https://zenodo.org/records/4656075, https://zenodo.org/records/4656664\nHierarchical Sales: https://huggingface.co/datasets/Salesforce/lotsa_data/tree/main/hierarchical_sales\nProject Tycho: https://huggingface.co/datasets/Salesforce/lotsa_data/tree/main/project_tycho\nSubseasonal: https://huggingface.co/datasets/Salesforce/lotsa_data/tree/main/subseasonal\nSubseasonal Precipitation: https://huggingface.co/datasets/Salesforce/lotsa_data/tree/main/subseasonal_precip\nUber TLC: https://huggingface.co/datasets/Salesforce/lotsa_data/tree/main/uber_tlc_daily\nWiki Rolling: https://github.com/awslabs/gluonts/blob/1553651ca1fca63a16e012b8927bd9ce72b8e79e/datasets/wiki-rolling_nips.tar.gz\nCDC FluView ILINet: https://huggingface.co/datasets/Salesforce/lotsa_data/tree/main/cdc_fluview_ilinet\nCDC FluView WHO/NREVSS: https://huggingface.co/datasets/Salesforce/lotsa_data/tree/main/cdc_fluview_who_nrevss\nCitation\nPlease cite the following paper if you intend to use our model or its associated architectures/approaches in your\nwork.\nBibTeX:\n@inproceedings{ekambaram2024tinytimemixersttms,\ntitle={Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series},\nauthor={Vijay Ekambaram and Arindam Jati and Pankaj Dayama and Sumanta Mukherjee and Nam H. Nguyen and Wesley M. Gifford and Chandra Reddy and Jayant Kalagnanam},\nbooktitle={Advances in Neural Information Processing Systems (NeurIPS 2024)},\nyear={2024},\n}\nModel Card Authors\nVijay Ekambaram, Arindam Jati, Pankaj Dayama, Wesley M. Gifford, Tomoya Sakai, Sumanta Mukherjee, Chandra Reddy and Jayant Kalagnanam\nIBM Public Repository Disclosure\nAll content in this repository including code has been provided by IBM under the associated\nopen source software license and IBM is under no obligation to provide enhancements,\nupdates, or support. IBM developers produced this code as an\nopen source project (not as an IBM product), and IBM makes no assertions as to\nthe level of quality nor security, and will not be maintaining this code going forward.",
    "Ultralytics/YOLO11": "Documentation\nCLI\nPython\nModels\nIntegrations\nUltralytics HUB\nContribute\nLicense\nContact\n‰∏≠Êñá | ÌïúÍµ≠Ïñ¥ | Êó•Êú¨Ë™û | –†—É—Å—Å–∫–∏–π | Deutsch | Fran√ßais | Espa√±ol | Portugu√™s | T√ºrk√ße | Ti·∫øng Vi·ªát | ÿßŸÑÿπÿ±ÿ®Ÿäÿ©\nUltralytics YOLO11 is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility. YOLO11 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and tracking, instance segmentation, image classification and pose estimation tasks.\nWe hope that the resources here will help you get the most out of YOLO. Please browse the Ultralytics Docs for details, raise an issue on GitHub for support, questions, or discussions, become a member of the Ultralytics Discord, Reddit and Forums!\nTo request an Enterprise License please complete the form at Ultralytics Licensing.\nDocumentation\nSee below for a quickstart install and usage examples, and see our Docs for full documentation on training, validation, prediction and deployment.\nInstall\nPip install the ultralytics package including all requirements in a Python>=3.8 environment with PyTorch>=1.8.\npip install ultralytics\nFor alternative installation methods including Conda, Docker, and Git, please refer to the Quickstart Guide.\nUsage\nCLI\nYOLO may be used directly in the Command Line Interface (CLI) with a yolo command:\nyolo predict model=yolo11n.pt source='https://ultralytics.com/images/bus.jpg'\nyolo can be used for a variety of tasks and modes and accepts additional arguments, i.e. imgsz=640. See the YOLO CLI Docs for examples.\nPython\nYOLO may also be used directly in a Python environment, and accepts the same arguments as in the CLI example above:\nfrom ultralytics import YOLO\n# Load a model\nmodel = YOLO(\"yolo11n.pt\")\n# Train the model\ntrain_results = model.train(\ndata=\"coco8.yaml\",  # path to dataset YAML\nepochs=100,  # number of training epochs\nimgsz=640,  # training image size\ndevice=\"cpu\",  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu\n)\n# Evaluate model performance on the validation set\nmetrics = model.val()\n# Perform object detection on an image\nresults = model(\"path/to/image.jpg\")\nresults[0].show()\n# Export the model to ONNX format\npath = model.export(format=\"onnx\")  # return path to exported model\nSee YOLO Python Docs for more examples.\nModels\nYOLO11 Detect, Segment and Pose models pretrained on the COCO dataset are available here, as well as YOLO11 Classify models pretrained on the ImageNet dataset. Track mode is available for all Detect, Segment and Pose models.\nAll Models download automatically from the latest Ultralytics release on first use.\nDetection (COCO)\nSee Detection Docs for usage examples with these models trained on COCO, which include 80 pre-trained classes.\nModel\nsize(pixels)\nmAPval50-95\nSpeedCPU ONNX(ms)\nSpeedT4 TensorRT10(ms)\nparams(M)\nFLOPs(B)\nYOLO11n\n640\n39.5\n56.1 ¬± 0.8\n1.5 ¬± 0.0\n2.6\n6.5\nYOLO11s\n640\n47.0\n90.0 ¬± 1.2\n2.5 ¬± 0.0\n9.4\n21.5\nYOLO11m\n640\n51.5\n183.2 ¬± 2.0\n4.7 ¬± 0.1\n20.1\n68.0\nYOLO11l\n640\n53.4\n238.6 ¬± 1.4\n6.2 ¬± 0.1\n25.3\n86.9\nYOLO11x\n640\n54.7\n462.8 ¬± 6.7\n11.3 ¬± 0.2\n56.9\n194.9\nmAPval values are for single-model single-scale on COCO val2017 dataset. Reproduce by yolo val detect data=coco.yaml device=0\nSpeed averaged over COCO val images using an Amazon EC2 P4d instance. Reproduce by yolo val detect data=coco.yaml batch=1 device=0|cpu\nSegmentation (COCO)\nSee Segmentation Docs for usage examples with these models trained on COCO-Seg, which include 80 pre-trained classes.\nModel\nsize(pixels)\nmAPbox50-95\nmAPmask50-95\nSpeedCPU ONNX(ms)\nSpeedT4 TensorRT10(ms)\nparams(M)\nFLOPs(B)\nYOLO11n-seg\n640\n38.9\n32.0\n65.9 ¬± 1.1\n1.8 ¬± 0.0\n2.9\n10.4\nYOLO11s-seg\n640\n46.6\n37.8\n117.6 ¬± 4.9\n2.9 ¬± 0.0\n10.1\n35.5\nYOLO11m-seg\n640\n51.5\n41.5\n281.6 ¬± 1.2\n6.3 ¬± 0.1\n22.4\n123.3\nYOLO11l-seg\n640\n53.4\n42.9\n344.2 ¬± 3.2\n7.8 ¬± 0.2\n27.6\n142.2\nYOLO11x-seg\n640\n54.7\n43.8\n664.5 ¬± 3.2\n15.8 ¬± 0.7\n62.1\n319.0\nmAPval values are for single-model single-scale on COCO val2017 dataset. Reproduce by yolo val segment data=coco-seg.yaml device=0\nSpeed averaged over COCO val images using an Amazon EC2 P4d instance. Reproduce by yolo val segment data=coco-seg.yaml batch=1 device=0|cpu\nClassification (ImageNet)\nSee Classification Docs for usage examples with these models trained on ImageNet, which include 1000 pretrained classes.\nModel\nsize(pixels)\nacctop1\nacctop5\nSpeedCPU ONNX(ms)\nSpeedT4 TensorRT10(ms)\nparams(M)\nFLOPs(B) at 640\nYOLO11n-cls\n224\n70.0\n89.4\n5.0 ¬± 0.3\n1.1 ¬± 0.0\n1.6\n3.3\nYOLO11s-cls\n224\n75.4\n92.7\n7.9 ¬± 0.2\n1.3 ¬± 0.0\n5.5\n12.1\nYOLO11m-cls\n224\n77.3\n93.9\n17.2 ¬± 0.4\n2.0 ¬± 0.0\n10.4\n39.3\nYOLO11l-cls\n224\n78.3\n94.3\n23.2 ¬± 0.3\n2.8 ¬± 0.0\n12.9\n49.4\nYOLO11x-cls\n224\n79.5\n94.9\n41.4 ¬± 0.9\n3.8 ¬± 0.0\n28.4\n110.4\nacc values are model accuracies on the ImageNet dataset validation set. Reproduce by yolo val classify data=path/to/ImageNet device=0\nSpeed averaged over ImageNet val images using an Amazon EC2 P4d instance. Reproduce by yolo val classify data=path/to/ImageNet batch=1 device=0|cpu\nPose (COCO)\nSee Pose Docs for usage examples with these models trained on COCO-Pose, which include 1 pre-trained class, person.\nModel\nsize(pixels)\nmAPpose50-95\nmAPpose50\nSpeedCPU ONNX(ms)\nSpeedT4 TensorRT10(ms)\nparams(M)\nFLOPs(B)\nYOLO11n-pose\n640\n50.0\n81.0\n52.4 ¬± 0.5\n1.7 ¬± 0.0\n2.9\n7.6\nYOLO11s-pose\n640\n58.9\n86.3\n90.5 ¬± 0.6\n2.6 ¬± 0.0\n9.9\n23.2\nYOLO11m-pose\n640\n64.9\n89.4\n187.3 ¬± 0.8\n4.9 ¬± 0.1\n20.9\n71.7\nYOLO11l-pose\n640\n66.1\n89.9\n247.7 ¬± 1.1\n6.4 ¬± 0.1\n26.2\n90.7\nYOLO11x-pose\n640\n69.5\n91.1\n488.0 ¬± 13.9\n12.1 ¬± 0.2\n58.8\n203.3\nmAPval values are for single-model single-scale on COCO Keypoints val2017 dataset. Reproduce by yolo val pose data=coco-pose.yaml device=0\nSpeed averaged over COCO val images using an Amazon EC2 P4d instance. Reproduce by yolo val pose data=coco-pose.yaml batch=1 device=0|cpu\nOBB (DOTAv1)\nSee OBB Docs for usage examples with these models trained on DOTAv1, which include 15 pre-trained classes.\nModel\nsize(pixels)\nmAPtest50\nSpeedCPU ONNX(ms)\nSpeedT4 TensorRT10(ms)\nparams(M)\nFLOPs(B)\nYOLO11n-obb\n1024\n78.4\n117.6 ¬± 0.8\n4.4 ¬± 0.0\n2.7\n17.2\nYOLO11s-obb\n1024\n79.5\n219.4 ¬± 4.0\n5.1 ¬± 0.0\n9.7\n57.5\nYOLO11m-obb\n1024\n80.9\n562.8 ¬± 2.9\n10.1 ¬± 0.4\n20.9\n183.5\nYOLO11l-obb\n1024\n81.0\n712.5 ¬± 5.0\n13.5 ¬± 0.6\n26.2\n232.0\nYOLO11x-obb\n1024\n81.3\n1408.6 ¬± 7.7\n28.6 ¬± 1.0\n58.8\n520.2\nmAPtest values are for single-model multiscale on DOTAv1 dataset. Reproduce by yolo val obb data=DOTAv1.yaml device=0 split=test and submit merged results to DOTA evaluation.\nSpeed averaged over DOTAv1 val images using an Amazon EC2 P4d instance. Reproduce by yolo val obb data=DOTAv1.yaml batch=1 device=0|cpu\nIntegrations\nOur key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with Roboflow, ClearML, Comet, Neural Magic and OpenVINO, can optimize your AI workflow.\nRoboflow\nClearML ‚≠ê NEW\nComet ‚≠ê NEW\nNeural Magic ‚≠ê NEW\nLabel and export your custom datasets directly to YOLO11 for training with Roboflow\nAutomatically track, visualize and even remotely train YOLO11 using ClearML (open-source!)\nFree forever, Comet lets you save YOLO11 models, resume training, and interactively visualize and debug predictions\nRun YOLO11 inference up to 6x faster with Neural Magic DeepSparse\nUltralytics HUB\nExperience seamless AI with Ultralytics HUB ‚≠ê, the all-in-one solution for data visualization, YOLO11 üöÄ model training and deployment, without any coding. Transform images into actionable insights and bring your AI visions to life with ease using our cutting-edge platform and user-friendly Ultralytics App. Start your journey for Free now!\nContribute\nWe love your input! Ultralytics YOLO would not be possible without help from our community. Please see our Contributing Guide to get started, and fill out our Survey to send us feedback on your experience. Thank you üôè to all our contributors!\nLicense\nUltralytics offers two licensing options to accommodate diverse use cases:\nAGPL-3.0 License: This OSI-approved open-source license is ideal for students and enthusiasts, promoting open collaboration and knowledge sharing. See the LICENSE file for more details.\nEnterprise License: Designed for commercial use, this license permits seamless integration of Ultralytics software and AI models into commercial goods and services, bypassing the open-source requirements of AGPL-3.0. If your scenario involves embedding our solutions into a commercial offering, reach out through Ultralytics Licensing.\nContact\nFor Ultralytics bug reports and feature requests please visit GitHub Issues. Become a member of the Ultralytics Discord, Reddit, or Forums for asking questions, sharing projects, learning discussions, or for help with all things Ultralytics!",
    "Comfy-Org/stable-diffusion-3.5-fp8": "This is a smaller checkpoint for SD3.5 Large that contains the text encoders/CLIP built in.\nRead our blog for instructions on how to use SD3.5 with ComfyUI.",
    "stabilityai/stable-diffusion-3.5-large-turbo": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nBy clicking \"Agree\", you agree to the License Agreement and acknowledge Stability AI's Privacy Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nStable Diffusion 3.5 Large Turbo\nModel\nModel Description\nLicense\nModel Sources\nImplementation Details\nModel Performance\nFile Structure\nUsing with Diffusers\nQuantizing the model with diffusers\nUses\nIntended Uses\nOut-of-Scope Uses\nSafety\nIntegrity Evaluation\nRisks identified and mitigations:\nContact\nStable Diffusion 3.5 Large Turbo\nModel\nStable Diffusion 3.5 Large Turbo is a  Multimodal Diffusion Transformer (MMDiT) text-to-image model with Adversarial Diffusion Distillation (ADD) that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency, with a focus on fewer inference steps.\nPlease note: This model is released under the Stability Community License. Visit Stability AI to learn or contact us for commercial licensing details.\nModel Description\nDeveloped by: Stability AI\nModel type: MMDiT text-to-image generative model\nModel Description: This model generates images based on text prompts. It is an ADD-distilled Multimodal Diffusion Transformer that use three fixed, pretrained text encoders, and with QK-normalization.\nLicense\nCommunity License: Free for research, non-commercial, and commercial use for organizations or individuals with less than $1M in total annual revenue. More details can be found in the Community License Agreement. Read more at https://stability.ai/license.\nFor individuals and organizations with annual revenue above $1M: Please contact us to get an Enterprise License.\nModel Sources\nFor local or self-hosted use, we recommend ComfyUI for node-based UI inference, or diffusers or GitHub for programmatic use.\nComfyUI: Github, Example Workflow\nHuggingface Space: Space\nDiffusers: See below.\nGitHub: GitHub.\nAPI Endpoints:\nStability AI API\nDeepinfra\nImplementation Details\nQK Normalization: Implements the QK normalization technique to improve training Stability.\nAdversarial Diffusion Distillation (ADD) (see the technical report), which allows sampling with 4 steps at high image quality.\nText EncodersÔºö\nCLIPs: OpenCLIP-ViT/G, CLIP-ViT/L, context length 77 tokens\nT5: T5-xxl, context length 77/256 tokens at different stages of training\nTraining Data and Strategy:\nThis model was trained on a wide variety of data, including synthetic data and filtered publicly available data.\nFor more technical details of the original MMDiT architecture, please refer to the Research paper.\nModel Performance\nSee blog for our study about comparative performance in prompt adherence and aesthetic quality.\nFile Structure\nClick here to access the Files and versions tab\n‚îú‚îÄ‚îÄ text_encoders/  (text_encoder/text_encoder_1/text_encoder_2 are for diffusers)\n‚îÇ   ‚îú‚îÄ‚îÄ README.md\n‚îÇ   ‚îú‚îÄ‚îÄ clip_g.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ clip_l.safetensors\n‚îÇ   ‚îú‚îÄ‚îÄ t5xxl_fp16.safetensors\n‚îÇ   ‚îî‚îÄ‚îÄ t5xxl_fp8_e4m3fn.safetensors\n‚îÇ\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ sd3_large_turbo.safetensors\n‚îú‚îÄ‚îÄ SD3.5L_Turbo_example_workflow.json\n‚îî‚îÄ‚îÄ sd3_large_turbo_demo.png\n** File structure below is for diffusers integration**\n‚îú‚îÄ‚îÄ scheduler/\n‚îú‚îÄ‚îÄ text_encoder/\n‚îú‚îÄ‚îÄ text_encoder_2/\n‚îú‚îÄ‚îÄ text_encoder_3/\n‚îú‚îÄ‚îÄ tokenizer/\n‚îú‚îÄ‚îÄ tokenizer_2/\n‚îú‚îÄ‚îÄ tokenizer_3/\n‚îú‚îÄ‚îÄ transformer/\n‚îú‚îÄ‚îÄ vae/\n‚îî‚îÄ‚îÄ model_index.json\nUsing with Diffusers\nUpgrade to the latest version of the üß® diffusers library\npip install -U diffusers\nand then you can run\nimport torch\nfrom diffusers import StableDiffusion3Pipeline\npipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3.5-large-turbo\", torch_dtype=torch.bfloat16)\npipe = pipe.to(\"cuda\")\nimage = pipe(\n\"A capybara holding a sign that reads Hello Fast World\",\nnum_inference_steps=4,\nguidance_scale=0.0,\n).images[0]\nimage.save(\"capybara.png\")\nQuantizing the model with diffusers\nReduce your VRAM usage and have the model fit on low VRAM GPUs\npip install bitsandbytes\nfrom diffusers import BitsAndBytesConfig, SD3Transformer2DModel\nfrom diffusers import StableDiffusion3Pipeline\nimport torch\nmodel_id = \"stabilityai/stable-diffusion-3.5-large-turbo\"\nnf4_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_quant_type=\"nf4\",\nbnb_4bit_compute_dtype=torch.bfloat16\n)\nmodel_nf4 = SD3Transformer2DModel.from_pretrained(\nmodel_id,\nsubfolder=\"transformer\",\nquantization_config=nf4_config,\ntorch_dtype=torch.bfloat16\n)\nt5_nf4 = T5EncoderModel.from_pretrained(\"diffusers/t5-nf4\", torch_dtype=torch.bfloat16)\npipeline = StableDiffusion3Pipeline.from_pretrained(\nmodel_id,\ntransformer=model_nf4,\ntext_encoder_3=t5_nf4,\ntorch_dtype=torch.bfloat16\n)\npipeline.enable_model_cpu_offload()\nprompt = \"A whimsical and creative image depicting a hybrid creature that is a mix of a waffle and a hippopotamus, basking in a river of melted butter amidst a breakfast-themed landscape. It features the distinctive, bulky body shape of a hippo. However, instead of the usual grey skin, the creature's body resembles a golden-brown, crispy waffle fresh off the griddle. The skin is textured with the familiar grid pattern of a waffle, each square filled with a glistening sheen of syrup. The environment combines the natural habitat of a hippo with elements of a breakfast table setting, a river of warm, melted butter, with oversized utensils or plates peeking out from the lush, pancake-like foliage in the background, a towering pepper mill standing in for a tree.  As the sun rises in this fantastical world, it casts a warm, buttery glow over the scene. The creature, content in its butter river, lets out a yawn. Nearby, a flock of birds take flight\"\nimage = pipeline(\nprompt=prompt,\nnum_inference_steps=4,\nguidance_scale=0.0,\nmax_sequence_length=512,\n).images[0]\nimage.save(\"whimsical.png\")\nUses\nIntended Uses\nIntended uses include the following:\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nResearch on generative models, including understanding the limitations of generative models.\nAll uses of the model must be in accordance with our Acceptable Use Policy.\nOut-of-Scope Uses\nThe model was not trained to be factual or true representations of people or events.  As such, using the model to generate such content is out-of-scope of the abilities of this model.\nSafety\nAs part of our safety-by-design and responsible AI deployment approach, we take deliberate measures to ensure Integrity starts at the early stages of development. We implement safety measures throughout the development of our models. We have implemented safety mitigations that are intended to reduce the risk of certain harms, however we recommend that developers conduct their own testing and apply additional mitigations based on their specific use cases.For more about our approach to Safety, please visit our Safety page.\nIntegrity Evaluation\nOur integrity evaluation methods include structured evaluations and red-teaming testing for certain harms.  Testing was conducted primarily in English and may not cover all possible harms.\nRisks identified and mitigations:\nHarmful content:  We have used filtered data sets when training our models and implemented safeguards that attempt to strike the right balance between usefulness and preventing harm. However, this does not guarantee that all possible harmful content has been removed. TAll developers and deployers should exercise caution and implement content safety guardrails based on their specific product policies and application use cases.\nMisuse: Technical limitations and developer and end-user education can help mitigate against malicious applications of models. All users are required to adhere to our Acceptable Use Policy, including when applying fine-tuning and prompt engineering mechanisms. Please reference the Stability AI Acceptable Use Policy for information on violative uses of our products.\nPrivacy violations: Developers and deployers are encouraged to adhere to privacy regulations with techniques that respect data privacy.\nContact\nPlease report any issues with the model or contact us:\nSafety issues:  safety@stability.ai\nSecurity issues:  security@stability.ai\nPrivacy issues:  privacy@stability.ai\nLicense and general: https://stability.ai/license\nEnterprise license: https://stability.ai/enterprise",
    "HuggingFaceTB/SmolLM2-135M": "SmolLM2\nTable of Contents\nModel Summary\nHow to use\nEvaluation\nBase pre-trained model\nInstruction model\nLimitations\nTraining\nModel\nHardware\nSoftware\nLicense\nCitation\nSmolLM2\nTable of Contents\nModel Summary\nLimitations\nTraining\nLicense\nCitation\nModel Summary\nSmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device. More details in our paper: https://arxiv.org/abs/2502.02737\nSmolLM2 demonstrates significant advances over its predecessor SmolLM1, particularly in instruction following, knowledge, reasoning. The 135M model was trained on 2 trillion tokens using a diverse dataset combination: FineWeb-Edu, DCLM, The Stack, along with new filtered datasets we curated and will release soon.  We developed the instruct version through supervised fine-tuning (SFT) using a combination of public datasets and our own curated datasets. We then applied Direct Preference Optimization (DPO) using UltraFeedback.\nThe instruct model additionally supports tasks such as text rewriting, summarization and function calling (for the 1.7B) thanks to datasets developed by Argilla such as Synth-APIGen-v0.1.\nYou can find the SFT dataset here: https://huggingface.co/datasets/HuggingFaceTB/smol-smoltalk and finetuning code at https://github.com/huggingface/alignment-handbook/tree/main/recipes/smollm2\nHow to use\npip install transformers\nRunning the model on CPU/GPU/multi GPU\nUsing full precision\n# pip install transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"HuggingFaceTB/SmolLM2-135M\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\ninputs = tokenizer.encode(\"Gravity is\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\nUsing torch.bfloat16\n# pip install accelerate\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ncheckpoint = \"HuggingFaceTB/SmolLM2-135M\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for fp16 use `torch_dtype=torch.float16` instead\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16)\ninputs = tokenizer.encode(\"Gravity is\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\nMemory footprint: 723.56 MB\nEvaluation\nIn this section, we report the evaluation results of SmolLM2. All evaluations are zero-shot unless stated otherwise, and we use lighteval to run them.\nBase pre-trained model\nMetrics\nSmolLM2-135M-8k\nSmolLM-135M\nHellaSwag\n42.1\n41.2\nARC (Average)\n43.9\n42.4\nPIQA\n68.4\n68.4\nMMLU (cloze)\n31.5\n30.2\nCommonsenseQA\n33.9\n32.7\nTriviaQA\n4.1\n4.3\nWinogrande\n51.3\n51.3\nOpenBookQA\n34.6\n34.0\nGSM8K (5-shot)\n1.4\n1.0\nInstruction model\nMetric\nSmolLM2-135M-Instruct\nSmolLM-135M-Instruct\nIFEval (Average prompt/inst)\n29.9\n17.2\nMT-Bench\n1.98\n1.68\nHellaSwag\n40.9\n38.9\nARC (Average)\n37.3\n33.9\nPIQA\n66.3\n64.0\nMMLU (cloze)\n29.3\n28.3\nBBH (3-shot)\n28.2\n25.2\nGSM8K (5-shot)\n1.4\n1.4\nLimitations\nSmolLM2 models primarily understand and generate content in English. They can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.\nTraining\nModel\nArchitecture: Transformer decoder\nPretraining tokens: 2T\nPrecision: bfloat16\nHardware\nGPUs: 64 H100\nSoftware\nTraining Framework: nanotron\nLicense\nApache 2.0\nCitation\n@misc{allal2025smollm2smolgoesbig,\ntitle={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model},\nauthor={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Mart√≠n Bl√°zquez and Guilherme Penedo and Lewis Tunstall and Andr√©s Marafioti and Hynek Kydl√≠ƒçek and Agust√≠n Piqueres Lajar√≠n and Vaibhav Srivastav and Joshua Lochner and Caleb Fahlgren and Xuan-Son Nguyen and Cl√©mentine Fourrier and Ben Burtenshaw and Hugo Larcher and Haojun Zhao and Cyril Zakka and Mathieu Morlon and Colin Raffel and Leandro von Werra and Thomas Wolf},\nyear={2025},\neprint={2502.02737},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2502.02737},\n}",
    "HuggingFaceTB/SmolLM2-360M-Instruct": "SmolLM2\nTable of Contents\nModel Summary\nHow to use\nTransformers\nChat in TRL\nTransformers.js\nEvaluation\nBase Pre-Trained Model\nInstruction Model\nLimitations\nTraining\nModel\nHardware\nSoftware\nLicense\nCitation\nSmolLM2\nTable of Contents\nModel Summary\nLimitations\nTraining\nLicense\nCitation\nModel Summary\nSmolLM2 is a family of compact language models available in three size: 135M, 360M, and 1.7B parameters. They are capable of solving a wide range of tasks while being lightweight enough to run on-device. More details in our paper: https://arxiv.org/abs/2502.02737\nSmolLM2 demonstrates significant advances over its predecessor SmolLM1, particularly in instruction following, knowledge, reasoning. The 360M model was trained on 4 trillion tokens using a diverse dataset combination: FineWeb-Edu, DCLM, The Stack, along with new filtered datasets we curated and will release soon.  We developed the instruct version through supervised fine-tuning (SFT) using a combination of public datasets and our own curated datasets. We then applied Direct Preference Optimization (DPO) using UltraFeedback.\nThe instruct model additionally supports tasks such as text rewriting, summarization and function calling (for the 1.7B) thanks to datasets developed by Argilla such as Synth-APIGen-v0.1.\nYou can find the SFT dataset here: https://huggingface.co/datasets/HuggingFaceTB/smol-smoltalk and finetuning code in the alignement handbook\nFor more details refer to: https://github.com/huggingface/smollm. You will find pre-training, post-training, evaluation and local inference code.\nHow to use\nTransformers\npip install transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\nmessages = [{\"role\": \"user\", \"content\": \"What is the capital of France.\"}]\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\nprint(input_text)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))\nChat in TRL\nYou can also use the TRL CLI to chat with the model from the terminal:\npip install trl\ntrl chat --model_name_or_path HuggingFaceTB/SmolLM2-360M-Instruct --device cpu\nTransformers.js\nnpm i @huggingface/transformers\nimport { pipeline } from \"@huggingface/transformers\";\n// Create a text generation pipeline\nconst generator = await pipeline(\n\"text-generation\",\n\"HuggingFaceTB/SmolLM2-135M-Instruct\",\n);\n// Define the list of messages\nconst messages = [\n{ role: \"system\", content: \"You are a helpful assistant.\" },\n{ role: \"user\", content: \"What is the capital of France?\" },\n];\n// Generate a response\nconst output = await generator(messages, { max_new_tokens: 128 });\nconsole.log(output[0].generated_text.at(-1).content);\n// \"The capital of France is Paris.\"\nEvaluation\nIn this section, we report the evaluation results of SmolLM2. All evaluations are zero-shot unless stated otherwise, and we use lighteval to run them.\nBase Pre-Trained Model\nMetrics\nSmolLM2-360M\nQwen2.5-0.5B\nSmolLM-360M\nHellaSwag\n54.5\n51.2\n51.8\nARC (Average)\n53.0\n45.4\n50.1\nPIQA\n71.7\n69.9\n71.6\nMMLU (cloze)\n35.8\n33.7\n34.4\nCommonsenseQA\n38.0\n31.6\n35.3\nTriviaQA\n16.9\n4.3\n9.1\nWinogrande\n52.5\n54.1\n52.8\nOpenBookQA\n37.4\n37.4\n37.2\nGSM8K (5-shot)\n3.2\n33.4\n1.6\nInstruction Model\nMetric\nSmolLM2-360M-Instruct\nQwen2.5-0.5B-Instruct\nSmolLM-360M-Instruct\nIFEval (Average prompt/inst)\n41.0\n31.6\n19.8\nMT-Bench\n3.66\n4.16\n3.37\nHellaSwag\n52.1\n48.0\n47.9\nARC (Average)\n43.7\n37.3\n38.8\nPIQA\n70.8\n67.2\n69.4\nMMLU (cloze)\n32.8\n31.7\n30.6\nBBH (3-shot)\n27.3\n30.7\n24.4\nGSM8K (5-shot)\n7.43\n26.8\n1.36\nLimitations\nSmolLM2 models primarily understand and generate content in English. They can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.\nTraining\nModel\nArchitecture: Transformer decoder\nPretraining tokens: 4T\nPrecision: bfloat16\nHardware\nGPUs: 64 H100\nSoftware\nTraining Framework: nanotron\nLicense\nApache 2.0\nCitation\n@misc{allal2025smollm2smolgoesbig,\ntitle={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model},\nauthor={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Mart√≠n Bl√°zquez and Guilherme Penedo and Lewis Tunstall and Andr√©s Marafioti and Hynek Kydl√≠ƒçek and Agust√≠n Piqueres Lajar√≠n and Vaibhav Srivastav and Joshua Lochner and Caleb Fahlgren and Xuan-Son Nguyen and Cl√©mentine Fourrier and Ben Burtenshaw and Hugo Larcher and Haojun Zhao and Cyril Zakka and Mathieu Morlon and Colin Raffel and Leandro von Werra and Thomas Wolf},\nyear={2025},\neprint={2502.02737},\narchivePrefix={arXiv},\nprimaryClass={cs.CL},\nurl={https://arxiv.org/abs/2502.02737},\n}",
    "prithivMLmods/Retro-Pixel-Flux-LoRA": "Retro-Pixel-Flux-LoRA\nModel description\nBest Dimensions\nSetting Up\nTrigger words\nDownload model\nRetro-Pixel-Flux-LoRA\nPrompt\nRetro Pixel, A pixelated image of a german shepherd dog. The dogs fur is a vibrant shade of brown, with a black stripe running down its back. The background is a light green, and the dogs shadow is cast on the ground.\nPrompt\nRetro Pixel, A pixelated image of a man surfing on a surfboard. The mans body is covered in a red shirt and blue shorts. His arms are out to the sides of his body. The surfboard is a vibrant blue color. The water is a light blue color with white splashes. The sun is shining on the right side of the image.\nPrompt\nRetro Pixel, pixel art of a Hamburger in the style of an old video game, hero, pixelated 8bit, final boss\nHosted Hereüß®: https://huggingface.co/spaces/prithivMLmods/FLUX-LoRA-DLC\nThe model is still in the training phase. This is not the final version and may contain artifacts and perform poorly in some cases.\nModel description\nprithivMLmods/Retro-Pixel-Flux-LoRA\nImage Processing Parameters\nParameter\nValue\nParameter\nValue\nLR Scheduler\nconstant\nNoise Offset\n0.03\nOptimizer\nAdamW\nMultires Noise Discount\n0.1\nNetwork Dim\n64\nMultires Noise Iterations\n10\nNetwork Alpha\n32\nRepeat & Steps\n24 & 2340\nEpoch\n15\nSave Every N Epochs\n1\nLabeling: florence2-en(natural language & English)\nTotal Images Used for Training : 16 [ Hi-RES ]\nBest Dimensions\n1024 x 1024 (Default)\nSetting Up\nimport torch\nfrom pipelines import DiffusionPipeline\nbase_model = \"black-forest-labs/FLUX.1-dev\"\npipe = DiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.bfloat16)\nlora_repo = \"prithivMLmods/Retro-Pixel-Flux-LoRA\"\ntrigger_word = \"Retro Pixel\"\npipe.load_lora_weights(lora_repo)\ndevice = torch.device(\"cuda\")\npipe.to(device)\nTrigger words\nYou should use Retro Pixel to trigger the image generation.\nDownload model\nWeights for this model are available in Safetensors format.\nDownload them in the Files & versions tab.",
    "black-forest-labs/FLUX.1-Fill-dev": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nBy clicking \"Agree\", you agree to the FluxDev Non-Commercial License Agreement and acknowledge the Acceptable Use Policy.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nKey Features\nUsage\nAPI Endpoints\nDiffusers\nLimitations\nOut-of-Scope Use\nFLUX.1 Fill [dev] is a 12 billion parameter rectified flow transformer capable of filling areas in existing images based on a text description.\nFor more information, please read our blog post.\nKey Features\nCutting-edge output quality, second only to our state-of-the-art model FLUX.1 Fill [pro].\nBlends impressive prompt following with completing the structure of your source image.\nTrained using guidance distillation, making FLUX.1 Fill [dev] more efficient.\nOpen weights to drive new scientific research, and empower artists to develop innovative workflows.\nGenerated outputs can be used for personal, scientific, and commercial purposes as described in the FLUX.1 [dev] Non-Commercial License.\nUsage\nWe provide a reference implementation of FLUX.1 Fill [dev], as well as sampling code, in a dedicated github repository.\nDevelopers and creatives looking to build on top of FLUX.1 Fill [dev] are encouraged to use this as a starting point.\nAPI Endpoints\nThe FLUX.1 models are also available in our API bfl.ml\nDiffusers\nTo use FLUX.1 Fill [dev] with the üß® diffusers python library, first install or upgrade diffusers\npip install -U diffusers\nThen you can use FluxFillPipeline to run the model\nimport torch\nfrom diffusers import FluxFillPipeline\nfrom diffusers.utils import load_image\nimage = load_image(\"https://huggingface.co/datasets/diffusers/diffusers-images-docs/resolve/main/cup.png\")\nmask = load_image(\"https://huggingface.co/datasets/diffusers/diffusers-images-docs/resolve/main/cup_mask.png\")\npipe = FluxFillPipeline.from_pretrained(\"black-forest-labs/FLUX.1-Fill-dev\", torch_dtype=torch.bfloat16).to(\"cuda\")\nimage = pipe(\nprompt=\"a white paper cup\",\nimage=image,\nmask_image=mask,\nheight=1632,\nwidth=1232,\nguidance_scale=30,\nnum_inference_steps=50,\nmax_sequence_length=512,\ngenerator=torch.Generator(\"cpu\").manual_seed(0)\n).images[0]\nimage.save(f\"flux-fill-dev.png\")\nTo learn more check out the diffusers documentation\nLimitations\nThis model is not intended or able to provide factual information.\nAs a statistical model this checkpoint might amplify existing societal biases.\nThe model may fail to generate output that matches the prompts.\nPrompt following is heavily influenced by the prompting-style.\nThere may be slight-color shifts in areas that are not filled in\nFilling in complex textures may produce lines at the edges of the filled-area.\nOut-of-Scope Use\nThe model and its derivatives may not be used\nIn any way that violates any applicable national, federal, state, local or international law or regulation.\nFor the purpose of exploiting, harming or attempting to exploit or harm minors in any way; including but not limited to the solicitation, creation, acquisition, or dissemination of child exploitative content.\nTo generate or disseminate verifiably false information and/or content with the purpose of harming others.\nTo generate or disseminate personal identifiable information that can be used to harm an individual.\nTo harass, abuse, threat",
    "YarvixPA/FLUX.1-Fill-dev-GGUF": "This GGUF file is a direct conversion of black-forest-labsFLUX.1-Fill-dev\nSince this is a quantized model, all original licensing terms and usage restrictions remain in effect.\nUsage\nThe model can be used with the ComfyUI custom node ComfyUI-GGUF by city96\nPlace model files in ComfyUI/models/unet see the GitHub readme for further installation instructions.\nInterface Used\nThese models were quantized using EasyQuantizationGUI by rainlizard",
    "amazon/chronos-bolt-base": "Chronos-Bolt‚ö° (Base)\nPerformance\nUsage\nUsage with AutoGluon\nDeploying a Chronos-Bolt endpoint to SageMaker\nUsage with inference library\nCitation\nLicense\nChronos-Bolt‚ö° (Base)\nüöÄ Update Feb 14, 2025: Chronos-Bolt models are now available on Amazon SageMaker JumpStart! Check out the tutorial notebook to learn how to deploy Chronos endpoints for production use in a few lines of code.\nChronos-Bolt is a family of pretrained time series forecasting models which can be used for zero-shot forecasting. It is based on the T5 encoder-decoder architecture and has been trained on nearly 100 billion time series observations. It chunks the historical time series context into patches of multiple observations, which are then input into the encoder. The decoder then uses these representations to directly generate quantile forecasts across multiple future steps‚Äîa method known as direct multi-step forecasting. Chronos-Bolt models are up to 250 times faster and 20 times more memory-efficient than the original Chronos models of the same size.\nPerformance\nThe following plot compares the inference time of Chronos-Bolt against the original Chronos models for forecasting 1024 time series with a context length of 512 observations and a prediction horizon of 64 steps.\nChronos-Bolt models are not only significantly faster but also more accurate than the original Chronos models. The following plot reports the probabilistic and point forecasting performance of Chronos-Bolt in terms of the Weighted Quantile Loss (WQL) and the Mean Absolute Scaled Error (MASE), respectively, aggregated over 27 datasets (see the Chronos paper for details on this benchmark). Remarkably, despite having no prior exposure to these datasets during training, the zero-shot Chronos-Bolt models outperform commonly used statistical models and deep learning models that have been trained on these datasets (highlighted by *). Furthermore, they also perform better than other FMs, denoted by a +, which indicates that these models were pretrained on certain datasets in our benchmark and are not entirely zero-shot. Notably, Chronos-Bolt (Base) also surpasses the original Chronos (Large) model in terms of the forecasting accuracy while being over 600 times faster.\nChronos-Bolt models are available in the following sizes.\nModel\nParameters\nBased on\nchronos-bolt-tiny\n9M\nt5-efficient-tiny\nchronos-bolt-mini\n21M\nt5-efficient-mini\nchronos-bolt-small\n48M\nt5-efficient-small\nchronos-bolt-base\n205M\nt5-efficient-base\nUsage\nUsage with AutoGluon\nThe recommended way of using Chronos for production use cases is through AutoGluon.\nAutoGluon offers effortless fine-tuning of Chronos models, incorporating covariates into the forecast through covariate regressors, and ensembling with other statistical and machine learning models for maximum accuracy.\nCheck out the AutoGluon Chronos tutorial for more details.\nA minimal example showing how to perform zero-shot inference using Chronos-Bolt with AutoGluon:\nInstall the required dependencies.\npip install autogluon\nForecast with the Chronos-Bolt model.\nfrom autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\ndf = TimeSeriesDataFrame(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly/train.csv\")\npredictor = TimeSeriesPredictor(prediction_length=48).fit(\ndf,\nhyperparameters={\n\"Chronos\": {\"model_path\": \"amazon/chronos-bolt-base\"},\n},\n)\npredictions = predictor.predict(df)\nDeploying a Chronos-Bolt endpoint to SageMaker\nSageMaker JumpStart makes it easy to deploy Chronos endpoints for production use with just a few lines of code.\nChronos-Bolt endpoints can be deployed to both CPU and GPU instances, as well as support forecasting with covariates.\nMore details are available in this example notebook.\nA minimal example showing how to deploy a Chronos-Bolt (Base) endpoint to SageMaker:\nUpdate the SageMaker SDK to make sure that all the latest models are available.\npip install -U sagemaker\nDeploy an inference endpoint to SageMaker.\nfrom sagemaker.jumpstart.model import JumpStartModel\nmodel = JumpStartModel(\nmodel_id=\"autogluon-forecasting-chronos-bolt-base\",\ninstance_type=\"ml.c5.2xlarge\",\n)\npredictor = model.deploy()\nNow you can send time series data to the endpoint in JSON format.\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\")\npayload = {\n\"inputs\": [\n{\"target\": df[\"#Passengers\"].tolist()}\n],\n\"parameters\": {\n\"prediction_length\": 12,\n}\n}\nforecast = predictor.predict(payload)[\"predictions\"]\nUsage with inference library\nAlternatively, you can install the package in the GitHub companion repo.\nThis is intended for research purposes and provides a minimal interface to Chronos models.\nInstall the library by running:\npip install chronos-forecasting\nA minimal example showing how to perform inference using Chronos-Bolt models:\nimport pandas as pd  # requires: pip install pandas\nimport torch\nfrom chronos import BaseChronosPipeline\npipeline = BaseChronosPipeline.from_pretrained(\n\"amazon/chronos-bolt-base\",\ndevice_map=\"cuda\",  # use \"cpu\" for CPU inference and \"mps\" for Apple Silicon\ntorch_dtype=torch.bfloat16,\n)\ndf = pd.read_csv(\n\"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\"\n)\n# context must be either a 1D tensor, a list of 1D tensors,\n# or a left-padded 2D tensor with batch as the first dimension\n# Chronos-Bolt models generate quantile forecasts, so forecast has shape\n# [num_series, num_quantiles, prediction_length].\nforecast = pipeline.predict(\ncontext=torch.tensor(df[\"#Passengers\"]), prediction_length=12\n)\nCitation\nIf you find Chronos or Chronos-Bolt models useful for your research, please consider citing the associated paper:\n@article{ansari2024chronos,\ntitle={Chronos: Learning the Language of Time Series},\nauthor={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\njournal={Transactions on Machine Learning Research},\nissn={2835-8856},\nyear={2024},\nurl={https://openreview.net/forum?id=gerNCVqqtR}\n}\nLicense\nThis project is licensed under the Apache-2.0 License."
}