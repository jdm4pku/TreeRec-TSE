{
    "funasr/ct-punc": "FunASR: A Fundamental End-to-End Speech Recognition Toolkit\nHighlights\nInstallation\nModel Zoo\nQuick Start\nCommand-line usage\nSpeech Recognition (Non-streaming)\nSpeech Recognition (Streaming)\nVoice Activity Detection (Non-Streaming)\nVoice Activity Detection (Streaming)\nPunctuation Restoration\nTimestamp Prediction\nFunASR: A Fundamental End-to-End Speech Recognition Toolkit\nFunASR hopes to build a bridge between academic research and industrial applications on speech recognition. By supporting the training & finetuning of the industrial-grade speech recognition model, researchers and developers can conduct research and production of speech recognition models more conveniently, and promote the development of speech recognition ecology. ASR for FunÔºÅ\nHighlights\n| News\n| Installation\n| Quick Start\n| Runtime\n| Model Zoo\n| Contact\nHighlights\nFunASR is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR. FunASR provides convenient scripts and tutorials, supporting inference and fine-tuning of pre-trained models.\nWe have released a vast collection of academic and industrial pretrained models on the ModelScope and huggingface, which can be accessed through our Model Zoo. The representative Paraformer-large, a non-autoregressive end-to-end speech recognition model, has the advantages of high accuracy, high efficiency, and convenient deployment, supporting the rapid construction of speech recognition services. For more details on service deployment, please refer to the service deployment document.\nInstallation\npip3 install -U funasr\nOr install from source code\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\nInstall modelscope for the pretrained models (Optional)\npip3 install -U modelscope\nModel Zoo\nFunASR has open-sourced a large number of pre-trained models on industrial data. You are free to use, copy, modify, and share FunASR models under the Model License Agreement. Below are some representative models, for more models please refer to the Model Zoo.\n(Note: ü§ó represents the Huggingface model zoo link, ‚≠ê represents the ModelScope model zoo link)\nModel Name\nTask Details\nTraining Data\nParameters\nparaformer-zh  (‚≠ê ü§ó )\nspeech recognition, with timestamps, non-streaming\n60000 hours, Mandarin\n220M\nparaformer-zh-streaming  ( ‚≠ê ü§ó )\nspeech recognition, streaming\n60000 hours, Mandarin\n220M\nparaformer-en  ( ‚≠ê ü§ó )\nspeech recognition, with timestamps, non-streaming\n50000 hours, English\n220M\nconformer-en  ( ‚≠ê ü§ó )\nspeech recognition, non-streaming\n50000 hours, English\n220M\nct-punc  ( ‚≠ê ü§ó )\npunctuation restoration\n100M, Mandarin and English\n1.1G\nfsmn-vad  ( ‚≠ê ü§ó )\nvoice activity detection\n5000 hours, Mandarin and English\n0.4M\nfa-zh  ( ‚≠ê ü§ó )\ntimestamp prediction\n5000 hours, Mandarin\n38M\ncam++  ( ‚≠ê ü§ó )\nspeaker verification/diarization\n5000 hours\n7.2M\nQuick Start\nBelow is a quick start tutorial. Test audio files (Mandarin, English).\nCommand-line usage\nfunasr +model=paraformer-zh +vad_model=\"fsmn-vad\" +punc_model=\"ct-punc\" +input=asr_example_zh.wav\nNotes: Support recognition of single audio file, as well as file list in Kaldi-style wav.scp format: wav_id wav_pat\nSpeech Recognition (Non-streaming)\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\", model_revision=\"v2.0.4\",\nvad_model=\"fsmn-vad\", vad_model_revision=\"v2.0.4\",\npunc_model=\"ct-punc-c\", punc_model_revision=\"v2.0.4\",\n# spk_model=\"cam++\", spk_model_revision=\"v2.0.2\",\n)\nres = model.generate(input=f\"{model.model_path}/example/asr_example.wav\",\nbatch_size_s=300,\nhotword='È≠îÊê≠')\nprint(res)\nNote: model_hub: represents the model repository, ms stands for selecting ModelScope download, hf stands for selecting Huggingface download.\nSpeech Recognition (Streaming)\nfrom funasr import AutoModel\nchunk_size = [0, 10, 5] #[0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4 #number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1 #number of encoder chunks to lookback for decoder cross-attention\nmodel = AutoModel(model=\"paraformer-zh-streaming\", model_revision=\"v2.0.4\")\nimport soundfile\nimport os\nwav_file = os.path.join(model.model_path, \"example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960 # 600ms\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\nspeech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\nis_final = i == total_chunk_num - 1\nres = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size, encoder_chunk_look_back=encoder_chunk_look_back, decoder_chunk_look_back=decoder_chunk_look_back)\nprint(res)\nNote: chunk_size is the configuration for streaming latency. [0,10,5] indicates that the real-time display granularity is 10*60=600ms, and the lookahead information is 5*60=300ms. Each inference input is 600ms (sample points are 16000*0.6=960), and the output is the corresponding text. For the last speech segment input, is_final=True needs to be set to force the output of the last word.\nVoice Activity Detection (Non-Streaming)\nfrom funasr import AutoModel\nmodel = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\")\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\nVoice Activity Detection (Streaming)\nfrom funasr import AutoModel\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\")\nimport soundfile\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\nspeech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\nis_final = i == total_chunk_num - 1\nres = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\nif len(res[0][\"value\"]):\nprint(res)\nPunctuation Restoration\nfrom funasr import AutoModel\nmodel = AutoModel(model=\"ct-punc\", model_revision=\"v2.0.4\")\nres = model.generate(input=\"ÈÇ£‰ªäÂ§©ÁöÑ‰ºöÂ∞±Âà∞ËøôÈáåÂêß happy new year ÊòéÂπ¥ËßÅ\")\nprint(res)\nTimestamp Prediction\nfrom funasr import AutoModel\nmodel = AutoModel(model=\"fa-zh\", model_revision=\"v2.0.4\")\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\nMore examples ref to docs",
    "funasr/paraformer-zh": "FunASR: A Fundamental End-to-End Speech Recognition Toolkit\nHighlights\nInstallation\nModel Zoo\nQuick Start\nCommand-line usage\nSpeech Recognition (Non-streaming)\nSpeech Recognition (Streaming)\nVoice Activity Detection (Non-Streaming)\nVoice Activity Detection (Streaming)\nPunctuation Restoration\nTimestamp Prediction\nFunASR: A Fundamental End-to-End Speech Recognition Toolkit\nFunASR hopes to build a bridge between academic research and industrial applications on speech recognition. By supporting the training & finetuning of the industrial-grade speech recognition model, researchers and developers can conduct research and production of speech recognition models more conveniently, and promote the development of speech recognition ecology. ASR for FunÔºÅ\nHighlights\n| News\n| Installation\n| Quick Start\n| Runtime\n| Model Zoo\n| Contact\nHighlights\nFunASR is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR. FunASR provides convenient scripts and tutorials, supporting inference and fine-tuning of pre-trained models.\nWe have released a vast collection of academic and industrial pretrained models on the ModelScope and huggingface, which can be accessed through our Model Zoo. The representative Paraformer-large, a non-autoregressive end-to-end speech recognition model, has the advantages of high accuracy, high efficiency, and convenient deployment, supporting the rapid construction of speech recognition services. For more details on service deployment, please refer to the service deployment document.\nInstallation\npip3 install -U funasr\nOr install from source code\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\nInstall modelscope for the pretrained models (Optional)\npip3 install -U modelscope\nModel Zoo\nFunASR has open-sourced a large number of pre-trained models on industrial data. You are free to use, copy, modify, and share FunASR models under the Model License Agreement. Below are some representative models, for more models please refer to the Model Zoo.\n(Note: ü§ó represents the Huggingface model zoo link, ‚≠ê represents the ModelScope model zoo link)\nModel Name\nTask Details\nTraining Data\nParameters\nparaformer-zh  (‚≠ê ü§ó )\nspeech recognition, with timestamps, non-streaming\n60000 hours, Mandarin\n220M\nparaformer-zh-streaming  ( ‚≠ê ü§ó )\nspeech recognition, streaming\n60000 hours, Mandarin\n220M\nparaformer-en  ( ‚≠ê ü§ó )\nspeech recognition, with timestamps, non-streaming\n50000 hours, English\n220M\nconformer-en  ( ‚≠ê ü§ó )\nspeech recognition, non-streaming\n50000 hours, English\n220M\nct-punc  ( ‚≠ê ü§ó )\npunctuation restoration\n100M, Mandarin and English\n1.1G\nfsmn-vad  ( ‚≠ê ü§ó )\nvoice activity detection\n5000 hours, Mandarin and English\n0.4M\nfa-zh  ( ‚≠ê ü§ó )\ntimestamp prediction\n5000 hours, Mandarin\n38M\ncam++  ( ‚≠ê ü§ó )\nspeaker verification/diarization\n5000 hours\n7.2M\nQuick Start\nBelow is a quick start tutorial. Test audio files (Mandarin, English).\nCommand-line usage\nfunasr +model=paraformer-zh +vad_model=\"fsmn-vad\" +punc_model=\"ct-punc\" +input=asr_example_zh.wav\nNotes: Support recognition of single audio file, as well as file list in Kaldi-style wav.scp format: wav_id wav_pat\nSpeech Recognition (Non-streaming)\nfrom funasr import AutoModel\n# paraformer-zh is a multi-functional asr model\n# use vad, punc, spk or not as you need\nmodel = AutoModel(model=\"paraformer-zh\", model_revision=\"v2.0.4\",\nvad_model=\"fsmn-vad\", vad_model_revision=\"v2.0.4\",\npunc_model=\"ct-punc-c\", punc_model_revision=\"v2.0.4\",\n# spk_model=\"cam++\", spk_model_revision=\"v2.0.2\",\n)\nres = model.generate(input=f\"{model.model_path}/example/asr_example.wav\",\nbatch_size_s=300,\nhotword='È≠îÊê≠')\nprint(res)\nNote: model_hub: represents the model repository, ms stands for selecting ModelScope download, hf stands for selecting Huggingface download.\nSpeech Recognition (Streaming)\nfrom funasr import AutoModel\nchunk_size = [0, 10, 5]  # [0, 10, 5] 600ms, [0, 8, 4] 480ms\nencoder_chunk_look_back = 4  # number of chunks to lookback for encoder self-attention\ndecoder_chunk_look_back = 1  # number of encoder chunks to lookback for decoder cross-attention\nmodel = AutoModel(model=\"paraformer-zh-streaming\", model_revision=\"v2.0.4\")\nimport soundfile\nimport os\nwav_file = os.path.join(model.model_path, \"../fa-zh/example/asr_example.wav\")\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = chunk_size[1] * 960  # 600ms\ncache = {}\ntotal_chunk_num = int(len((speech) - 1) / chunk_stride + 1)\nfor i in range(total_chunk_num):\nspeech_chunk = speech[i * chunk_stride:(i + 1) * chunk_stride]\nis_final = i == total_chunk_num - 1\nres = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size,\nencoder_chunk_look_back=encoder_chunk_look_back,\ndecoder_chunk_look_back=decoder_chunk_look_back)\nprint(res)\nNote: chunk_size is the configuration for streaming latency. [0,10,5] indicates that the real-time display granularity is 10*60=600ms, and the lookahead information is 5*60=300ms. Each inference input is 600ms (sample points are 16000*0.6=960), and the output is the corresponding text. For the last speech segment input, is_final=True needs to be set to force the output of the last word.\nVoice Activity Detection (Non-Streaming)\nfrom funasr import AutoModel\nmodel = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\")\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\nres = model.generate(input=wav_file)\nprint(res)\nVoice Activity Detection (Streaming)\nfrom funasr import AutoModel\nchunk_size = 200 # ms\nmodel = AutoModel(model=\"fsmn-vad\", model_revision=\"v2.0.4\")\nimport soundfile\nwav_file = f\"{model.model_path}/example/vad_example.wav\"\nspeech, sample_rate = soundfile.read(wav_file)\nchunk_stride = int(chunk_size * sample_rate / 1000)\ncache = {}\ntotal_chunk_num = int(len((speech)-1)/chunk_stride+1)\nfor i in range(total_chunk_num):\nspeech_chunk = speech[i*chunk_stride:(i+1)*chunk_stride]\nis_final = i == total_chunk_num - 1\nres = model.generate(input=speech_chunk, cache=cache, is_final=is_final, chunk_size=chunk_size)\nif len(res[0][\"value\"]):\nprint(res)\nPunctuation Restoration\nfrom funasr import AutoModel\nmodel = AutoModel(model=\"ct-punc\", model_revision=\"v2.0.4\")\nres = model.generate(input=\"ÈÇ£‰ªäÂ§©ÁöÑ‰ºöÂ∞±Âà∞ËøôÈáåÂêß happy new year ÊòéÂπ¥ËßÅ\")\nprint(res)\nTimestamp Prediction\nfrom funasr import AutoModel\nmodel = AutoModel(model=\"fa-zh\", model_revision=\"v2.0.4\")\nwav_file = f\"{model.model_path}/example/asr_example.wav\"\ntext_file = f\"{model.model_path}/example/text.txt\"\nres = model.generate(input=(wav_file, text_file), data_type=(\"sound\", \"text\"))\nprint(res)\nMore examples ref to docs",
    "avsolatorio/GIST-small-Embedding-v0": "Data\nUsage\nTraining Parameters\nEvaluation\nCitation\nAcknowledgements\nGIST small Embedding v0\nGISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning\nThe model is fine-tuned on top of the BAAI/bge-small-en-v1.5 using the MEDI dataset augmented with mined triplets from the MTEB Classification training dataset (excluding data from the Amazon Polarity Classification task).\nThe model does not require any instruction for generating embeddings. This means that queries for retrieval tasks can be directly encoded without crafting instructions.\nTechnical paper: GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning\nData\nThe dataset used is a compilation of the MEDI and MTEB Classification training datasets. Third-party datasets may be subject to additional terms and conditions under their associated licenses. A HuggingFace Dataset version of the compiled dataset, and the specific revision used to train the model, is available:\nDataset: avsolatorio/medi-data-mteb_avs_triplets\nRevision: 238a0499b6e6b690cc64ea56fde8461daa8341bb\nThe dataset contains a task_type key, which can be used to select only the mteb classification tasks (prefixed with mteb_).\nThe MEDI Dataset is published in the following paper: One Embedder, Any Task: Instruction-Finetuned Text Embeddings.\nThe MTEB Benchmark results of the GIST embedding model, compared with the base model, suggest that the fine-tuning dataset has perturbed the model considerably, which resulted in significant improvements in certain tasks while adversely degrading performance in some.\nThe retrieval performance for the TRECCOVID task is of note. The fine-tuning dataset does not contain significant knowledge about COVID-19, which could have caused the observed performance degradation. We found some evidence, detailed in the paper, that thematic coverage of the fine-tuning data can affect downstream performance.\nUsage\nThe model can be easily loaded using the Sentence Transformers library.\nimport torch.nn.functional as F\nfrom sentence_transformers import SentenceTransformer\nrevision = None  # Replace with the specific revision to ensure reproducibility if the model is updated.\nmodel = SentenceTransformer(\"avsolatorio/GIST-small-Embedding-v0\", revision=revision)\ntexts = [\n\"Illustration of the REaLTabFormer model. The left block shows the non-relational tabular data model using GPT-2 with a causal LM head. In contrast, the right block shows how a relational dataset's child table is modeled using a sequence-to-sequence (Seq2Seq) model. The Seq2Seq model uses the observations in the parent table to condition the generation of the observations in the child table. The trained GPT-2 model on the parent table, with weights frozen, is also used as the encoder in the Seq2Seq model.\",\n\"Predicting human mobility holds significant practical value, with applications ranging from enhancing disaster risk planning to simulating epidemic spread. In this paper, we present the GeoFormer, a decoder-only transformer model adapted from the GPT architecture to forecast human mobility.\",\n\"As the economies of Southeast Asia continue adopting digital technologies, policy makers increasingly ask how to prepare the workforce for emerging labor demands. However, little is known about the skills that workers need to adapt to these changes\"\n]\n# Compute embeddings\nembeddings = model.encode(texts, convert_to_tensor=True)\n# Compute cosine-similarity for each pair of sentences\nscores = F.cosine_similarity(embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=-1)\nprint(scores.cpu().numpy())\nTraining Parameters\nBelow are the training parameters used to fine-tune the model:\nEpochs = 40\nWarmup ratio = 0.1\nLearning rate = 5e-6\nBatch size = 16\nCheckpoint step = 102000\nContrastive loss temperature = 0.01\nEvaluation\nThe model was evaluated using the MTEB Evaluation suite.\nCitation\nPlease cite our work if you use GISTEmbed or the datasets we published in your projects or research. ü§ó\n@article{solatorio2024gistembed,\ntitle={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning},\nauthor={Aivin V. Solatorio},\njournal={arXiv preprint arXiv:2402.16829},\nyear={2024},\nURL={https://arxiv.org/abs/2402.16829}\neprint={2402.16829},\narchivePrefix={arXiv},\nprimaryClass={cs.LG}\n}\nAcknowledgements\nThis work is supported by the \"KCP IV - Exploring Data Use in the Development Economics Literature using Large Language Models (AI and LLMs)\" project funded by the Knowledge for Change Program (KCP) of the World Bank - RA-P503405-RESE-TF0C3444.\nThe findings, interpretations, and conclusions expressed in this material are entirely those of the authors. They do not necessarily represent the views of the International Bank for Reconstruction and Development/World Bank and its affiliated organizations, or those of the Executive Directors of the World Bank or the governments they represent.",
    "HFatemeH/vilt_finetuned_200": "No model card",
    "oliverbob/bible-v1": "Uploaded  model\nUploaded  model\nDeveloped by: oliverbob\nLicense: apache-2.0\nFinetuned from model : unsloth/tinyllama-bnb-4bit\nThis llama model was trained 2x faster with Unsloth and Huggingface's TRL library.",
    "defog/sqlcoder-7b-2": "Update notice\nModel Card for SQLCoder-7B-2\nModel Details\nModel Description\nModel Sources [optional]\nUses\nHow to Get Started with the Model\nPrompt\nEvaluation\nResults\nModel Card Contact\nUpdate notice\nThe model weights were updated at 7 AM UTC on Feb 7, 2024. The new model weights lead to a much more performant model ‚Äì¬†particularly for joins.\nIf you downloaded the model before that, please redownload the weights for best performance.\nModel Card for SQLCoder-7B-2\nA capable large language model for natural language to SQL generation.\nModel Details\nModel Description\nThis is the model card of a ü§ó transformers model that has been pushed on the Hub. This model card has been automatically generated.\nDeveloped by: Defog, Inc\nModel type: [Text to SQL]\nLicense: [CC-by-SA-4.0]\nFinetuned from model: [CodeLlama-7B]\nModel Sources [optional]\nHuggingFace:\nGitHub:\nDemo:\nUses\nThis model is intended to be used by non-technical users to understand data inside their SQL databases. It is meant as an analytics tool, and not as a database admin tool.\nThis model has not been trained to reject malicious requests from users with write access to databases, and should only be used by users with read-only access.\nHow to Get Started with the Model\nUse the code here to get started with the model.\nPrompt\nPlease use the following prompt for optimal results. Please remember to use do_sample=False and num_beams=4 for optimal results.\n### Task\nGenerate a SQL query to answer [QUESTION]{user_question}[/QUESTION]\n### Database Schema\nThe query will run on a database with the following schema:\n{table_metadata_string_DDL_statements}\n### Answer\nGiven the database schema, here is the SQL query that [QUESTION]{user_question}[/QUESTION]\n[SQL]\nEvaluation\nThis model was evaluated on SQL-Eval, a PostgreSQL based evaluation framework developed by Defog for testing and alignment of model capabilities.\nYou can read more about the methodology behind SQLEval here.\nResults\nWe classified each generated question into one of 6 categories. The table displays the percentage of questions answered correctly by each model, broken down by category.\ndate\ngroup_by\norder_by\nratio\njoin\nwhere\nsqlcoder-70b\n96\n91.4\n97.1\n85.7\n97.1\n91.4\nsqlcoder-7b-2\n96\n91.4\n94.3\n91.4\n94.3\n77.1\nsqlcoder-34b\n80\n94.3\n85.7\n77.1\n85.7\n80\ngpt-4\n72\n94.3\n97.1\n80\n91.4\n80\ngpt-4-turbo\n76\n91.4\n91.4\n62.8\n88.6\n77.1\nnatural-sql-7b\n56\n88.6\n85.7\n60\n88.6\n80\nsqlcoder-7b\n64\n82.9\n74.3\n54.3\n74.3\n74.3\ngpt-3.5\n72\n77.1\n82.8\n34.3\n65.7\n71.4\nclaude-2\n52\n71.4\n74.3\n57.1\n65.7\n62.9\nModel Card Contact\nContact us on X at @defogdata, or on email at founders@defog.ai",
    "RUCKBReasoning/TableLLM-13b": "TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios\nEvaluation Results\nPrompt Template\nCode Solution\nText Answer\nTableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios\n| Paper | Training set | Github | Homepage |\nWe present TableLLM, a powerful large language model designed to handle tabular data manipulation tasks efficiently, whether they are embedded in spreadsheets or documents, meeting the demands of real office scenarios. The TableLLM series encompasses two distinct scales: TableLLM-7B and TableLLM-13B, which are fine-tuned based on CodeLlama-7b-Instruct-hf and CodeLlama-13b-Instruct-hf.\nTableLLM generates either a code solution or a direct text answer to handle tabular data manipulation tasks based on different scenarios. Code generation is used for handling spreadsheet-embedded tabular data, which often involves the insert, delete, update, query, merge, and plot operations of tables. Text generation is used for handling document-embedded tabular data, which often involves the query operation of short tables.\nEvaluation Results\nWe evaluate the code solution generation ability of TableLLM on three benchmarks: WikiSQL, Spider and Self-created table operation benchmark. The text answer generation ability is tested on four benchmarks: WikiTableQuestion (WikiTQ), TAT-QA, FeTaQA and OTTQA. The evaluation result is shown below:\nModel\nWikiTQ\nTAT-QA\nFeTaQA\nOTTQA\nWikiSQL\nSpider\nSelf-created\nAverage\nTaPEX\n38.5\n‚Äì\n‚Äì\n‚Äì\n83.9\n15.0\n/\n45.8\nTaPas\n31.5\n‚Äì\n‚Äì\n‚Äì\n74.2\n23.1\n/\n42.92\nTableLlama\n24.0\n22.2\n20.5\n6.4\n43.7\n9.0\n/\n20.7\nGPT3.5\n58.5\n72.1\n71.2\n60.8\n81.7\n67.4\n77.1\n69.8\nGPT4\n74.1\n77.1\n78.4\n69.5\n84.0\n69.5\n77.8\n75.8\nLlama2-Chat (13B)\n48.8\n49.6\n67.7\n61.5\n‚Äì\n‚Äì\n‚Äì\n56.9\nCodeLlama (13B)\n43.4\n47.2\n57.2\n49.7\n38.3\n21.9\n47.6\n43.6\nDeepseek-Coder (33B)\n6.5\n11.0\n7.1\n7.4\n72.5\n58.4\n73.9\n33.8\nStructGPT (GPT3.5)\n52.5\n27.5\n11.8\n14.0\n67.8\n84.8\n/\n48.9\nBinder (GPT3.5)\n61.6\n12.8\n6.8\n5.1\n78.6\n52.6\n/\n42.5\nDATER (GPT3.5)\n53.4\n28.4\n18.3\n13.0\n58.2\n26.5\n/\n37.0\nTableLLM-7B (Ours)\n58.8\n66.9\n72.6\n63.1\n86.6\n82.6\n78.8\n72.8\nTableLLM-13B (Ours)\n62.4\n68.2\n74.5\n62.5\n90.7\n83.4\n80.8\n74.7\nPrompt Template\nThe prompts we used for generating code solutions and text answers are introduced below.\nCode Solution\nThe prompt template for the insert, delete, update, query, and plot operations on a single table.\n[INST]Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\nHeader and first few lines of CSV file:\n{csv_data}\nQuestion: {question}[/INST]\nThe prompt template for the merge operation on two tables.\n[INST]Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\nHeader and first few lines of CSV file 1:\n{csv_data1}\nHeader and first few lines of CSV file 2:\n{csv_data2}\nQuestion: {question}[/INST]\nThe csv_data field is filled with the first few lines of your provided table file. Below is an example:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nText Answer\nThe prompt template for direct text answer generation on short tables.\n[INST]Offer a thorough and accurate solution that directly addresses the Question outlined in the [Question].\n### [Table Text]\n{table_descriptions}\n### [Table]\n\n### [Question]\n{question}\n### [Solution][INST/]\nFor more details about how to use TableLLM, please refer to our GitHub page: https://github.com/TableLLM/TableLLM",
    "ashawkey/LGM": "LGM\nIntroduction\nModel Details\nUsage\nCitation\nLGM\nThis model contains the pretrained weights for LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation.\nProject Page\nIntroduction\nLGM can generate 3D objects from image or text within 5 seconds at high-resolution based on Gaussian Splatting.\nModel Details\nThe model is trained on a ~80K subset of Objaverse.\nFor more details, please refer to our paper.\nUsage\nTo download the model:\nfrom huggingface_hub import hf_hub_download\nckpt_path = hf_hub_download(repo_id=\"ashawkey/LGM\", filename=\"model_fp16_fixrot.safetensors\")\nPlease refer to our repo for more details on loading and inference.\nCitation\n@article{tang2024lgm,\ntitle={LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation},\nauthor={Tang, Jiaxiang and Chen, Zhaoxi and Chen, Xiaokang and Wang, Tengfei and Zeng, Gang and Liu, Ziwei},\njournal={arXiv preprint arXiv:2402.05054},\nyear={2024}\n}",
    "BanglaLLM/bangla-llama-13b-base-v0.1": "Bangla LLaMA 13B Base v0.1 [pre-trained]\nModel description\nRelated Models\nUsage Note\nMeet the Developers\nCitation\nBangla LLaMA 13B Base v0.1 [pre-trained]\nWelcome to the inaugural release of the Bangla LLaMA 13B base model ‚Äì an important step in advancing LLMs for the Bangla language. This model is ready for immediate inference and is also primed for further fine-tuning to cater to your specific NLP tasks.\nPlease Note: This model, labeled as a foundational Bangla Language Model (LLM), is designed primarily for Causal Language Modeling (LM) purposes. In other words, if you are looking for an instruction following model in Bangla, you may find BanglaLLM/bangla-llama-13b-instruct-v0.1 more suitable for your needs.\nModel description\nThe Bangla LLaMA models have been enhanced and tailored specifically with an extensive Bangla vocabulary of 16,000 tokens, building upon the foundation set by the original LLaMA-2.\nModel type: A 13B parameter model for Causal LM pre-trained on CulturaX dataset's Bangla subset.\nLanguage(s): Bangla and English\nLicense: GNU General Public License v3.0\nSource Model: meta-llama/Llama-2-13b-hf\nTraining Precision: float16\nCode: GitHub\nRelated Models\nModel\nType\nData\nBase Model\n# Params\nDownload Links\nBangla LLaMA 7B Base\nBase model\n12GB\nLLaMA 7B\n7B\nHF Hub\nBangla LLaMA 13B Base\nBase model\n4GB\nLLaMA 13B\n13B\nHF Hub\nBangla LLaMA 7B Instruct\nInstruction following model\n145k instructions\nBangla LLaMA 7B Base\n7B\nHF Hub\nBangla LLaMA 13B Instruct\nInstruction following model\n145k instructions\nBangla LLaMA 13B Base\n13B\nHF Hub\nUsage Note\nIt's important to note that the models have not undergone detoxification. Therefore, while they possess impressive linguistic capabilities, there is a possibility for them to generate content that could be deemed harmful or offensive. We urge users to exercise discretion and supervise the model's outputs closely, especially in public or sensitive applications.\nMeet the Developers\nGet to know the creators behind this innovative model and follow their contributions to the field:\nAbdullah Khan Zehady\nCitation\nWe hope this model serves as a valuable tool in your NLP toolkit and look forward to seeing the advancements it will enable in the understanding and generation of the Bangla language.",
    "Anzhc/Anzhcs_YOLOs": "Description\nAvailable Models\nFace segmentation:\nEyes segmentation:\nHead+Hair segmentation:\nBreasts:\nDrone detection\nAnime Art Scoring\nSupport\nDescription\nYOLOs in this repo are trained with datasets that i have annotated myself, or with the help of my friends(They will be appropriately mentioned in those cases). YOLOs on open datasets will have their own pages.\nWant to request a model?\nIm open to commissions, hit me up in Discord - anzhc\nTable of Contents\nFace segmentation\nUniversal\nReal Face, gendered\nEyes segmentation\nHead+Hair segmentation\nBreasts\nBreasts Segmentation\nBreast size detection/classification\nDrone detection\nAnime Art Scoring\nSupport\nP.S. All model names in tables have download links attached :3\nAvailable Models\nFace segmentation:\nUniversal:\nSeries of models aiming at detecting and segmenting face accurately. Trained on closed dataset i annotated myself.\nModel\nTarget\nmAP 50\nmAP 50-95\nClasses\nDataset size\nTraining Resolution\nAnzhc Face -seg.pt\nFace: illustration, real\nLOST DATA\nLOST DATA\n2(male, female)\nLOST DATA\n640\nAnzhc Face seg 640 v2 y8n.pt\nFace: illustration, real\n0.791(box) 0.765(mask)\n0.608(box) 0.445(mask)\n1(face)\n~500\n640\nAnzhc Face seg 768 v2 y8n.pt\nFace: illustration, real\n0.765(box) 0.748(mask)\n0.572(box) 0.431(mask)\n1(face)\n~500\n768\nAnzhc Face seg 768MS v2 y8n.pt\nFace: illustration, real\n0.807(box) 0.770(mask)\n0.601(box) 0.432(mask)\n1(face)\n~500\n768\nAnzhc Face seg 1024 v2 y8n.pt\nFace: illustration, real\n0.768(box) 0.740(mask)\n0.557(box) 0.394(mask)\n1(face)\n~500\n1024\nAnzhc Face seg 640 v3 y11n.pt\nFace: illustration\n0.882(box) 0.871(mask)\n0.689(box) 0.570(mask)\n1(face)\n~660\n640\nUPDATE: v3 model has a bit different face target compared to v2, so stats of v2 models suffer compared to v3 in newer benchmark, especially in mask, while box is +- same.\nDataset for v3 and above is going to be targeting inclusion of eyebrows and full eyelashes, for better adetailer experience without large dillution parameter.\nAlso starting from v3, im moving to yolo11 models, as they seem to be direct upgrade over v8. v12 did not show significant improvement while requiring 50% more time to train, even with installed Flash Attention, so it's unlikely i will switch to it anytime soon.\nBenchmark was performed in 640px.\nDifference in v2 models are only in their target resolution, so their performance spread is marginal.\nReal Face, gendered:\nTrained only on real photos for the most part, so will perform poorly with illustrations, but is gendered, and can be used for male/female detection stack.\nModel\nTarget\nmAP 50\nmAP 50-95\nClasses\nDataset size\nTraining Resolution\nAnzhcs ManFace v02 1024 y8n.pt\nFace: real\n0.883(box),0.883(mask)\n0.778(box), 0.704(mask)\n1(face)\n~340\n1024\nAnzhcs WomanFace v05 1024 y8n.pt\nFace: real\n0.82(box),0.82(mask)\n0.713(box), 0.659(mask)\n1(face)\n~600\n1024\nBenchmark was performed in 640px.\nEyes segmentation:\nWas trained for the purpose of inpainting eyes with Adetailer extension, and specializes on detecting anime eyes, particularly - sclera area, without adding eyelashes and outer eye area to detection.\nCurrent benchmark is likely inaccurate (but it is all i have), due to data being re-scrambled multi times (dataset expansion for future versions).\nModel\nTarget\nmAP 50\nmAP 50-95\nClasses\nDataset size\nTraining Resolution\nAnzhc Eyes -seg-hd.pt\nEyes: illustration\n0.925(box),0.868(mask)\n0.721(box), 0.511(mask)\n1(eye)\n~500(?)\n1024\nHead+Hair segmentation:\nAn old model (one of my first). Detects head + hair. Can be useful in likeness inpaint pipelines that need to be automated.\nModel\nTarget\nmAP 50\nmAP 50-95\nClasses\nDataset size\nTraining Resolution\nAnzhc HeadHair seg y8n.pt\nHead: illustration, real\n0.775(box),0.777(mask)\n0.576(box), 0.552(mask)\n1(head)\n~3180\n640\nAnzhc HeadHair seg y8m.pt\nHead: illustration, real\n0.867(box),0.862(mask)\n0.674(box), 0.626(mask)\n1(head)\n~3180\n640\nBreasts:\nBreasts segmentation:\nModel for segmenting breasts. Was trained on anime images only, therefore has very weak realistic performance, but still is possible.\nModel\nTarget\nmAP 50\nmAP 50-95\nClasses\nDataset size\nTraining Resolution\nAnzhc Breasts Seg v1 1024n.pt\nBreasts: illustration\n0.742(box),0.73(mask)\n0.563(box), 0.535(mask)\n1(breasts)\n~2000\n1024\nAnzhc Breasts Seg v1 1024s.pt\nBreasts: illustration\n0.768(box),0.763(mask)\n0.596(box), 0.575(mask)\n1(breasts)\n~2000\n1024\nAnzhc Breasts Seg v1 1024m.pt\nBreasts: illustration\n0.782(box),0.775(mask)\n0.644(box), 0.614(mask)\n1(breasts)\n~2000\n1024\nBreast size detection and classification:\nModel for Detecting and classifying breast size. Can be used for tagging and moderating content.\nUtilizes custom scale, combining default Booru sizes with quite freeform upper range of scale from rule34, simplifying and standartizing it.\nSize range is established relative to body proportion, instead of relative to scene, to not be confused in cases of gigantism and be disentangled from scene.And of course it's subjective, since i was the only one annotating data.\nModel\nTarget\nClasses\nDataset size\nTraining Resolution\nAnzhcs Breast Size det cls v8 y11m.pt\nBreasts: illustration and real\n15(size range)\n~16100\n640\nmAPs are not displayed in table, because i think we need more complex stats for this model.\nAccurate ratio - correct predictions, exactly matching val.+1, -1, +-1 ratio - expanded range of acceptable predictions, by +,- and +-1 class. I suggest using this stat as main accuracy, because +-1 range is likely an acceptable margin of error.At annotation, usual rate of error of original data according to this size scale was in range of +-2 to +-3 in some cases, so +-1 should be quite good.Miscalss ratio - Correct detection, but classification goes beyond +-1 error.Miss ratio - Not seen by model, completely missed.False-Positive ratio - Detection of something that isn't there.In case of this model i suspect that FPR is also including confusion rate. In some cases multiple detection will be made for single instance, and only 1 will be accepted.That can be counted as false-positive, while it will be covered in +-1 acc. Actual FPR should be lower than reported, as tested manually.GT Instances - amount of instances of data per class in dataset.\nWith that established,v8 provides pretty decent quality detection and classification, except for extremes of class 11+, and class 0(flat chest), well, since it's not too simple to detect what's not there.Class 2(medium) is one of the most confusing in this case, and has lowest accuracy. From charts, it's mostly mistaken with class 1.Rest of classes with reasonable amount of data perform quite well, and achieve high 70s to mid 80s for normal sizes, and up to high 90s for bigger size range.Misclassification is quite rare, and im happy with model performance in that regard. Average rate of misclassification is just ~3%.Missing predictions is unfortunately over 10%, but data is highly skewed with classes 0-2, which are hard to detect.FPR for v8 is very reasonable, assuming confused detections(of 2 classes at once) are counted as FPR. Size range is smooth, and lots of cases where both classes could be applied.\nLast class(unmeasurable) is used for classifying outliers that are hard to measure in currently visible area(e.g. mostly out of frame), but model will try to reasonably predict obstructed and partially visible instances.\nAll ratios are calculated relative to their respective GT instance count.\nI will continue to use this benchmark approach for future detection models.\nDrone detection\nModel for segmenting and detecting drones. What a wild swing after entry for breast model, huh. I don't really know, just had an idea, made it work, here we are.\nI would highly advice against using it in anything serious.\nStarting from v03. Consider it as v1, since v03 is my internal iteration.\nHIGHLY SENSITIVE TO DRONE MODELS - will have hard time detecting certain types, especially close-up.\nPerforms poorly on cluttered background.\nModel\nTarget\nmAP 50\nmAP 50-95\nClasses\nDataset size\nTraining Resolution\nAnzhcs Drones v03 1024 y11n.pt\nDrones\n0.927(box) 0.888(mask)\n0.753(box) 0.508(mask)\n1(drone)\n~3460\n1024\nAnime Art Scoring\nA classification model trained to assign a percentile group based on human preference, instead of trying to directly assign a \"quality\" label.Dataset was composed of about 100k images aged from 1 to 2 years on Danbooru (newer and older images were not used). That limits data to images that were sufficiently viewed and rated, while not being overly exposed due to age, nor underexposed.Scores were used and split into percentile groups, each 10%.\nMain interest in making this one was to find out if there is a significant discoverable correlation between scores and image quality.Here are my custom charts:\n(top100 is second class due to alphabetical sorting, but for margin acceptance chart it was re-sorted)\nFrom this chart, considering there are 10 classes in total, i found weak-to-modest correlation between scores and upper half of chart, negative correlation with middle-low part, weak for low, and moderate for lowest.\nWhat does that mean?\nIt means that there is meaningful correlation between scoring of people relative to features of art in question, but there is no meaningful correlation between art that is scoring neutrally.Negative scoring (top80-100) has moderate correlation, which suggests that there are some uniform negative features we can infere.Top60 class is very interesting, because it presents no correlation between provided images, even in top-3 accuracy(it performs at near-random selection in that case(10%)).That suggests that there is no feature correlation between art being not noticed, at least not the one YOLO was able to find.\nWe can reasonably predict art that will end up in top of the chart by human score, but we are not able to predict middle-of-the line art, which would constitute majority of art in real case.We can predict low quality based on human preference reasonably well, but far from ideal.\nMargin acceptance charts - A top-1 accuracy, but with margin of class acceptance(1, 2 and 3(starts with -1, then adds +1 and then -2 class)(it/s not +-1-3 as naming suggests))This allows us to see how well are classes correlate. If we see significant increase relative to first chart, that means that second best prediction was selected as top-1.We can also see extended correlation trend across classes. We once again can see that middle classes have very low correlation and accuracy, suggesting no meaningful features.That kinda suggests to me that there is no reason for art that ended up in middle of dataset to be there, and it would end up higher or lower in perfect world.\nTop10-40 correlates very well, and that can be used for human preference detection. Funny note on that: bigger the breasts - better the score.And i wholeheartedly support that notion.NSFW art in general will have higher preference score, well, what an unexpected outcome, amirite? Dataset was composed ~50/50% from Danbooru/Safebooru(safebooru.donmai.us), so it's not due to overrepresentation of NSFW.That is also why you should not use scores for quality tagging, but if you are looking for a thing to maintain high compatibility with current anime models - be my guest.Correlation between bottom scores(that you'd use for low quality/worst quality) is weaker, so be conservative with that.\nBigger model and data will likely see more correlation, but from quick test of just running larger variation did not lead me to better performance.\nModel\nTarget\nTop-1 acc/(w/ margin(1/2/3))\nTop-2 acc\nTop-3 acc\nClasses\nDataset size\nTraining Resolution\nAnzhcs Anime Score CLS v1.pt\nAnime illustration\n0.336(0.467/0.645/0.679)\n0.566\n0.696\n10(top10 to top100)\n~98000\n224\nAdditionally, i will provide a script for tagging your datasets with that, if you want - Simple Utility Scripts repo\nSupport\nIf you want to support me, feel free to donate on ko-fi:https://ko-fi.com/anzhc\nOr send me some BTC:bc1qpc5kmxrpqp6x8ykdu6976s4rvsz0utk22h80j9\n/--UNDER CONSTRUCTION--/",
    "yanolja/YanoljaNEXT-EEVE-10.8B": "EEVE-Korean-10.8B-v1.0\nJoin Our Community on Discord!\nOur Dedicated Team (Alphabetical Order)\nAbout the Model\nTechnical Deep Dive\nUsage and Limitations\nTraining Details\nCitation\nEEVE-Korean-10.8B-v1.0\nJoin Our Community on Discord!\nIf you're passionate about the field of Large Language Models and wish to exchange knowledge and insights, we warmly invite you to join our Discord server. It's worth noting that Korean is the primary language used in this server. The landscape of LLM is evolving rapidly, and without active sharing, our collective knowledge risks becoming outdated swiftly. Let's collaborate and drive greater impact together! Join us here: Discord Link.\nOur Dedicated Team (Alphabetical Order)\nResearch\nEngineering\nProduct Management\nUX Design\nMyeongho Jeong\nGeon Kim\nBokyung Huh\nEunsue Choi\nSeungduk Kim\nRifqi Alfi\nSeungtaek Choi\nSanghoon Han\nSuhyun Kang\nAbout the Model\nThis model is a Korean vocabulary-extended version of upstage/SOLAR-10.7B-v1.0, specifically fine-tuned on various Korean web-crawled datasets available on HuggingFace. Our approach was to expand the model's understanding of Korean by pre-training the embeddings for new tokens and partially fine-tuning the lm_head embeddings for the already existing tokens while preserving the original parameters of the base model.\nTechnical Deep Dive\nTo adapt foundational models from English to Korean, we use subword-based embedding with a seven-stage training process involving parameter freezing.\nThis approach progressively trains from input embeddings to full parameters, efficiently extending the model's vocabulary to include Korean.\nOur method enhances the model's cross-linguistic applicability by carefully integrating new linguistic tokens, focusing on causal language modeling pre-training.\nWe leverage the inherent capabilities of foundational models trained on English to efficiently transfer knowledge and reasoning to Korean, optimizing the adaptation process.\nFor more details, please refer to our technical report: Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models.\nHere‚Äôs an simplified code for our key approach:\n# number_of_old_tokens is the size of tokenizer before vocab extension. For example, in case of EEVE-Korean-10.8B-v1.0, number_of_old_tokens is 32000.\ndef freeze_partial_embedding_hook(grad):\ngrad[:number_of_old_tokens] = 0\nreturn grad\nfor name, param in model.named_parameters():\nif (\"lm_head\" in name or \"embed_tokens\" in name) and \"original\" not in name:\nparam.requires_grad = True\nif \"embed_tokens\" in name:\nparam.register_hook(freeze_partial_embedding_hook)\nelse:\nparam.requires_grad = False\nUsage and Limitations\nKeep in mind that this model hasn't been fine-tuned with instruction-based training. While it excels in Korean language tasks, we advise careful consideration and further training for specific applications.\nTraining Details\nOur model‚Äôs training was comprehensive and diverse:\nVocabulary Expansion:\nWe meticulously selected 8,960 Korean tokens based on their frequency in our Korean web corpus. This process involved multiple rounds of tokenizer training, manual curation, and token frequency analysis, ensuring a rich and relevant vocabulary for our model.\nInitial Tokenizer Training: We trained an intermediate tokenizer on a Korean web corpus, with a vocabulary of 40,000 tokens.\nExtraction of New Korean Tokens: From the intermediate tokenizer, we identified all Korean tokens not present in the original SOLAR's tokenizer.\nManual Tokenizer Construction: We then built the target tokenizer, focusing on these new Korean tokens.\nFrequency Analysis: Using the target tokenizer, we processed a 100GB Korean corpus to count each token's frequency.\nRefinement of Token List: We removed tokens appearing less than 6,000 times, ensuring to secure enough tokens to train models later.\nInclusion of Single-Letter Characters: Counted missing Korean single-letter characters and added them to the target tokenizer that appeared more than 6,000 times.\nIterative Refinement: We repeated steps 2 to 6 until there were no tokens to drop or add.\nTraining Bias Towards New Tokens: Our training data was biased to include more texts with new tokens, for effective learning.\nThis rigorous approach ensured a comprehensive and contextually rich Korean vocabulary for the model.\nCitation\n@misc{kim2024efficient,\ntitle={Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models},\nauthor={Seungduk Kim and Seungtaek Choi and Myeongho Jeong},\nyear={2024},\neprint={2402.14714},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "Ttimofeyka/MistralRP-Noromaid-NSFW-Mistral-7B-GGUF": "Model\nModel Details\nPrompt template: Alpaca\nMerge Details\nMerge Method\nModels Merged\nConfiguration\nModel\nThis is a merge model of pre-trained language models created using mergekit.\nModel Details\nPrompt template: Alpaca\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{prompt}\n### Response:\nMerge Details\nMerge Method\nThis model was merged using the SLERP merge method.\nModels Merged\nThe following models were included in the merge:\nUndi95/Mistral-RP-0.1-7B\nMaziyarPanahi/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1\nConfiguration\nThe following YAML configuration was used to produce this model:\nslices:\n- sources:\n- model: Undi95/Mistral-RP-0.1-7B\nlayer_range: [0, 32]\n- model: MaziyarPanahi/NSFW_DPO_Noromaid-7b-Mistral-7B-Instruct-v0.1\nlayer_range: [0, 32]\nmerge_method: slerp\nbase_model: Undi95/Mistral-RP-0.1-7B\nparameters:\nt:\n- filter: self_attn\nvalue: [0, 0.5, 0.3, 0.7, 1]\n- filter: mlp\nvalue: [1, 0.5, 0.7, 0.3, 0]\n- value: 0.5 # fallback for rest of tensors\ndtype: bfloat16",
    "minhtoan/t5-translate-lao-english": "Lao to English Translation Model\nHow to use\nOn GPU\nOn CPU\nAuthor\nLao to English Translation Model\nWelcome to the forefront of linguistic innovation with our groundbreaking T5 language model designed specifically for Lao to English translation. In a rapidly globalizing world where effective communication is paramount, our T5 model stands as a beacon of excellence, offering unparalleled accuracy, fluency, and efficiency in bridging the language gap between Lao and English.\nBuilt on state-of-the-art deep learning architecture and trained on vast datasets of Lao and English texts, our language model (LLM) harnesses the power of transformer-based technology to deliver seamless and precise translations. Whether you're a business expanding into Laotian markets, a researcher seeking to access Lao-language resources, or an individual connecting with Lao-speaking communities, our T5 model is your ultimate solution for unlocking linguistic barriers and fostering meaningful cross-cultural exchanges.\nWith a commitment to quality and innovation, our translation model not only translates words but also preserves context, tone, and cultural nuances, ensuring that the essence of the original message remains intact in every translated sentence. Whether it's documents, websites, or multimedia content, our LLM model offers unmatched versatility and reliability, empowering users to communicate effortlessly across languages and borders.\nHow to use\nOn GPU\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"minhtoan/t5-translate-lao-english\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"minhtoan/t5-translate-lao-english\")\nmodel.cuda()\nsrc = \"‡∫Ç‡ªâ‡∫≠‡∫ç‚Äã‡∫Æ‡∫±‡∫Å‚Äã‡ªÄ‡∫à‡∫ª‡ªâ‡∫≤\"\ntokenized_text = tokenizer.encode(src, return_tensors=\"pt\").cuda()\nmodel.eval()\ntranslate_ids = model.generate(tokenized_text, max_length=140)\noutput = tokenizer.decode(translate_ids[0], skip_special_tokens=True)\noutput\n'I love you'\nOn CPU\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"minhtoan/t5-translate-lao-english\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"minhtoan/t5-translate-lao-english\")\nsrc = \"‡∫Ç‡ªâ‡∫≠‡∫ç‚Äã‡∫Æ‡∫±‡∫Å‚Äã‡ªÄ‡∫à‡∫ª‡ªâ‡∫≤\"\ninput_ids = tokenizer(src, max_length=200, return_tensors=\"pt\", padding=\"max_length\", truncation=True).input_ids\noutputs = model.generate(input_ids=input_ids, max_new_tokens=140)\noutput = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\noutput\n'I love you'\nAuthor\nPhan Minh Toan",
    "Lykon/dreamshaper-xl-v2-turbo": "Dreamshaper XL v2 Turbo\nDiffusers\nDreamshaper XL v2 Turbo\nlykon/dreamshaper-xl-v2-turbo is a Stable Diffusion model that has been fine-tuned on stabilityai/stable-diffusion-xl-base-1.0.\nPlease consider supporting me:\non Patreon\nor buy me a coffee\nDiffusers\nFor more general information on how to run text-to-image models with üß® Diffusers, see the docs.\nInstallation\npip install diffusers transformers accelerate\nRun\nfrom diffusers import AutoPipelineForText2Image, DPMSolverMultistepScheduler\nimport torch\npipe = AutoPipelineForText2Image.from_pretrained('lykon/dreamshaper-xl-v2-turbo', torch_dtype=torch.float16, variant=\"fp16\")\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe = pipe.to(\"cuda\")\nprompt = \"portrait photo of muscular bearded guy in a worn mech suit, light bokeh, intricate, steel metal, elegant, sharp focus, soft lighting, vibrant colors\"\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=6, guidance_scale=2).images[0]\nimage.save(\"./image.png\")",
    "Yellow-AI-NLP/komodo-7b-base": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nModel Card for Komodo-7B-Base\nModel Details\nModel Description\nUsage Example\nTechnical Specifications\nModel Architecture and Objective\nTokenizer Details\nTraining Data\nTraining Procedure\nEvaluation & Results\nInfrastructure\nCitation\nModel Card Authors\nModel Card for Komodo-7B-Base\nKomodo-7B-Base is a large language model that is developed through incremental pretraining and vocabulary expansion on top of Llama-2-7B-Base. This model can handle Indonesian, English and 11 regional languages of Indonesia.\nDisclaimer : This is not an instruction-tuned model, further fine-tuning is needed for downstream tasks. For example, people usually utilize the Alpaca dataset for further fine-tuning on top of Llama-2-7B-Base model. Hence, there is no prompt template for this model.\nModel Details\nModel Description\nMore details can be found in our paper:  https://arxiv.org/abs/2403.09362\nDeveloped by: Yellow.ai\nModel type: Decoder\nLanguages: English, Indonesian, Acehnese, Balinese, Banjarese, Buginese, Madurese, Minangkabau, Javanese, Dayak Ngaju, Sundanese, Toba Batak, Lampungnese\nLicense: llama2\nUsage Example\nSince this is a gated model, you need to logged in to your HF account before using the model. Below is one way to do this. You can get the HF Token from your profile (Profile -> Settings -> Access Tokens)\nimport huggingface_hub\nhuggingface_hub.login(\"YOUR_HF_TOKEN\")\nOnce you are logged in, you can start download and load the model & tokenizer. We wrote a custom decoding function for Komodo-7B, that's why we need to pass the trust_remote_code=True. The code also works without this parameter, but decoding process will not work as expected.\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = AutoTokenizer.from_pretrained(\"Yellow-AI-NLP/komodo-7b-base\",trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"Yellow-AI-NLP/komodo-7b-base\",trust_remote_code=True)\nmodel = model.to(device)\nThen, you can try using the model.\nfull_prompt = \"Candi borobudur adalah\"\ntokens = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\noutput = model.generate(tokens[\"input_ids\"], eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n# Candi borobudur adalah candi yang terletak di Magelang, Jawa Tengah.\nTechnical Specifications\nModel Architecture and Objective\nKomodo-7B is a decoder model using the Llama-2 architecture.\nParameter\nKomodo-7B\nLayers\n32\nd_model\n4096\nhead_dim\n32\nVocabulary\n35008\nSequence Length\n4096\nTokenizer Details\nRecognizing the importance of linguistic diversity, we focused on enhancing our language model's proficiency in both Indonesian and regional languages. To achieve this, we systematically expanded the tokenizer's vocabulary by identifying and incorporating approximately 2,000 frequently used words specific to Indonesian and 1,000 words for regional languages that were absent in the Llama-2 model.\nThe standard method for enhancing a vocabulary typically involves developing a new tokenizer and integrating it with the existing one. This technique has shown impressive results in projects like Chinese-LLaMA and Open-Hathi. The effectiveness of this strategy can be attributed to the significant linguistic distinctions between languages such as Chinese and Hindi when compared to English. In contrast, the Indonesian language employs the same Latin script as English, which presents a different set of challenges.\nWe tested the traditional method, as well as a new approach where we included the top n words (not tokens) from the Indonesian vocabulary. We discovered that with the new approach, we could achieve better fertility scores by adding around 3000 new vocabulary words. Adding more than 3000 words did not significantly improve the fertility score further, but it increased the size of the embedding matrix, leading to longer training times.\nMore details can be found in our paper:  https://arxiv.org/abs/2403.09362\nTraining Data\nMore details can be found in our paper:  https://arxiv.org/abs/2403.09362\nTraining Procedure\nMore details can be found in our paper:  https://arxiv.org/abs/2403.09362\nPreprocessing\nMore details can be found in our paper:  https://arxiv.org/abs/2403.09362\nEvaluation & Results\nPlease note that the benchmarking values below are based on our SFT Model, Komodo-7B-Instruct, while here we only release the base model, Komodo-7B-base.\nOrganization\nModel Name\nIndo MMLU\nID-EN\nXCOPA-ID\nIntent Classification\nColloquial Detection\nNusaX-Senti\nID-Hate Speech\nTydiQA-ID\nIndosum\nAverage\nOpenAI\nGPT-3.5-turbo-0301\n51.3\n64.5\n70.0\n82.0\n64.1\n47.2\n68.0\n85.3\n41.0\n63.7\nOpenAI\nGPT-3.5-turbo-0613\n52.7\n66.8\n88.2\n84.0\n75.1\n63.3\n63.7\n86.4\n40.0\n68.9\nOpenAI\nGPT-3.5-turbo-1106\n53.3\n69.7\n89.3\n84.0\n64.2\n59.8\n56.6\n88.0\n42.0\n67.4\nOpenAI\nGPT-4-preview-1106\n69.8\n78.0\n98.3\n89.0\n92.7\n66.1\n73.4\n72.0\n33.0\n74.7\nMeta\nLlama-2-7B-Chat\n30.4\n45.6\n41.5\n57.0\n31.4\n2.9\n41.3\n11.7\n34.0\n32.9\nMeta\nLlama-2-13B-Chat\n32.0\n61.7\n38.0\n59.0\n31.1\n58.7\n57.2\n71.9\n40.0\n50.0\nGoogle\nGemma-7B-it\n37.4\n73.6\n57.7\n77.1\n18.8\n44.2\n54.8\n73.3\n44.0\n53.4\nMistral\nMixtral-8x7B-v0.1-Instruct\n45.2\n57.8\n88.7\n86.0\n41.1\n52.8\n68.8\n90.3\n14.0\n60.5\nAISingapore\nSealion-7B-Instruct-NC\n23.9\n26.9\n41.3\n37.0\n41.8\n30.7\n57.3\n65.3\n26.0\n38.9\nCohere\nAya-101-13B\n47.7\n47.3\n84.0\n64.0\n18.9\n74.6\n72.7\n81.3\n39.0\n58.8\nMBZUAI\nBactrian-X-Llama-7B\n23.6\n43.2\n45.3\n42.0\n50.3\n44.5\n42.4\n65.0\n15.0\n41.3\nAlibaba\nQwen-1.5-7B-chat\n40.0\n56.0\n29.5\n85.0\n41.8\n58.7\n63.9\n51.22\n29.0\n50.6\nYellow.ai\nKomodo-7B-Instruct\n43.2\n90.5\n79.6\n84.0\n73.6\n79.3\n56.2\n90.3\n43.0\n71.1\nMore details can be found in our paper:  https://arxiv.org/abs/2403.09362\nInfrastructure\nTraining Details\nKomodo-7B\nAWS EC2 p4d.24xlarge\n1 instances\nNvidia A100 40GB GPU\n8\nTraining Duration\n300 hours\nCitation\n@misc{owen2024komodo,\ntitle={Komodo: A Linguistic Expedition into Indonesia's Regional Languages},\nauthor={Louis Owen and Vishesh Tripathi and Abhay Kumar and Biddwan Ahmed},\nyear={2024},\neprint={2403.09362},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\nModel Card Authors\nLouis Owen\nVishesh Tripathi\nAbhay Kumar\nBiddwan Ahmed",
    "minhtoan/t5-translate-english-lao": "English to Lao Translation Model\nHow to use\nOn GPU\nOn CPU\nAuthor\nEnglish to Lao Translation Model\nWelcome to the forefront of linguistic innovation with our groundbreaking T5 language model designed specifically for English to Lao translation. In a rapidly globalizing world where effective communication is paramount, our T5 model stands as a beacon of excellence, offering unparalleled accuracy, fluency, and efficiency in bridging the language gap between Lao and English.\nBuilt on state-of-the-art deep learning architecture and trained on vast datasets of Lao and English texts, our language model (LLM) harnesses the power of transformer-based technology to deliver seamless and precise translations. Whether you're a business expanding into Laotian markets, a researcher seeking to access Lao-language resources, or an individual connecting with Lao-speaking communities, our T5 model is your ultimate solution for unlocking linguistic barriers and fostering meaningful cross-cultural exchanges.\nWith a commitment to quality and innovation, our translation model not only translates words but also preserves context, tone, and cultural nuances, ensuring that the essence of the original message remains intact in every translated sentence. Whether it's documents, websites, or multimedia content, our LLM model offers unmatched versatility and reliability, empowering users to communicate effortlessly across languages and borders.\nHow to use\nOn GPU\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"minhtoan/t5-translate-english-lao\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"minhtoan/t5-translate-english-lao\")\nmodel.cuda()\nsrc = \"I want to buy a new book\"\ntokenized_text = tokenizer.encode(src, return_tensors=\"pt\").cuda()\nmodel.eval()\ntranslate_ids = model.generate(tokenized_text, max_length=200)\noutput = tokenizer.decode(translate_ids[0], skip_special_tokens=True)\noutput\n'‡∫Ç‡ªâ‡∫≠‡∫ç‡∫¢‡∫≤‡∫Å‡∫ä‡∫∑‡ªâ‡∫õ‡∫∂‡ªâ‡∫°‡ªÉ‡∫´‡∫°‡ªà'\nOn CPU\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"minhtoan/t5-translate-english-lao\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"minhtoan/t5-translate-english-lao\")\nsrc = \"I want to buy a new book\"\ninput_ids = tokenizer(src, max_length=140, return_tensors=\"pt\", padding=\"max_length\", truncation=True).input_ids\noutputs = model.generate(input_ids=input_ids, max_new_tokens=200)\noutput = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\noutput\n'‡∫Ç‡ªâ‡∫≠‡∫ç‡∫¢‡∫≤‡∫Å‡∫ä‡∫∑‡ªâ‡∫õ‡∫∂‡ªâ‡∫°‡ªÉ‡∫´‡∫°‡ªà'\nAuthor\nPhan Minh Toan",
    "BlinkDL/rwkv-6-world": "RWKV-6 World\nModel Description\nRWKV-6 World\nRWKV-6 paper: https://arxiv.org/abs/2404.05892\nUse rwkv pip package 0.8.24+ for RWKV-6 inference: https://pypi.org/project/rwkv/ (pipeline = PIPELINE(model, \"rwkv_vocab_v20230424\") for rwkv-world models)\nOnline Demo 1: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2\nOnline Demo 2: https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-1\nGUI: https://github.com/josStorer/RWKV-Runner (see Releases)\nFor developer: https://github.com/BlinkDL/ChatRWKV/blob/main/API_DEMO_CHAT.py\nhttps://github.com/BlinkDL/ChatRWKV/blob/main/RWKV_v6_demo.py\nhttps://www.rwkv.com/\nRWKV-6 7B v3 MMLU = 54.2% (using the same \"47.9%\" code)\nRWKV-6 7B v2.1 MMLU = 47.9%: https://github.com/Jellyfish042/rwkv_mmlu\nRWKV-6 0.1B (using pythia-160m tokenizer): https://huggingface.co/BlinkDL/temp-latest-training-models/blob/main/temp/rwkv-x060-173m-pile-20240515-ctx4k.pth\nModel Description\nRWKV-6 trained on 100+ world languages (70% English, 15% multilang, 15% code).\nWorld = Some_Pile + Some_SlimPajama + Some_StarCoder + Some_OSCAR + All_Wikipedia + All_ChatGPT_Data_I_can_find\nWorld v1 = 0.59T tokens\nWorld v2 = 1.12T tokens\nWorld v2.1 = 1.42T tokens\nRecommended fine-tuning format (use \\n for newlines):\nUser: xxxxxxxxxxxxxxx\nAssistant: xxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx\nUser: xxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx\nAssistant: xxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx\nxxxxxxxxxxxxxxx\nA good chat prompt (better replace \\n\\n in xxx to \\n, such that there will be no newlines in xxx):\nUser: hi\nAssistant: Hi. I am your assistant and I will provide expert full response in full details. Please feel free to ask any question and I will always answer it.\nUser: xxx\nAssistant:\nQA prompt (better replace \\n\\n in xxx to \\n, such that there will be no newlines in xxx):\nQuestion: xxx\nAnswer:\nand\nInstruction: xxx\nInput: xxx\nResponse:\n!!! There should not be any space after your final \":\" or you will upset the tokenizer and see non-English reponse !!!\n!!! There should not be any space after your final \":\" or you will upset the tokenizer and see non-English reponse !!!\n!!! There should not be any space after your final \":\" or you will upset the tokenizer and see non-English reponse !!!",
    "spyrosbriakos/greek_legal_bert_v2": "This model was produced as part of respective B.Sc. Thesis: NLP Tasks with GreekLegalBERT v2.\nAs far as we can discern, there are two unique models in the Greek NLP era: the general-purpose Greek-BERT model and the specific-domain Greek-Legal-BERT-v1 model. In this\nthesis, we focus on the generation and representation of the second version of GreekLegal-BERT, namely GreekLegalBERT v2, which was provided with more Legal Data than the first version.\nCombined dataset that was used for current model's pretraining purposes is comprised of:\nThe Raptarchis dataset, also known as RAPTARCHIS47k, consisting of approximately 47 thousand legal resources, is a comprehensive collection of Greek legislation dating from the founding of the Greek state in 1834 through 2015.\nNomothesi@, a platform that makes Greek legislation available on the Web as linked open data, was built on the basis of the aforementioned principles.\nEuroParl, Philipp Koehn‚Äôs team in Edinburgh was able to collect corpus parallel text from the European Parliament sessions in 11 languages from European Union, including Greek.\nEUR-LEX provides online access to European Union (EU) legal documents that is both official and comprehensive, containing 57 thousand Greek EU legislative documents from the EUR-LEX portal.\nHellenic Parliament Sessions, All the available minutes of the plenary sessions of the Greek or Hellenic Parliament, from 3 July 1989 to 24 August 2021,\nThe current thesis' goal is to compare the three dinstict Greek NLP models, based on BERT model, between different downstream NLP tasks, notably in Named Entity\nRecognition, Natural Language Inference and Multiclass Classification on Raptarchis dataset.",
    "blaze999/Medical-NER": "deberta-med-ner-2\nModel description\nTraining hyperparameters\nUsage\nAuthor\nFramework versions\ndeberta-med-ner-2\nThis model is a fine-tuned version of DeBERTa on the PubMED Dataset.\nModel description\nMedical NER Model finetuned on BERT to recognize 41 Medical entities.\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 2e-05\ntrain_batch_size: 8\neval_batch_size: 16\nseed: 42\ngradient_accumulation_steps: 2\ntotal_train_batch_size: 16\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\nlr_scheduler_type: cosine\nlr_scheduler_warmup_ratio: 0.1\nnum_epochs: 30\nmixed_precision_training: Native AMP\nUsage\nThe easiest way is to load the inference api from huggingface and second method is through the pipeline object offered by transformers library.\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\npipe = pipeline(\"token-classification\", model=\"Clinical-AI-Apollo/Medical-NER\", aggregation_strategy='simple')\nresult = pipe('45 year old woman diagnosed with CAD')\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\ntokenizer = AutoTokenizer.from_pretrained(\"Clinical-AI-Apollo/Medical-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"Clinical-AI-Apollo/Medical-NER\")\nAuthor\nAuthor: Saketh Mattupalli\nFramework versions\nTransformers 4.37.0\nPytorch 2.1.2\nDatasets 2.1.0\nTokenizers 0.15.1",
    "ResembleAI/auxiliary_models": "README.md exists but content is empty.",
    "nomic-ai/nomic-embed-text-v1.5": "nomic-embed-text-v1.5: Resizable Production Embeddings with Matryoshka Representation Learning\nUsage\nTask instruction prefixes\nsearch_document\nsearch_query\nclustering\nclassification\nSentence Transformers\nTransformers\nTransformers.js\nNomic API\nInfinity\nAdjusting Dimensionality\nTraining\nJoin the Nomic Community\nCitation\nnomic-embed-text-v1.5: Resizable Production Embeddings with Matryoshka Representation Learning\nBlog | Technical Report | AWS SageMaker | Nomic Platform\nExciting Update!: nomic-embed-text-v1.5 is now multimodal! nomic-embed-vision-v1.5 is aligned to the embedding space of nomic-embed-text-v1.5, meaning any text embedding is multimodal!\nUsage\nImportant: the text prompt must include a task instruction prefix, instructing the model which task is being performed.\nFor example, if you are implementing a RAG application, you embed your documents as search_document: <text here> and embed your user queries as search_query: <text here>.\nTask instruction prefixes\nsearch_document\nPurpose: embed texts as documents from a dataset\nThis prefix is used for embedding texts as documents, for example as documents for a RAG index.\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\nsentences = ['search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten']\nembeddings = model.encode(sentences)\nprint(embeddings)\nsearch_query\nPurpose: embed texts as questions to answer\nThis prefix is used for embedding texts as questions that documents from a dataset could resolve, for example as queries to be answered by a RAG application.\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\nsentences = ['search_query: Who is Laurens van Der Maaten?']\nembeddings = model.encode(sentences)\nprint(embeddings)\nclustering\nPurpose: embed texts to group them into clusters\nThis prefix is used for embedding texts in order to group them into clusters, discover common topics, or remove semantic duplicates.\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\nsentences = ['clustering: the quick brown fox']\nembeddings = model.encode(sentences)\nprint(embeddings)\nclassification\nPurpose: embed texts to classify them\nThis prefix is used for embedding texts into vectors that will be used as features for a classification model\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\nsentences = ['classification: the quick brown fox']\nembeddings = model.encode(sentences)\nprint(embeddings)\nSentence Transformers\nimport torch.nn.functional as F\nfrom sentence_transformers import SentenceTransformer\nmatryoshka_dim = 512\nmodel = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\nsentences = ['search_query: What is TSNE?', 'search_query: Who is Laurens van der Maaten?']\nembeddings = model.encode(sentences, convert_to_tensor=True)\nembeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))\nembeddings = embeddings[:, :matryoshka_dim]\nembeddings = F.normalize(embeddings, p=2, dim=1)\nprint(embeddings)\nTransformers\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\ndef mean_pooling(model_output, attention_mask):\ntoken_embeddings = model_output[0]\ninput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\nreturn torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\nsentences = ['search_query: What is TSNE?', 'search_query: Who is Laurens van der Maaten?']\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nmodel = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True, safe_serialization=True)\nmodel.eval()\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n+ matryoshka_dim = 512\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\nembeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n+ embeddings = F.layer_norm(embeddings, normalized_shape=(embeddings.shape[1],))\n+ embeddings = embeddings[:, :matryoshka_dim]\nembeddings = F.normalize(embeddings, p=2, dim=1)\nprint(embeddings)\nThe model natively supports scaling of the sequence length past 2048 tokens. To do so,\n- tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n+ tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', model_max_length=8192)\n- model = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True)\n+ model = AutoModel.from_pretrained('nomic-ai/nomic-embed-text-v1.5', trust_remote_code=True, rotary_scaling_factor=2)\nTransformers.js\nimport { pipeline, layer_norm } from '@huggingface/transformers';\n// Create a feature extraction pipeline\nconst extractor = await pipeline('feature-extraction', 'nomic-ai/nomic-embed-text-v1.5');\n// Define sentences\nconst texts = ['search_query: What is TSNE?', 'search_query: Who is Laurens van der Maaten?'];\n// Compute sentence embeddings\nlet embeddings = await extractor(texts, { pooling: 'mean' });\nconsole.log(embeddings); // Tensor of shape [2, 768]\nconst matryoshka_dim = 512;\nembeddings = layer_norm(embeddings, [embeddings.dims[1]])\n.slice(null, [0, matryoshka_dim])\n.normalize(2, -1);\nconsole.log(embeddings.tolist());\nNomic API\nThe easiest way to use Nomic Embed is through the Nomic Embedding API.\nGenerating embeddings with the nomic Python client is as easy as\nfrom nomic import embed\noutput = embed.text(\ntexts=['Nomic Embedding API', '#keepAIOpen'],\nmodel='nomic-embed-text-v1.5',\ntask_type='search_document',\ndimensionality=256,\n)\nprint(output)\nFor more information, see the API reference\nInfinity\nUsage with Infinity.\ndocker run --gpus all -v $PWD/data:/app/.cache -e HF_TOKEN=$HF_TOKEN -p \"7997\":\"7997\" \\\nmichaelf34/infinity:0.0.70 \\\nv2 --model-id nomic-ai/nomic-embed-text-v1.5 --revision \"main\" --dtype float16 --batch-size 8 --engine torch --port 7997 --no-bettertransformer\nAdjusting Dimensionality\nnomic-embed-text-v1.5 is an improvement upon Nomic Embed that utilizes Matryoshka Representation Learning which gives developers the flexibility to trade off the embedding size for a negligible reduction in performance.\nName\nSeqLen\nDimension\nMTEB\nnomic-embed-text-v1\n8192\n768\n62.39\nnomic-embed-text-v1.5\n8192\n768\n62.28\nnomic-embed-text-v1.5\n8192\n512\n61.96\nnomic-embed-text-v1.5\n8192\n256\n61.04\nnomic-embed-text-v1.5\n8192\n128\n59.34\nnomic-embed-text-v1.5\n8192\n64\n56.10\nTraining\nClick the Nomic Atlas map below to visualize a 5M sample of our contrastive pretraining data!\nWe train our embedder using a multi-stage training pipeline. Starting from a long-context BERT model,\nthe first unsupervised contrastive stage trains on a dataset generated from weakly related text pairs, such as question-answer pairs from forums like StackExchange and Quora, title-body pairs from Amazon reviews, and summarizations from news articles.\nIn the second finetuning stage, higher quality labeled datasets such as search queries and answers from web searches are leveraged. Data curation and hard-example mining is crucial in this stage.\nFor more details, see the Nomic Embed Technical Report and corresponding blog post.\nTraining data to train the models is released in its entirety. For more details, see the contrastors repository\nJoin the Nomic Community\nNomic: https://nomic.ai\nDiscord: https://discord.gg/myY5YDR8z8\nTwitter: https://twitter.com/nomic_ai\nCitation\nIf you find the model, dataset, or training code useful, please cite our work\n@misc{nussbaum2024nomic,\ntitle={Nomic Embed: Training a Reproducible Long Context Text Embedder},\nauthor={Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar},\nyear={2024},\neprint={2402.01613},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}",
    "imagepipeline/NightVisionXL": "NightVisionXL\nHow to try this model ?\nAPI Reference\nGenerate Image\nFeedback\nüîó Visit Website\nNightVisionXL\nThis checkpoint model is uploaded on imagepipeline.io\nModel details - NightVision XL, a lightly trained SDXL model, is refined with community input to achieve its current state. Specialized for photorealistic portraits, it's perfect for social media. With strong coherency and minimized biases, it avoids common issues seen in similar models. Notably, it excels in rendering rich blacks and captivating evening scenes, even producing incredibly bright outputs. This versatile tool caters to both SFW and NSFW content. User-friendly and efficient, NightVision XL requires minimal input, allowing the model's capabilities to shine in scene creation. Experience the next level of digital artistry effortlessly with NightVision XL.\nHow to try this model ?\nYou can try using it locally or send an API call to test the output quality.\nGet your API_KEY from  imagepipeline.io. No payment required.\nCoding in php javascript node etc ? Checkout our documentation\nimport requests\nimport json\nurl =  \"https://imagepipeline.io/sdxl/text2image/v1/run\"\npayload = json.dumps({\n\"model_id\":  \"50c8df55-adb9-4036-bd88-d57f8751dc27\",\n\"prompt\":  \"ultra realistic close up portrait ((beautiful pale cyberpunk female with heavy black eyeliner)), blue eyes, shaved side haircut, hyper detail, cinematic lighting, magic neon, dark red city, Canon EOS R3, nikon, f/1.4, ISO 200, 1/160s, 8K, RAW, unedited, symmetrical balance, in-frame, 8K\",\n\"negative_prompt\":  \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\",\n\"width\":  \"512\",\n\"height\":  \"512\",\n\"samples\":  \"1\",\n\"num_inference_steps\":  \"30\",\n\"safety_checker\":  false,\n\"guidance_scale\":  7.5,\n\"multi_lingual\":  \"no\",\n\"embeddings\":  \"\",\n\"lora_models\": \"\",\n\"lora_weights\":  \"\"\n})\nheaders =  {\n'Content-Type':  'application/json',\n'API-Key': 'your_api_key'\n}\nresponse = requests.request(\"POST\", url, headers=headers, data=payload)\nprint(response.text)\n}\nGet more ready to use MODELS like this for SD 1.5 and SDXL :\nAPI Reference\nGenerate Image\nhttps://api.imagepipeline.io/sdxl/text2image/v1\nHeaders\nType\nDescription\nAPI-Key\nstr\nGet your API_KEY from  imagepipeline.io\nContent-Type\nstr\napplication/json - content type of the request body\nParameter\nType\nDescription\nmodel_id\nstr\nYour base model, find available lists in  models page or upload your own\nprompt\nstr\nText Prompt. Check our Prompt Guide for tips\nnum_inference_steps\nint [1-50]\nNoise is removed with each step, resulting in a higher-quality image over time. Ideal value 30-50 (without LCM)\nguidance_scale\nfloat [1-20]\nHigher guidance scale prioritizes text prompt relevance but sacrifices image quality. Ideal value 7.5-12.5\nlora_models\nstr, array\nPass the model_id(s) of LoRA models that can be found in models page\nlora_weights\nstr, array\nStrength of the LoRA effect\nlicense: creativeml-openrail-m\ntags:\nimagepipeline\nimagepipeline.io\ntext-to-image\nultra-realistic\npinned: false\npipeline_tag: text-to-image\nFeedback\nIf you have any feedback, please reach out to us at hello@imagepipeline.io\nüîó Visit Website\nIf you are the original author of this model, please click here to add credits",
    "SG161222/RealVisXL_V4.0": "Check my exclusive models on Mage: ParagonXL / NovaXL / NovaXL Lightning / NovaXL V2 / NovaXL Pony / NovaXL Pony Lightning / RealDreamXL / RealDreamXL Lightning\nThis model is available on Mage.Space (main sponsor)\nYou can support me directly on Boosty - https://boosty.to/sg_161222\nIt's important! Read it!\nThe model is still in the training phase. This is not the final version and may contain artifacts and perform poorly in some cases.\nThe model is aimed at photorealism. Can produce sfw and nsfw images of decent quality.\nCivitAI Page: https://civitai.com/models/139562/realvisxl-v40-turbo\nRecommended Negative Prompt:\n(face asymmetry, eyes asymmetry, deformed eyes, open mouth)\nor another negative prompt\nRecommended Generation Parameters:\nSampling Steps: 25+\nSampling Method: DPM++ 2M Karras\nRecommended Hires Fix Parameters:\nHires steps: 10+\nUpscaler: 4x-UltraSharp upscaler / or another\nDenoising strength: 0.1 - 0.5\nUpscale by: 1.1-2.0",
    "keriati/deepseek-coder-1.3b-typescript-GGUF": "deepseek-coder-1.3b-typescript\nHow to Use\nRunning with Ollama\nRunning with Ollama and CodeGPT Autocomplete in VSCode\nFill In the Middle (FIM)\nTraining procedure\nTraining hyperparameters\nTraining results\nFramework versions\n[CodeGPT.co]  |  [ü¶ô Ollama]  |  [Discord]  |  [VSCode Extension]\nSee axolotl config\naxolotl version: 0.3.0\nbase_model: deepseek-ai/deepseek-coder-1.3b-base\nmodel_type: AutoModelForCausalLM\ntrust_remote_code: true\nload_in_8bit: false\nload_in_4bit: false\nstrict: false\ndatasets:\n- path: CodeGPTPlus/typescript-0-500000-seq1024\ntype: completion\nfield: text\nval_set_size: 0.001\noutput_dir:  ./fft-out\nsequence_len: 1024\nadapter:\nlora_model_dir:\nlora_r:\nlora_alpha:\nlora_dropout:\nlora_target_linear:\nlora_fan_in_fan_out:\nlora_modules_to_save:\nwandb_project: deepseek_1.3_fft\nwandb_entity:\nwandb_watch:\nwandb_name: aws_a10g\nwandb_log_model: end\ngradient_accumulation_steps: 2\nmicro_batch_size: 20\nnum_epochs: 1\noptimizer: adamw_bnb_8bit\nadam_beta1: 0.9\nadam_beta2: 0.999\nadam_epsilon: 0.000001\nmax_grad_norm: 1.0\nweight_decay: 0.1\nlr_scheduler: cosine\nlearning_rate: 0.00002\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: true\nfp16: false\ntf32: false\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\nloss_watchdog_threshold: 5.0\nloss_watchdog_patience: 3\nhub_model_id: CodeGPTPlus/deepseek_coder_1.3b_typescript\nhub_strategy: every_save\nwarmup_ratio: 0.01\nevals_per_epoch: 20\nsaves_per_epoch: 3\ndebug:\ndeepspeed:\nfsdp:\nfsdp_config:\nspecial_tokens:\nbos_token: \"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>\"\neos_token: \"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\"\npad_token: \"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\"\ndeepseek-coder-1.3b-typescript\nCodeGPTPlus/deepseek-coder-1.3b-typescript, emerges as a fine-tuned iteration of deepseek-ai/deepseek-coder-1.3b-base, meticulously crafted by the CodeGPT team to excel in generating expert code in TypeScript. With specific fine-tuning for TypeScript and a dataset of 0.5B tokens, this model excels in producing precise and efficient solutions in this programming language.\nThe 16K window size and an additional fill-in-the-middle task are employed to deliver project-level code completion.\nThis new model stands as the ideal choice for those seeking a specialized code generator for TypeScript, backed by the expertise of the CodeGPT team.\nIt achieves the following results on the evaluation set:\nLoss: 0.7681\nModel Developers CodeGPT Team\nVariations  1.3B\nInput Models input text only.\nOutput Models generate text only.\nHow to Use\nThis model is for completion purposes only. Here give some examples of how to use the model.\nRunning the model on a GPU\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"CodeGPTPlus/deepseek-coder-1.3b-typescript\",\ntrust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"CodeGPTPlus/deepseek-coder-1.3b-typescript\",\ntrust_remote_code=True).cuda()\ninput_text = \"\"\"<ÔΩúfim‚ñÅbeginÔΩú>function quickSort(arr: number[]): number[] {\nif (arr.length <= 1) {\nreturn arr;\n}\nconst pivot = arr[0];\nconst left = [];\nconst right = [];\n<ÔΩúfim‚ñÅholeÔΩú>\nreturn [...quickSort(left), pivot, ...quickSort(right)];\n}<ÔΩúfim‚ñÅendÔΩú>\"\"\"\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(**inputs, max_length=256)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nRunning with Ollama\nModel: https://ollama.ai/codegpt/deepseek-coder-1.3b-typescript\nollama run codegpt/deepseek-coder-1.3b-typescript\nRunning with Ollama and CodeGPT Autocomplete in VSCode\nDocumentation: https://docs.codegpt.co/docs/tutorial-features/code_autocompletion\nSelect \"Ollama - codegpt/deepseek-coder-1.3b-typescript\" in the autocomplete model selector.\nThen, write any code or comment in the vscode text editor, and the model will provide you with code suggestions through the CodeGPT code autocomplete.\nFill In the Middle (FIM)\n<ÔΩúfim‚ñÅbeginÔΩú>function quickSort(arr: number[]): number[] {\nif (arr.length <= 1) {\nreturn arr;\n}\nconst pivot = arr[0];\nconst left = [];\nconst right = [];\n<ÔΩúfim‚ñÅholeÔΩú>\nreturn [...quickSort(left), pivot, ...quickSort(right)];\n}<ÔΩúfim‚ñÅendÔΩú>\nTraining procedure\nTraining hyperparameters\nThe following hyperparameters were used during training:\nlearning_rate: 2e-05\ntrain_batch_size: 20\neval_batch_size: 20\nseed: 42\ngradient_accumulation_steps: 2\ntotal_train_batch_size: 40\noptimizer: Adam with betas=(0.9,0.999) and epsilon=1e-06\nlr_scheduler_type: cosine\nlr_scheduler_warmup_steps: 261\nnum_epochs: 1\nTraining results\nTraining Loss\nEpoch\nStep\nValidation Loss\n1.0745\n0.0\n1\n0.8681\n1.2267\n0.05\n1308\n0.8130\n1.1594\n0.1\n2616\n0.8018\n0.7674\n0.15\n3924\n0.7942\n0.6443\n0.2\n5232\n0.7889\n0.9155\n0.25\n6540\n0.7847\n0.7501\n0.3\n7848\n0.7819\n0.8835\n0.35\n9156\n0.7792\n0.7261\n0.4\n10464\n0.7769\n0.9746\n0.45\n11772\n0.7748\n0.6884\n0.5\n13080\n0.7734\n0.6104\n0.55\n14388\n0.7722\n0.8876\n0.6\n15696\n0.7710\n0.9567\n0.65\n17004\n0.7703\n0.6915\n0.7\n18312\n0.7696\n0.8874\n0.75\n19620\n0.7691\n0.6124\n0.8\n20928\n0.7686\n0.8147\n0.85\n22236\n0.7684\n0.8021\n0.9\n23544\n0.7683\n0.8665\n0.95\n24852\n0.7681\nFramework versions\nTransformers 4.37.0.dev0\nPytorch 2.0.1+cu118\nDatasets 2.16.1\nTokenizers 0.15.0",
    "google/gemma-7b-it": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged-in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma Model Card\nModel Information\nDescription\nUsage\nChat Template\nInputs and outputs\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma Model Card\nModel Page: Gemma\nThis model card corresponds to the 7B instruct version of the Gemma model. You can also visit the model card of the 2B base model, 7B base model, and 2B instruct model.\nResources and Technical Documentation:\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights, pre-trained variants, and instruction-tuned variants. Gemma\nmodels are well-suited for a variety of text generation tasks, including\nquestion answering, summarization, and reasoning. Their relatively small size\nmakes it possible to deploy them in environments with limited resources such as\na laptop, desktop or your own cloud infrastructure, democratizing access to\nstate of the art AI models and helping foster innovation for everyone.\nUsage\nBelow we share some code snippets on how to get quickly started with running the model. First make sure to pip install -U transformers, then copy the snippet from the section that is relevant for your usecase.\nFine-tuning the model\nYou can find fine-tuning scripts and notebook under the examples/ directory of google/gemma-7b repository. To adapt it to this model, simply change the model-id to google/gemma-7b-it.\nIn that repository, we provide:\nA script to perform Supervised Fine-Tuning (SFT) on UltraChat dataset using QLoRA\nA script to perform SFT using FSDP on TPU devices\nA notebook that you can run on a free-tier Google Colab instance to perform SFT on English quotes dataset\nRunning the model on a CPU\nAs explained below, we recommend torch.bfloat16 as the default dtype. You can use a different precision if necessary.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-7b-it\",\ntorch_dtype=torch.bfloat16\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a single / multi GPU\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-7b-it\",\ndevice_map=\"auto\",\ntorch_dtype=torch.bfloat16\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a GPU using different precisions\nThe native weights of this model were exported in bfloat16 precision. You can use float16, which may be faster on certain hardware, indicating the torch_dtype when loading the model. For convenience, the float16 revision of the repo contains a copy of the weights already converted to that precision.\nYou can also use float32 if you skip the dtype, but no precision increase will occur (model weights will just be upcasted to float32). See examples below.\nUsing torch.float16\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-7b-it\",\ndevice_map=\"auto\",\ntorch_dtype=torch.float16,\nrevision=\"float16\",\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nUsing torch.bfloat16\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", device_map=\"auto\", torch_dtype=torch.bfloat16)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nUpcasting to torch.float32\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\"google/gemma-7b-it\",\ndevice_map=\"auto\"\n)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nQuantized Versions through bitsandbytes\nUsing 8-bit precision (int8)\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", quantization_config=quantization_config)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nUsing 4-bit precision\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", quantization_config=quantization_config)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nOther optimizations\nFlash Attention 2\nFirst make sure to install flash-attn in your environment pip install flash-attn\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\n+   attn_implementation=\"flash_attention_2\"\n).to(0)\nChat Template\nThe instruction-tuned models use a chat template that must be adhered to for conversational use.\nThe easiest way to apply it is using the tokenizer's built-in chat template, as shown in the following snippet.\nLet's load the model and apply the chat template to a conversation. In this example, we'll start with a single user interaction:\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\nmodel_id = \"google/gemma-7b-it\"\ndtype = torch.bfloat16\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ndevice_map=\"cuda\",\ntorch_dtype=dtype,\n)\nchat = [\n{ \"role\": \"user\", \"content\": \"Write a hello world program\" },\n]\nprompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\nAt this point, the prompt contains the following text:\n<bos><start_of_turn>user\nWrite a hello world program<end_of_turn>\n<start_of_turn>model\nAs you can see, each turn is preceded by a <start_of_turn> delimiter and then the role of the entity\n(either user, for content supplied by the user, or model for LLM responses). Turns finish with\nthe <end_of_turn> token.\nYou can follow this format to build the prompt manually, if you need to do it without the tokenizer's\nchat template.\nAfter the prompt is ready, generation can be performed like this:\ninputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\nprint(tokenizer.decode(outputs[0]))\nInputs and outputs\nInput: Text string, such as a question, a prompt, or a document to be\nsummarized.\nOutput: Generated English-language text in response to the input, such\nas an answer to a question, or a summary of a document.\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety\nof sources, totaling 6 trillion tokens. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model is exposed\nto a broad range of linguistic styles, topics, and vocabulary. Primarily\nEnglish-language content.\nCode: Exposing the model to code helps it to learn the syntax and patterns of\nprogramming languages, which improves its ability to generate code or\nunderstand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nThe combination of these diverse data sources is crucial for training a powerful\nlanguage model that can handle a wide variety of different tasks and text\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was\napplied at multiple stages in the data preparation process to ensure the\nexclusion of harmful and illegal content\nSensitive Data Filtering: As part of making Gemma pre-trained models safe and\nreliable, automated techniques were used to filter out certain personal\ninformation and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safely in line with\nour policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using the latest generation of\nTensor Processing Unit (TPU) hardware (TPUv5e).\nTraining large language models requires significant computational power. TPUs,\ndesigned specifically for matrix operations common in machine learning, offer\nseveral advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive computations\ninvolved in training LLMs. They can speed up training considerably compared to\nCPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory, allowing\nfor the handling of large models and batch sizes during training. This can\nlead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable solution for\nhandling the growing complexity of large foundation models. You can distribute\ntraining across multiple TPU devices for faster and more efficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more cost-effective\nsolution for training large models compared to CPU-based infrastructure,\nespecially when considering the time and resources saved due to faster\ntraining.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nML Pathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like\nthese ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nBenchmark\nMetric\n2B Params\n7B Params\nMMLU\n5-shot, top-1\n42.3\n64.3\nHellaSwag\n0-shot\n71.4\n81.2\nPIQA\n0-shot\n77.3\n81.2\nSocialIQA\n0-shot\n49.7\n51.8\nBooIQ\n0-shot\n69.4\n83.2\nWinoGrande\npartial score\n65.4\n72.3\nCommonsenseQA\n7-shot\n65.3\n71.3\nOpenBookQA\n47.8\n52.8\nARC-e\n73.2\n81.5\nARC-c\n42.1\n53.2\nTriviaQA\n5-shot\n53.2\n63.4\nNatural Questions\n5-shot\n12.5\n23\nHumanEval\npass@1\n22.0\n32.3\nMBPP\n3-shot\n29.2\n44.4\nGSM8K\nmaj@1\n17.7\n46.4\nMATH\n4-shot\n11.8\n24.3\nAGIEval\n24.2\n41.7\nBIG-Bench\n35.2\n55.1\n------------------------------\n-------------\n-----------\n---------\nAverage\n45.0\n56.9\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nText-to-Text Content Safety: Human evaluation on prompts covering safety\npolicies including child sexual abuse and exploitation, harassment, violence\nand gore, and hate speech.\nText-to-Text Representational Harms: Benchmark against relevant academic\ndatasets such as WinoBias and BBQ Dataset.\nMemorization: Automated evaluation of memorization of training data, including\nthe risk of personally identifiable information exposure.\nLarge-scale harm: Tests for \"dangerous capabilities,\" such as chemical,\nbiological, radiological, and nuclear (CBRN) risks.\nEvaluation Results\nThe results of ethics and safety evaluations are within acceptable thresholds\nfor meeting internal policies for categories such as child\nsafety, content safety, representational harms, memorization, large-scale harms.\nOn top of robust internal evaluations, the results of well known safety\nbenchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA\nare shown here.\nBenchmark\nMetric\n2B Params\n7B Params\nRealToxicity\naverage\n6.86\n7.90\nBOLD\n45.57\n49.08\nCrowS-Pairs\ntop-1\n45.82\n51.33\nBBQ Ambig\n1-shot, top-1\n62.58\n92.54\nBBQ Disambig\ntop-1\n54.62\n71.99\nWinogender\ntop-1\n51.25\n54.17\nTruthfulQA\n44.84\n31.81\nWinobias 1_2\n56.12\n59.09\nWinobias 2_2\n91.10\n92.23\nToxigen\n29.77\n39.59\n------------------------------\n-------------\n-----------\n---------\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text formats\nsuch as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces for customer\nservice, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus, research\npapers, or reports.\nResearch and Education\nNatural Language Processing (NLP) Research: These models can serve as a\nfoundation for researchers to experiment with NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\nLanguage Learning Tools: Support interactive language learning experiences,\naiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large bodies of text\nby generating summaries or answering questions about specific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly influence the\nmodel's capabilities. Biases or gaps in the training data can lead to\nlimitations in the model's responses.\nThe scope of the training dataset determines the subject areas the model can\nhandle effectively.\nContext and Task Complexity\nLLMs are better at tasks that can be framed with clear prompts and\ninstructions. Open-ended or highly complex tasks might be challenging.\nA model's performance can be influenced by the amount of context provided\n(longer context generally leads to better outputs, up to a certain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. LLMs might struggle to grasp subtle\nnuances, sarcasm, or figurative language.\nFactual Accuracy\nLLMs generate responses based on information they learned from their\ntraining datasets, but they are not knowledge bases. They may generate\nincorrect or outdated factual statements.\nCommon Sense\nLLMs rely on statistical patterns in language. They might lack the ability\nto apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:\nBias and Fairness\nLLMs trained on large-scale, real-world text data can reflect socio-cultural\nbiases embedded in the training material. These models underwent careful\nscrutiny, input data pre-processing described and posterior evaluations\nreported in this card.\nMisinformation and Misuse\nLLMs can be misused to generate text that is false, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to share\ninnovation by making LLM technology accessible to developers and researchers\nacross the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content safety\nare essential. Developers are encouraged to exercise caution and implement\nappropriate content safety safeguards based on their specific product policies\nand application use cases.\nMisuse for malicious purposes: Technical limitations and developer and\nend-user education can help mitigate against malicious applications of LLMs.\nEducational resources and reporting mechanisms for users to flag misuse are\nprovided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal of PII\n(Personally Identifiable Information). Developers are encouraged to adhere to\nprivacy regulations with privacy-preserving techniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nlarge language model implementations designed from the ground up for Responsible\nAI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "TRI-ML/prismatic-vlms": "PRISM Models\nIntended use\nLicensing\nTraining Procedures\nEvaluation Procedures\nPRISM Models\nAll models trained as part of the paper Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models\nby Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh.\nThese models were trained in January 2024 in the open source codebase prismatic-vlms. The goal\nof releasing these models is to provide a thorough understanding of what design choices matter when training visually-conditioned language models\nin addition to providing a number of strong open-source VLMs for the community to build on.\nIntended use\nThe primary use of PRISMs are for research and development on visually-conditioned language models. The intended users are members of the machine learning and artificial intelligence research community.\nLicensing\nPRISM models are released under an MIT License. Copyright (c) Toyota Research Institute, Inc. Toyota did not provide any of the materials used to train these models. They are here for reference and verification and evaluation of the training procedures described in the paper and as enabled in the code. See the paper and the README in the codebase for more details.\nThese models are provided as-is. Toyota Research Institute disclaims all warranties, express or implied, including any warranty of merchantability and fitness for a particular purpose.\nTraining Procedures\nAll models are trained as described in the paper using the associated training codebase. The following datasets are used for training:\nAll LLaVA 1.5 Training Data\nLVIS-Instruct-4V\nLRV-Instruct\nEvaluation Procedures\nModels are evaluated as described in the paper using the associated evaluation codebase. Evaluation datasets span a number of visual reasoning tasks including:\nGeneral visual question answering\nBounding box prediction\nChallenge sets which evaluate counting, identifying spatial relationships, and propensity to hallucinate",
    "NbAiLab/nb-whisper-large": "NB-Whisper Large\nHow to Use the Models\nOnline Demos\nLocal Setup with HuggingFace\nWhisper CPP\nWhisperX and Speaker Diarization\nAPI\nTraining Data\nDownstream Use\nBias, Risks, and Limitations\nSoftware\nCitation & Contributors\nDisclaimer\nAttribution\nAcknowledgements\nContact\nNB-Whisper Large\nIntroducing the Norwegian NB-Whisper Large model, proudly developed by the National Library of Norway. NB-Whisper is a cutting-edge series of models designed for automatic speech recognition (ASR) and speech translation. These models are based on the work of OpenAI's Whisper. Each model in the series has been trained for 250,000 steps, utilizing a diverse dataset of 8 million samples. These samples consist of aligned audio clips, each 30 seconds long, culminating in a staggering 66,000 hours of speech. For an in-depth understanding of our training methodology and dataset composition, keep an eye out for our upcoming article.\nModel Size\nParameters\nModel\nTiny\n39M\nNB-Whisper Tiny\nBase\n74M\nNB-Whisper Base\nSmall\n244M\nNB-Whisper Small\nMedium\n769M\nNB-Whisper Medium\nLarge\n1550M\nNB-Whisper Large\nVerbatim Model\nWhile the main models are suitable for most transcription task, we demonstrate how easy it is to change the output of the main model. The following models are trained 250 additional steps from the main models above, and might be suitable for more targetted use cases:\nVerbatim version: This lower-cased variant is more literal and suitable for tasks requiring detailed transcription, such as linguistic analysis.\nModel Size\nParameters\nSemantic version\nTiny\n39M\nTiny - semantic\nBase\n74M\nBase - semantic\nSmall\n244M\nSmall - semantic\nMedium\n769M\nMedium - semantic\nLarge\n1550M\nLarge - semantic\nModel Description\nDeveloped by: NB AI-Lab\nShared by: NB AI-Lab\nModel type: whisper\nLanguage(s) (NLP): Norwegian, Norwegian Bokm√•l, Norwegian Nynorsk, English\nLicense: Apache 2.0\nTrained from model: openai/whisper-large-v3\nCode Repository: https://github.com/NbAiLab/nb-whisper/\nPaper: Coming soon\nDemo: See Spaces on this page\nHow to Use the Models\nOnline Demos\nYou can try the models directly through the HuggingFace Inference API, accessible on the right side of this page. Be aware that initially, the model needs to load and will run on limited CPU capacity, which might be slow. To enhance your experience, we are temporarily hosting some models on TPUs for a few days, significantly boosting their performance. Explore these under the Spaces section on the Main Page.\nLocal Setup with HuggingFace\nAlternatively, you can run the models locally. The Tiny, Base, and Small models are optimized for CPU execution. For the Medium and Large models, we recommend a system equipped with a GPU to ensure efficient processing. Setting up and using these models with HuggingFace's Transformers is straightforward, provided you have Python installed on your machine. For practical demonstrations, refer to examples using this sample mp3 file.\n# Download the sample file\n$ wget -N https://github.com/NbAiLab/nb-whisper/raw/main/audio/king.mp3\n# Install necessary libraries.\n$ pip install transformers>=4.35.2\nAfter this is done, you should be able to run this in Python:\nfrom transformers import pipeline\n# Load the model\nasr = pipeline(\"automatic-speech-recognition\", \"NbAiLabBeta/nb-whisper-large\")\n#transcribe\nasr(\"king.mp3\", generate_kwargs={'task': 'transcribe', 'language': 'no'})\nExpected output\n{\n{'text': ' Nordmenn er nordlendinger, tr√∏ndere, s√∏rlendinger og folk fra alle andre regioner. Nordmenn er ogs√• innvandret fra Afghanistan, Pakistan, Polen, Sverige, Somalia og Syria. Det er ikke alltid s√• lett √• si hvor vi er fra, hvilken nasjonalitet vi er fra. Hvilken nasjonalitet vi er fra. Hvilken nasjonalitet vi er fra. Hvilken nasjonalitet vi er fra. Hvilken nasjonalitet vi er fra. Hvilken nasjonalitet vi er fra. Hvilken nasjonalitet vi er fra.'}\n}\nExtended HuggingFace\nExamining the output above, we see that there are multiple repetitions at the end. This is because the video is longer than 30 seconds. By passing the chunk_lengt_s argument, we can transcribe longer file. Our experience is that we get slightly better result by setting that to 28 seconds instead of the default 30 seconds. We also recommend setting the beam size to 5 if possible. This greatly increases the accuracy but takes a bit longer and requires slightly more memory. The examples below also illustrates how to transcribe to English or Nynorsk, and how to get timestamps for sentences and words.\n# Long Transcripts\nasr(\"king.mp3\", chunk_length_s=28, generate_kwargs={'task': 'transcribe', 'language': 'no'})\n# Increase accuracy by setting beam size to 5\nasr(\"king.mp3\", chunk_length_s=28, return_timestamps=True, generate_kwargs={'num_beams': 5, 'task': 'transcribe', 'language': 'no'})\n# Return Timestamps\nasr(\"king.mp3\", chunk_length_s=28, return_timestamps=True, generate_kwargs={'task': 'transcribe', 'language': 'no'})\n# Return Word Level Timestamps\nasr(\"king.mp3\", chunk_length_s=28, return_timestamps=\"word\", generate_kwargs={'task': 'transcribe', 'language': 'no'})\n# Transcribe to Nynorsk\nasr(\"king.mp3\", chunk_length_s=28, generate_kwargs={'task': 'transcribe', 'language': 'nn'})\n# Transcribe to English\nasr(\"king.mp3\", chunk_length_s=28, generate_kwargs={'task': 'transcribe', 'language': 'en'})\nExpected output\nLong transcripts:\n{\n{'text': ' Nordmenn er nordlendinger, tr√∏ndere, s√∏rlendinger og folk fra alle andre regioner. Nordmenn er ogs√• innvandret fra Afghanistan, Pakistan, Polen, Sverige, Somalia og Syria. Det er ikke alltid s√• lett √• si hvor vi er fra, hvilken nasjonalitet vi er fra. Hvilken nasjonalitet vi er fra. Hvilken nasjonalitet vi er fra. Hvilken nasjonalitet vi er fra. Hvilken nasjonalitet vi er fra. Hvilken nasjonalitet vi er fra, hvilken nasjonalitet vi tilh√∏rer. Det vi kaller hjem, er der hjertet v√•rt er, og det kan ikke alltid plasseres innenfor landegrenser. Nordmenn er jenter som er glad i jenter, gutter som er glad i gutter, og jenter og gutter som er glad i hverandre. Nordmenn trommer p√• Gud, Allah, Altet og ingenting. Nordmenn liker Grieg, Kygo, Helbilis og Kari Bremnes. Med andre ord, Norge er dere. Norge er oss. Mitt st√∏rste h√•p for Norge er at vi skal klare √• ta vare p√• hverandre, at vi skal bygge dette landet videre p√• tillit, fellesskap og raushet.'}\n}\nTimestamps:\n{\n{'text': ' Nordmenn er nordlendinger, tr√∏ndere, s√∏rlendinger og folk fra alle andre regioner. Nordmenn er ogs√• innvandret fra Afghanistan, Pakistan, Polen, Sverige, Somalia og Syria. Det er ikke alltid s√• lett √• si hvor vi er fra, hvilken nasjonalitet vi er fra. Hvilken nasjonalitet vi er fra. hvilken nasjonalitet vi tilh√∏rer. Det vi kaller hjem, er der hjertet v√•rt er, og det kan ikke alltid plasseres innenfor landegrenser. Nordmenn er jenter som er glad i jenter, gutter som er glad i gutter, og jenter og gutter som er glad i hverandre. Nordmenn trommer p√• Gud, Allah, Altet og ingenting. Nordmenn liker Grieg, Kygo, Helbiles og Kari Bremnes. Med andre ord, Norge er dere. Norge er oss. Mitt st√∏rste h√•p for Norge er at vi skal klare √• ta vare p√• hverandre, at vi skal bygge dette landet videre p√• tillit, fellesskap og raushet.',\n'chunks': [{'timestamp': (0.0, 5.46),\n'text': ' Nordmenn er nordlendinger, tr√∏ndere, s√∏rlendinger'},\n{'timestamp': (5.52, 8.68), 'text': ' og folk fra alle andre regioner.'},\n{'timestamp': (8.68, 16.64),\n'text': ' Nordmenn er ogs√• innvandret fra Afghanistan, Pakistan, Polen, Sverige, Somalia og Syria.'},\n{'timestamp': (16.64, 13.3),\n'text': ' Det er ikke alltid s√• lett √• si hvor vi er fra, hvilken nasjonalitet vi er fra.'},\n{'timestamp': (13.32, 30.28),\n'text': ' Hvilken nasjonalitet vi er fra. hvilken nasjonalitet vi tilh√∏rer.'},\n{'timestamp': (32.52, 39.16),\n'text': ' Det vi kaller hjem, er der hjertet v√•rt er, og det kan ikke alltid plasseres'},\n{'timestamp': (39.16, 42.0), 'text': ' innenfor landegrenser.'},\n{'timestamp': (42.0, 46.74),\n'text': ' Nordmenn er jenter som er glad i jenter, gutter som er glad i gutter,'},\n{'timestamp': (46.74, 51.12),\n'text': ' og jenter og gutter som er glad i hverandre.'},\n{'timestamp': (51.16, 57.42),\n'text': ' Nordmenn trommer p√• Gud, Allah, Altet og ingenting.'},\n{'timestamp': (57.42, 64.3),\n'text': ' Nordmenn liker Grieg, Kygo, Helbiles og Kari Bremnes.'},\n{'timestamp': (64.34, 71.24),\n'text': ' Med andre ord, Norge er dere. Norge er oss.'},\n{'timestamp': (71.24, 78.04),\n'text': ' Mitt st√∏rste h√•p for Norge er at vi skal klare √• ta vare p√• hverandre,'},\n{'timestamp': (78.12, 84.68),\n'text': ' at vi skal bygge dette landet videre p√• tillit, fellesskap og raushet.'}]}\n}\nWord Level Timestamps:\n{\n{\"text\": \"Nordmenn er nordlendinger, tr√∏ndere, s√∏rlendinger og folk fra alle andre regioner. Nordmenn er ogs√• innvandret fra Afghanistan, Pakistan, Polen, Sverige, Somalia og Syria. Det er ikke alltid s√• lett √• si hvor vi er fra, hvilken nasjonalitet vi tilh√∏rer. Det vi kaller hjem, er der hjertet v√•rt er, og det kan ikke alltid plasseres innenfor landegrenser. Nordmenn er jenter som er glad i jenter, gutter som er glad i gutter, og jenter og gutter som er glad i hverandre. Nordmenn trommer p√• Gud, Allah, Altet og ingenting. Nordmenn liker Grieg, Kygo, Helbilis og Kari Bremnes. Med andre ord, Norge er dere. Norge er oss. Mitt st√∏rste h√•p for Norge er at vi skal klare √• ta vare p√• hverandre, at vi skal bygge dette landet videre p√• tillit, fellesskap og raushet.\",\n\"chunks\": [\n{\"text\": \"Nordmenn\", \"timestamp\": [0.72, 1.42]},\n{\"text\": \"er\", \"timestamp\": [1.42, 1.74]},\n// ... more chunks ...\n{\"text\": \"raushet.\", \"timestamp\": [83.1, 84.88]}\n]\n}\n}\nNynorsk:\n{\n{\"text\": \"Nordmenn er nordlendingar, tr√∏ndarar, s√∏rlendingar og folk fr√• alle andre regionar. Nordmenn er ogs√• innvandra fr√• Afghanistan, Pakistan, Polen, Sverige, Somalia og Syria. Det er ikkje alltid s√• lett √• seie kvar vi er fr√•, kva nasjonalitet vi tilh√∏yrer. Det vi kallar heim, er der hjartet v√•rt er, og det kan ikkje alltid plasserast innanfor landegrenser. Nordmenn er jenter som er glad i jenter, gutar som erade i gutar, og jenter og gutar som er glade i kvarandre. Nordmenn trommar p√• Gud, Allah, Altet og ingenting. Nordmenn liker Grieg, Kygo, Helbiles og Kari Bremnes. Med andre ord, Noreg er dere! Noreg er oss. Mitt st√∏rste h√•p for Noreg er at vi skal klare √• ta vare p√• kvarandre, at vi skal byggje dette landet vidare p√• tillit, fellesskap og raushet.\"}\n}\nEnglish:\n{\n{\"text\": \"Norwegians are Norwegians, tr√∏nders, southerners and people from all other regions. Norwegians are also invaded from Afghanistan, Pakistan, Poland, Sweden, Somalia and Suria. It is not always so easy to say where we are from, what nationality we belong to. What we call home is where our heart is, and it cannot always be placed within national borders. Norwegians are girls who like girls, boys who like boys, and girls and boys who like each other. Norwegians thrump on God, Allah, Altet and nothing. Norwegians like Grieg, Kygo, Helbilis and Kari Bremnes. In other words, Norway is you. Norway is us. My biggest hope for Norway is that we should be able to take care of each other, that we should build this country on trust, community and generosity.\"}\n}\nWhisper CPP\nWhisper CPP is a C++ implementation of the Whisper model, offering the same functionalities with the added benefits of C++ efficiency and performance optimizations. This allows embedding any Whisper model into a binary file, facilitating the development of real applications. However, it requires some familiarity with compiling C++ programs. Their homepage provides examples of how to build applications, including real-time transcription.\nWe have converted this model to the ggml-format model used by Whisper CPP binaries. The file can be downloaded here, and a q5_0 quantized version is also available here.\n# We can download and compile whisper.cpp\n$ git clone --depth 1 https://github.com/ggerganov/whisper.cpp --branch v1.5.1\n$ cd whisper.cpp/\n$ make\n#¬†We also need to convert the audio to WAV as that is the only format supported by whisper.cpp\n$ wget -N https://github.com/NbAiLab/nb-whisper/raw/main/audio/king.mp3\n$ ffmpeg -i king.mp3 -ar 16000 -ac 1 -c:a pcm_s16le king.wav\n# Lets download the two ggml-files from this site\nwget -N https://huggingface.co/NbAiLab/nb-whisper-large/resolve/main/ggml-model.bin -O models/nb-large-ggml-model.bin\nwget -N https://huggingface.co/NbAiLab/nb-whisper-large/resolve/main/ggml-model-q5_0.bin -O models/nb-large-ggml-model-q5_0.bin\n#¬†And run it with the f16 default model\n$ ./main -l no -m models/nb-large-ggml-model.bin king.wav\n# Or the quantized version\n$ ./main -l no -m models/nb-large-ggml-model-q5_0.bin king.wav\nWhisperX and Speaker Diarization\nSpeaker diarization is a technique in natural language processing and automatic speech recognition that identifies and separates different speakers in an audio recording. It segments the audio into parts based on who is speaking, enhancing the quality of transcribing meetings or phone calls. We find that WhisperX is the easiest way to use our models for diarizing speech. In addition, WhisperX is using phoneme-based Wav2Vec-models for improving the alignment of the timestamps. As of December 2023 it also has native support for using the nb-wav2vec-models. It currently uses PyAnnote-audio for doing the actual diarization. This package has a fairly strict licence where you have to agree to user terms. Follow the instructions below.\n# Follow the install instructions on https://github.com/m-bain/whisperX\n# Make sure you have a HuggingFace account and have agreed to the pyannote terms\n# Log in (or supply HF Token in command line)\nhuggingface-cli login\n# Download a test file\nwget -N https://github.com/NbAiLab/nb-whisper/raw/main/audio/knuthamsun.mp3\n# Optional. If you get complains about not support for Norwegian, do:\npip uninstall whisperx && pip install git+https://github.com/m-bain/whisperx.git@8540ff5985fceee764acbed94f656063d7f56540\n# Transcribe the test file. All transcripts will end up in the directory of the mp3-file\nwhisperx knuthamsun.mp3 --model NbAiLabBeta/nb-whisper-large --language no --diarize\nYou can also run WhisperX from Python. Please take a look at the instructions on WhisperX homepage.\nAPI\nInstructions for accessing the models via a simple API are included in the demos under Spaces. Note that these demos are temporary and will only be available for a few weeks.\nTraining Data\nThe training data originates from Spr√•kbanken and the National Library of Norway's digital collection, including:\nNST Norwegian ASR Database (16 kHz) and its corresponding dataset\nTranscribed speeches from the Norwegian Parliament by Spr√•kbanken\nTV broadcast (NRK) subtitles (NLN digital collection)\nAudiobooks (NLN digital collection)\nDownstream Use\nThe models, especially the smaller ones, may exhibit occasional hallucinations and may drop parts of the transcript. They are designed to convert spoken language into grammatically correct written sentences, which might not always be word-for-word translations. We have made two extra model variant for users that want a different transcription style. We encourage users to try the models themselves to get a better understanding.\nBias, Risks, and Limitations\nUsing these models without adequate risk assessment and mitigation could be considered irresponsible. They may contain biases or other undesirable distortions. Users who deploy these models or integrate them into systems or services are responsible for mitigating risks and complying with applicable AI regulations. The National Library of Norway, as the model owner, disclaims liability for any outcomes resulting from third-party use of these models.\nSoftware\nThe model was trained using Jax/Flax and converted to PyTorch, Tensorflow, whisper.cpp, and ONXX formats. These are available under Files and versions. We welcome requests for conversion to other formats. All training code and scripts are released under the Apache License 2.0 in the GitHub repository nb-whisper.\nCitation & Contributors\nThe NB-Whisper Large model is a product of the NoSTram project led by Per Egil Kummervold (@pere) at the National Library of Norway. Key contributors include Javier de la Rosa (@versae), Freddy Wetjen (@freddyw), and Rolv-Arild Braaten (@Rolv-Arild). NB AI-Lab, under the direction of Svein Arne Brygfjeld (@Brygfjeld), supported the project's successful completion. A detailed paper on our process and findings is forthcoming.\nDisclaimer\nThe models published in this repository are intended for a generalist purpose and are available to third parties. These models may have bias and/or any other undesirable distortions. When third parties, deploy or provide systems and/or services to other parties using any of these models (or using systems based on these models) or become users of the models, they should note that it is their responsibility to mitigate the risks arising from their use and, in any event, to comply with applicable regulations, including regulations regarding the use of artificial intelligence. In no event shall the owner of the models (The National Library of Norway) be liable for any results arising from the use made by third parties of these models.\nAttribution\nThis model is released under the Apache-2.0 license. Note that for downloads made in Norway, the requirements for attribution specified in the Norwegian copyright act still apply where relevant, even if not explicitly mentioned in the Apache License. Although attribution might not be required if the model is downloaded and used in other countries, we strongly encourage following the practive of marking subtitles with ‚ÄúUndertekster generert av NB-Whisper Medium v1.0‚Äù or ‚ÄúSubtitles generated by NB-Whisper Medium v1.0.‚Äù This will also ensure that future ASR programs are not trained on machine-generated subtitles.\nAcknowledgements\nOur gratitude extends to Google TPU Research Cloud for training resources, Google Cloud for translation credits, and HuggingFace's Sanchit Ghandi for technical support. A special thank you to Per Erik Solberg at Spr√•kbanken for the collaboration on the Stortinget corpus.\nContact\nFor feedback, technical concerns, or collaboration inquiries, please contact ailab@nb.no. If you plan to include this model in your research, contact us for the latest information on our upcoming paper for citation purposes.",
    "nomic-ai/nomic-embed-text-v1-GGUF": "A newer version of this model is available:\nnomic-ai/nomic-embed-text-v1.5-GGUF\nnomic-embed-text-v1 - GGUF\nUsage\nDescription\nExample llama.cpp Command\nCompatibility\nProvided Files\nnomic-embed-text-v1 - GGUF\nOriginal model: nomic-embed-text-v1\nUsage\nEmbedding text with nomic-embed-text requires task instruction prefixes at the beginning of each string.\nFor example, the code below shows how to use the search_query prefix to embed user questions, e.g. in a RAG application.\nTo see the full set of task instructions available & how they are designed to be used, visit the model card for nomic-embed-text-v1.\nDescription\nThis repo contains llama.cpp-compatible files for nomic-embed-text-v1 in GGUF format.\nllama.cpp will default to 2048 tokens of context with these files. For the full 8192 token context length, you will have to choose a context extension method. The ü§ó Transformers model uses Dynamic NTK-Aware RoPE scaling, but that is not currently available in llama.cpp.\nExample llama.cpp Command\nCompute a single embedding:\n./embedding -ngl 99 -m nomic-embed-text-v1.f16.gguf -c 8192 -b 8192 --rope-scaling yarn --rope-freq-scale .75 -p 'search_query: What is TSNE?'\nYou can also submit a batch of texts to embed, as long as the total number of tokens does not exceed the context length. Only the first three embeddings are shown by the embedding example.\ntexts.txt:\nsearch_query: What is TSNE?\nsearch_query: Who is Laurens Van der Maaten?\nCompute multiple embeddings:\n./embedding -ngl 99 -m nomic-embed-text-v1.f16.gguf -c 8192 -b 8192 --rope-scaling yarn --rope-freq-scale .75 -f texts.txt\nCompatibility\nThese files are compatible with llama.cpp as of commit 4524290e8 from 2/15/2024.\nProvided Files\nThe below table shows the mean squared error of the embeddings produced by these quantizations of Nomic Embed relative to the Sentence Transformers implementation.\nName\nQuant\nSize\nMSE\nnomic-embed-text-v1.Q2_K.gguf\nQ2_K\n48 MiB\n2.36e-03\nnomic-embed-text-v1.Q3_K_S.gguf\nQ3_K_S\n57 MiB\n1.31e-03\nnomic-embed-text-v1.Q3_K_M.gguf\nQ3_K_M\n65 MiB\n8.73e-04\nnomic-embed-text-v1.Q3_K_L.gguf\nQ3_K_L\n69 MiB\n8.68e-04\nnomic-embed-text-v1.Q4_0.gguf\nQ4_0\n75 MiB\n6.87e-04\nnomic-embed-text-v1.Q4_K_S.gguf\nQ4_K_S\n75 MiB\n6.81e-04\nnomic-embed-text-v1.Q4_K_M.gguf\nQ4_K_M\n81 MiB\n3.12e-04\nnomic-embed-text-v1.Q5_0.gguf\nQ5_0\n91 MiB\n2.79e-04\nnomic-embed-text-v1.Q5_K_S.gguf\nQ5_K_S\n91 MiB\n2.61e-04\nnomic-embed-text-v1.Q5_K_M.gguf\nQ5_K_M\n95 MiB\n7.34e-05\nnomic-embed-text-v1.Q6_K.gguf\nQ6_K\n108 MiB\n6.29e-05\nnomic-embed-text-v1.Q8_0.gguf\nQ8_0\n140 MiB\n6.34e-06\nnomic-embed-text-v1.f16.gguf\nF16\n262 MiB\n5.62e-10\nnomic-embed-text-v1.f32.gguf\nF32\n262 MiB\n9.34e-11",
    "dtarnow/UPscaler": "No model card",
    "GnanaPrasath/ocr_tamil": "Languages Supported üîõ\nAccuracy üéØ\nComparison between Tesseract OCR and OCR Tamil ‚öñÔ∏è\nHow to Install and Use OCR Tamil üë®üèº‚Äçüíª\nQuick linksüåê\nPip install instructionsüêç\nPython Usage - Single image inference\nBatch inference mode üíª\nApplications‚ö°\nLimitations‚õî\nAcknowledgements üëè\nCitation\nOCR Tamil - Easy, Accurate and Simple to use Tamil OCR - (‡Æí‡Æ≥‡Æø ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æ£‡Æ∞‡Æø)\n‚ù§Ô∏èÔ∏è‚ù§Ô∏èÔ∏èPlease star‚ú® it if you like‚ù§Ô∏èÔ∏è‚ù§Ô∏èÔ∏è\nOCR Tamil can help you extract text from signboard, nameplates, storefronts etc., from Natural Scenes with high accuracy. This version of OCR is much more robust to tilted text compared to the Tesseract, Paddle OCR and Easy OCR as they are primarily built to work on the documents texts and not on natural scenes. This model is work in progress, feel free to contribute!!!\nLanguages Supported üîõ\n‚û°Ô∏è English\n‚û°Ô∏è Tamil (‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç)\nAccuracy üéØ\n‚úîÔ∏è English > 98%\n‚úîÔ∏è Tamil > 95%\nComparison between Tesseract OCR and OCR Tamil ‚öñÔ∏è\nInput Image\nOCR TAMIL   üèÜ\nTesseract\nEasyOCR\n‡Æµ‡Ææ‡Æ¥‡Øç‡Æï‡Æµ‡Æ≥‡ÆÆ‡ØÅ‡Æü‡Æ©‡Øç‚úÖ\n‡Æï‡Øç‚Äå ‡Æï‡Øç‡Æï‡Æ∏‡Ææ‡Æ∞‡Æï‡Æ≥‡Æ≥‡ØÆ‡Æä‡Æï‡Æé‡Æ≥‡ÆÆ‡ØÅ‡Æü‡Æ©‡Øç‚Äå ‚ùå\n‡Æµ‡Ææ‡Æ¥‡Æï ‡Æµ‡Æ≥‡ÆÆ‡ØÅ‡Æü‡Æ©‡Øç‚ùå\n‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æµ‡Ææ‡Æ¥‡Øç‡Æï‚úÖ\nNO OUTPUT ‚ùå\n‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æµ‡Ææ‡Æ¥‡Øç‡Æï‚úÖ\n‡Æï‡Øã‡Æ™‡Æø ‚úÖ\nNO OUTPUT ‚ùå\n‡Æ™99‚ùå\n‡Æ§‡Ææ‡ÆÆ‡Øç‡Æ™‡Æ∞‡ÆÆ‡Øç ‚úÖ\nNO OUTPUT ‚ùå\n‡Æ§‡Ææ‡ÆÆ‡Øç‡Æ™‡Æ∞‡ÆÆ‚ùå\n‡Æ®‡ØÜ‡Æü‡ØÅ‡Æû‡Øç‡Æö‡Ææ‡Æ≤‡Øà‡Æ§‡Øç ‚úÖ\nNO OUTPUT ‚ùå\n‡Æ®‡ØÜ‡Æü‡ØÅ‡Æû‡Øç‡Æö‡Ææ‡Æ≤‡Øà‡Æ§‡Øç ‚úÖ\n‡ÆÖ‡Æ£‡Øç‡Æ£‡Ææ‡Æö‡Ææ‡Æ≤‡Øà ‚úÖ\nNO OUTPUT ‚ùå\n‡Æ≤@I9‚ùå\n‡Æ∞‡ØÜ‡Æü‡Æø‡ÆÆ‡Øá‡Æü‡Æ∏‡Øç ‚ùå\nNO OUTPUT ‚ùå\n‡Æ∞‡ØÜ‡Æü‡Æø‡ÆÆ‡Øá‡Æü‡Æ∏‡Øç ‚ùå\nObtained Tesseract and EasyOCR results using the Colab notebook with Tamil and english as language\nHow to Install and Use OCR Tamil üë®üèº‚Äçüíª\nQuick linksüåê\nüìî Detailed explanation on Medium article.\n‚úçÔ∏è Experiment in Colab notebook\nü§ó Test it in Huggingface spaces\nPip install instructionsüêç\nIn your command line, run the following command pip install ocr_tamil\nIf you are using jupyter notebook , install like !pip install ocr_tamil\nPython Usage - Single image inference\nText Recognition only\nfrom ocr_tamil.ocr import OCR\nimage_path = r\"test_images\\1.jpg\" # insert your own path here\nocr = OCR()\ntext_list = ocr.predict(image_path)\nprint(text_list[0])\n## OUTPUT : ‡Æ®‡ØÜ‡Æü‡ØÅ‡Æû‡Øç‡Æö‡Ææ‡Æ≤‡Øà‡Æ§‡Øç\nText Detect + Recognition\nfrom ocr_tamil.ocr import OCR\nimage_path = r\"test_images\\0.jpg\" # insert your own image path here\nocr = OCR(detect=True)\ntexts = ocr.predict(image_path)\nprint(text_list[0])\n## OUTPUT : ‡Æï‡Øä‡Æü‡Øà‡Æï‡Øç‡Æï‡Ææ‡Æ©‡Æ≤‡Øç Kodaikanal\nBatch inference mode üíª\nText Recognition only\nfrom ocr_tamil.ocr import OCR\nimage_path = [r\"test_images\\1.jpg\",r\"test_images\\2.jpg\"] # insert your own image paths here\nocr = OCR()\ntext_list = ocr.predict(image_path)\nfor text in text_list:\nprint(text)\n## OUTPUT : ‡Æ®‡ØÜ‡Æü‡ØÅ‡Æû‡Øç‡Æö‡Ææ‡Æ≤‡Øà‡Æ§‡Øç\n## OUTPUT : ‡Æï‡Øã‡Æ™‡Æø\nText Detect + Recognition\nfrom ocr_tamil.ocr import OCR\nimage_path = [r\"test_images\\0.jpg\",r\"test_images\\tamil_sentence.jpg\"] # insert your own image paths here\nocr = OCR(detect=True)\ntext_list = ocr.predict(image_path)\nfor text in text_list:\nprint(text)\n## OUTPUT : ‡Æï‡Øä‡Æü‡Øà‡Æï‡Øç‡Æï‡Ææ‡Æ©‡Æ≤‡Øç Kodaikanal\n## OUTPUT : ‡Æö‡ØÜ‡Æ∞‡Æø‡ÆØ‡Æ∞‡Øç ‡ÆØ‡Æ±‡Øç‡Æï‡Øà ‡ÆÆ‡ØÇ‡Æ≤‡Æø‡Æï‡Øà‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æá‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ ‡Æà‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÜ‡Æü‡ØÅ‡Æï‡Øç‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡Æµ‡ØÄ‡Æ∞‡Æø‡ÆØ ‡Æâ‡Æü‡Øç‡Æ™‡Øä‡Æ∞‡ØÅ‡Æü‡Øç‡Æï‡Æ≥‡Øà ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æü‡Æï‡Øç‡Æï‡Æø ‡Æé‡Æ®‡Øç‡Æ§ ‡Æá‡Æ∞‡Æö‡Ææ‡ÆØ‡Æ© ‡Æö‡Øá‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Øà‡Æï‡Æ≥‡ØÅ‡ÆÆ‡Øç ‡Æá‡Æ≤‡Øç‡Æ≤‡Ææ‡ÆÆ‡Æ≤‡Øç ‡Æâ‡Æ∞‡ØÅ‡Æµ‡Ææ‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡Æá‡Æ®‡Øç‡Æ§‡Æø‡ÆØ‡Ææ‡Æµ‡Æø‡Æ©‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Øç ‡Æö‡Æø‡Æ§‡Øç‡Æ§ ‡Æ§‡ÆØ‡Ææ‡Æ∞‡Æø‡Æ™‡Øç‡Æ™‡ØÅ\nTested using Python 3.10 on Windows & Linux (Ubuntu 22.04) Machines\nApplications‚ö°\nADAS system navigation based on the signboards + maps (hybrid approach) üöÅ\nLicense plate recognition üöò\nLimitations‚õî\nUnable to read the text if they are present in rotated forms\nCurrently supports Only English and Tamil Language\nDocument Text reading capability is limited. Auto identification of Paragraph, line are not supported along with Text detection model inability to detect and crop the Tamil text leads to accuracy decrease (WORKAROUND Can use your own text detection model along with OCR tamil text recognition model)\nCropped Text from Text detection Model\nCharacter **‡Æá** missing due to text detection model error\n**?**‡ÆØ‡Æ±‡Øç‡Æï‡Øà ‡ÆÆ‡ØÇ‡Æ≤‡Æø‡Æï‡Øà‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æá‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ ‡Æà‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÜ‡Æü‡ØÅ‡Æï‡Øç‡Æï‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡Æµ‡ØÄ‡Æ∞‡Æø‡ÆØ ‡Æâ‡Æü‡Øç‡Æ™‡Øä‡Æ∞‡ØÅ‡Æü‡Øç‡Æï‡Æ≥‡Øà ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æü‡Æï‡Øç‡Æï‡Æø ‡Æé‡Æ®‡Øç‡Æ§ ‡Æá‡Æ∞‡Æö‡Ææ‡ÆØ‡Æ© ‡Æö‡Øá‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Øà‡Æï‡Æ≥‡ØÅ‡ÆÆ‡Øç ‡Æá‡Æ≤‡Øç‡Æ≤‡Ææ‡ÆÆ‡Æ≤‡Øç ‡Æâ‡Æ∞‡ØÅ‡Æµ‡Ææ‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü ‡Æá‡Æ®‡Øç‡Æ§‡Æø‡ÆØ‡Ææ‡Æµ‡Æø‡Æ©‡Øç ‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Øç ‡Æö‡Æø‡Æ§‡Øç‡Æ§ ‡Æ§‡ÆØ‡Ææ‡Æ∞‡Æø‡Æ™‡Øç‡Æ™‡ØÅ\nAcknowledgements üëè\nText detection - CRAFT TEXT DECTECTION\nText recognition - PARSEQ\n@InProceedings{bautista2022parseq,\ntitle={Scene Text Recognition with Permuted Autoregressive Sequence Models},\nauthor={Bautista, Darwin and Atienza, Rowel},\nbooktitle={European Conference on Computer Vision},\npages={178--196},\nmonth={10},\nyear={2022},\npublisher={Springer Nature Switzerland},\naddress={Cham},\ndoi={10.1007/978-3-031-19815-1_11},\nurl={https://doi.org/10.1007/978-3-031-19815-1_11}\n}\n@inproceedings{baek2019character,\ntitle={Character Region Awareness for Text Detection},\nauthor={Baek, Youngmin and Lee, Bado and Han, Dongyoon and Yun, Sangdoo and Lee, Hwalsuk},\nbooktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\npages={9365--9374},\nyear={2019}\n}\nCitation\n@InProceedings{GnanaPrasath,\ntitle={Tamil OCR},\nauthor={Gnana Prasath D},\nmonth={01},\nyear={2024},\nurl={https://github.com/gnana70/tamil_ocr}\n}"
}