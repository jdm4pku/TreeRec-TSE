{
    "Salesforce/blip-image-captioning-base": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\nTL;DR\nUsage\nUsing the Pytorch model\nEthical Considerations\nBibTex and citation info\nBLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\nModel card for image captioning pretrained on COCO dataset - base architecture (with ViT base backbone).\nPull figure from BLIP official repo\nTL;DR\nAuthors from the paper write in the abstract:\nVision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.\nUsage\nYou can use this model for conditional and un-conditional image captioning\nUsing the Pytorch model\nRunning the model on CPU\nClick to expand\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n# conditional image captioning\ntext = \"a photography of\"\ninputs = processor(raw_image, text, return_tensors=\"pt\")\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\")\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\nRunning the model on GPU\nIn full precision\nClick to expand\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n# conditional image captioning\ntext = \"a photography of\"\ninputs = processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\")\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\nIn half precision (float16)\nClick to expand\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", torch_dtype=torch.float16).to(\"cuda\")\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n# conditional image captioning\ntext = \"a photography of\"\ninputs = processor(raw_image, text, return_tensors=\"pt\").to(\"cuda\", torch.float16)\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\nEthical Considerations\nThis release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people‚Äôs lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.\nBibTex and citation info\n@misc{https://doi.org/10.48550/arxiv.2201.12086,\ndoi = {10.48550/ARXIV.2201.12086},\nurl = {https://arxiv.org/abs/2201.12086},\nauthor = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\nkeywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\ntitle = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\npublisher = {arXiv},\nyear = {2022},\ncopyright = {Creative Commons Attribution 4.0 International}\n}",
    "lj1995/VoiceConversionWebUI": "README.md exists but content is empty.",
    "timbrooks/instruct-pix2pix": "InstructPix2Pix: Learning to Follow Image Editing Instructions\nExample\nInstructPix2Pix: Learning to Follow Image Editing Instructions\nGitHub: https://github.com/timothybrooks/instruct-pix2pix\nExample\nTo use InstructPix2Pix, install diffusers using main for now. The pipeline will be available in the next release\npip install diffusers accelerate safetensors transformers\nimport PIL\nimport requests\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\nmodel_id = \"timbrooks/instruct-pix2pix\"\npipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\npipe.to(\"cuda\")\npipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\nurl = \"https://raw.githubusercontent.com/timothybrooks/instruct-pix2pix/main/imgs/example.jpg\"\ndef download_image(url):\nimage = PIL.Image.open(requests.get(url, stream=True).raw)\nimage = PIL.ImageOps.exif_transpose(image)\nimage = image.convert(\"RGB\")\nreturn image\nimage = download_image(url)\nprompt = \"turn him into cyborg\"\nimages = pipe(prompt, image=image, num_inference_steps=10, image_guidance_scale=1).images\nimages[0]",
    "microsoft/BioGPT-Large": "BioGPT\nCitation\nBioGPT\nPre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.\nCitation\nIf you find BioGPT useful in your research, please cite the following paper:\n@article{10.1093/bib/bbac409,\nauthor = {Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},\ntitle = \"{BioGPT: generative pre-trained transformer for biomedical text generation and mining}\",\njournal = {Briefings in Bioinformatics},\nvolume = {23},\nnumber = {6},\nyear = {2022},\nmonth = {09},\nabstract = \"{Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98\\%, 38.42\\% and 40.76\\% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2\\% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.}\",\nissn = {1477-4054},\ndoi = {10.1093/bib/bbac409},\nurl = {https://doi.org/10.1093/bib/bbac409},\nnote = {bbac409},\neprint = {https://academic.oup.com/bib/article-pdf/23/6/bbac409/47144271/bbac409.pdf},\n}",
    "lllyasviel/sd-controlnet-canny": "Controlnet - Canny Version\nModel Details\nIntroduction\nReleased Checkpoints\nExample\nTraining\nBlog post\nControlnet - Canny Version\nControlNet is a neural network structure to control diffusion models by adding extra conditions.\nThis checkpoint corresponds to the ControlNet conditioned on Canny edges.\nIt can be used in combination with Stable Diffusion.\nModel Details\nDeveloped by: Lvmin Zhang, Maneesh Agrawala\nModel type: Diffusion-based text-to-image generation model\nLanguage(s): English\nLicense: The CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based.\nResources for more information: GitHub Repository, Paper.\nCite as:\n@misc{zhang2023adding,\ntitle={Adding Conditional Control to Text-to-Image Diffusion Models},\nauthor={Lvmin Zhang and Maneesh Agrawala},\nyear={2023},\neprint={2302.05543},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}\nIntroduction\nControlnet was proposed in Adding Conditional Control to Text-to-Image Diffusion Models by\nLvmin Zhang, Maneesh Agrawala.\nThe abstract reads as follows:\nWe present a neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions.\nThe ControlNet learns task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small (< 50k).\nMoreover, training a ControlNet is as fast as fine-tuning a diffusion model, and the model can be trained on a personal devices.\nAlternatively, if powerful computation clusters are available, the model can scale to large amounts (millions to billions) of data.\nWe report that large diffusion models like Stable Diffusion can be augmented with ControlNets to enable conditional inputs like edge maps, segmentation maps, keypoints, etc.\nThis may enrich the methods to control large diffusion models and further facilitate related applications.\nReleased Checkpoints\nThe authors released 8 different checkpoints, each trained with Stable Diffusion v1-5\non a different type of conditioning:\nModel Name\nControl Image Overview\nControl Image Example\nGenerated Image Example\nlllyasviel/sd-controlnet-canny Trained with canny edge detection\nA monochrome image with white edges on a black background.\nlllyasviel/sd-controlnet-depth Trained with Midas depth estimation\nA grayscale image with black representing deep areas and white representing shallow areas.\nlllyasviel/sd-controlnet-hed Trained with HED edge detection (soft edge)\nA monochrome image with white soft edges on a black background.\nlllyasviel/sd-controlnet-mlsd Trained with M-LSD line detection\nA monochrome image composed only of white straight lines on a black background.\nlllyasviel/sd-controlnet-normal Trained with normal map\nA normal mapped image.\nlllyasviel/sd-controlnet_openpose Trained with OpenPose bone image\nA OpenPose bone image.\nlllyasviel/sd-controlnet_scribble Trained with human scribbles\nA hand-drawn monochrome image with white outlines on a black background.\nlllyasviel/sd-controlnet_segTrained with semantic segmentation\nAn ADE20K's segmentation protocol image.\nExample\nIt is recommended to use the checkpoint with Stable Diffusion v1-5 as the checkpoint\nhas been trained on it.\nExperimentally, the checkpoint can be used with other diffusion models such as dreamboothed stable diffusion.\nNote: If you want to process an image to create the auxiliary conditioning, external dependencies are required as shown below:\nInstall opencv\n$ pip install opencv-contrib-python\nLet's install diffusers and related packages:\n$ pip install diffusers transformers accelerate\nRun code:\nimport cv2\nfrom PIL import Image\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nimport torch\nimport numpy as np\nfrom diffusers.utils import load_image\nimage = load_image(\"https://huggingface.co/lllyasviel/sd-controlnet-hed/resolve/main/images/bird.png\")\nimage = np.array(image)\nlow_threshold = 100\nhigh_threshold = 200\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\nimage = Image.fromarray(image)\ncontrolnet = ControlNetModel.from_pretrained(\n\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16\n)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n)\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n# Remove if you do not have xformers installed\n# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers\n# for installation instructions\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_model_cpu_offload()\nimage = pipe(\"bird\", image, num_inference_steps=20).images[0]\nimage.save('images/bird_canny_out.png')\nTraining\nThe canny edge model was trained on 3M edge-image, caption pairs. The model was trained for 600 GPU-hours with Nvidia A100 80G using Stable Diffusion 1.5 as a base model.\nBlog post\nFor more information, please also have a look at the official ControlNet Blog Post.",
    "QuixiAI/WizardLM-7B-Uncensored": "Join our Discord! https://discord.gg/cognitivecomputations\nThis is WizardLM trained with a subset of the dataset - responses that contained alignment / moralizing were removed.  The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.\nShout out to the open source AI/ML community, and everyone who helped me out.\nNote:\nAn uncensored model has no guardrails.\nYou are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.\nPublishing anything this model generates is the same as publishing it yourself.\nYou are responsible for the content you publish, and you cannot blame the model any more than you can blame the knife, gun, lighter, or car for what you do with it.",
    "lxyuan/distilbert-base-multilingual-cased-sentiments-student": "distilbert-base-multilingual-cased-sentiments-student\nInference example\nTraining procedure\nTraining hyperparameters\nTraining log\nFramework versions\ndistilbert-base-multilingual-cased-sentiments-student\nThis model is distilled from the zero-shot classification pipeline on the Multilingual Sentiment\ndataset using this script.\nIn reality the multilingual-sentiment dataset is annotated of course,\nbut we'll pretend and ignore the annotations for the sake of example.\nTeacher model: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\nTeacher hypothesis template: \"The sentiment of this text is {}.\"\nStudent model: distilbert-base-multilingual-cased\nInference example\nfrom transformers import pipeline\ndistilled_student_sentiment_classifier = pipeline(\nmodel=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\nreturn_all_scores=True\n)\n# english\ndistilled_student_sentiment_classifier (\"I love this movie and i would watch it again and again!\")\n>> [[{'label': 'positive', 'score': 0.9731044769287109},\n{'label': 'neutral', 'score': 0.016910076141357422},\n{'label': 'negative', 'score': 0.009985478594899178}]]\n# malay\ndistilled_student_sentiment_classifier(\"Saya suka filem ini dan saya akan menontonnya lagi dan lagi!\")\n[[{'label': 'positive', 'score': 0.9760093688964844},\n{'label': 'neutral', 'score': 0.01804516464471817},\n{'label': 'negative', 'score': 0.005945465061813593}]]\n# japanese\ndistilled_student_sentiment_classifier(\"ÁßÅ„ÅØ„Åì„ÅÆÊò†Áîª„ÅåÂ§ßÂ•Ω„Åç„Åß„ÄÅ‰ΩïÂ∫¶„ÇÇË¶ã„Åæ„ÅôÔºÅ\")\n>> [[{'label': 'positive', 'score': 0.9342429041862488},\n{'label': 'neutral', 'score': 0.040193185210227966},\n{'label': 'negative', 'score': 0.025563929229974747}]]\nTraining procedure\nNotebook link: here\nTraining hyperparameters\nResult can be reproduce using the following commands:\npython transformers/examples/research_projects/zero-shot-distillation/distill_classifier.py \\\n--data_file ./multilingual-sentiments/train_unlabeled.txt \\\n--class_names_file ./multilingual-sentiments/class_names.txt \\\n--hypothesis_template \"The sentiment of this text is {}.\" \\\n--teacher_name_or_path MoritzLaurer/mDeBERTa-v3-base-mnli-xnli \\\n--teacher_batch_size 32 \\\n--student_name_or_path distilbert-base-multilingual-cased \\\n--output_dir ./distilbert-base-multilingual-cased-sentiments-student \\\n--per_device_train_batch_size 16 \\\n--fp16\nIf you are training this model on Colab, make the following code changes to avoid Out-of-memory error message:\n###### modify L78 to disable fast tokenizer\ndefault=False,\n###### update dataset map part at L313\ndataset = dataset.map(tokenizer, input_columns=\"text\", fn_kwargs={\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 512})\n###### add following lines to L213\ndel model\nprint(f\"Manually deleted Teacher model, free some memory for student model.\")\n###### add following lines to L337\ntrainer.push_to_hub()\ntokenizer.push_to_hub(\"distilbert-base-multilingual-cased-sentiments-student\")\nTraining log\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n{'train_runtime': 2009.8864, 'train_samples_per_second': 73.0, 'train_steps_per_second': 4.563, 'train_loss': 0.6473459283913797, 'epoch': 1.0}\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9171/9171 [33:29<00:00,  4.56it/s]\n[INFO|trainer.py:762] 2023-05-06 10:56:18,555 >> The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n[INFO|trainer.py:3129] 2023-05-06 10:56:18,557 >> ***** Running Evaluation *****\n[INFO|trainer.py:3131] 2023-05-06 10:56:18,557 >>   Num examples = 146721\n[INFO|trainer.py:3134] 2023-05-06 10:56:18,557 >>   Batch size = 128\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1147/1147 [08:59<00:00,  2.13it/s]\n05/06/2023 11:05:18 - INFO - __main__ - Agreement of student and teacher predictions: 88.29%\n[INFO|trainer.py:2868] 2023-05-06 11:05:18,251 >> Saving model checkpoint to ./distilbert-base-multilingual-cased-sentiments-student\n[INFO|configuration_utils.py:457] 2023-05-06 11:05:18,251 >> Configuration saved in ./distilbert-base-multilingual-cased-sentiments-student/config.json\n[INFO|modeling_utils.py:1847] 2023-05-06 11:05:18,905 >> Model weights saved in ./distilbert-base-multilingual-cased-sentiments-student/pytorch_model.bin\n[INFO|tokenization_utils_base.py:2171] 2023-05-06 11:05:18,905 >> tokenizer config file saved in ./distilbert-base-multilingual-cased-sentiments-student/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2178] 2023-05-06 11:05:18,905 >> Special tokens file saved in ./distilbert-base-multilingual-cased-sentiments-student/special_tokens_map.json\nFramework versions\nTransformers 4.28.1\nPytorch 2.0.0+cu118\nDatasets 2.11.0\nTokenizers 0.13.3",
    "colbert-ir/colbertv2.0": "ColBERT (v2)\nüö® Announcements\nColBERTv1\nInstallation\nOverview\nAPI Usage Notebook\nData\nIndexing\nRetrieval\nTraining\nRunning a lightweight ColBERTv2 server\nBranches\nSupported branches\nDeprecated branches\nAcknowledgments\nColBERT (v2)\nColBERT is a fast and accurate retrieval model, enabling scalable BERT-based search over large text collections in tens of milliseconds.\nFigure 1: ColBERT's late interaction, efficiently scoring the fine-grained similarity between a queries and a passage.\nAs Figure 1 illustrates, ColBERT relies on fine-grained contextual late interaction: it encodes each passage into a matrix of token-level embeddings (shown above in blue). Then at search time, it embeds every query into another matrix (shown in green) and efficiently finds passages that contextually match the query using scalable vector-similarity (MaxSim) operators.\nThese rich interactions allow ColBERT to surpass the quality of single-vector representation models, while scaling efficiently to large corpora. You can read more in our papers:\nColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT (SIGIR'20).\nRelevance-guided Supervision for OpenQA with ColBERT (TACL'21).\nBaleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval (NeurIPS'21).\nColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction (NAACL'22).\nPLAID: An Efficient Engine for Late Interaction Retrieval (CIKM'22).\nüö® Announcements\n(1/29/23) We have merged a new index updater feature and support for additional Hugging Face models! These are in beta so please give us feedback as you try them out.\n(1/24/23) If you're looking for the DSP framework for composing ColBERTv2 and LLMs, it's at: https://github.com/stanfordnlp/dsp\nColBERTv1\nThe ColBERTv1 code from the SIGIR'20 paper is in the colbertv1 branch. See here for more information on other branches.\nInstallation\nColBERT requires Python 3.7+ and Pytorch 1.9+ and uses the Hugging Face Transformers library.\nWe strongly recommend creating a conda environment using the commands below. (If you don't have conda, follow the official conda installation guide.)\nWe have also included a new environment file specifically for CPU-only environments (conda_env_cpu.yml), but note that if you are testing CPU execution on a machine that includes GPUs you might need to specify CUDA_VISIBLE_DEVICES=\"\" as part of your command. Note that a GPU is required for training and indexing.\nconda env create -f conda_env[_cpu].yml\nconda activate colbert\nIf you face any problems, please open a new issue and we'll help you promptly!\nOverview\nUsing ColBERT on a dataset typically involves the following steps.\nStep 0: Preprocess your collection. At its simplest, ColBERT works with tab-separated (TSV) files: a file (e.g., collection.tsv) will contain all passages and another (e.g., queries.tsv) will contain a set of queries for searching the collection.\nStep 1: Download the pre-trained ColBERTv2 checkpoint. This checkpoint has been trained on the MS MARCO Passage Ranking task. You can also optionally train your own ColBERT model.\nStep 2: Index your collection. Once you have a trained ColBERT model, you need to index your collection to permit fast retrieval. This step encodes all passages into matrices, stores them on disk, and builds data structures for efficient search.\nStep 3: Search the collection with your queries. Given the model and index, you can issue queries over the collection to retrieve the top-k passages for each query.\nBelow, we illustrate these steps via an example run on the MS MARCO Passage Ranking task.\nAPI Usage Notebook\nNEW: We have an experimental notebook on Google Colab that you can use with free GPUs. Indexing 10,000 on the free Colab T4 GPU takes six minutes.\nThis Jupyter notebook docs/intro.ipynb notebook illustrates using the key features of ColBERT with the new Python API.\nIt includes how to download the ColBERTv2 model checkpoint trained on MS MARCO Passage Ranking and how to download our new LoTTE benchmark.\nData\nThis repository works directly with a simple tab-separated file format to store queries, passages, and top-k ranked lists.\nQueries: each line is qid \\t query text.\nCollection: each line is pid \\t passage text.\nTop-k Ranking: each line is qid \\t pid \\t rank.\nThis works directly with the data format of the MS MARCO Passage Ranking dataset. You will need the training triples (triples.train.small.tar.gz), the official top-1000 ranked lists for the dev set queries (top1000.dev), and the dev set relevant passages (qrels.dev.small.tsv). For indexing the full collection, you will also need the list of passages (collection.tar.gz).\nIndexing\nFor fast retrieval, indexing precomputes the ColBERT representations of passages.\nExample usage:\nfrom colbert.infra import Run, RunConfig, ColBERTConfig\nfrom colbert import Indexer\nif __name__=='__main__':\nwith Run().context(RunConfig(nranks=1, experiment=\"msmarco\")):\nconfig = ColBERTConfig(\nnbits=2,\nroot=\"/path/to/experiments\",\n)\nindexer = Indexer(checkpoint=\"/path/to/checkpoint\", config=config)\nindexer.index(name=\"msmarco.nbits=2\", collection=\"/path/to/MSMARCO/collection.tsv\")\nRetrieval\nWe typically recommend that you use ColBERT for end-to-end retrieval, where it directly finds its top-k passages from the full collection:\nfrom colbert.data import Queries\nfrom colbert.infra import Run, RunConfig, ColBERTConfig\nfrom colbert import Searcher\nif __name__=='__main__':\nwith Run().context(RunConfig(nranks=1, experiment=\"msmarco\")):\nconfig = ColBERTConfig(\nroot=\"/path/to/experiments\",\n)\nsearcher = Searcher(index=\"msmarco.nbits=2\", config=config)\nqueries = Queries(\"/path/to/MSMARCO/queries.dev.small.tsv\")\nranking = searcher.search_all(queries, k=100)\nranking.save(\"msmarco.nbits=2.ranking.tsv\")\nYou can optionally specify the ncells, centroid_score_threshold, and ndocs search hyperparameters to trade off between speed and result quality. Defaults for different values of k are listed in colbert/searcher.py.\nWe can evaluate the MSMARCO rankings using the following command:\npython -m utility.evaluate.msmarco_passages --ranking \"/path/to/msmarco.nbits=2.ranking.tsv\" --qrels \"/path/to/MSMARCO/qrels.dev.small.tsv\"\nTraining\nWe provide a pre-trained model checkpoint, but we also detail how to train from scratch here.\nNote that this example demonstrates the ColBERTv1 style of training, but the provided checkpoint was trained with ColBERTv2.\nTraining requires a JSONL triples file with a [qid, pid+, pid-] list per line. The query IDs and passage IDs correspond to the specified queries.tsv and collection.tsv files respectively.\nExample usage (training on 4 GPUs):\nfrom colbert.infra import Run, RunConfig, ColBERTConfig\nfrom colbert import Trainer\nif __name__=='__main__':\nwith Run().context(RunConfig(nranks=4, experiment=\"msmarco\")):\nconfig = ColBERTConfig(\nbsize=32,\nroot=\"/path/to/experiments\",\n)\ntrainer = Trainer(\ntriples=\"/path/to/MSMARCO/triples.train.small.tsv\",\nqueries=\"/path/to/MSMARCO/queries.train.small.tsv\",\ncollection=\"/path/to/MSMARCO/collection.tsv\",\nconfig=config,\n)\ncheckpoint_path = trainer.train()\nprint(f\"Saved checkpoint to {checkpoint_path}...\")\nRunning a lightweight ColBERTv2 server\nWe provide a script to run a lightweight server which serves k (upto 100) results in ranked order for a given search query, in JSON format. This script can be used to power DSP programs.\nTo run the server, update the environment variables INDEX_ROOT and INDEX_NAME in the .env file to point to the appropriate ColBERT index. The run the following command:\npython server.py\nA sample query:\nhttp://localhost:8893/api/search?query=Who won the 2022 FIFA world cup&k=25\nBranches\nSupported branches\nmain: Stable branch with ColBERTv2 + PLAID.\ncolbertv1: Legacy branch for ColBERTv1.\nDeprecated branches\nnew_api: Base ColBERTv2 implementation.\ncpu_inference: ColBERTv2 implementation with CPU search support.\nfast_search: ColBERTv2 implementation with PLAID.\nbinarization: ColBERT with a baseline binarization-based compression strategy (as opposed to ColBERTv2's residual compression, which we found to be more robust).\nAcknowledgments\nColBERT logo designed by Chuyi Zhang.",
    "facebook/dinov2-base": "Vision Transformer (base-sized model) trained using DINOv2\nModel description\nIntended uses & limitations\nHow to use\nBibTeX entry and citation info\nVision Transformer (base-sized model) trained using DINOv2\nVision Transformer (ViT) model trained using the DINOv2 method. It was introduced in the paper DINOv2: Learning Robust Visual Features without Supervision by Oquab et al. and first released in this repository.\nDisclaimer: The team releasing DINOv2 did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a self-supervised fashion.\nImages are presented to the model as a sequence of fixed-size patches, which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\nNote that this model does not include any fine-tuned heads.\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\nIntended uses & limitations\nYou can use the raw model for feature extraction. See the model hub to look for\nfine-tuned versions on a task that interests you.\nHow to use\nHere is how to use this model:\nfrom transformers import AutoImageProcessor, AutoModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\nmodel = AutoModel.from_pretrained('facebook/dinov2-base')\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\nBibTeX entry and citation info\nmisc{oquab2023dinov2,\ntitle={DINOv2: Learning Robust Visual Features without Supervision},\nauthor={Maxime Oquab and Timoth√©e Darcet and Th√©o Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herv√© Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},\nyear={2023},\neprint={2304.07193},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "lmsys/vicuna-7b-v1.5": "Vicuna Model Card\nModel Details\nModel Sources\nUses\nHow to Get Started with the Model\nTraining Details\nEvaluation\nDifference between different versions of Vicuna\nVicuna Model Card\nModel Details\nVicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT.\nDeveloped by: LMSYS\nModel type: An auto-regressive language model based on the transformer architecture\nLicense: Llama 2 Community License Agreement\nFinetuned from model: Llama 2\nModel Sources\nRepository: https://github.com/lm-sys/FastChat\nBlog: https://lmsys.org/blog/2023-03-30-vicuna/\nPaper: https://arxiv.org/abs/2306.05685\nDemo: https://chat.lmsys.org/\nUses\nThe primary use of Vicuna is research on large language models and chatbots.\nThe primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.\nHow to Get Started with the Model\nCommand line interface: https://github.com/lm-sys/FastChat#vicuna-weights\nAPIs (OpenAI API, Huggingface API): https://github.com/lm-sys/FastChat/tree/main#api\nTraining Details\nVicuna v1.5 is fine-tuned from Llama 2 with supervised instruction fine-tuning.\nThe training data is around 125K conversations collected from ShareGPT.com.\nSee more details in the \"Training Details of Vicuna Models\" section in the appendix of this paper.\nEvaluation\nVicuna is evaluated with standard benchmarks, human preference, and LLM-as-a-judge. See more details in this paper and leaderboard.\nDifference between different versions of Vicuna\nSee vicuna_weights_version.md",
    "BAAI/bge-large-zh-v1.5": "News\nModel List\nFrequently asked questions\nUsage\nUsage for Embedding Model\nUsing FlagEmbedding\nUsing Sentence-Transformers\nUsing Langchain\nUsing HuggingFace Transformers\nUsage for Reranker\nUsing FlagEmbedding\nUsing Huggingface transformers\nEvaluation\nTrain\nBAAI Embedding\nBGE Reranker\nContact\nCitation\nLicense\nFlagEmbedding\nModel List |\nFAQ |\nUsage  |\nEvaluation |\nTrain |\nContact |\nCitation |\nLicense\nFor more details please refer to our Github: FlagEmbedding.\nIf you are looking for a model that supports more languages, longer texts, and other retrieval methods, you can try using bge-m3.\nEnglish | ‰∏≠Êñá\nFlagEmbedding focuses on retrieval-augmented LLMs, consisting of the following projects currently:\nLong-Context LLM: Activation Beacon\nFine-tuning of LM : LM-Cocktail\nDense Retrieval: BGE-M3, LLM Embedder, BGE Embedding\nReranker Model: BGE Reranker\nBenchmark: C-MTEB\nNews\n1/30/2024: Release BGE-M3, a new member to BGE model series! M3 stands for Multi-linguality (100+ languages), Multi-granularities (input length up to 8192), Multi-Functionality (unification of dense, lexical, multi-vec/colbert retrieval).\nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\nTechnical Report and Code. :fire:\n1/9/2024: Release Activation-Beacon, an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. Technical Report :fire:\n12/24/2023: Release LLaRA, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. Technical Report :fire:\n11/23/2023: Release LM-Cocktail, a method to maintain general capabilities during fine-tuning by merging multiple language models. Technical Report :fire:\n10/12/2023: Release LLM-Embedder, a unified embedding model to support diverse retrieval augmentation needs for LLMs. Technical Report\n09/15/2023: The technical report and massive training data of BGE has been released\n09/12/2023: New models:\nNew reranker model: release cross-encoder models BAAI/bge-reranker-base and BAAI/bge-reranker-large, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models.\nupdate embedding model: release bge-*-v1.5 embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\nMore\n09/07/2023: Update fine-tune code: Add script to mine hard negatives and support adding instruction during fine-tuning.\n08/09/2023: BGE Models are integrated into Langchain, you can use it like this; C-MTEB leaderboard is available.\n08/05/2023: Release base-scale and small-scale models, best performance among the models of the same size ü§ó\n08/02/2023: Release bge-large-*(short for BAAI General Embedding) Models, rank 1st on MTEB and C-MTEB benchmark! :tada: :tada:\n08/01/2023: We release the Chinese Massive Text Embedding Benchmark (C-MTEB), consisting of 31 test dataset.\nModel List\nbge is short for BAAI general embedding.\nModel\nLanguage\nDescription\nquery instruction for retrieval [1]\nBAAI/bge-m3\nMultilingual\nInference Fine-tune\nMulti-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens)\nBAAI/llm-embedder\nEnglish\nInference Fine-tune\na unified embedding model to support diverse retrieval augmentation needs for LLMs\nSee README\nBAAI/bge-reranker-large\nChinese and English\nInference Fine-tune\na cross-encoder model which is more accurate but less efficient [2]\nBAAI/bge-reranker-base\nChinese and English\nInference Fine-tune\na cross-encoder model which is more accurate but less efficient [2]\nBAAI/bge-large-en-v1.5\nEnglish\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\nRepresent this sentence for searching relevant passages:\nBAAI/bge-base-en-v1.5\nEnglish\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\nRepresent this sentence for searching relevant passages:\nBAAI/bge-small-en-v1.5\nEnglish\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\nRepresent this sentence for searching relevant passages:\nBAAI/bge-large-zh-v1.5\nChinese\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\nBAAI/bge-base-zh-v1.5\nChinese\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\nBAAI/bge-small-zh-v1.5\nChinese\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\nBAAI/bge-large-en\nEnglish\nInference Fine-tune\n:trophy: rank 1st in MTEB leaderboard\nRepresent this sentence for searching relevant passages:\nBAAI/bge-base-en\nEnglish\nInference Fine-tune\na base-scale model but with similar ability to bge-large-en\nRepresent this sentence for searching relevant passages:\nBAAI/bge-small-en\nEnglish\nInference Fine-tune\na small-scale model but with competitive performance\nRepresent this sentence for searching relevant passages:\nBAAI/bge-large-zh\nChinese\nInference Fine-tune\n:trophy: rank 1st in C-MTEB benchmark\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\nBAAI/bge-base-zh\nChinese\nInference Fine-tune\na base-scale model but with similar ability to bge-large-zh\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\nBAAI/bge-small-zh\nChinese\nInference Fine-tune\na small-scale model but with competitive performance\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\n[1]: If you need to search the relevant passages to a query, we suggest to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, no instruction needs to be added to passages.\n[2]: Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. To balance the accuracy and time cost, cross-encoder is widely used to re-rank top-k documents retrieved by other simple models.\nFor examples, use bge embedding model to retrieve top 100 relevant documents, and then use bge reranker to re-rank the top 100 document to get the final top-3 results.\nAll models have been uploaded to Huggingface Hub, and you can see them at https://huggingface.co/BAAI.\nIf you cannot open the Huggingface Hub, you also can download the models at https://model.baai.ac.cn/models .\nFrequently asked questions\n1. How to fine-tune bge embedding model?\nFollowing this example to prepare data and fine-tune your model.\nSome suggestions:\nMine hard negatives following this example, which can improve the retrieval performance.\nIf you pre-train bge on your data, the pre-trained model cannot be directly used to calculate similarity, and it must be fine-tuned with contrastive learning before computing similarity.\nIf the accuracy of the fine-tuned model is still not high, it is recommended to use/fine-tune the cross-encoder model (bge-reranker) to re-rank top-k results. Hard negatives also are needed to fine-tune reranker.\n2. The similarity score between two dissimilar sentences is higher than 0.5\nSuggest to use bge v1.5, which alleviates the issue of the similarity distribution.\nSince we finetune the models by contrastive learning with a temperature of 0.01,\nthe similarity distribution of the current BGE model is about in the interval [0.6, 1].\nSo a similarity score greater than 0.5 does not indicate that the two sentences are similar.\nFor downstream tasks, such as passage retrieval or semantic similarity,\nwhat matters is the relative order of the scores, not the absolute value.\nIf you need to filter similar sentences based on a similarity threshold,\nplease select an appropriate similarity threshold based on the similarity distribution on your data (such as 0.8, 0.85, or even 0.9).\n3. When does the query instruction need to be used\nFor the bge-*-v1.5, we improve its retrieval ability when not using instruction.\nNo instruction only has a slight degradation in retrieval performance compared with using instruction.\nSo you can generate embedding without instruction in all cases for convenience.\nFor a retrieval task that uses short queries to find long related documents,\nit is recommended to add instructions for these short queries.\nThe best method to decide whether to add instructions for queries is choosing the setting that achieves better performance on your task.\nIn all cases, the documents/passages do not need to add the instruction.\nUsage\nUsage for Embedding Model\nHere are some examples for using bge models with\nFlagEmbedding, Sentence-Transformers, Langchain, or Huggingface Transformers.\nUsing FlagEmbedding\npip install -U FlagEmbedding\nIf it doesn't work for you, you can see FlagEmbedding for more methods to install FlagEmbedding.\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"Ê†∑‰æãÊï∞ÊçÆ-1\", \"Ê†∑‰æãÊï∞ÊçÆ-2\"]\nsentences_2 = [\"Ê†∑‰æãÊï∞ÊçÆ-3\", \"Ê†∑‰æãÊï∞ÊçÆ-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5',\nquery_instruction_for_retrieval=\"‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\",\nuse_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\nqueries = ['query_1', 'query_2']\npassages = [\"Ê†∑‰æãÊñáÊ°£-1\", \"Ê†∑‰æãÊñáÊ°£-2\"]\nq_embeddings = model.encode_queries(queries)\np_embeddings = model.encode(passages)\nscores = q_embeddings @ p_embeddings.T\nFor the value of the argument query_instruction_for_retrieval, see Model List.\nBy default, FlagModel will use all available GPUs when encoding. Please set os.environ[\"CUDA_VISIBLE_DEVICES\"] to select specific GPUs.\nYou also can set os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\" to make all GPUs unavailable.\nUsing Sentence-Transformers\nYou can also use the bge models with sentence-transformers:\npip install -U sentence-transformers\nfrom sentence_transformers import SentenceTransformer\nsentences_1 = [\"Ê†∑‰æãÊï∞ÊçÆ-1\", \"Ê†∑‰æãÊï∞ÊçÆ-2\"]\nsentences_2 = [\"Ê†∑‰æãÊï∞ÊçÆ-3\", \"Ê†∑‰æãÊï∞ÊçÆ-4\"]\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nembeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\nembeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\nFor s2p(short query to long passage) retrieval task,\neach short query should start with an instruction (instructions see Model List).\nBut the instruction is not needed for passages.\nfrom sentence_transformers import SentenceTransformer\nqueries = ['query_1', 'query_2']\npassages = [\"Ê†∑‰æãÊñáÊ°£-1\", \"Ê†∑‰æãÊñáÊ°£-2\"]\ninstruction = \"‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\"\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nq_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)\np_embeddings = model.encode(passages, normalize_embeddings=True)\nscores = q_embeddings @ p_embeddings.T\nUsing Langchain\nYou can use bge in langchain like this:\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\nmodel = HuggingFaceBgeEmbeddings(\nmodel_name=model_name,\nmodel_kwargs=model_kwargs,\nencode_kwargs=encode_kwargs,\nquery_instruction=\"‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\"\n)\nmodel.query_instruction = \"‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\"\nUsing HuggingFace Transformers\nWith the transformers package, you can use the model like this: First, you pass your input through the transformer model, then you select the last hidden state of the first token (i.e., [CLS]) as the sentence embedding.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Sentences we want sentence embeddings for\nsentences = [\"Ê†∑‰æãÊï∞ÊçÆ-1\", \"Ê†∑‰æãÊï∞ÊçÆ-2\"]\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel.eval()\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, cls pooling.\nsentence_embeddings = model_output[0][:, 0]\n# normalize embeddings\nsentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\", sentence_embeddings)\nUsage for Reranker\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding.\nYou can get a relevance score by inputting query and passage to the reranker.\nThe reranker is optimized based cross-entropy loss, so the relevance score is not bounded to a specific range.\nUsing FlagEmbedding\npip install -U FlagEmbedding\nGet relevance scores (higher scores indicate more relevance):\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\nUsing Huggingface transformers\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\nmodel.eval()\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\ninputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\nscores = model(**inputs, return_dict=True).logits.view(-1, ).float()\nprint(scores)\nEvaluation\nbaai-general-embedding models achieve state-of-the-art performance on both MTEB and C-MTEB leaderboard!\nFor more details and evaluation tools see our scripts.\nMTEB:\nModel Name\nDimension\nSequence Length\nAverage (56)\nRetrieval (15)\nClustering (11)\nPair Classification (3)\nReranking (4)\nSTS (10)\nSummarization (1)\nClassification (12)\nBAAI/bge-large-en-v1.5\n1024\n512\n64.23\n54.29\n46.08\n87.12\n60.03\n83.11\n31.61\n75.97\nBAAI/bge-base-en-v1.5\n768\n512\n63.55\n53.25\n45.77\n86.55\n58.86\n82.4\n31.07\n75.53\nBAAI/bge-small-en-v1.5\n384\n512\n62.17\n51.68\n43.82\n84.92\n58.36\n81.59\n30.12\n74.14\nbge-large-en\n1024\n512\n63.98\n53.9\n46.98\n85.8\n59.48\n81.56\n32.06\n76.21\nbge-base-en\n768\n512\n63.36\n53.0\n46.32\n85.86\n58.7\n81.84\n29.27\n75.27\ngte-large\n1024\n512\n63.13\n52.22\n46.84\n85.00\n59.13\n83.35\n31.66\n73.33\ngte-base\n768\n512\n62.39\n51.14\n46.2\n84.57\n58.61\n82.3\n31.17\n73.01\ne5-large-v2\n1024\n512\n62.25\n50.56\n44.49\n86.03\n56.61\n82.05\n30.19\n75.24\nbge-small-en\n384\n512\n62.11\n51.82\n44.31\n83.78\n57.97\n80.72\n30.53\n74.37\ninstructor-xl\n768\n512\n61.79\n49.26\n44.74\n86.62\n57.29\n83.06\n32.32\n61.79\ne5-base-v2\n768\n512\n61.5\n50.29\n43.80\n85.73\n55.91\n81.05\n30.28\n73.84\ngte-small\n384\n512\n61.36\n49.46\n44.89\n83.54\n57.7\n82.07\n30.42\n72.31\ntext-embedding-ada-002\n1536\n8192\n60.99\n49.25\n45.9\n84.89\n56.32\n80.97\n30.8\n70.93\ne5-small-v2\n384\n512\n59.93\n49.04\n39.92\n84.67\n54.32\n80.39\n31.16\n72.94\nsentence-t5-xxl\n768\n512\n59.51\n42.24\n43.72\n85.06\n56.42\n82.63\n30.08\n73.42\nall-mpnet-base-v2\n768\n514\n57.78\n43.81\n43.69\n83.04\n59.36\n80.28\n27.49\n65.07\nsgpt-bloom-7b1-msmarco\n4096\n2048\n57.59\n48.22\n38.93\n81.9\n55.65\n77.74\n33.6\n66.19\nC-MTEB:We create the benchmark C-MTEB for Chinese text embedding which consists of 31 datasets from 6 tasks.\nPlease refer to C_MTEB for a detailed introduction.\nModel\nEmbedding dimension\nAvg\nRetrieval\nSTS\nPairClassification\nClassification\nReranking\nClustering\nBAAI/bge-large-zh-v1.5\n1024\n64.53\n70.46\n56.25\n81.6\n69.13\n65.84\n48.99\nBAAI/bge-base-zh-v1.5\n768\n63.13\n69.49\n53.72\n79.75\n68.07\n65.39\n47.53\nBAAI/bge-small-zh-v1.5\n512\n57.82\n61.77\n49.11\n70.41\n63.96\n60.92\n44.18\nBAAI/bge-large-zh\n1024\n64.20\n71.53\n54.98\n78.94\n68.32\n65.11\n48.39\nbge-large-zh-noinstruct\n1024\n63.53\n70.55\n53\n76.77\n68.58\n64.91\n50.01\nBAAI/bge-base-zh\n768\n62.96\n69.53\n54.12\n77.5\n67.07\n64.91\n47.63\nmultilingual-e5-large\n1024\n58.79\n63.66\n48.44\n69.89\n67.34\n56.00\n48.23\nBAAI/bge-small-zh\n512\n58.27\n63.07\n49.45\n70.35\n63.64\n61.48\n45.09\nm3e-base\n768\n57.10\n56.91\n50.47\n63.99\n67.52\n59.34\n47.68\nm3e-large\n1024\n57.05\n54.75\n50.42\n64.3\n68.2\n59.66\n48.88\nmultilingual-e5-base\n768\n55.48\n61.63\n46.49\n67.07\n65.35\n54.35\n40.68\nmultilingual-e5-small\n384\n55.38\n59.95\n45.27\n66.45\n65.85\n53.86\n45.26\ntext-embedding-ada-002(OpenAI)\n1536\n53.02\n52.0\n43.35\n69.56\n64.31\n54.28\n45.68\nluotuo\n1024\n49.37\n44.4\n42.78\n66.62\n61\n49.25\n44.39\ntext2vec-base\n768\n47.63\n38.79\n43.41\n67.41\n62.19\n49.45\n37.66\ntext2vec-large\n1024\n47.36\n41.94\n44.97\n70.86\n60.66\n49.16\n30.02\nReranking:\nSee C_MTEB for evaluation script.\nModel\nT2Reranking\nT2RerankingZh2En*\nT2RerankingEn2Zh*\nMMarcoReranking\nCMedQAv1\nCMedQAv2\nAvg\ntext2vec-base-multilingual\n64.66\n62.94\n62.51\n14.37\n48.46\n48.6\n50.26\nmultilingual-e5-small\n65.62\n60.94\n56.41\n29.91\n67.26\n66.54\n57.78\nmultilingual-e5-large\n64.55\n61.61\n54.28\n28.6\n67.42\n67.92\n57.4\nmultilingual-e5-base\n64.21\n62.13\n54.68\n29.5\n66.23\n66.98\n57.29\nm3e-base\n66.03\n62.74\n56.07\n17.51\n77.05\n76.76\n59.36\nm3e-large\n66.13\n62.72\n56.1\n16.46\n77.76\n78.27\n59.57\nbge-base-zh-v1.5\n66.49\n63.25\n57.02\n29.74\n80.47\n84.88\n63.64\nbge-large-zh-v1.5\n65.74\n63.39\n57.03\n28.74\n83.45\n85.44\n63.97\nBAAI/bge-reranker-base\n67.28\n63.95\n60.45\n35.46\n81.26\n84.1\n65.42\nBAAI/bge-reranker-large\n67.6\n64.03\n61.44\n37.16\n82.15\n84.18\n66.09\n* : T2RerankingZh2En and T2RerankingEn2Zh are cross-language retrieval tasks\nTrain\nBAAI Embedding\nWe pre-train the models using retromae and train them on large-scale pairs data using contrastive learning.\nYou can fine-tune the embedding model on your data following our examples.\nWe also provide a pre-train example.\nNote that the goal of pre-training is to reconstruct the text, and the pre-trained model cannot be used for similarity calculation directly, it needs to be fine-tuned.\nMore training details for bge see baai_general_embedding.\nBGE Reranker\nCross-encoder will perform full-attention over the input pair,\nwhich is more accurate than embedding model (i.e., bi-encoder) but more time-consuming than embedding model.\nTherefore, it can be used to re-rank the top-k documents returned by embedding model.\nWe train the cross-encoder on a multilingual pair data,\nThe data format is the same as embedding model, so you can fine-tune it easily following our example.\nMore details please refer to ./FlagEmbedding/reranker/README.md\nContact\nIf you have any question or suggestion related to this project, feel free to open an issue or pull request.\nYou also can email Shitao Xiao(stxiao@baai.ac.cn) and Zheng Liu(liuzheng@baai.ac.cn).\nCitation\nIf you find this repository useful, please consider giving a star :star: and citation\n@misc{bge_embedding,\ntitle={C-Pack: Packaged Resources To Advance General Chinese Embedding},\nauthor={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\nyear={2023},\neprint={2309.07597},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\nLicense\nFlagEmbedding is licensed under the MIT License. The released models can be used for commercial purposes free of charge.",
    "BAAI/bge-reranker-large": "News\nModel List\nFrequently asked questions\nUsage\nUsage for Embedding Model\nUsing FlagEmbedding\nUsing Sentence-Transformers\nUsing Langchain\nUsing HuggingFace Transformers\nUsage for Reranker\nUsing FlagEmbedding\nUsing Huggingface transformers\nUsage reranker with the ONNX files\nUsage reranker with infinity\nEvaluation\nTrain\nBAAI Embedding\nBGE Reranker\nCitation\nLicense\nWe have updated the new reranker, supporting larger lengths, more languages, and achieving better performance.\nFlagEmbedding\nModel List |\nFAQ |\nUsage  |\nEvaluation |\nTrain |\nCitation |\nLicense\nMore details please refer to our Github: FlagEmbedding.\nEnglish | ‰∏≠Êñá\nFlagEmbedding focuses on retrieval-augmented LLMs, consisting of the following projects currently:\nLong-Context LLM: Activation Beacon\nFine-tuning of LM : LM-Cocktail\nEmbedding Model: Visualized-BGE, BGE-M3, LLM Embedder, BGE Embedding\nReranker Model: llm rerankers, BGE Reranker\nBenchmark: C-MTEB\nNews\n3/18/2024: Release new rerankers, built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation.\n3/18/2024: Release Visualized-BGE, equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data.\n1/30/2024: Release BGE-M3, a new member to BGE model series! M3 stands for Multi-linguality (100+ languages), Multi-granularities (input length up to 8192), Multi-Functionality (unification of dense, lexical, multi-vec/colbert retrieval).\nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\nTechnical Report and Code. :fire:\n1/9/2024: Release Activation-Beacon, an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. Technical Report :fire:\n12/24/2023: Release LLaRA, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. Technical Report\n11/23/2023: Release LM-Cocktail, a method to maintain general capabilities during fine-tuning by merging multiple language models. Technical Report :fire:\n10/12/2023: Release LLM-Embedder, a unified embedding model to support diverse retrieval augmentation needs for LLMs. Technical Report\n09/15/2023: The technical report of BGE has been released\n09/15/2023: The massive training data of BGE has been released\n09/12/2023: New models:\nNew reranker model: release cross-encoder models BAAI/bge-reranker-base and BAAI/bge-reranker-large, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models.\nupdate embedding model: release bge-*-v1.5 embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\nMore\n09/07/2023: Update fine-tune code: Add script to mine hard negatives and support adding instruction during fine-tuning.\n08/09/2023: BGE Models are integrated into Langchain, you can use it like this; C-MTEB leaderboard is available.\n08/05/2023: Release base-scale and small-scale models, best performance among the models of the same size ü§ó\n08/02/2023: Release bge-large-*(short for BAAI General Embedding) Models, rank 1st on MTEB and C-MTEB benchmark! :tada: :tada:\n08/01/2023: We release the Chinese Massive Text Embedding Benchmark (C-MTEB), consisting of 31 test dataset.\nModel List\nbge is short for BAAI general embedding.\nModel\nLanguage\nDescription\nquery instruction for retrieval [1]\nBAAI/bge-m3\nMultilingual\nInference Fine-tune\nMulti-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens)\nBAAI/llm-embedder\nEnglish\nInference Fine-tune\na unified embedding model to support diverse retrieval augmentation needs for LLMs\nSee README\nBAAI/bge-reranker-large\nChinese and English\nInference Fine-tune\na cross-encoder model which is more accurate but less efficient [2]\nBAAI/bge-reranker-base\nChinese and English\nInference Fine-tune\na cross-encoder model which is more accurate but less efficient [2]\nBAAI/bge-large-en-v1.5\nEnglish\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\nRepresent this sentence for searching relevant passages:\nBAAI/bge-base-en-v1.5\nEnglish\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\nRepresent this sentence for searching relevant passages:\nBAAI/bge-small-en-v1.5\nEnglish\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\nRepresent this sentence for searching relevant passages:\nBAAI/bge-large-zh-v1.5\nChinese\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\nBAAI/bge-base-zh-v1.5\nChinese\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\nBAAI/bge-small-zh-v1.5\nChinese\nInference Fine-tune\nversion 1.5 with more reasonable similarity distribution\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\nBAAI/bge-large-en\nEnglish\nInference Fine-tune\n:trophy: rank 1st in MTEB leaderboard\nRepresent this sentence for searching relevant passages:\nBAAI/bge-base-en\nEnglish\nInference Fine-tune\na base-scale model but with similar ability to bge-large-en\nRepresent this sentence for searching relevant passages:\nBAAI/bge-small-en\nEnglish\nInference Fine-tune\na small-scale model but with competitive performance\nRepresent this sentence for searching relevant passages:\nBAAI/bge-large-zh\nChinese\nInference Fine-tune\n:trophy: rank 1st in C-MTEB benchmark\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\nBAAI/bge-base-zh\nChinese\nInference Fine-tune\na base-scale model but with similar ability to bge-large-zh\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\nBAAI/bge-small-zh\nChinese\nInference Fine-tune\na small-scale model but with competitive performance\n‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\n[1]: If you need to search the relevant passages to a query, we suggest to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, no instruction needs to be added to passages.\n[2]: Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. To balance the accuracy and time cost, cross-encoder is widely used to re-rank top-k documents retrieved by other simple models.\nFor examples, use bge embedding model to retrieve top 100 relevant documents, and then use bge reranker to re-rank the top 100 document to get the final top-3 results.\nAll models have been uploaded to Huggingface Hub, and you can see them at https://huggingface.co/BAAI.\nIf you cannot open the Huggingface Hub, you also can download the models at https://model.baai.ac.cn/models .\nFrequently asked questions\n1. How to fine-tune bge embedding model?\nFollowing this example to prepare data and fine-tune your model.\nSome suggestions:\nMine hard negatives following this example, which can improve the retrieval performance.\nIf you pre-train bge on your data, the pre-trained model cannot be directly used to calculate similarity, and it must be fine-tuned with contrastive learning before computing similarity.\nIf the accuracy of the fine-tuned model is still not high, it is recommended to use/fine-tune the cross-encoder model (bge-reranker) to re-rank top-k results.\nHard negatives also are needed to fine-tune reranker. Refer to this example for the fine-tuning for reranker\n2. The similarity score between two dissimilar sentences is higher than 0.5\nSuggest to use bge v1.5, which alleviates the issue of the similarity distribution.\nSince we finetune the models by contrastive learning with a temperature of 0.01,\nthe similarity distribution of the current BGE model is about in the interval [0.6, 1].\nSo a similarity score greater than 0.5 does not indicate that the two sentences are similar.\nFor downstream tasks, such as passage retrieval or semantic similarity,\nwhat matters is the relative order of the scores, not the absolute value.\nIf you need to filter similar sentences based on a similarity threshold,\nplease select an appropriate similarity threshold based on the similarity distribution on your data (such as 0.8, 0.85, or even 0.9).\n3. When does the query instruction need to be used\nFor the bge-*-v1.5, we improve its retrieval ability when not using instruction.\nNo instruction only has a slight degradation in retrieval performance compared with using instruction.\nSo you can generate embedding without instruction in all cases for convenience.\nFor a retrieval task that uses short queries to find long related documents,\nit is recommended to add instructions for these short queries.\nThe best method to decide whether to add instructions for queries is choosing the setting that achieves better performance on your task.\nIn all cases, the documents/passages do not need to add the instruction.\nUsage\nUsage for Embedding Model\nHere are some examples for using bge models with\nFlagEmbedding, Sentence-Transformers, Langchain, or Huggingface Transformers.\nUsing FlagEmbedding\npip install -U FlagEmbedding\nIf it doesn't work for you, you can see FlagEmbedding for more methods to install FlagEmbedding.\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"Ê†∑‰æãÊï∞ÊçÆ-1\", \"Ê†∑‰æãÊï∞ÊçÆ-2\"]\nsentences_2 = [\"Ê†∑‰æãÊï∞ÊçÆ-3\", \"Ê†∑‰æãÊï∞ÊçÆ-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5',\nquery_instruction_for_retrieval=\"‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\",\nuse_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\nqueries = ['query_1', 'query_2']\npassages = [\"Ê†∑‰æãÊñáÊ°£-1\", \"Ê†∑‰æãÊñáÊ°£-2\"]\nq_embeddings = model.encode_queries(queries)\np_embeddings = model.encode(passages)\nscores = q_embeddings @ p_embeddings.T\nFor the value of the argument query_instruction_for_retrieval, see Model List.\nBy default, FlagModel will use all available GPUs when encoding. Please set os.environ[\"CUDA_VISIBLE_DEVICES\"] to select specific GPUs.\nYou also can set os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\" to make all GPUs unavailable.\nUsing Sentence-Transformers\nYou can also use the bge models with sentence-transformers:\npip install -U sentence-transformers\nfrom sentence_transformers import SentenceTransformer\nsentences_1 = [\"Ê†∑‰æãÊï∞ÊçÆ-1\", \"Ê†∑‰æãÊï∞ÊçÆ-2\"]\nsentences_2 = [\"Ê†∑‰æãÊï∞ÊçÆ-3\", \"Ê†∑‰æãÊï∞ÊçÆ-4\"]\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nembeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\nembeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\nFor s2p(short query to long passage) retrieval task,\neach short query should start with an instruction (instructions see Model List).\nBut the instruction is not needed for passages.\nfrom sentence_transformers import SentenceTransformer\nqueries = ['query_1', 'query_2']\npassages = [\"Ê†∑‰æãÊñáÊ°£-1\", \"Ê†∑‰æãÊñáÊ°£-2\"]\ninstruction = \"‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\"\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nq_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)\np_embeddings = model.encode(passages, normalize_embeddings=True)\nscores = q_embeddings @ p_embeddings.T\nUsing Langchain\nYou can use bge in langchain like this:\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\nmodel = HuggingFaceBgeEmbeddings(\nmodel_name=model_name,\nmodel_kwargs=model_kwargs,\nencode_kwargs=encode_kwargs,\nquery_instruction=\"‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\"\n)\nmodel.query_instruction = \"‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö\"\nUsing HuggingFace Transformers\nWith the transformers package, you can use the model like this: First, you pass your input through the transformer model, then you select the last hidden state of the first token (i.e., [CLS]) as the sentence embedding.\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Sentences we want sentence embeddings for\nsentences = [\"Ê†∑‰æãÊï∞ÊçÆ-1\", \"Ê†∑‰æãÊï∞ÊçÆ-2\"]\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel.eval()\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n# Compute token embeddings\nwith torch.no_grad():\nmodel_output = model(**encoded_input)\n# Perform pooling. In this case, cls pooling.\nsentence_embeddings = model_output[0][:, 0]\n# normalize embeddings\nsentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\", sentence_embeddings)\nUsage for Reranker\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding.\nYou can get a relevance score by inputting query and passage to the reranker.\nThe reranker is optimized based cross-entropy loss, so the relevance score is not bounded to a specific range.\nUsing FlagEmbedding\npip install -U FlagEmbedding\nGet relevance scores (higher scores indicate more relevance):\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\nUsing Huggingface transformers\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\nmodel.eval()\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\ninputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\nscores = model(**inputs, return_dict=True).logits.view(-1, ).float()\nprint(scores)\nUsage reranker with the ONNX files\nfrom optimum.onnxruntime import ORTModelForSequenceClassification  # type: ignore\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base')\nmodel_ort = ORTModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base', file_name=\"onnx/model.onnx\")\n# Sentences we want sentence embeddings for\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\n# Tokenize sentences\nencoded_input = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt')\nscores_ort = model_ort(**encoded_input, return_dict=True).logits.view(-1, ).float()\n# Compute token embeddings\nwith torch.inference_mode():\nscores = model_ort(**encoded_input, return_dict=True).logits.view(-1, ).float()\n# scores and scores_ort are identical\nUsage reranker with infinity\nIts also possible to deploy the onnx/torch files with the infinity_emb pip package.\nimport asyncio\nfrom infinity_emb import AsyncEmbeddingEngine, EngineArgs\nquery='what is a panda?'\ndocs = ['The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear', \"Paris is in France.\"]\nengine = AsyncEmbeddingEngine.from_args(\nEngineArgs(model_name_or_path = \"BAAI/bge-reranker-base\", device=\"cpu\", engine=\"torch\" # or engine=\"optimum\" for onnx\n))\nasync def main():\nasync with engine:\nranking, usage = await engine.rerank(query=query, docs=docs)\nprint(list(zip(ranking, docs)))\nasyncio.run(main())\nEvaluation\nbaai-general-embedding models achieve state-of-the-art performance on both MTEB and C-MTEB leaderboard!\nFor more details and evaluation tools see our scripts.\nMTEB:\nModel Name\nDimension\nSequence Length\nAverage (56)\nRetrieval (15)\nClustering (11)\nPair Classification (3)\nReranking (4)\nSTS (10)\nSummarization (1)\nClassification (12)\nBAAI/bge-large-en-v1.5\n1024\n512\n64.23\n54.29\n46.08\n87.12\n60.03\n83.11\n31.61\n75.97\nBAAI/bge-base-en-v1.5\n768\n512\n63.55\n53.25\n45.77\n86.55\n58.86\n82.4\n31.07\n75.53\nBAAI/bge-small-en-v1.5\n384\n512\n62.17\n51.68\n43.82\n84.92\n58.36\n81.59\n30.12\n74.14\nbge-large-en\n1024\n512\n63.98\n53.9\n46.98\n85.8\n59.48\n81.56\n32.06\n76.21\nbge-base-en\n768\n512\n63.36\n53.0\n46.32\n85.86\n58.7\n81.84\n29.27\n75.27\ngte-large\n1024\n512\n63.13\n52.22\n46.84\n85.00\n59.13\n83.35\n31.66\n73.33\ngte-base\n768\n512\n62.39\n51.14\n46.2\n84.57\n58.61\n82.3\n31.17\n73.01\ne5-large-v2\n1024\n512\n62.25\n50.56\n44.49\n86.03\n56.61\n82.05\n30.19\n75.24\nbge-small-en\n384\n512\n62.11\n51.82\n44.31\n83.78\n57.97\n80.72\n30.53\n74.37\ninstructor-xl\n768\n512\n61.79\n49.26\n44.74\n86.62\n57.29\n83.06\n32.32\n61.79\ne5-base-v2\n768\n512\n61.5\n50.29\n43.80\n85.73\n55.91\n81.05\n30.28\n73.84\ngte-small\n384\n512\n61.36\n49.46\n44.89\n83.54\n57.7\n82.07\n30.42\n72.31\ntext-embedding-ada-002\n1536\n8192\n60.99\n49.25\n45.9\n84.89\n56.32\n80.97\n30.8\n70.93\ne5-small-v2\n384\n512\n59.93\n49.04\n39.92\n84.67\n54.32\n80.39\n31.16\n72.94\nsentence-t5-xxl\n768\n512\n59.51\n42.24\n43.72\n85.06\n56.42\n82.63\n30.08\n73.42\nall-mpnet-base-v2\n768\n514\n57.78\n43.81\n43.69\n83.04\n59.36\n80.28\n27.49\n65.07\nsgpt-bloom-7b1-msmarco\n4096\n2048\n57.59\n48.22\n38.93\n81.9\n55.65\n77.74\n33.6\n66.19\nC-MTEB:We create the benchmark C-MTEB for Chinese text embedding which consists of 31 datasets from 6 tasks.\nPlease refer to C_MTEB for a detailed introduction.\nModel\nEmbedding dimension\nAvg\nRetrieval\nSTS\nPairClassification\nClassification\nReranking\nClustering\nBAAI/bge-large-zh-v1.5\n1024\n64.53\n70.46\n56.25\n81.6\n69.13\n65.84\n48.99\nBAAI/bge-base-zh-v1.5\n768\n63.13\n69.49\n53.72\n79.75\n68.07\n65.39\n47.53\nBAAI/bge-small-zh-v1.5\n512\n57.82\n61.77\n49.11\n70.41\n63.96\n60.92\n44.18\nBAAI/bge-large-zh\n1024\n64.20\n71.53\n54.98\n78.94\n68.32\n65.11\n48.39\nbge-large-zh-noinstruct\n1024\n63.53\n70.55\n53\n76.77\n68.58\n64.91\n50.01\nBAAI/bge-base-zh\n768\n62.96\n69.53\n54.12\n77.5\n67.07\n64.91\n47.63\nmultilingual-e5-large\n1024\n58.79\n63.66\n48.44\n69.89\n67.34\n56.00\n48.23\nBAAI/bge-small-zh\n512\n58.27\n63.07\n49.45\n70.35\n63.64\n61.48\n45.09\nm3e-base\n768\n57.10\n56.91\n50.47\n63.99\n67.52\n59.34\n47.68\nm3e-large\n1024\n57.05\n54.75\n50.42\n64.3\n68.2\n59.66\n48.88\nmultilingual-e5-base\n768\n55.48\n61.63\n46.49\n67.07\n65.35\n54.35\n40.68\nmultilingual-e5-small\n384\n55.38\n59.95\n45.27\n66.45\n65.85\n53.86\n45.26\ntext-embedding-ada-002(OpenAI)\n1536\n53.02\n52.0\n43.35\n69.56\n64.31\n54.28\n45.68\nluotuo\n1024\n49.37\n44.4\n42.78\n66.62\n61\n49.25\n44.39\ntext2vec-base\n768\n47.63\n38.79\n43.41\n67.41\n62.19\n49.45\n37.66\ntext2vec-large\n1024\n47.36\n41.94\n44.97\n70.86\n60.66\n49.16\n30.02\nReranking:\nSee C_MTEB for evaluation script.\nModel\nT2Reranking\nT2RerankingZh2En*\nT2RerankingEn2Zh*\nMMarcoReranking\nCMedQAv1\nCMedQAv2\nAvg\ntext2vec-base-multilingual\n64.66\n62.94\n62.51\n14.37\n48.46\n48.6\n50.26\nmultilingual-e5-small\n65.62\n60.94\n56.41\n29.91\n67.26\n66.54\n57.78\nmultilingual-e5-large\n64.55\n61.61\n54.28\n28.6\n67.42\n67.92\n57.4\nmultilingual-e5-base\n64.21\n62.13\n54.68\n29.5\n66.23\n66.98\n57.29\nm3e-base\n66.03\n62.74\n56.07\n17.51\n77.05\n76.76\n59.36\nm3e-large\n66.13\n62.72\n56.1\n16.46\n77.76\n78.27\n59.57\nbge-base-zh-v1.5\n66.49\n63.25\n57.02\n29.74\n80.47\n84.88\n63.64\nbge-large-zh-v1.5\n65.74\n63.39\n57.03\n28.74\n83.45\n85.44\n63.97\nBAAI/bge-reranker-base\n67.28\n63.95\n60.45\n35.46\n81.26\n84.1\n65.42\nBAAI/bge-reranker-large\n67.6\n64.03\n61.44\n37.16\n82.15\n84.18\n66.09\n* : T2RerankingZh2En and T2RerankingEn2Zh are cross-language retrieval tasks\nTrain\nBAAI Embedding\nWe pre-train the models using retromae and train them on large-scale pairs data using contrastive learning.\nYou can fine-tune the embedding model on your data following our examples.\nWe also provide a pre-train example.\nNote that the goal of pre-training is to reconstruct the text, and the pre-trained model cannot be used for similarity calculation directly, it needs to be fine-tuned.\nMore training details for bge see baai_general_embedding.\nBGE Reranker\nCross-encoder will perform full-attention over the input pair,\nwhich is more accurate than embedding model (i.e., bi-encoder) but more time-consuming than embedding model.\nTherefore, it can be used to re-rank the top-k documents returned by embedding model.\nWe train the cross-encoder on a multilingual pair data,\nThe data format is the same as embedding model, so you can fine-tune it easily following our example.\nMore details please refer to ./FlagEmbedding/reranker/README.md\nCitation\nIf you find this repository useful, please consider giving a star :star: and citation\n@misc{bge_embedding,\ntitle={C-Pack: Packaged Resources To Advance General Chinese Embedding},\nauthor={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\nyear={2023},\neprint={2309.07597},\narchivePrefix={arXiv},\nprimaryClass={cs.CL}\n}\nLicense\nFlagEmbedding is licensed under the MIT License. The released models can be used for commercial purposes free of charge.",
    "cis-lmu/glotlid": "GlotLID\nDescription\nHow to use\nLicense\nVersion\nReferences\nGlotLID\nDescription\nGlotLID is a Fasttext language identification (LID) model that supports more than 2000 labels.\nLatest: GlotLID is now updated to V3. V3 supports 2102 labels (three-letter ISO codes with script). For more details on the supported languages and performance, as well as significant changes from previous versions, please refer to https://github.com/cisnlp/GlotLID/blob/main/languages-v3.md.\nDemo: huggingface\nRepository: github\nPaper: paper (EMNLP 2023)\nPoint of Contact: amir@cis.lmu.de\nHow to use\nHere is how to use this model to detect the language of a given text:\n>>> import fasttext\n>>> from huggingface_hub import hf_hub_download\n# model.bin is the latest version always\n>>> model_path = hf_hub_download(repo_id=\"cis-lmu/glotlid\", filename=\"model.bin\")\n>>> model = fasttext.load_model(model_path)\n>>> model.predict(\"Hello, world!\")\nIf you are not a fan of huggingface_hub, then download the model directyly:\n>>> ! wget https://huggingface.co/cis-lmu/glotlid/resolve/main/model.bin\n>>> import fasttext\n>>> model = fasttext.load_model(\"/path/to/model.bin\")\n>>> model.predict(\"Hello, world!\")\nLicense\nThe model is distributed under the Apache License, Version 2.0.\nVersion\nWe always maintain the previous version of GlotLID in our repository.\nTo access a specific version, simply append the version number to the filename.\nFor v1: model_v1.bin (introduced in the GlotLID paper and used in all experiments).\nFor v2: model_v2.bin (an edited version of v1, featuring more languages, and cleaned from noisy corpora based on the analysis of v1).\nFor v3: model_v3.bin (an edited version of v2, featuring more languages, excluding macro languages, further cleaned from noisy corpora and incorrect metadata labels based on the analysis of v2, supporting \"zxx\" and \"und\" series labels)\nmodel.bin always refers to the latest version (v3).\nReferences\nIf you use this model, please cite the following paper:\n@inproceedings{\nkargaran2023glotlid,\ntitle={{GlotLID}: Language Identification for Low-Resource Languages},\nauthor={Kargaran, Amir Hossein and Imani, Ayyoob and Yvon, Fran{\\c{c}}ois and Sch{\\\"u}tze, Hinrich},\nbooktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},\nyear={2023},\nurl={https://openreview.net/forum?id=dl4e3EBz5j}\n}",
    "stabilityai/sdxl-turbo": "SDXL-Turbo Model Card\nModel Details\nModel Description\nModel Sources\nEvaluation\nUses\nDirect Use\nDiffusers\nOut-of-Scope Use\nLimitations and Bias\nLimitations\nRecommendations\nHow to Get Started with the Model\nSDXL-Turbo Model Card\nSDXL-Turbo is a fast generative text-to-image model that can synthesize photorealistic images from a text prompt in a single network evaluation.\nA real-time demo is available here: http://clipdrop.co/stable-diffusion-turbo\nPlease note: For commercial use, please refer to https://stability.ai/license.\nModel Details\nModel Description\nSDXL-Turbo is a distilled version of SDXL 1.0, trained for real-time synthesis.\nSDXL-Turbo is based on a novel training method called Adversarial Diffusion Distillation (ADD) (see the technical report), which allows sampling large-scale foundational\nimage diffusion models in 1 to 4 steps at high image quality.\nThis approach uses score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal and combines this with an\nadversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps.\nDeveloped by: Stability AI\nFunded by: Stability AI\nModel type: Generative text-to-image model\nFinetuned from model: SDXL 1.0 Base\nModel Sources\nFor research purposes, we recommend our generative-models Github repository (https://github.com/Stability-AI/generative-models),\nwhich implements the most popular diffusion frameworks (both training and inference).\nRepository: https://github.com/Stability-AI/generative-models\nPaper: https://stability.ai/research/adversarial-diffusion-distillation\nDemo: http://clipdrop.co/stable-diffusion-turbo\nEvaluation\nThe charts above evaluate user preference for SDXL-Turbo over other single- and multi-step models.\nSDXL-Turbo evaluated at a single step is preferred by human voters in terms of image quality and prompt following over LCM-XL evaluated at four (or fewer) steps.\nIn addition, we see that using four steps for SDXL-Turbo further improves performance.\nFor details on the user study, we refer to the research paper.\nUses\nDirect Use\nThe model is intended for both non-commercial and commercial usage. You can use this model for non-commercial or research purposes under this license. Possible research areas and tasks include\nResearch on generative models.\nResearch on real-time applications of generative models.\nResearch on the impact of real-time generative models.\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nFor commercial use, please refer to https://stability.ai/membership.\nExcluded uses are described below.\nDiffusers\npip install diffusers transformers accelerate --upgrade\nText-to-image:\nSDXL-Turbo does not make use of guidance_scale or negative_prompt, we disable it with guidance_scale=0.0.\nPreferably, the model generates images of size 512x512 but higher image sizes work as well.\nA single step is enough to generate high quality images.\nfrom diffusers import AutoPipelineForText2Image\nimport torch\npipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.to(\"cuda\")\nprompt = \"A cinematic shot of a baby racoon wearing an intricate italian priest robe.\"\nimage = pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\nImage-to-image:\nWhen using SDXL-Turbo for image-to-image generation, make sure that num_inference_steps * strength is larger or equal\nto 1. The image-to-image pipeline will run for int(num_inference_steps * strength) steps, e.g. 0.5 * 2.0 = 1 step in our example\nbelow.\nfrom diffusers import AutoPipelineForImage2Image\nfrom diffusers.utils import load_image\nimport torch\npipe = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipe.to(\"cuda\")\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\").resize((512, 512))\nprompt = \"cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k\"\nimage = pipe(prompt, image=init_image, num_inference_steps=2, strength=0.5, guidance_scale=0.0).images[0]\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events,\nand therefore using the model to generate such content is out-of-scope for the abilities of this model.\nThe model should not be used in any way that violates Stability AI's Acceptable Use Policy.\nLimitations and Bias\nLimitations\nThe generated images are of a fixed resolution (512x512 pix), and the model does not achieve perfect photorealism.\nThe model cannot render legible text.\nFaces and people in general may not be generated properly.\nThe autoencoding part of the model is lossy.\nRecommendations\nThe model is intended for both non-commercial and commercial usage.\nHow to Get Started with the Model\nCheck out https://github.com/Stability-AI/generative-models",
    "deepseek-ai/deepseek-llm-7b-chat": "1. Introduction of Deepseek LLM\n2. Model Summary\n3. How to Use\nChat Completion\n4. License\n5. Contact\n[üè†Homepage]  |  [ü§ñ Chat with DeepSeek LLM]  |  [Discord]  |  [Wechat(ÂæÆ‰ø°)]\n1. Introduction of Deepseek LLM\nIntroducing DeepSeek LLM, an advanced language model comprising 7 billion parameters. It has been trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese. In order to foster research, we have made DeepSeek LLM 7B/67B Base and DeepSeek LLM 7B/67B Chat open source for the research community.\n2. Model Summary\ndeepseek-llm-7b-chat is a 7B parameter model initialized from deepseek-llm-7b-base and fine-tuned on extra instruction data.\nHome Page: DeepSeek\nRepository: deepseek-ai/deepseek-LLM\nChat With DeepSeek LLM: DeepSeek-LLM\n3. How to Use\nHere give some examples of how to use our model.\nChat Completion\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\nmodel_name = \"deepseek-ai/deepseek-llm-7b-chat\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\nmessages = [\n{\"role\": \"user\", \"content\": \"Who are you?\"}\n]\ninput_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\noutputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)\nresult = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\nprint(result)\nAvoiding the use of the provided function apply_chat_template, you can also interact with our model following the sample template. Note that messages should be replaced by your input.\nUser: {messages[0]['content']}\nAssistant: {messages[1]['content']}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>User: {messages[2]['content']}\nAssistant:\nNote: By default (add_special_tokens=True), our tokenizer automatically adds a bos_token (<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>) before the input text. Additionally, since the system prompt is not compatible with this version of our models, we DO NOT RECOMMEND including the system prompt in your input.\n4. License\nThis code repository is licensed under the MIT License. The use of DeepSeek LLM models is subject to the Model License. DeepSeek LLM supports commercial use.\nSee the LICENSE-MODEL for more details.\n5. Contact\nIf you have any questions, please raise an issue or contact us at service@deepseek.com.",
    "facebook/seamless-m4t-v2-large": "SeamlessM4T v2\nSeamlessM4T  models\nEvaluating SeamlessM4T models\nFinetuning SeamlessM4T models\nTransformers usage\nSupported Languages:\nCitation\nSeamlessM4T v2\nSeamlessM4T is our foundational all-in-one Massively Multilingual and Multimodal Machine Translation model delivering high-quality translation for speech and text in nearly 100 languages.\nSeamlessM4T models support the tasks of:\nSpeech-to-speech translation (S2ST)\nSpeech-to-text translation (S2TT)\nText-to-speech translation (T2ST)\nText-to-text translation (T2TT)\nAutomatic speech recognition (ASR).\nSeamlessM4T models support:\nüé§ 101 languages for speech input.\nüí¨ 96 Languages for text input/output.\nüîä 35 languages for speech output.\nüåü We are releasing SeamlessM4T v2, an updated version with our novel UnitY2 architecture.\nThis new model improves over SeamlessM4T v1 in quality as well as inference speed in speech generation tasks.\nThe v2 version of SeamlessM4T is a multitask adaptation of our novel UnitY2 architecture.\nUnity2 with its hierarchical character-to-unit upsampling and non-autoregressive text-to-unit decoding considerably improves over SeamlessM4T v1 in quality and inference speed.\nSeamlessM4T v2 is also supported by ü§ó Transformers, more on it in the dedicated section below.\nSeamlessM4T  models\nModel Name\n#params\ncheckpoint\nmetrics\nSeamlessM4T-Large v2\n2.3B\ncheckpoint\nmetrics\nSeamlessM4T-Large (v1)\n2.3B\ncheckpoint\nmetrics\nSeamlessM4T-Medium (v1)\n1.2B\ncheckpoint\nmetrics\nWe provide the extensive evaluation results of seamlessM4T-Large and SeamlessM4T-Medium reported in the paper (as averages) in the metrics files above.\nThe evaluation data ids for FLEURS, CoVoST2 and CVSS-C can be found here\nEvaluating SeamlessM4T models\nTo reproduce our results or to evaluate using the same metrics over your own test sets, please check out the Evaluation README here.\nFinetuning SeamlessM4T models\nPlease check out the Finetuning README here.\nTransformers usage\nSeamlessM4T is available in the ü§ó Transformers library, requiring minimal dependencies. Steps to get started:\nFirst install the ü§ó Transformers library from main and sentencepiece:\npip install git+https://github.com/huggingface/transformers.git sentencepiece\nRun the following Python code to generate speech samples. Here the target language is Russian:\nfrom transformers import AutoProcessor, SeamlessM4Tv2Model\nimport torchaudio\nprocessor = AutoProcessor.from_pretrained(\"facebook/seamless-m4t-v2-large\")\nmodel = SeamlessM4Tv2Model.from_pretrained(\"facebook/seamless-m4t-v2-large\")\n# from text\ntext_inputs = processor(text = \"Hello, my dog is cute\", src_lang=\"eng\", return_tensors=\"pt\")\naudio_array_from_text = model.generate(**text_inputs, tgt_lang=\"rus\")[0].cpu().numpy().squeeze()\n# from audio\naudio, orig_freq =  torchaudio.load(\"https://www2.cs.uic.edu/~i101/SoundFiles/preamble10.wav\")\naudio =  torchaudio.functional.resample(audio, orig_freq=orig_freq, new_freq=16_000) # must be a 16 kHz waveform array\naudio_inputs = processor(audios=audio, return_tensors=\"pt\")\naudio_array_from_audio = model.generate(**audio_inputs, tgt_lang=\"rus\")[0].cpu().numpy().squeeze()\nListen to the audio samples either in an ipynb notebook:\nfrom IPython.display import Audio\nsample_rate = model.config.sampling_rate\nAudio(audio_array_from_text, rate=sample_rate)\n# Audio(audio_array_from_audio, rate=sample_rate)\nOr save them as a .wav file using a third-party library, e.g. scipy:\nimport scipy\nsample_rate = model.config.sampling_rate\nscipy.io.wavfile.write(\"out_from_text.wav\", rate=sample_rate, data=audio_array_from_text)\n# scipy.io.wavfile.write(\"out_from_audio.wav\", rate=sample_rate, data=audio_array_from_audio)\nFor more details on using the SeamlessM4T model for inference using the ü§ó Transformers library, refer to the\nSeamlessM4T v2 docs or to this hands-on Google Colab.\nSupported Languages:\nListed below, are the languages supported by SeamlessM4T-large (v1/v2).\nThe source column specifies whether a language is supported as source speech (Sp) and/or source text (Tx).\nThe target column specifies whether a language is supported as target speech (Sp) and/or target text (Tx).\ncode\nlanguage\nscript\nSource\nTarget\nafr\nAfrikaans\nLatn\nSp, Tx\nTx\namh\nAmharic\nEthi\nSp, Tx\nTx\narb\nModern Standard Arabic\nArab\nSp, Tx\nSp, Tx\nary\nMoroccan Arabic\nArab\nSp, Tx\nTx\narz\nEgyptian Arabic\nArab\nSp, Tx\nTx\nasm\nAssamese\nBeng\nSp, Tx\nTx\nast\nAsturian\nLatn\nSp\n--\nazj\nNorth Azerbaijani\nLatn\nSp, Tx\nTx\nbel\nBelarusian\nCyrl\nSp, Tx\nTx\nben\nBengali\nBeng\nSp, Tx\nSp, Tx\nbos\nBosnian\nLatn\nSp, Tx\nTx\nbul\nBulgarian\nCyrl\nSp, Tx\nTx\ncat\nCatalan\nLatn\nSp, Tx\nSp, Tx\nceb\nCebuano\nLatn\nSp, Tx\nTx\nces\nCzech\nLatn\nSp, Tx\nSp, Tx\nckb\nCentral Kurdish\nArab\nSp, Tx\nTx\ncmn\nMandarin Chinese\nHans\nSp, Tx\nSp, Tx\ncmn_Hant\nMandarin Chinese\nHant\nSp, Tx\nSp, Tx\ncym\nWelsh\nLatn\nSp, Tx\nSp, Tx\ndan\nDanish\nLatn\nSp, Tx\nSp, Tx\ndeu\nGerman\nLatn\nSp, Tx\nSp, Tx\nell\nGreek\nGrek\nSp, Tx\nTx\neng\nEnglish\nLatn\nSp, Tx\nSp, Tx\nest\nEstonian\nLatn\nSp, Tx\nSp, Tx\neus\nBasque\nLatn\nSp, Tx\nTx\nfin\nFinnish\nLatn\nSp, Tx\nSp, Tx\nfra\nFrench\nLatn\nSp, Tx\nSp, Tx\nfuv\nNigerian Fulfulde\nLatn\nSp, Tx\nTx\ngaz\nWest Central Oromo\nLatn\nSp, Tx\nTx\ngle\nIrish\nLatn\nSp, Tx\nTx\nglg\nGalician\nLatn\nSp, Tx\nTx\nguj\nGujarati\nGujr\nSp, Tx\nTx\nheb\nHebrew\nHebr\nSp, Tx\nTx\nhin\nHindi\nDeva\nSp, Tx\nSp, Tx\nhrv\nCroatian\nLatn\nSp, Tx\nTx\nhun\nHungarian\nLatn\nSp, Tx\nTx\nhye\nArmenian\nArmn\nSp, Tx\nTx\nibo\nIgbo\nLatn\nSp, Tx\nTx\nind\nIndonesian\nLatn\nSp, Tx\nSp, Tx\nisl\nIcelandic\nLatn\nSp, Tx\nTx\nita\nItalian\nLatn\nSp, Tx\nSp, Tx\njav\nJavanese\nLatn\nSp, Tx\nTx\njpn\nJapanese\nJpan\nSp, Tx\nSp, Tx\nkam\nKamba\nLatn\nSp\n--\nkan\nKannada\nKnda\nSp, Tx\nTx\nkat\nGeorgian\nGeor\nSp, Tx\nTx\nkaz\nKazakh\nCyrl\nSp, Tx\nTx\nkea\nKabuverdianu\nLatn\nSp\n--\nkhk\nHalh Mongolian\nCyrl\nSp, Tx\nTx\nkhm\nKhmer\nKhmr\nSp, Tx\nTx\nkir\nKyrgyz\nCyrl\nSp, Tx\nTx\nkor\nKorean\nKore\nSp, Tx\nSp, Tx\nlao\nLao\nLaoo\nSp, Tx\nTx\nlit\nLithuanian\nLatn\nSp, Tx\nTx\nltz\nLuxembourgish\nLatn\nSp\n--\nlug\nGanda\nLatn\nSp, Tx\nTx\nluo\nLuo\nLatn\nSp, Tx\nTx\nlvs\nStandard Latvian\nLatn\nSp, Tx\nTx\nmai\nMaithili\nDeva\nSp, Tx\nTx\nmal\nMalayalam\nMlym\nSp, Tx\nTx\nmar\nMarathi\nDeva\nSp, Tx\nTx\nmkd\nMacedonian\nCyrl\nSp, Tx\nTx\nmlt\nMaltese\nLatn\nSp, Tx\nSp, Tx\nmni\nMeitei\nBeng\nSp, Tx\nTx\nmya\nBurmese\nMymr\nSp, Tx\nTx\nnld\nDutch\nLatn\nSp, Tx\nSp, Tx\nnno\nNorwegian Nynorsk\nLatn\nSp, Tx\nTx\nnob\nNorwegian Bokm√•l\nLatn\nSp, Tx\nTx\nnpi\nNepali\nDeva\nSp, Tx\nTx\nnya\nNyanja\nLatn\nSp, Tx\nTx\noci\nOccitan\nLatn\nSp\n--\nory\nOdia\nOrya\nSp, Tx\nTx\npan\nPunjabi\nGuru\nSp, Tx\nTx\npbt\nSouthern Pashto\nArab\nSp, Tx\nTx\npes\nWestern Persian\nArab\nSp, Tx\nSp, Tx\npol\nPolish\nLatn\nSp, Tx\nSp, Tx\npor\nPortuguese\nLatn\nSp, Tx\nSp, Tx\nron\nRomanian\nLatn\nSp, Tx\nSp, Tx\nrus\nRussian\nCyrl\nSp, Tx\nSp, Tx\nslk\nSlovak\nLatn\nSp, Tx\nSp, Tx\nslv\nSlovenian\nLatn\nSp, Tx\nTx\nsna\nShona\nLatn\nSp, Tx\nTx\nsnd\nSindhi\nArab\nSp, Tx\nTx\nsom\nSomali\nLatn\nSp, Tx\nTx\nspa\nSpanish\nLatn\nSp, Tx\nSp, Tx\nsrp\nSerbian\nCyrl\nSp, Tx\nTx\nswe\nSwedish\nLatn\nSp, Tx\nSp, Tx\nswh\nSwahili\nLatn\nSp, Tx\nSp, Tx\ntam\nTamil\nTaml\nSp, Tx\nTx\ntel\nTelugu\nTelu\nSp, Tx\nSp, Tx\ntgk\nTajik\nCyrl\nSp, Tx\nTx\ntgl\nTagalog\nLatn\nSp, Tx\nSp, Tx\ntha\nThai\nThai\nSp, Tx\nSp, Tx\ntur\nTurkish\nLatn\nSp, Tx\nSp, Tx\nukr\nUkrainian\nCyrl\nSp, Tx\nSp, Tx\nurd\nUrdu\nArab\nSp, Tx\nSp, Tx\nuzn\nNorthern Uzbek\nLatn\nSp, Tx\nSp, Tx\nvie\nVietnamese\nLatn\nSp, Tx\nSp, Tx\nxho\nXhosa\nLatn\nSp\n--\nyor\nYoruba\nLatn\nSp, Tx\nTx\nyue\nCantonese\nHant\nSp, Tx\nTx\nzlm\nColloquial Malay\nLatn\nSp\n--\nzsm\nStandard Malay\nLatn\nTx\nTx\nzul\nZulu\nLatn\nSp, Tx\nTx\nNote that seamlessM4T-medium supports 200 languages in the text modality, and is based on NLLB-200 (see full list in asset card)\nCitation\nFor SeamlessM4T v2, please cite :\n@inproceedings{seamless2023,\ntitle=\"Seamless: Multilingual Expressive and Streaming Speech Translation\",\nauthor=\"{Seamless Communication}, Lo{\\\"i}c Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, John Hoffman, Min-Jae Hwang, Hirofumi Inaguma, Christopher Klaiber, Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, Ruslan Mavlyutov, Alice Rakotoarison, Kaushik Ram Sadagopan, Abinesh Ramakrishnan, Tuan Tran, Guillaume Wenzek, Yilin Yang, Ethan Ye, Ivan Evtimov, Pierre Fernandez, Cynthia Gao, Prangthip Hansanti, Elahe Kalbassi, Amanda Kallet, Artyom Kozhevnikov, Gabriel Mejia, Robin San Roman, Christophe Touret, Corinne Wong, Carleigh Wood, Bokai Yu, Pierre Andrews, Can Balioglu, Peng-Jen Chen, Marta R. Costa-juss{\\`a}, Maha Elbayad, Hongyu Gong, Francisco Guzm{\\'a}n, Kevin Heffernan, Somya Jain, Justine Kao, Ann Lee, Xutai Ma, Alex Mourachko, Benjamin Peloquin, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Anna Sun, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang, Mary Williamson\",\njournal={ArXiv},\nyear={2023}\n}",
    "microsoft/phi-2": "Model Summary\nHow to Use\nIntended Uses\nQA Format:\nChat Format:\nCode Format:\nSample Code\nLimitations of Phi-2\nTraining\nModel\nSoftware\nLicense\nTrademarks\nModel Summary\nPhi-2 is a Transformer with 2.7 billion parameters. It was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value). When assessed against benchmarks testing common sense, language understanding, and logical reasoning, Phi-2 showcased a nearly state-of-the-art performance among models with less than 13 billion parameters.\nOur model hasn't been fine-tuned through reinforcement learning from human feedback. The intention behind crafting this open-source model is to provide the research community with a non-restricted small model to explore vital safety challenges, such as reducing toxicity, understanding societal biases, enhancing controllability, and more.\nHow to Use\nPhi-2 has been integrated in the transformers version 4.37.0, please ensure that you are using a version equal or higher than it.\nPhi-2 is known for having an attention overflow issue (with FP16). If you are facing this issue, please enable/disable autocast on the PhiAttention.forward() function.\nIntended Uses\nGiven the nature of the training data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format.\nQA Format:\nYou can provide the prompt as a standalone question as follows:\nWrite a detailed analogy between mathematics and a lighthouse.\nwhere the model generates the text after \".\" .\nTo encourage the model to write more concise answers, you can also try the following QA format using \"Instruct: <prompt>\\nOutput:\"\nInstruct: Write a detailed analogy between mathematics and a lighthouse.\nOutput: Mathematics is like a lighthouse. Just as a lighthouse guides ships safely to shore, mathematics provides a guiding light in the world of numbers and logic. It helps us navigate through complex problems and find solutions. Just as a lighthouse emits a steady beam of light, mathematics provides a consistent framework for reasoning and problem-solving. It illuminates the path to understanding and helps us make sense of the world around us.\nwhere the model generates the text after \"Output:\".\nChat Format:\nAlice: I don't know why, I'm struggling to maintain focus while studying. Any suggestions?\nBob: Well, have you tried creating a study schedule and sticking to it?\nAlice: Yes, I have, but it doesn't seem to help much.\nBob: Hmm, maybe you should try studying in a quiet environment, like the library.\nAlice: ...\nwhere the model generates the text after the first \"Bob:\".\nCode Format:\ndef print_prime(n):\n\"\"\"\nPrint all primes between 1 and n\n\"\"\"\nprimes = []\nfor num in range(2, n+1):\nis_prime = True\nfor i in range(2, int(math.sqrt(num))+1):\nif num % i == 0:\nis_prime = False\nbreak\nif is_prime:\nprimes.append(num)\nprint(primes)\nwhere the model generates the text after the comments.\nNotes:\nPhi-2 is intended for QA, chat, and code purposes. The model-generated text/code should be treated as a starting point rather than a definitive solution for potential use cases. Users should be cautious when employing these models in their applications.\nDirect adoption for production tasks without evaluation is out of scope of this project. As a result, the Phi-2 model has not been tested to ensure that it performs adequately for any production-level application. Please refer to the limitation sections of this document for more details.\nIf you are using transformers<4.37.0, always load the model with trust_remote_code=True to prevent side-effects.\nSample Code\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntorch.set_default_device(\"cuda\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\ninputs = tokenizer('''def print_prime(n):\n\"\"\"\nPrint all primes between 1 and n\n\"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\nprint(text)\nLimitations of Phi-2\nGenerate Inaccurate Code and Facts: The model may produce incorrect code snippets and statements. Users should treat these outputs as suggestions or starting points, not as definitive or accurate solutions.\nLimited Scope for code: Majority of Phi-2 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\nUnreliable Responses to Instruction: The model has not undergone instruction fine-tuning. As a result, it may struggle or fail to adhere to intricate or nuanced instructions provided by users.\nLanguage Limitations: The model is primarily designed to understand standard English. Informal English, slang, or any other languages might pose challenges to its comprehension, leading to potential misinterpretations or errors in response.\nPotential Societal Biases: Phi-2 is not entirely free from societal biases despite efforts in assuring training data safety. There's a possibility it may generate content that mirrors these societal biases, particularly if prompted or instructed to do so. We urge users to be aware of this and to exercise caution and critical thinking when interpreting model outputs.\nToxicity: Despite being trained with carefully selected data, the model can still produce harmful content if explicitly prompted or instructed to do so. We chose to release the model to help the open-source community develop the most effective ways to reduce the toxicity of a model directly after pretraining.\nVerbosity: Phi-2 being a base model often produces irrelevant or extra text and responses following its first answer to user prompts within a single turn. This is due to its training dataset being primarily textbooks, which results in textbook-like responses.\nTraining\nModel\nArchitecture: a Transformer-based model with next-word prediction objective\nContext length: 2048 tokens\nDataset size: 250B tokens, combination of NLP synthetic data created by AOAI GPT-3.5 and filtered web data from Falcon RefinedWeb and SlimPajama, which was assessed by AOAI GPT-4.\nTraining tokens: 1.4T tokens\nGPUs: 96xA100-80G\nTraining time: 14 days\nSoftware\nPyTorch\nDeepSpeed\nFlash-Attention\nLicense\nThe model is licensed under the MIT license.\nTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow‚ÄØMicrosoft‚Äôs Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies.",
    "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF": "Tinyllama 1.1B Chat v1.0 - GGUF\nDescription\nAbout GGUF\nRepositories available\nPrompt template: Zephyr\nCompatibility\nExplanation of quantisation methods\nProvided files\nHow to download GGUF files\nIn text-generation-webui\nOn the command line, including multiple files at once\nExample llama.cpp command\nHow to run in text-generation-webui\nHow to run from Python code\nHow to load this model in Python code, using llama-cpp-python\nHow to use with LangChain\nDiscord\nThanks, and how to contribute\nOriginal model card: TinyLlama's Tinyllama 1.1B Chat v1.0\nTinyLlama-1.1B\nChat & support: TheBloke's Discord server\nWant to contribute? TheBloke's Patreon page\nTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)\nTinyllama 1.1B Chat v1.0 - GGUF\nModel creator: TinyLlama\nOriginal model: Tinyllama 1.1B Chat v1.0\nDescription\nThis repo contains GGUF format model files for TinyLlama's Tinyllama 1.1B Chat v1.0.\nThese files were quantised using hardware kindly provided by Massed Compute.\nAbout GGUF\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\nHere is an incomplete list of clients and libraries that are known to support GGUF:\nllama.cpp. The source project for GGUF. Offers a CLI and a server option.\ntext-generation-webui, the most widely used web UI, with many features and powerful extensions. Supports GPU acceleration.\nKoboldCpp, a fully featured web UI, with GPU accel across all platforms and GPU architectures. Especially good for story telling.\nGPT4All, a free and open source local running GUI, supporting Windows, Linux and macOS with full GPU accel.\nLM Studio, an easy-to-use and powerful local GUI for Windows and macOS (Silicon), with GPU acceleration. Linux available, in beta as of 27/11/2023.\nLoLLMS Web UI, a great web UI with many interesting and unique features, including a full model library for easy model selection.\nFaraday.dev, an attractive and easy to use character-based chat GUI for Windows and macOS (both Silicon and Intel), with GPU acceleration.\nllama-cpp-python, a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\ncandle, a Rust ML framework with a focus on performance, including GPU support, and ease of use.\nctransformers, a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server. Note, as of time of writing (November 27th 2023), ctransformers has not been updated in a long time and does not support many recent models.\nRepositories available\nAWQ model(s) for GPU inference.\nGPTQ models for GPU inference, with multiple quantisation parameter options.\n2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference\nTinyLlama's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions\nPrompt template: Zephyr\n<|system|>\n{system_message}</s>\n<|user|>\n{prompt}</s>\n<|assistant|>\nCompatibility\nThese quantised GGUFv2 files are compatible with llama.cpp from August 27th onwards, as of commit d0cee0d\nThey are also compatible with many third party UIs and libraries - please see the list at the top of this README.\nExplanation of quantisation methods\nClick to see details\nThe new methods available are:\nGGML_TYPE_Q2_K - \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\nGGML_TYPE_Q3_K - \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\nGGML_TYPE_Q4_K - \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\nGGML_TYPE_Q5_K - \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\nGGML_TYPE_Q6_K - \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\nRefer to the Provided Files table below to see what files use which methods, and how.\nProvided files\nName\nQuant method\nBits\nSize\nMax RAM required\nUse case\ntinyllama-1.1b-chat-v1.0.Q2_K.gguf\nQ2_K\n2\n0.48 GB\n2.98 GB\nsmallest, significant quality loss - not recommended for most purposes\ntinyllama-1.1b-chat-v1.0.Q3_K_S.gguf\nQ3_K_S\n3\n0.50 GB\n3.00 GB\nvery small, high quality loss\ntinyllama-1.1b-chat-v1.0.Q3_K_M.gguf\nQ3_K_M\n3\n0.55 GB\n3.05 GB\nvery small, high quality loss\ntinyllama-1.1b-chat-v1.0.Q3_K_L.gguf\nQ3_K_L\n3\n0.59 GB\n3.09 GB\nsmall, substantial quality loss\ntinyllama-1.1b-chat-v1.0.Q4_0.gguf\nQ4_0\n4\n0.64 GB\n3.14 GB\nlegacy; small, very high quality loss - prefer using Q3_K_M\ntinyllama-1.1b-chat-v1.0.Q4_K_S.gguf\nQ4_K_S\n4\n0.64 GB\n3.14 GB\nsmall, greater quality loss\ntinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\nQ4_K_M\n4\n0.67 GB\n3.17 GB\nmedium, balanced quality - recommended\ntinyllama-1.1b-chat-v1.0.Q5_0.gguf\nQ5_0\n5\n0.77 GB\n3.27 GB\nlegacy; medium, balanced quality - prefer using Q4_K_M\ntinyllama-1.1b-chat-v1.0.Q5_K_S.gguf\nQ5_K_S\n5\n0.77 GB\n3.27 GB\nlarge, low quality loss - recommended\ntinyllama-1.1b-chat-v1.0.Q5_K_M.gguf\nQ5_K_M\n5\n0.78 GB\n3.28 GB\nlarge, very low quality loss - recommended\ntinyllama-1.1b-chat-v1.0.Q6_K.gguf\nQ6_K\n6\n0.90 GB\n3.40 GB\nvery large, extremely low quality loss\ntinyllama-1.1b-chat-v1.0.Q8_0.gguf\nQ8_0\n8\n1.17 GB\n3.67 GB\nvery large, extremely low quality loss - not recommended\nNote: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\nHow to download GGUF files\nNote for manual downloaders: You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\nLM Studio\nLoLLMS Web UI\nFaraday.dev\nIn text-generation-webui\nUnder Download Model, you can enter the model repo: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF and below it, a specific filename to download, such as: tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf.\nThen click Download.\nOn the command line, including multiple files at once\nI recommend using the huggingface-hub Python library:\npip3 install huggingface-hub\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\nhuggingface-cli download TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nMore advanced huggingface-cli download usage (click to read)\nYou can also download multiple files at once with a pattern:\nhuggingface-cli download TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF --local-dir . --local-dir-use-symlinks False --include='*Q4_K*gguf'\nFor more documentation on downloading with huggingface-cli, please see: HF -> Hub Python Library -> Download files -> Download from the CLI.\nTo accelerate downloads on fast connections (1Gbit/s or higher), install hf_transfer:\npip3 install hf_transfer\nAnd set environment variable HF_HUB_ENABLE_HF_TRANSFER to 1:\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\nWindows Command Line users: You can set the environment variable by running set HF_HUB_ENABLE_HF_TRANSFER=1 before the download command.\nExample llama.cpp command\nMake sure you are using llama.cpp from commit d0cee0d or later.\n./main -ngl 35 -m tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<|system|>\\n{system_message}</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\"\nChange -ngl 32 to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration.\nChange -c 2048 to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF file and set by llama.cpp automatically. Note that longer sequence lengths require much more resources, so you may need to reduce this value.\nIf you want to have a chat-style conversation, replace the -p <PROMPT> argument with -i -ins\nFor other parameters and how to use them, please refer to the llama.cpp documentation\nHow to run in text-generation-webui\nFurther instructions can be found in the text-generation-webui documentation, here: text-generation-webui/docs/04 ‚Äê Model Tab.md.\nHow to run from Python code\nYou can use GGUF models from Python using the llama-cpp-python or ctransformers libraries. Note that at the time of writing (Nov 27th 2023), ctransformers has not been updated for some time and is not compatible with some recent models. Therefore I recommend you use llama-cpp-python.\nHow to load this model in Python code, using llama-cpp-python\nFor full documentation, please see: llama-cpp-python docs.\nFirst install the package\nRun one of the following commands, according to your system:\n# Base ctransformers with no GPU acceleration\npip install llama-cpp-python\n# With NVidia CUDA acceleration\nCMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n# Or with OpenBLAS acceleration\nCMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n# Or with CLBLast acceleration\nCMAKE_ARGS=\"-DLLAMA_CLBLAST=on\" pip install llama-cpp-python\n# Or with AMD ROCm GPU acceleration (Linux only)\nCMAKE_ARGS=\"-DLLAMA_HIPBLAS=on\" pip install llama-cpp-python\n# Or with Metal GPU acceleration for macOS systems only\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python\n# In windows, to set the variables CMAKE_ARGS in PowerShell, follow this format; eg for NVidia CUDA:\n$env:CMAKE_ARGS = \"-DLLAMA_OPENBLAS=on\"\npip install llama-cpp-python\nSimple llama-cpp-python example code\nfrom llama_cpp import Llama\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = Llama(\nmodel_path=\"./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\",  # Download the model file first\nn_ctx=2048,  # The max sequence length to use - note that longer sequence lengths require much more resources\nn_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\nn_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n)\n# Simple inference example\noutput = llm(\n\"<|system|>\\n{system_message}</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\", # Prompt\nmax_tokens=512,  # Generate up to 512 tokens\nstop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\necho=True        # Whether to echo the prompt\n)\n# Chat Completion API\nllm = Llama(model_path=\"./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\", chat_format=\"llama-2\")  # Set chat_format according to the model you are using\nllm.create_chat_completion(\nmessages = [\n{\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n{\n\"role\": \"user\",\n\"content\": \"Write a story about llamas.\"\n}\n]\n)\nHow to use with LangChain\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\nLangChain + llama-cpp-python\nLangChain + ctransformers\nDiscord\nFor further support, and discussions on these models and AI in general, join us at:\nTheBloke AI's Discord server\nThanks, and how to contribute\nThanks to the chirper.ai team!\nThanks to Clay from gpus.llm-utils.org!\nI've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\nIf you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\nPatreon: https://patreon.com/TheBlokeAI\nKo-Fi: https://ko-fi.com/TheBlokeAI\nSpecial thanks to: Aemon Algiz.\nPatreon special mentions: Michael Levine, ÈòøÊòé, Trailburnt, Nikolai Manek, John Detwiler, Randy H, Will Dee, Sebastain Graf, NimbleBox.ai, Eugene Pentland, Emad Mostaque, Ai Maven, Jim Angel, Jeff Scroggin, Michael Davis, Manuel Alberto Morcote, Stephen Murray, Robert, Justin Joy, Luke @flexchar, Brandon Frisco, Elijah Stavena, S_X, Dan Guido, Undi ., Komninos Chatzipapas, Shadi, theTransient, Lone Striker, Raven Klaugh, jjj, Cap'n Zoog, Michel-Marie MAUDET (LINAGORA), Matthew Berman, David, Fen Risland, Omer Bin Jawed, Luke Pendergrass, Kalila, OG, Erik Bj√§reholt, Rooh Singh, Joseph William Delisle, Dan Lewis, TL, John Villwock, AzureBlack, Brad, Pedro Madruga, Caitlyn Gatomon, K, jinyuan sun, Mano Prime, Alex, Jeffrey Morgan, Alicia Loh, Illia Dulskyi, Chadd, transmissions 11, fincy, Rainer Wilmers, ReadyPlayerEmma, knownsqashed, Mandus, biorpg, Deo Leter, Brandon Phillips, SuperWojo, Sean Connelly, Iucharbius, Jack West, Harry Royden McLaughlin, Nicholas, terasurfer, Vitor Caleffi, Duane Dunston, Johann-Peter Hartmann, David Ziegler, Olakabola, Ken Nordquist, Trenton Dambrowitz, Tom X Nguyen, Vadim, Ajan Kanaga, Leonard Tan, Clay Pascal, Alexandros Triantafyllidis, JM33133, Xule, vamX, ya boyyy, subjectnull, Talal Aujan, Alps Aficionado, wassieverse, Ari Malik, James Bentley, Woland, Spencer Kim, Michael Dempsey, Fred von Graf, Elle, zynix, William Richards, Stanislav Ovsiannikov, Edmond Seymore, Jonathan Leane, Martin Kemka, usrbinkat, Enrico Ros\nThank you to all my generous patrons and donaters!\nAnd thank you again to a16z for their generous grant.\nOriginal model card: TinyLlama's Tinyllama 1.1B Chat v1.0\nTinyLlama-1.1B\nhttps://github.com/jzhang38/TinyLlama\nThe TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens. With some proper optimization, we can achieve this within a span of \"just\" 90 days using 16 A100-40G GPUs üöÄüöÄ. The training has started on 2023-09-01.\nWe adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint.\nThis Model\nThis is the chat model finetuned on top of TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T. We follow HF's Zephyr's training recipe. The model was \" initially fine-tuned on a variant of the UltraChat dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT.\nWe then further aligned the model with ü§ó TRL's DPOTrainer on the openbmb/UltraFeedback dataset, which contain 64k prompts and model completions that are ranked by GPT-4.\"\nHow to use\nYou will need the transformers>=4.34\nDo check the TinyLlama github page for more information.\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\nimport torch\nfrom transformers import pipeline\npipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n{\n\"role\": \"system\",\n\"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n},\n{\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# ...",
    "stabilityai/stable-video-diffusion-img2vid-xt-1-1": "You need to agree to share your contact information to access this model\nStable Video Diffusion 1.1 License Agreement\nSTABILITY AI COMMUNITY LICENSE AGREEMENT Last Updated: July 5, 2024 1. INTRODUCTIONThis Agreement applies to any individual person or entity (‚ÄúYou‚Äù, ‚ÄúYour‚Äù or ‚ÄúLicensee‚Äù) that uses or distributes any portion or element of the Stability AI Materials  or Derivative Works thereof for any Research & Non-Commercial or Commercial purpose. Capitalized terms not otherwise defined herein are defined in Section V below.This Agreement is intended to allow research, non-commercial, and limited commercial uses of the Models free of charge. In order to ensure that certain limited commercial uses of the Models continue to be allowed, this Agreement  preserves free access to the Models for people or organizations  generating annual revenue of less than US $1,000,000 (or local currency equivalent).By clicking ‚ÄúI Accept‚Äù  or by using or distributing or using any portion or element of the Stability Materials or Derivative Works, You agree that You have read, understood and are bound by the terms of this Agreement. If You are acting on behalf of a company, organization or other entity, then ‚ÄúYou‚Äù includes you and that entity, and You agree that You: (i) are an authorized representative of such entity with the authority to bind such entity to this Agreement, and (ii) You agree to the terms of this Agreement on that entity‚Äôs behalf.2. RESEARCH & NON-COMMERCIAL USE LICENSESubject to the terms of this Agreement, Stability AI grants You a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable and royalty-free limited license under Stability AI‚Äôs intellectual property or other rights owned by Stability AI embodied in the Stability AI Materials to use, reproduce, distribute, and create Derivative Works of, and make modifications to, the Stability AI Materials for any Research or Non-Commercial Purpose. ‚ÄúResearch Purpose‚Äù means academic or scientific advancement, and in each case, is not primarily intended for commercial advantage or monetary compensation to You or others. ‚ÄúNon-Commercial Purpose‚Äù means any purpose other than a Research Purpose that is not primarily intended for commercial advantage or monetary compensation to You or others, such as personal use (i.e., hobbyist) or evaluation and testing.3. COMMERCIAL USE LICENSESubject to the terms of this Agreement (including the remainder of this Section III), Stability AI grants You a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable and royalty-free limited license under Stability AI‚Äôs intellectual property or other rights owned by Stability AI embodied in the Stability AI Materials to use, reproduce, distribute, and create Derivative Works of, and make modifications to, the Stability AI Materials for any Commercial Purpose. ‚ÄúCommercial Purpose‚Äù means any purpose other than a Research Purpose or Non-Commercial Purpose that is primarily intended for commercial advantage or monetary compensation to You or others, including but not limited to, (i) creating, modifying, or distributing Your product or service, including via a hosted service or application programming interface, and (ii) for Your business‚Äôs or organization‚Äôs internal operations.  If You are using or distributing the Stability AI Materials for a Commercial Purpose, You must register with Stability AI at (https://stability.ai/community-license). If at any time You or Your Affiliate(s), either individually or in aggregate, generate more than USD $1,000,000 in annual revenue (or the equivalent thereof in Your local currency), regardless of whether that revenue is generated directly or indirectly from the Stability AI Materials or Derivative Works, any licenses granted to You under this Agreement shall terminate as of such date. You must request a license from Stability AI at (https://stability.ai/enterprise) , which Stability AI may grant to You in its sole discretion. If you receive Stability AI Materials, or any Derivative Works thereof, from a Licensee as part of an integrated end user product, then Section III of this Agreement will not apply to you.4. GENERAL TERMSYour Research, Non-Commercial, and Commercial License(s) under this Agreement are subject to the following terms. a.  Distribution & Attribution. If You distribute or make available the Stability AI Materials or a Derivative Work to a third party, or a product or service that uses any portion of them, You shall: (i) provide a copy of this Agreement to that third party, (ii) retain the following attribution notice within a \"Notice\" text file distributed as a part of such copies: \"This Stability AI Model is licensed under the Stability AI Community License, Copyright ¬©  Stability AI Ltd. All Rights Reserved‚Äù, and (iii) prominently display ‚ÄúPowered by Stability AI‚Äù on a related website, user interface, blogpost, about page, or product documentation.  If You create a Derivative Work, You may add your own attribution notice(s) to the ‚ÄúNotice‚Äù text file included with that Derivative Work, provided that You clearly indicate which attributions apply to the Stability AI Materials and state in the ‚ÄúNotice‚Äù text file that You changed the Stability AI Materials and how it was modified. b.  Use Restrictions. Your use of the Stability AI Materials and Derivative Works, including any output or results of the Stability AI Materials or Derivative Works, must comply with applicable laws and regulations (including Trade Control Laws and equivalent regulations) and adhere to the Documentation and Stability AI‚Äôs AUP, which is hereby incorporated by reference. Furthermore, You will not use the Stability AI Materials or Derivative Works, or any output or results of the Stability AI Materials or Derivative Works, to create or improve any foundational generative AI model (excluding the Models or Derivative Works).  c.  Intellectual Property.  (i) Trademark License.  No trademark licenses are granted under this Agreement, and in connection with the Stability AI Materials or Derivative Works, You may not use any name or mark owned by or associated with Stability AI or any of its Affiliates, except as required under Section IV(a) herein. (ii)  Ownership of Derivative Works.  As between You and Stability AI, You are the owner of Derivative Works You create, subject to Stability AI‚Äôs ownership of the Stability AI Materials and any Derivative Works made by or for Stability AI. (iii)  Ownership of Outputs. As between You and Stability AI, You own any outputs generated from the Models or Derivative Works to the extent permitted by applicable law.  (iv)  Disputes.  If You or Your Affiliate(s) institute litigation or other proceedings against Stability AI (including a cross-claim or counterclaim in a lawsuit) alleging that the Stability AI Materials, Derivative Works or associated outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by You, then any licenses granted to You under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Stability AI from and against any claim by any third party arising out of or related to Your use or distribution of the Stability AI Materials or Derivative Works in violation of this Agreement. (v)  Feedback.  From time to time, You may provide Stability AI with verbal and/or written suggestions, comments or other feedback related to Stability AI‚Äôs existing or prospective technology, products or services (collectively, ‚ÄúFeedback‚Äù). You are not obligated to provide Stability AI with Feedback, but to the extent that You do, You hereby grant Stability AI a perpetual, irrevocable, royalty-free, fully-paid, sub-licensable, transferable, non-exclusive, worldwide right and license to exploit the Feedback in any manner without restriction. Your Feedback is provided ‚ÄúAS IS‚Äù and You make no warranties whatsoever about any Feedback. d.  Disclaimer Of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE STABILITY AI MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OR LAWFULNESS OF USING OR REDISTRIBUTING THE STABILITY AI MATERIALS, DERIVATIVE WORKS OR ANY OUTPUT OR RESULTS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE STABILITY AI MATERIALS, DERIVATIVE WORKS AND ANY OUTPUT AND RESULTS. e.  Limitation Of Liability. IN NO EVENT WILL STABILITY AI OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY DIRECT, INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF STABILITY AI OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING. f.  Term And Termination. The term of this Agreement will commence upon Your acceptance of this Agreement or access to the Stability AI Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Stability AI may terminate this Agreement if You are in breach of any term or condition of this Agreement. Upon termination of this Agreement, You shall delete and cease use of any Stability AI Materials or Derivative Works. Section IV(d), (e), and (g) shall survive the termination of this Agreement. g.  Governing Law.  This Agreement will be governed by and constructed in accordance with the laws of the United States and the State of California without regard to choice of law principles, and the UN Convention on Contracts for International Sale of Goods does not apply to this Agreement.5. DEFINITIONS‚ÄúAffiliate(s)‚Äù means any entity that directly or indirectly controls, is controlled by, or is under common control with the subject entity; for purposes of this definition, ‚Äúcontrol‚Äù means direct or indirect ownership or control of more than 50% of the voting interests of the subject entity.\"Agreement\" means this Stability AI Community License Agreement.‚ÄúAUP‚Äù means the Stability AI Acceptable Use Policy available at (https://stability.ai/use-policy), as may be updated from time to time.\"Derivative Work(s)‚Äù means (a) any derivative work of the Stability AI Materials as recognized by U.S. copyright laws and (b) any modifications to a Model, and any other model created which is based on or derived from the Model or the Model‚Äôs output, including ‚Äúfine tune‚Äù and ‚Äúlow-rank adaptation‚Äù models derived from a Model or a Model‚Äôs output, but do not include the output of any Model.‚ÄúDocumentation‚Äù means any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software or Models.‚ÄúModel(s)\" means, collectively, Stability AI‚Äôs proprietary models and algorithms, including machine-learning models, trained model weights and other elements of the foregoing listed on Stability‚Äôs Core Models Webpage available at (https://stability.ai/core-models), as may be updated from time to time.\"Stability AI\" or \"we\" means Stability AI Ltd. and its Affiliates.\"Software\" means Stability AI‚Äôs proprietary software made available under this Agreement now or in the future.‚ÄúStability AI Materials‚Äù means, collectively, Stability‚Äôs proprietary Models, Software and Documentation (and any portion or combination thereof) made available under this Agreement.‚ÄúTrade Control Laws‚Äù means any applicable U.S. and non-U.S. export control and trade sanctions laws and regulations.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nStable Video Diffusion 1.1 Image-to-Video Model Card\nModel Details\nModel Description\nModel Sources\nUses\nDirect Use\nOut-of-Scope Use\nLimitations and Bias\nLimitations\nRecommendations\nHow to Get Started with the Model\nStable Video Diffusion 1.1 Image-to-Video Model Card\nStable Video Diffusion (SVD) 1.1 Image-to-Video is a diffusion model that takes in a still image as a conditioning frame, and generates a video from it.\nPlease note: For commercial use, please refer to https://stability.ai/license.\nModel Details\nModel Description\n(SVD 1.1) Image-to-Video is a latent diffusion model trained to generate short video clips from an image conditioning.\nThis model was trained to generate 25 frames at resolution 1024x576 given a context frame of the same size, finetuned from SVD Image-to-Video [25 frames].\nFine tuning was performed with fixed conditioning at 6FPS and Motion Bucket Id 127 to improve the consistency of outputs without the need to adjust hyper parameters. These conditions are still adjustable and have not been removed. Performance outside of the fixed conditioning settings may vary compared to SVD 1.0.\nDeveloped by: Stability AI\nFunded by: Stability AI\nModel type: Generative image-to-video model\nFinetuned from model: SVD Image-to-Video [25 frames]\nModel Sources\nFor research purposes, we recommend our generative-models Github repository (https://github.com/Stability-AI/generative-models),\nwhich implements the most popular diffusion frameworks (both training and inference).\nRepository: https://github.com/Stability-AI/generative-models\nPaper: https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets\nUses\nDirect Use\nThe model is intended for both non-commercial and commercial usage. You can use this model for non-commercial or research purposes under the following license. Possible research areas and tasks include\nResearch on generative models.\nSafe deployment of models which have the potential to generate harmful content.\nProbing and understanding the limitations and biases of generative models.\nGeneration of artworks and use in design and other artistic processes.\nApplications in educational or creative tools.\nFor commercial use, please refer to https://stability.ai/license.\nExcluded uses are described below.\nOut-of-Scope Use\nThe model was not trained to be factual or true representations of people or events,\nand therefore using the model to generate such content is out-of-scope for the abilities of this model.\nThe model should not be used in any way that violates Stability AI's Acceptable Use Policy.\nLimitations and Bias\nLimitations\nThe generated videos are rather short (<= 4sec), and the model does not achieve perfect photorealism.\nThe model may generate videos without motion, or very slow camera pans.\nThe model cannot be controlled through text.\nThe model cannot render legible text.\nFaces and people in general may not be generated properly.\nThe autoencoding part of the model is lossy.\nRecommendations\nThe model is intended for both non-commercial and commercial usage.\nHow to Get Started with the Model\nCheck out https://github.com/Stability-AI/generative-models",
    "intfloat/multilingual-e5-large-instruct": "Multilingual-E5-large-instruct\nUsage\nTransformers\nSentence Transformers\nInfinity\nSupported Languages\nTraining Details\nMTEB Benchmark Evaluation\nFAQ\nCitation\nLimitations\nMultilingual-E5-large-instruct\nMultilingual E5 Text Embeddings: A Technical Report.\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, arXiv 2024\nThis model has 24 layers and the embedding size is 1024.\nUsage\nBelow are examples to encode queries and passages from the MS-MARCO passage ranking dataset.\nTransformers\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\ndef average_pool(last_hidden_states: Tensor,\nattention_mask: Tensor) -> Tensor:\nlast_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\nreturn last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery: {query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'how much protein should a female eat'),\nget_detailed_instruct(task, 'ÂçóÁìúÁöÑÂÆ∂Â∏∏ÂÅöÊ≥ï')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n\"1.Ê∏ÖÁÇíÂçóÁìú‰∏ù ÂéüÊñô:Â´©ÂçóÁìúÂçä‰∏™ Ë∞ÉÊñô:Ëë±„ÄÅÁõê„ÄÅÁôΩÁ≥ñ„ÄÅÈ∏°Á≤æ ÂÅöÊ≥ï: 1„ÄÅÂçóÁìúÁî®ÂàÄËñÑËñÑÁöÑÂâäÂéªË°®Èù¢‰∏ÄÂ±ÇÁöÆ,Áî®Âã∫Â≠êÂàÆÂéªÁì§ 2„ÄÅÊì¶ÊàêÁªÜ‰∏ù(Ê≤°ÊúâÊì¶ËèúÊùøÂ∞±Áî®ÂàÄÊÖ¢ÊÖ¢ÂàáÊàêÁªÜ‰∏ù) 3„ÄÅÈîÖÁÉßÁÉ≠ÊîæÊ≤π,ÂÖ•Ëë±Ëä±ÁÖ∏Âá∫È¶ôÂë≥ 4„ÄÅÂÖ•ÂçóÁìú‰∏ùÂø´ÈÄüÁøªÁÇí‰∏ÄÂàÜÈíüÂ∑¶Âè≥,ÊîæÁõê„ÄÅ‰∏ÄÁÇπÁôΩÁ≥ñÂíåÈ∏°Á≤æË∞ÉÂë≥Âá∫ÈîÖ 2.È¶ôËë±ÁÇíÂçóÁìú ÂéüÊñô:ÂçóÁìú1Âè™ Ë∞ÉÊñô:È¶ôËë±„ÄÅËíúÊú´„ÄÅÊ©ÑÊ¶ÑÊ≤π„ÄÅÁõê ÂÅöÊ≥ï: 1„ÄÅÂ∞ÜÂçóÁìúÂéªÁöÆ,ÂàáÊàêÁâá 2„ÄÅÊ≤πÈîÖ8ÊàêÁÉ≠Âêé,Â∞ÜËíúÊú´ÊîæÂÖ•ÁàÜÈ¶ô 3„ÄÅÁàÜÈ¶ôÂêé,Â∞ÜÂçóÁìúÁâáÊîæÂÖ•,ÁøªÁÇí 4„ÄÅÂú®ÁøªÁÇíÁöÑÂêåÊó∂,ÂèØ‰ª•‰∏çÊó∂Âú∞ÂæÄÈîÖÈáåÂä†Ê∞¥,‰ΩÜ‰∏çË¶ÅÂ§™Â§ö 5„ÄÅÊîæÂÖ•Áõê,ÁÇíÂåÄ 6„ÄÅÂçóÁìúÂ∑Æ‰∏çÂ§öËΩØÂíåÁªµ‰∫Ü‰πãÂêé,Â∞±ÂèØ‰ª•ÂÖ≥ÁÅ´ 7„ÄÅÊííÂÖ•È¶ôËë±,Âç≥ÂèØÂá∫ÈîÖ\"\n]\ninput_texts = queries + documents\ntokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large-instruct')\nmodel = AutoModel.from_pretrained('intfloat/multilingual-e5-large-instruct')\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\noutputs = model(**batch_dict)\nembeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\n# => [[91.92852783203125, 67.580322265625], [70.3814468383789, 92.1330795288086]]\nSentence Transformers\nfrom sentence_transformers import SentenceTransformer\ndef get_detailed_instruct(task_description: str, query: str) -> str:\nreturn f'Instruct: {task_description}\\nQuery: {query}'\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\nget_detailed_instruct(task, 'how much protein should a female eat'),\nget_detailed_instruct(task, 'ÂçóÁìúÁöÑÂÆ∂Â∏∏ÂÅöÊ≥ï')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n\"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n\"1.Ê∏ÖÁÇíÂçóÁìú‰∏ù ÂéüÊñô:Â´©ÂçóÁìúÂçä‰∏™ Ë∞ÉÊñô:Ëë±„ÄÅÁõê„ÄÅÁôΩÁ≥ñ„ÄÅÈ∏°Á≤æ ÂÅöÊ≥ï: 1„ÄÅÂçóÁìúÁî®ÂàÄËñÑËñÑÁöÑÂâäÂéªË°®Èù¢‰∏ÄÂ±ÇÁöÆ,Áî®Âã∫Â≠êÂàÆÂéªÁì§ 2„ÄÅÊì¶ÊàêÁªÜ‰∏ù(Ê≤°ÊúâÊì¶ËèúÊùøÂ∞±Áî®ÂàÄÊÖ¢ÊÖ¢ÂàáÊàêÁªÜ‰∏ù) 3„ÄÅÈîÖÁÉßÁÉ≠ÊîæÊ≤π,ÂÖ•Ëë±Ëä±ÁÖ∏Âá∫È¶ôÂë≥ 4„ÄÅÂÖ•ÂçóÁìú‰∏ùÂø´ÈÄüÁøªÁÇí‰∏ÄÂàÜÈíüÂ∑¶Âè≥,ÊîæÁõê„ÄÅ‰∏ÄÁÇπÁôΩÁ≥ñÂíåÈ∏°Á≤æË∞ÉÂë≥Âá∫ÈîÖ 2.È¶ôËë±ÁÇíÂçóÁìú ÂéüÊñô:ÂçóÁìú1Âè™ Ë∞ÉÊñô:È¶ôËë±„ÄÅËíúÊú´„ÄÅÊ©ÑÊ¶ÑÊ≤π„ÄÅÁõê ÂÅöÊ≥ï: 1„ÄÅÂ∞ÜÂçóÁìúÂéªÁöÆ,ÂàáÊàêÁâá 2„ÄÅÊ≤πÈîÖ8ÊàêÁÉ≠Âêé,Â∞ÜËíúÊú´ÊîæÂÖ•ÁàÜÈ¶ô 3„ÄÅÁàÜÈ¶ôÂêé,Â∞ÜÂçóÁìúÁâáÊîæÂÖ•,ÁøªÁÇí 4„ÄÅÂú®ÁøªÁÇíÁöÑÂêåÊó∂,ÂèØ‰ª•‰∏çÊó∂Âú∞ÂæÄÈîÖÈáåÂä†Ê∞¥,‰ΩÜ‰∏çË¶ÅÂ§™Â§ö 5„ÄÅÊîæÂÖ•Áõê,ÁÇíÂåÄ 6„ÄÅÂçóÁìúÂ∑Æ‰∏çÂ§öËΩØÂíåÁªµ‰∫Ü‰πãÂêé,Â∞±ÂèØ‰ª•ÂÖ≥ÁÅ´ 7„ÄÅÊííÂÖ•È¶ôËë±,Âç≥ÂèØÂá∫ÈîÖ\"\n]\ninput_texts = queries + documents\nmodel = SentenceTransformer('intfloat/multilingual-e5-large-instruct')\nembeddings = model.encode(input_texts, convert_to_tensor=True, normalize_embeddings=True)\nscores = (embeddings[:2] @ embeddings[2:].T) * 100\nprint(scores.tolist())\n# [[91.92853546142578, 67.5802993774414], [70.38143157958984, 92.13307189941406]]\nInfinity\nUsage with Infinity:\ndocker run --gpus all -v $PWD/data:/app/.cache -e HF_TOKEN=$HF_TOKEN -p \"7997\":\"7997\" \\\nmichaelf34/infinity:0.0.68 \\\nv2 --model-id intfloat/multilingual-e5-large-instruct --revision \"main\" --dtype float16 --batch-size 32 --engine torch --port 7997\nSupported Languages\nThis model is initialized from xlm-roberta-large\nand continually trained on a mixture of multilingual datasets.\nIt supports 100 languages from xlm-roberta,\nbut low-resource languages may see performance degradation.\nTraining Details\nInitialization: xlm-roberta-large\nFirst stage: contrastive pre-training with 1 billion weakly supervised text pairs.\nSecond stage: fine-tuning on datasets from the E5-mistral paper.\nMTEB Benchmark Evaluation\nCheck out unilm/e5 to reproduce evaluation results\non the BEIR and MTEB benchmark.\nFAQ\n1. Do I need to add instructions to the query?\nYes, this is how the model is trained, otherwise you will see a performance degradation.\nThe task definition should be a one-sentence instruction that describes the task.\nThis is a way to customize text embeddings for different scenarios through natural language instructions.\nPlease check out unilm/e5/utils.py for instructions we used for evaluation.\nOn the other hand, there is no need to add instructions to the document side.\n2. Why are my reproduced results slightly different from reported in the model card?\nDifferent versions of transformers and pytorch could cause negligible but non-zero performance differences.\n3. Why does the cosine similarity scores distribute around 0.7 to 1.0?\nThis is a known and expected behavior as we use a low temperature 0.01 for InfoNCE contrastive loss.\nFor text embedding tasks like text retrieval or semantic similarity,\nwhat matters is the relative order of the scores instead of the absolute values,\nso this should not be an issue.\nCitation\nIf you find our paper or models helpful, please consider cite as follows:\n@article{wang2024multilingual,\ntitle={Multilingual E5 Text Embeddings: A Technical Report},\nauthor={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\njournal={arXiv preprint arXiv:2402.05672},\nyear={2024}\n}\nLimitations\nLong texts will be truncated to at most 512 tokens.",
    "google/gemma-7b": "Access Gemma on Hugging Face\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nTo access Gemma on Hugging Face, you‚Äôre required to review and agree to Google‚Äôs usage license. To do this, please ensure you‚Äôre logged-in to Hugging Face and click below. Requests are processed immediately.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nGemma Model Card\nModel Information\nDescription\nContext Length\nUsage\nInputs and outputs\nModel Data\nTraining Dataset\nData Preprocessing\nImplementation Information\nHardware\nSoftware\nEvaluation\nBenchmark Results\nEthics and Safety\nEvaluation Approach\nEvaluation Results\nUsage and Limitations\nIntended Usage\nLimitations\nEthical Considerations and Risks\nBenefits\nGemma Model Card\nModel Page: Gemma\nThis model card corresponds to the 7B base version of the Gemma model. You can also visit the model card of the 2B base model, 7B instruct model, and 2B instruct model.\nResources and Technical Documentation:\nGemma Technical Report\nResponsible Generative AI Toolkit\nGemma on Kaggle\nGemma on Vertex Model Garden\nTerms of Use: Terms\nAuthors: Google\nModel Information\nSummary description and brief definition of inputs and outputs.\nDescription\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights, pre-trained variants, and instruction-tuned variants. Gemma\nmodels are well-suited for a variety of text generation tasks, including\nquestion answering, summarization, and reasoning. Their relatively small size\nmakes it possible to deploy them in environments with limited resources such as\na laptop, desktop or your own cloud infrastructure, democratizing access to\nstate of the art AI models and helping foster innovation for everyone.\nContext Length\nModels are trained on a context length of 8192 tokens.\nUsage\nBelow we share some code snippets on how to get quickly started with running the model. First make sure to pip install -U transformers, then copy the snippet from the section that is relevant for your usecase.\nFine-tuning examples\nYou can find fine-tuning notebooks under the examples/ directory. We provide:\nA script to perform Supervised Fine-Tuning (SFT) on UltraChat dataset using QLoRA\nA script to perform SFT using FSDP on TPU devices\nA notebook that you can run on a free-tier Google Colab instance to perform SFT on English quotes dataset. You can also find the copy of the notebook here.\nRunning the model on a CPU\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\")\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a single / multi GPU\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", device_map=\"auto\")\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nRunning the model on a GPU using different precisions\nUsing torch.float16\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", device_map=\"auto\", revision=\"float16\")\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nUsing torch.bfloat16\n# pip install accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", device_map=\"auto\", torch_dtype=torch.bfloat16)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nQuantized Versions through bitsandbytes\nUsing 8-bit precision (int8)\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", quantization_config=quantization_config)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nUsing 4-bit precision\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\nmodel = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", quantization_config=quantization_config)\ninput_text = \"Write me a poem about Machine Learning.\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0]))\nOther optimizations\nFlash Attention 2\nFirst make sure to install flash-attn in your environment pip install flash-attn\nmodel = AutoModelForCausalLM.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\n+   attn_implementation=\"flash_attention_2\"\n).to(0)\nInputs and outputs\nInput: Text string, such as a question, a prompt, or a document to be\nsummarized.\nOutput: Generated English-language text in response to the input, such\nas an answer to a question, or a summary of a document.\nModel Data\nData used for model training and how the data was processed.\nTraining Dataset\nThese models were trained on a dataset of text data that includes a wide variety\nof sources, totaling 6 trillion tokens. Here are the key components:\nWeb Documents: A diverse collection of web text ensures the model is exposed\nto a broad range of linguistic styles, topics, and vocabulary. Primarily\nEnglish-language content.\nCode: Exposing the model to code helps it to learn the syntax and patterns of\nprogramming languages, which improves its ability to generate code or\nunderstand code-related questions.\nMathematics: Training on mathematical text helps the model learn logical\nreasoning, symbolic representation, and to address mathematical queries.\nThe combination of these diverse data sources is crucial for training a powerful\nlanguage model that can handle a wide variety of different tasks and text\nformats.\nData Preprocessing\nHere are the key data cleaning and filtering methods applied to the training\ndata:\nCSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was\napplied at multiple stages in the data preparation process to ensure the\nexclusion of harmful and illegal content\nSensitive Data Filtering: As part of making Gemma pre-trained models safe and\nreliable, automated techniques were used to filter out certain personal\ninformation and other sensitive data from training sets.\nAdditional methods: Filtering based on content quality and safely in line with\nour policies.\nImplementation Information\nDetails about the model internals.\nHardware\nGemma was trained using the latest generation of\nTensor Processing Unit (TPU) hardware (TPUv5e).\nTraining large language models requires significant computational power. TPUs,\ndesigned specifically for matrix operations common in machine learning, offer\nseveral advantages in this domain:\nPerformance: TPUs are specifically designed to handle the massive computations\ninvolved in training LLMs. They can speed up training considerably compared to\nCPUs.\nMemory: TPUs often come with large amounts of high-bandwidth memory, allowing\nfor the handling of large models and batch sizes during training. This can\nlead to better model quality.\nScalability: TPU Pods (large clusters of TPUs) provide a scalable solution for\nhandling the growing complexity of large foundation models. You can distribute\ntraining across multiple TPU devices for faster and more efficient processing.\nCost-effectiveness: In many scenarios, TPUs can provide a more cost-effective\nsolution for training large models compared to CPU-based infrastructure,\nespecially when considering the time and resources saved due to faster\ntraining.\nThese advantages are aligned with\nGoogle's commitments to operate sustainably.\nSoftware\nTraining was done using JAX and ML Pathways.\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\nML Pathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\nfoundation models, including large language models like\nthese ones.\nTogether, JAX and ML Pathways are used as described in the\npaper about the Gemini family of models; \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\nEvaluation\nModel evaluation metrics and results.\nBenchmark Results\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\nBenchmark\nMetric\n2B Params\n7B Params\nMMLU\n5-shot, top-1\n42.3\n64.3\nHellaSwag\n0-shot\n71.4\n81.2\nPIQA\n0-shot\n77.3\n81.2\nSocialIQA\n0-shot\n49.7\n51.8\nBooIQ\n0-shot\n69.4\n83.2\nWinoGrande\npartial score\n65.4\n72.3\nCommonsenseQA\n7-shot\n65.3\n71.3\nOpenBookQA\n47.8\n52.8\nARC-e\n73.2\n81.5\nARC-c\n42.1\n53.2\nTriviaQA\n5-shot\n53.2\n63.4\nNatural Questions\n5-shot\n12.5\n23\nHumanEval\npass@1\n22.0\n32.3\nMBPP\n3-shot\n29.2\n44.4\nGSM8K\nmaj@1\n17.7\n46.4\nMATH\n4-shot\n11.8\n24.3\nAGIEval\n24.2\n41.7\nBIG-Bench\n35.2\n55.1\n------------------------------\n-------------\n-----------\n---------\nAverage\n45.0\n56.9\nEthics and Safety\nEthics and safety evaluation approach and results.\nEvaluation Approach\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\nText-to-Text Content Safety: Human evaluation on prompts covering safety\npolicies including child sexual abuse and exploitation, harassment, violence\nand gore, and hate speech.\nText-to-Text Representational Harms: Benchmark against relevant academic\ndatasets such as WinoBias and BBQ Dataset.\nMemorization: Automated evaluation of memorization of training data, including\nthe risk of personally identifiable information exposure.\nLarge-scale harm: Tests for \"dangerous capabilities,\" such as chemical,\nbiological, radiological, and nuclear (CBRN) risks.\nEvaluation Results\nThe results of ethics and safety evaluations are within acceptable thresholds\nfor meeting internal policies for categories such as child\nsafety, content safety, representational harms, memorization, large-scale harms.\nOn top of robust internal evaluations, the results of well known safety\nbenchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA\nare shown here.\nBenchmark\nMetric\n2B Params\n7B Params\nRealToxicity\naverage\n6.86\n7.90\nBOLD\n45.57\n49.08\nCrowS-Pairs\ntop-1\n45.82\n51.33\nBBQ Ambig\n1-shot, top-1\n62.58\n92.54\nBBQ Disambig\ntop-1\n54.62\n71.99\nWinogender\ntop-1\n51.25\n54.17\nTruthfulQA\n44.84\n31.81\nWinobias 1_2\n56.12\n59.09\nWinobias 2_2\n91.10\n92.23\nToxigen\n29.77\n39.59\n------------------------------\n-------------\n-----------\n---------\nUsage and Limitations\nThese models have certain limitations that users should be aware of.\nIntended Usage\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\nContent Creation and Communication\nText Generation: These models can be used to generate creative text formats\nsuch as poems, scripts, code, marketing copy, and email drafts.\nChatbots and Conversational AI: Power conversational interfaces for customer\nservice, virtual assistants, or interactive applications.\nText Summarization: Generate concise summaries of a text corpus, research\npapers, or reports.\nResearch and Education\nNatural Language Processing (NLP) Research: These models can serve as a\nfoundation for researchers to experiment with NLP techniques, develop\nalgorithms, and contribute to the advancement of the field.\nLanguage Learning Tools: Support interactive language learning experiences,\naiding in grammar correction or providing writing practice.\nKnowledge Exploration: Assist researchers in exploring large bodies of text\nby generating summaries or answering questions about specific topics.\nLimitations\nTraining Data\nThe quality and diversity of the training data significantly influence the\nmodel's capabilities. Biases or gaps in the training data can lead to\nlimitations in the model's responses.\nThe scope of the training dataset determines the subject areas the model can\nhandle effectively.\nContext and Task Complexity\nLLMs are better at tasks that can be framed with clear prompts and\ninstructions. Open-ended or highly complex tasks might be challenging.\nA model's performance can be influenced by the amount of context provided\n(longer context generally leads to better outputs, up to a certain point).\nLanguage Ambiguity and Nuance\nNatural language is inherently complex. LLMs might struggle to grasp subtle\nnuances, sarcasm, or figurative language.\nFactual Accuracy\nLLMs generate responses based on information they learned from their\ntraining datasets, but they are not knowledge bases. They may generate\nincorrect or outdated factual statements.\nCommon Sense\nLLMs rely on statistical patterns in language. They might lack the ability\nto apply common sense reasoning in certain situations.\nEthical Considerations and Risks\nThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:\nBias and Fairness\nLLMs trained on large-scale, real-world text data can reflect socio-cultural\nbiases embedded in the training material. These models underwent careful\nscrutiny, input data pre-processing described and posterior evaluations\nreported in this card.\nMisinformation and Misuse\nLLMs can be misused to generate text that is false, misleading, or harmful.\nGuidelines are provided for responsible use with the model, see the\nResponsible Generative AI Toolkit.\nTransparency and Accountability:\nThis model card summarizes details on the models' architecture,\ncapabilities, limitations, and evaluation processes.\nA responsibly developed open model offers the opportunity to share\ninnovation by making LLM technology accessible to developers and researchers\nacross the AI ecosystem.\nRisks identified and mitigations:\nPerpetuation of biases: It's encouraged to perform continuous monitoring\n(using evaluation metrics, human review) and the exploration of de-biasing\ntechniques during model training, fine-tuning, and other use cases.\nGeneration of harmful content: Mechanisms and guidelines for content safety\nare essential. Developers are encouraged to exercise caution and implement\nappropriate content safety safeguards based on their specific product policies\nand application use cases.\nMisuse for malicious purposes: Technical limitations and developer and\nend-user education can help mitigate against malicious applications of LLMs.\nEducational resources and reporting mechanisms for users to flag misuse are\nprovided. Prohibited uses of Gemma models are outlined in the\nGemma Prohibited Use Policy.\nPrivacy violations: Models were trained on data filtered for removal of PII\n(Personally Identifiable Information). Developers are encouraged to adhere to\nprivacy regulations with privacy-preserving techniques.\nBenefits\nAt the time of release, this family of models provides high-performance open\nlarge language model implementations designed from the ground up for Responsible\nAI development compared to similarly sized models.\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.",
    "llava-hf/llava-v1.6-mistral-7b-hf": "LLaVa-Next, leveraging mistralai/Mistral-7B-Instruct-v0.2 as LLM\nModel description\nIntended uses & limitations\nHow to use\nModel optimization\nBibTeX entry and citation info\nLLaVa-Next, leveraging mistralai/Mistral-7B-Instruct-v0.2 as LLM\nThe LLaVA-NeXT model was proposed in LLaVA-NeXT: Improved reasoning, OCR, and world knowledge by Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee. LLaVa-NeXT (also called LLaVa-1.6) improves upon LLaVa-1.5 by increasing the input image resolution and training on an improved visual instruction tuning dataset to improve OCR and common sense reasoning.\nDisclaimer: The team releasing LLaVa-NeXT did not write a model card for this model so this model card has been written by the Hugging Face team.\nModel description\nLLaVa combines a pre-trained large language model with a pre-trained vision encoder for multimodal chatbot use cases. LLaVA 1.6 improves on LLaVA 1.5 BY:\nUsing Mistral-7B (for this checkpoint) and Nous-Hermes-2-Yi-34B which has better commercial licenses,\nand bilingual support\nMore diverse and high quality data mixture\nDynamic high resolution\nIntended uses & limitations\nYou can use the raw model for tasks like image captioning, visual question answering, multimodal chatbot use cases. See the model hub to look for\nother versions on a task that interests you.\nHow to use\nHere's the prompt template for this model but we recomment to use the chat templates to format the prompt with processor.apply_chat_template().\nThat will apply the correct template for a given checkpoint for you.\n\"[INST] <image>\\nWhat is shown in this image? [/INST]\"\nTo run the model with the pipeline, see the below example:\nfrom transformers import pipeline\npipe = pipeline(\"image-text-to-text\", model=\"llava-hf/llava-v1.6-mistral-7b-hf\")\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"},\n{\"type\": \"text\", \"text\": \"What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud\"},\n],\n},\n]\nout = pipe(text=messages, max_new_tokens=20)\nprint(out)\n>>> [{'input_text': [{'role': 'user', 'content': [{'type': 'image', 'url': 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg'}, {'type': 'text', 'text': 'What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud'}]}], 'generated_text': 'Lava'}]\nYou can also load and use the model like following:\nfrom transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\nimport torch\nfrom PIL import Image\nimport requests\nprocessor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\nmodel = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\nmodel.to(\"cuda:0\")\n# prepare image and text prompt, using the appropriate prompt template\nurl = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\nimage = Image.open(requests.get(url, stream=True).raw)\n# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\")\nconversation = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n{\"type\": \"image\"},\n],\n},\n]\nprompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\ninputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda:0\")\n# autoregressively complete prompt\noutput = model.generate(**inputs, max_new_tokens=100)\nprint(processor.decode(output[0], skip_special_tokens=True))\nFrom transformers>=v4.48, you can also pass image url or local path to the conversation history, and let the chat template handle the rest.\nChat template will load the image for you and return inputs in torch.Tensor which you can pass directly to model.generate()\nmessages = [\n{\n\"role\": \"user\",\n\"content\": [\n{\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n{\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n],\n},\n]\ninputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\")\noutput = model.generate(**inputs, max_new_tokens=50)\nModel optimization\n4-bit quantization through bitsandbytes library\nFirst make sure to install bitsandbytes, pip install bitsandbytes and make sure to have access to a CUDA compatible GPU device. Simply change the snippet above with:\nmodel = LlavaNextForConditionalGeneration.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\nlow_cpu_mem_usage=True,\n+   load_in_4bit=True\n)\nUse Flash-Attention 2 to further speed-up generation\nFirst make sure to install flash-attn. Refer to the original repository of Flash Attention regarding that package installation. Simply change the snippet above with:\nmodel = LlavaNextForConditionalGeneration.from_pretrained(\nmodel_id,\ntorch_dtype=torch.float16,\nlow_cpu_mem_usage=True,\n+   use_flash_attention_2=True\n).to(0)\nBibTeX entry and citation info\n@misc{liu2023improved,\ntitle={Improved Baselines with Visual Instruction Tuning},\nauthor={Haotian Liu and Chunyuan Li and Yuheng Li and Yong Jae Lee},\nyear={2023},\neprint={2310.03744},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "bigcode/starcoder2-7b": "StarCoder2\nTable of Contents\nModel Summary\nUse\nIntended use\nGeneration\nAttribution & Other Requirements\nLimitations\nTraining\nModel\nHardware\nSoftware\nLicense\nCitation\nStarCoder2\nTable of Contents\nModel Summary\nUse\nLimitations\nTraining\nLicense\nCitation\nModel Summary\nStarCoder2-7B model is a 7B parameter model trained on 17 programming languages from The Stack v2, with opt-out requests excluded. The model uses Grouped Query Attention, a context window of 16,384 tokens with a sliding window attention of 4,096 tokens,  and was trained using the Fill-in-the-Middle objective on 3.5+ trillion tokens.\nProject Website: bigcode-project.org\nPaper: Link\nPoint of Contact: contact@bigcode-project.org\nLanguages: 17 Programming languages\nUse\nIntended use\nThe model was trained on GitHub code as well as additional selected data sources such as Arxiv and Wikipedia. As such it is not an instruction model and commands like \"Write a function that computes the square root.\" do not work well.\nGeneration\nHere are some examples to get started with the model. You can find a script for fine-tuning in StarCoder2's GitHub repository.\nFirst, make sure to install transformers from source:\npip install git+https://github.com/huggingface/transformers.git\nRunning the model on CPU/GPU/multi GPU\nUsing full precision\n# pip install git+https://github.com/huggingface/transformers.git # TODO: merge PR to main\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ncheckpoint = \"bigcode/starcoder2-7b\"\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\nMemory footprint: 29232.57 MB\nUsing torch.bfloat16\n# pip install accelerate\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ncheckpoint = \"bigcode/starcoder2-7b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for fp16 use `torch_dtype=torch.float16` instead\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", torch_dtype=torch.bfloat16)\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\nMemory footprint: 14616.29 MB\nQuantized Versions through bitsandbytes\nUsing 8-bit precision (int8)\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n# to use 4bit use `load_in_4bit=True` instead\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\ncheckpoint = \"bigcode/starcoder2-7b\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, quantization_config=quantization_config)\ninputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\n# load_in_8bit\nMemory footprint: 7670.52 MB\n# load_in_4bit\n>>> print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\nMemory footprint: 4197.64 MB\nAttribution & Other Requirements\nThe pretraining dataset of the model was filtered for permissive licenses and code with no license only. Nevertheless, the model can generate source code verbatim from the dataset. The code's license might require attribution and/or other specific requirements that must be respected. We provide a search index that lets you search through the pretraining data to identify where the generated code came from and apply the proper attribution to your code.\nLimitations\nThe model has been trained on source code from 17 programming languages. The predominant language in source is English although other languages are also present. As such the model is capable of generating code snippets provided some context but the generated code is not guaranteed to work as intended. It can be inefficient and contain bugs or exploits. See the paper for an in-depth discussion of the model limitations.\nTraining\nModel\nArchitecture: Transformer decoder with grouped-query and sliding window attention and Fill-in-the-Middle objective\nPretraining steps: 1 million\nPretraining tokens: 3.5+ trillion\nPrecision: bfloat16\nHardware\nGPUs: 432 H100\nSoftware\nFramework: nanotron\nNeural networks: PyTorch\nLicense\nThe model is licensed under the BigCode OpenRAIL-M v1 license agreement. You can find the full agreement here.\nCitation\n@misc{lozhkov2024starcoder,\ntitle={StarCoder 2 and The Stack v2: The Next Generation},\nauthor={Anton Lozhkov and Raymond Li and Loubna Ben Allal and Federico Cassano and Joel Lamy-Poirier and Nouamane Tazi and Ao Tang and Dmytro Pykhtar and Jiawei Liu and Yuxiang Wei and Tianyang Liu and Max Tian and Denis Kocetkov and Arthur Zucker and Younes Belkada and Zijian Wang and Qian Liu and Dmitry Abulkhanov and Indraneil Paul and Zhuang Li and Wen-Ding Li and Megan Risdal and Jia Li and Jian Zhu and Terry Yue Zhuo and Evgenii Zheltonozhskii and Nii Osae Osae Dade and Wenhao Yu and Lucas Krau√ü and Naman Jain and Yixuan Su and Xuanli He and Manan Dey and Edoardo Abati and Yekun Chai and Niklas Muennighoff and Xiangru Tang and Muhtasham Oblokulov and Christopher Akiki and Marc Marone and Chenghao Mou and Mayank Mishra and Alex Gu and Binyuan Hui and Tri Dao and Armel Zebaze and Olivier Dehaene and Nicolas Patry and Canwen Xu and Julian McAuley and Han Hu and Torsten Scholak and Sebastien Paquet and Jennifer Robinson and Carolyn Jane Anderson and Nicolas Chapados and Mostofa Patwary and Nima Tajbakhsh and Yacine Jernite and Carlos Mu√±oz Ferrandis and Lingming Zhang and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},\nyear={2024},\neprint={2402.19173},\narchivePrefix={arXiv},\nprimaryClass={cs.SE}\n}",
    "ByteDance/SDXL-Lightning": "SDXL-Lightning\nDemos\nCheckpoints\nDiffusers Usage\n2-Step, 4-Step, 8-Step UNet\n2-Step, 4-Step, 8-Step LoRA\n1-Step UNet\nComfyUI Usage\n2-Step, 4-Step, 8-Step Full\n2-Step, 4-Step, 8-Step LoRA\n1-Step\nCite Our Work\nSDXL-Lightning\nSDXL-Lightning is a lightning-fast text-to-image generation model. It can generate high-quality 1024px images in a few steps. For more information, please refer to our research paper: SDXL-Lightning: Progressive Adversarial Diffusion Distillation. We open-source the model as part of the research.\nOur models are distilled from stabilityai/stable-diffusion-xl-base-1.0. This repository contains checkpoints for 1-step, 2-step, 4-step, and 8-step distilled models. The generation quality of our 2-step, 4-step, and 8-step model is amazing. Our 1-step model is more experimental.\nWe provide both full UNet and LoRA checkpoints. The full UNet models have the best quality while the LoRA models can be applied to other base models.\nDemos\nGenerate with all configurations, best quality: Demo\nCheckpoints\nsdxl_lightning_Nstep.safetensors: All-in-one checkpoint, for ComfyUI.\nsdxl_lightning_Nstep_unet.safetensors: UNet checkpoint only, for Diffusers.\nsdxl_lightning_Nstep_lora.safetensors: LoRA checkpoint, for Diffusers and ComfyUI.\nDiffusers Usage\nPlease always use the correct checkpoint for the corresponding inference steps.\n2-Step, 4-Step, 8-Step UNet\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, EulerDiscreteScheduler\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\nbase = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo = \"ByteDance/SDXL-Lightning\"\nckpt = \"sdxl_lightning_4step_unet.safetensors\" # Use the correct ckpt for your step setting!\n# Load model.\nunet = UNet2DConditionModel.from_config(base, subfolder=\"unet\").to(\"cuda\", torch.float16)\nunet.load_state_dict(load_file(hf_hub_download(repo, ckpt), device=\"cuda\"))\npipe = StableDiffusionXLPipeline.from_pretrained(base, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n# Ensure sampler uses \"trailing\" timesteps.\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n# Ensure using the same inference steps as the loaded model and CFG set to 0.\npipe(\"A girl smiling\", num_inference_steps=4, guidance_scale=0).images[0].save(\"output.png\")\n2-Step, 4-Step, 8-Step LoRA\nUse LoRA only if you are using non-SDXL base models. Otherwise use our UNet checkpoint for better quality.\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, EulerDiscreteScheduler\nfrom huggingface_hub import hf_hub_download\nbase = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo = \"ByteDance/SDXL-Lightning\"\nckpt = \"sdxl_lightning_4step_lora.safetensors\" # Use the correct ckpt for your step setting!\n# Load model.\npipe = StableDiffusionXLPipeline.from_pretrained(base, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\npipe.load_lora_weights(hf_hub_download(repo, ckpt))\npipe.fuse_lora()\n# Ensure sampler uses \"trailing\" timesteps.\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n# Ensure using the same inference steps as the loaded model and CFG set to 0.\npipe(\"A girl smiling\", num_inference_steps=4, guidance_scale=0).images[0].save(\"output.png\")\n1-Step UNet\nThe 1-step model is only experimental and the quality is much less stable. Consider using the 2-step model for much better quality.\nThe 1-step model uses \"sample\" prediction instead of \"epsilon\" prediction! The scheduler needs to be configured correctly.\nimport torch\nfrom diffusers import StableDiffusionXLPipeline, UNet2DConditionModel, EulerDiscreteScheduler\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\nbase = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo = \"ByteDance/SDXL-Lightning\"\nckpt = \"sdxl_lightning_1step_unet_x0.safetensors\" # Use the correct ckpt for your step setting!\n# Load model.\nunet = UNet2DConditionModel.from_config(base, subfolder=\"unet\").to(\"cuda\", torch.float16)\nunet.load_state_dict(load_file(hf_hub_download(repo, ckpt), device=\"cuda\"))\npipe = StableDiffusionXLPipeline.from_pretrained(base, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n# Ensure sampler uses \"trailing\" timesteps and \"sample\" prediction type.\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\", prediction_type=\"sample\")\n# Ensure using the same inference steps as the loaded model and CFG set to 0.\npipe(\"A girl smiling\", num_inference_steps=1, guidance_scale=0).images[0].save(\"output.png\")\nComfyUI Usage\nPlease always use the correct checkpoint for the corresponding inference steps.\nPlease use Euler sampler with sgm_uniform scheduler.\n2-Step, 4-Step, 8-Step Full\nDownload the full checkpoint (sdxl_lightning_Nstep.safetensors) to /ComfyUI/models/checkpoints.\nDownload our ComfyUI full workflow.\n2-Step, 4-Step, 8-Step LoRA\nUse LoRA only if you are using non-SDXL base models. Otherwise use our full checkpoint for better quality.\nPrepare your own base model.\nDownload the LoRA checkpoint (sdxl_lightning_Nstep_lora.safetensors) to /ComfyUI/models/loras\nDownload our ComfyUI LoRA workflow.\n1-Step\nThe 1-step model is only experimental and the quality is much less stable. Consider using the 2-step model for much better quality.\nUpdate your ComfyUI to the latest version.\nDownload the full checkpoint (sdxl_lightning_1step_x0.safetensors) to /ComfyUI/models/checkpoints.\nDownload our ComfyUI full 1-step workflow.\nCite Our Work\n@misc{lin2024sdxllightning,\ntitle={SDXL-Lightning: Progressive Adversarial Diffusion Distillation},\nauthor={Shanchuan Lin and Anran Wang and Xiao Yang},\nyear={2024},\neprint={2402.13929},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "mixedbread-ai/mxbai-embed-large-v1": "mixedbread-ai/mxbai-embed-large-v1\nQuickstart\nsentence-transformers\nTransformers\nTransformers.js\nUsing API\nInfinity\nEvaluation\nMatryoshka and Binary Quantization\nCommunity\nLicense\nCitation\nThe crispy sentence embedding family from Mixedbread.\nüçû Looking for a simple end-to-end retrieval solution? Meet Omni, our multimodal and multilingual model. Get in touch for access.\nmixedbread-ai/mxbai-embed-large-v1\nHere, we provide several ways to produce sentence embeddings. Please note that you have to provide the prompt Represent this sentence for searching relevant passages: for query if you want to use it for retrieval. Besides that you don't need any prompt. Our model also supports Matryoshka Representation Learning and binary quantization.\nQuickstart\nHere, we provide several ways to produce sentence embeddings. Please note that you have to provide the prompt Represent this sentence for searching relevant passages:  for query if you want to use it for retrieval. Besides that you don't need any prompt.\nsentence-transformers\npython -m pip install -U sentence-transformers\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import cos_sim\nfrom sentence_transformers.quantization import quantize_embeddings\n# 1. Specify preffered dimensions\ndimensions = 512\n# 2. load model\nmodel = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim=dimensions)\n# The prompt used for query retrieval tasks:\n# query_prompt = 'Represent this sentence for searching relevant passages: '\nquery = \"A man is eating a piece of bread\"\ndocs = [\n\"A man is eating food.\",\n\"A man is eating pasta.\",\n\"The girl is carrying a baby.\",\n\"A man is riding a horse.\",\n]\n# 2. Encode\nquery_embedding = model.encode(query, prompt_name=\"query\")\n# Equivalent Alternatives:\n# query_embedding = model.encode(query_prompt + query)\n# query_embedding = model.encode(query, prompt=query_prompt)\ndocs_embeddings = model.encode(docs)\n# Optional: Quantize the embeddings\nbinary_query_embedding = quantize_embeddings(query_embedding, precision=\"ubinary\")\nbinary_docs_embeddings = quantize_embeddings(docs_embeddings, precision=\"ubinary\")\nsimilarities = cos_sim(query_embedding, docs_embeddings)\nprint('similarities:', similarities)\nTransformers\nfrom typing import Dict\nimport torch\nimport numpy as np\nfrom transformers import AutoModel, AutoTokenizer\nfrom sentence_transformers.util import cos_sim\n# For retrieval you need to pass this prompt. Please find our more in our blog post.\ndef transform_query(query: str) -> str:\n\"\"\" For retrieval, add the prompt for query (not for documents).\n\"\"\"\nreturn f'Represent this sentence for searching relevant passages: {query}'\n# The model works really well with cls pooling (default) but also with mean pooling.\ndef pooling(outputs: torch.Tensor, inputs: Dict,  strategy: str = 'cls') -> np.ndarray:\nif strategy == 'cls':\noutputs = outputs[:, 0]\nelif strategy == 'mean':\noutputs = torch.sum(\noutputs * inputs[\"attention_mask\"][:, :, None], dim=1) / torch.sum(inputs[\"attention_mask\"], dim=1, keepdim=True)\nelse:\nraise NotImplementedError\nreturn outputs.detach().cpu().numpy()\n# 1. load model\nmodel_id = 'mixedbread-ai/mxbai-embed-large-v1'\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModel.from_pretrained(model_id).cuda()\ndocs = [\ntransform_query('A man is eating a piece of bread'),\n\"A man is eating food.\",\n\"A man is eating pasta.\",\n\"The girl is carrying a baby.\",\n\"A man is riding a horse.\",\n]\n# 2. encode\ninputs = tokenizer(docs, padding=True, return_tensors='pt')\nfor k, v in inputs.items():\ninputs[k] = v.cuda()\noutputs = model(**inputs).last_hidden_state\nembeddings = pooling(outputs, inputs, 'cls')\nsimilarities = cos_sim(embeddings[0], embeddings[1:])\nprint('similarities:', similarities)\nTransformers.js\nIf you haven't already, you can install the Transformers.js JavaScript library from NPM using:\nnpm i @xenova/transformers\nYou can then use the model to compute embeddings like this:\nimport { pipeline, cos_sim } from '@xenova/transformers';\n// Create a feature extraction pipeline\nconst extractor = await pipeline('feature-extraction', 'mixedbread-ai/mxbai-embed-large-v1', {\nquantized: false, // Comment out this line to use the quantized version\n});\n// Generate sentence embeddings\nconst docs = [\n'Represent this sentence for searching relevant passages: A man is eating a piece of bread',\n'A man is eating food.',\n'A man is eating pasta.',\n'The girl is carrying a baby.',\n'A man is riding a horse.',\n]\nconst output = await extractor(docs, { pooling: 'cls' });\n// Compute similarity scores\nconst [source_embeddings, ...document_embeddings ] = output.tolist();\nconst similarities = document_embeddings.map(x => cos_sim(source_embeddings, x));\nconsole.log(similarities); // [0.7919578577247139, 0.6369278664248345, 0.16512018371357193, 0.3620778366720027]\nUsing API\nYou can use the model via our API as follows:\nfrom mixedbread_ai.client import MixedbreadAI, EncodingFormat\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport os\nmxbai = MixedbreadAI(api_key=\"{MIXEDBREAD_API_KEY}\")\nenglish_sentences = [\n'What is the capital of Australia?',\n'Canberra is the capital of Australia.'\n]\nres = mxbai.embeddings(\ninput=english_sentences,\nmodel=\"mixedbread-ai/mxbai-embed-large-v1\",\nnormalized=True,\nencoding_format=[EncodingFormat.FLOAT, EncodingFormat.UBINARY, EncodingFormat.INT_8],\ndimensions=512\n)\nencoded_embeddings = res.data[0].embedding\nprint(res.dimensions, encoded_embeddings.ubinary, encoded_embeddings.float_, encoded_embeddings.int_8)\nThe API comes with native int8 and binary quantization support! Check out the docs for more information.\nInfinity\ndocker run --gpus all -v $PWD/data:/app/.cache -p \"7997\":\"7997\" \\\nmichaelf34/infinity:0.0.68 \\\nv2 --model-id mixedbread-ai/mxbai-embed-large-v1 --revision \"main\" --dtype float16 --engine torch --port 7997\nEvaluation\nAs of March 2024, our model archives SOTA performance for Bert-large sized models on the MTEB. It ourperforms commercial models like OpenAIs text-embedding-3-large and matches the performance of model 20x it's size like the echo-mistral-7b. Our model was trained with no overlap of the MTEB data, which indicates that our model generalizes well across several domains, tasks and text length. We know there are some limitations with this model, which will be fixed in v2.\nModel\nAvg (56 datasets)\nClassification (12 datasets)\nClustering (11 datasets)\nPairClassification (3 datasets)\nReranking (4 datasets)\nRetrieval (15 datasets)\nSTS (10 datasets)\nSummarization (1 dataset)\nmxbai-embed-large-v1\n64.68\n75.64\n46.71\n87.2\n60.11\n54.39\n85.00\n32.71\nbge-large-en-v1.5\n64.23\n75.97\n46.08\n87.12\n60.03\n54.29\n83.11\n31.61\nmxbai-embed-2d-large-v1\n63.25\n74.14\n46.07\n85.89\n58.94\n51.42\n84.9\n31.55\nnomic-embed-text-v1\n62.39\n74.12\n43.91\n85.15\n55.69\n52.81\n82.06\n30.08\njina-embeddings-v2-base-en\n60.38\n73.45\n41.73\n85.38\n56.98\n47.87\n80.7\n31.6\nProprietary Models\nOpenAI text-embedding-3-large\n64.58\n75.45\n49.01\n85.72\n59.16\n55.44\n81.73\n29.92\nCohere embed-english-v3.0\n64.47\n76.49\n47.43\n85.84\n58.01\n55.00\n82.62\n30.18\nOpenAI text-embedding-ada-002\n60.99\n70.93\n45.90\n84.89\n56.32\n49.25\n80.97\n30.80\nPlease find more information in our blog post.\nMatryoshka and Binary Quantization\nEmbeddings in their commonly used form (float arrays) have a high memory footprint when used at scale. Two approaches to solve this problem are Matryoshka Representation Learning (MRL) and (Binary) Quantization. While MRL reduces the number of dimensions of an embedding, binary quantization transforms the value of each dimension from a float32 into a lower precision (int8 or even binary).  The model supports both approaches!\nYou can also take it one step further, and combine both MRL and quantization. This combination of binary quantization and MRL allows you to reduce the memory usage of your embeddings significantly. This leads to much lower costs when using a vector database in particular. You can read more about the technology and its advantages in our blog post.\nCommunity\nPlease join our Discord Community and share your feedback and thoughts! We are here to help and also always happy to chat.\nLicense\nApache 2.0\nCitation\n@online{emb2024mxbai,\ntitle={Open Source Strikes Bread - New Fluffy Embeddings Model},\nauthor={Sean Lee and Aamir Shakir and Darius Koenig and Julius Lipp},\nyear={2024},\nurl={https://www.mixedbread.ai/blog/mxbai-embed-large-v1},\n}\n@article{li2023angle,\ntitle={AnglE-optimized Text Embeddings},\nauthor={Li, Xianming and Li, Jing},\njournal={arXiv preprint arXiv:2309.12871},\nyear={2023}\n}",
    "naver/splade-v3": "You need to agree to share your contact information to access this model\nThis repository is publicly accessible, but\nyou have to accept the conditions to access its files and content.\nLog in\nor\nSign Up\nto review the conditions and access this model content.\nSPLADE-v3\nSPLADE-v3 is the latest series of SPLADE models.\nPerformance\nModel Details\nModel Description\nFull Model Architecture\nUsage\nDirect Usage (Sentence Transformers)\nCitation\nSPLADE-v3\nSPLADE-v3 is the latest series of SPLADE models.\nThis checkpoint corresponds to a model that starts from SPLADE++SelfDistil (naver/splade-cocondenser-selfdistil), and is\ntrained with a mix of KL-Div and MarginMSE, with 8 negatives per query sampled from SPLADE++SelfDistil. We used the original MS MARCO\ncollection without the titles.\nFor more details, see our arXiv companion book: https://arxiv.org/abs/2403.06789To use SPLADE, please visit our GitHub repository: https://github.com/naver/splade\nPerformance\nMRR@10 (MS MARCO dev)\navg nDCG@10 (BEIR-13)\nnaver/splade-v3\n40.2\n51.7\nModel Details\nThis is a SPLADE Sparse Encoder model. It maps sentences & paragraphs to a 30522-dimensional sparse vector space and can be used for semantic search and sparse retrieval.\nModel Description\nModel Type: SPLADE Sparse Encoder\nBase model: naver/splade-cocondenser-selfdistil\nMaximum Sequence Length: 512 tokens (256 for evaluation reproduction)\nOutput Dimensionality: 30522 dimensions\nSimilarity Function: Dot Product\nFull Model Architecture\nSparseEncoder(\n(0): MLMTransformer({'max_seq_length': 512, 'do_lower_case': False}) with MLMTransformer model: BertForMaskedLM\n(1): SpladePooling({'pooling_strategy': 'max', 'activation_function': 'relu', 'word_embedding_dimension': 30522})\n)\nUsage\nDirect Usage (Sentence Transformers)\nFirst install the Sentence Transformers library:\npip install -U sentence-transformers\nThen you can load this model and run inference.\nfrom sentence_transformers import SparseEncoder\n# Download from the ü§ó Hub\nmodel = SparseEncoder(\"naver/splade-v3\")\n# Run inference\nqueries = [\"what causes aging fast\"]\ndocuments = [\n\"UV-A light, specifically, is what mainly causes tanning, skin aging, and cataracts, UV-B causes sunburn, skin aging and skin cancer, and UV-C is the strongest, and therefore most effective at killing microorganisms. Again √¢\\x80\\x93 single words and multiple bullets.\",\n\"Answers from Ronald Petersen, M.D. Yes, Alzheimer's disease usually worsens slowly. But its speed of progression varies, depending on a person's genetic makeup, environmental factors, age at diagnosis and other medical conditions. Still, anyone diagnosed with Alzheimer's whose symptoms seem to be progressing quickly √¢\\x80\\x94 or who experiences a sudden decline √¢\\x80\\x94 should see his or her doctor.\",\n\"Bell's palsy and Extreme tiredness and Extreme fatigue (2 causes) Bell's palsy and Extreme tiredness and Hepatitis (2 causes) Bell's palsy and Extreme tiredness and Liver pain (2 causes) Bell's palsy and Extreme tiredness and Lymph node swelling in children (2 causes)\",\n]\nquery_embeddings = model.encode_query(queries)\ndocument_embeddings = model.encode_document(documents)\nprint(query_embeddings.shape, document_embeddings.shape)\n# [1, 30522] [3, 30522]\n# Get the similarity scores for the embeddings\nsimilarities = model.similarity(query_embeddings, document_embeddings)\nprint(similarities)\n# tensor([[11.3768, 10.8296,  4.3457]])\nCitation\nIf you use our checkpoint, please cite our work:\n@misc{lassance2024spladev3,\ntitle={SPLADE-v3: New baselines for SPLADE},\nauthor={Carlos Lassance and Herv√© D√©jean and Thibault Formal and St√©phane Clinchant},\nyear={2024},\neprint={2403.06789},\narchivePrefix={arXiv},\nprimaryClass={cs.IR},\ncopyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}\n}",
    "yisol/IDM-VTON": "Check out more codes on our github repository!\nIDM-VTON : Improving Diffusion Models for Authentic Virtual Try-on in the Wild\nTODO LIST\nAcknowledgements\nCitation\nLicense\nCheck out more codes on our github repository!\nIDM-VTON : Improving Diffusion Models for Authentic Virtual Try-on in the Wild\nThis is an official implementation of paper 'Improving Diffusion Models for Authentic Virtual Try-on in the Wild'\npaper\nproject page\nü§ó Try our huggingface Demo\nTODO LIST\ndemo model\ninference code\ntraining code\nAcknowledgements\nFor the demo, GPUs are supported from zerogpu, and auto masking generation codes are based on OOTDiffusion and DCI-VTON.Parts of the code are based on IP-Adapter.\nCitation\n@article{choi2024improving,\ntitle={Improving Diffusion Models for Virtual Try-on},\nauthor={Choi, Yisol and Kwak, Sangkyung and Lee, Kyungmin and Choi, Hyungwon and Shin, Jinwoo},\njournal={arXiv preprint arXiv:2403.05139},\nyear={2024}\n}\nLicense\nThe codes and checkpoints in this repository are under the CC BY-NC-SA 4.0 license.",
    "ArmurAI/Pentest_AI": "Pentest AI\nKey Features of PentestAI:\nPentest AI\nPentestAI is an innovative assistant for penetration testing, we used the OpenHermes-2.5-Mistral-7B model, we jailbroke it, finetuned it with commands for popular Kali Linux tools and it's now able to provide guided, actionable steps and command automation for performing deep pen tests.\nThe innovative PentestAI offers a cutting-edge solution for penetration testing by leveraging the modified OpenHermes-2.5-Mistral-7B model. This model has been uniquely jailbroken and finetuned with commands tailored for the most commonly used tools in Kali Linux, enabling it to provide guided, actionable steps and automate command execution for comprehensive penetration testing.\nKey Features of PentestAI:\nGuided Penetration Testing: PentestAI simplifies the complexity of penetration testing by guiding you through each step of the process. Starting with the acquisition of the target IP, it offers customized advice tailored to the specific phase of the penetration test you are in.\nCommand Automation: Streamline your penetration testing with automated command execution. PentestAI incorporates extensive knowledge of Kali Linux tools, providing you with command examples that you can execute directly or modify as needed.\nAdaptive Learning: As you progress through your penetration testing tasks and share results, PentestAI dynamically adapts its suggestions to enhance your efficiency and effectiveness.\nEthical Framework: Throughout the testing process, PentestAI emphasizes adherence to ethical standards, ensuring that your penetration testing practices are responsible and legally compliant.\nUser-Friendly Interaction: You can interact seamlessly with PentestAI and conclude your session anytime by typing 'exit' or 'hacked' upon successfully compromising the target machine.\nBy integrating these features, PentestAI not only facilitates a smoother penetration testing experience but also ensures that it is performed efficiently and ethically. Whether you are a novice eager to learn the ropes or an experienced professional seeking to streamline your workflow, PentestAI provides valuable support tailored to your needs.\nFor more details and to access the tool, visit the GitHub repository.",
    "ByteDance/Hyper-SD": "Hyper-SD\nNewsüî•üî•üî•\nTry our Hugging Face demos:\nIntroduction\nCheckpoints\nText-to-Image Usage\nFLUX.1-dev-related models\nSD3-related models\nSDXL-related models\nSD1.5-related models\nControlNet Usage\nSDXL-related models\nSD1.5-related models\nComfyui Usage\nCitation\nHyper-SD\nOfficial Repository of the paper: Hyper-SD.\nProject Page: https://hyper-sd.github.io/\nNewsüî•üî•üî•\nAug.26, 2024. üí•üí•üí• Our 8-steps and 16-steps FLUX.1-dev-related LoRAs are available now! We recommend LoRA scales around 0.125 that is adaptive with training and guidance scale could be kept on 3.5. Lower step LoRAs would be coming soon. üí•üí•üí•\nAug.19, 2024. SD3-related CFG LoRAs are available now! We recommend setting guidance scale to 3.0/5.0/7.0 at 4/8/16-steps. Don't forget to fuse lora with a relatively small scale (e.g. 0.125 that is adaptive with training) before inference with diffusers. Note that 8-steps and 16-steps LoRA can also inference on a little bit smaller steps like 6-steps and 12-steps, respectively. Hope to hear your feedback, FLUX-related models will be coming next week.\nMay.13, 2024. The 12-Steps CFG-Preserved Hyper-SDXL-12steps-CFG-LoRA and Hyper-SD15-12steps-CFG-LoRA is also available now(support 5~8 guidance scales), this could be more practical with better trade-off between performance and speed. Enjoy!\nApr.30, 2024. Our 8-Steps CFG-Preserved Hyper-SDXL-8steps-CFG-LoRA and Hyper-SD15-8steps-CFG-LoRA is available now(support 5~8 guidance scales), we strongly recommend making the 8-step CFGLora a standard configuration for all SDXL and SD15 models!!!\nApr.28, 2024. ComfyUI workflows on 1-Step Unified LoRA ü•∞ with TCDScheduler to inference on different steps are released! Remember to install ‚≠ïÔ∏è ComfyUI-TCD in your ComfyUI/custom_nodes folder!!! You're encouraged to adjust the eta parameter to get better results üåü!\nApr.26, 2024. Thanks to @Pete for contributing to our scribble demo with larger canvas right now üëè.\nApr.24, 2024. The ComfyUI workflow and checkpoint on 1-Step SDXL UNet ‚ú® is also available! Don't forget ‚≠ïÔ∏è to install the custom scheduler in your ComfyUI/custom_nodes folder!!!\nApr.23, 2024. ComfyUI workflows on N-Steps LoRAs are released! Worth a try for creators üí•!\nApr.23, 2024. Our technical report üìö is uploaded to arXiv! Many implementation details are provided and we welcome more discussionsüëè.\nApr.21, 2024. Hyper-SD ‚ö°Ô∏è is highly compatible and work well with different base models and controlnets. To clarify, we also append the usage example of controlnet here.\nApr.20, 2024. Our checkpoints and two demos ü§ó (i.e. SD15-Scribble and SDXL-T2I) are publicly available on HuggingFace Repo.\nTry our Hugging Face demos:\nHyper-SD Scribble demo host on ü§ó scribble\nHyper-SDXL One-step Text-to-Image demo host on ü§ó T2I\nIntroduction\nHyper-SD is one of the new State-of-the-Art diffusion model acceleration techniques.\nIn this repository, we release the models distilled from FLUX.1-dev, SD3-Medium, SDXL Base 1.0 and Stable-Diffusion v1-5„ÄÇ\nCheckpoints\nHyper-FLUX.1-dev-Nsteps-lora.safetensors: Lora checkpoint, for FLUX.1-dev-related models.\nHyper-SD3-Nsteps-CFG-lora.safetensors: Lora checkpoint, for SD3-related models.\nHyper-SDXL-Nstep-lora.safetensors: Lora checkpoint, for SDXL-related models.\nHyper-SD15-Nstep-lora.safetensors: Lora checkpoint, for SD1.5-related models.\nHyper-SDXL-1step-unet.safetensors: Unet checkpoint distilled from SDXL-Base.\nText-to-Image Usage\nFLUX.1-dev-related models\nimport torch\nfrom diffusers import FluxPipeline\nfrom huggingface_hub import hf_hub_download\nbase_model_id = \"black-forest-labs/FLUX.1-dev\"\nrepo_name = \"ByteDance/Hyper-SD\"\n# Take 8-steps lora as an example\nckpt_name = \"Hyper-FLUX.1-dev-8steps-lora.safetensors\"\n# Load model, please fill in your access tokens since FLUX.1-dev repo is a gated model.\npipe = FluxPipeline.from_pretrained(base_model_id, token=\"xxx\")\npipe.load_lora_weights(hf_hub_download(repo_name, ckpt_name))\npipe.fuse_lora(lora_scale=0.125)\npipe.to(\"cuda\", dtype=torch.float16)\nimage=pipe(prompt=\"a photo of a cat\", num_inference_steps=8, guidance_scale=3.5).images[0]\nimage.save(\"output.png\")\nSD3-related models\nimport torch\nfrom diffusers import StableDiffusion3Pipeline\nfrom huggingface_hub import hf_hub_download\nbase_model_id = \"stabilityai/stable-diffusion-3-medium-diffusers\"\nrepo_name = \"ByteDance/Hyper-SD\"\n# Take 8-steps lora as an example\nckpt_name = \"Hyper-SD3-8steps-CFG-lora.safetensors\"\n# Load model, please fill in your access tokens since SD3 repo is a gated model.\npipe = StableDiffusion3Pipeline.from_pretrained(base_model_id, token=\"xxx\")\npipe.load_lora_weights(hf_hub_download(repo_name, ckpt_name))\npipe.fuse_lora(lora_scale=0.125)\npipe.to(\"cuda\", dtype=torch.float16)\nimage=pipe(prompt=\"a photo of a cat\", num_inference_steps=8, guidance_scale=5.0).images[0]\nimage.save(\"output.png\")\nSDXL-related models\n2-Steps, 4-Steps, 8-steps LoRA\nTake the 2-steps LoRA as an example, you can also use other LoRAs for the corresponding inference steps setting.\nimport torch\nfrom diffusers import DiffusionPipeline, DDIMScheduler\nfrom huggingface_hub import hf_hub_download\nbase_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo_name = \"ByteDance/Hyper-SD\"\n# Take 2-steps lora as an example\nckpt_name = \"Hyper-SDXL-2steps-lora.safetensors\"\n# Load model.\npipe = DiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\npipe.load_lora_weights(hf_hub_download(repo_name, ckpt_name))\npipe.fuse_lora()\n# Ensure ddim scheduler timestep spacing set as trailing !!!\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\n# lower eta results in more detail\nprompt=\"a photo of a cat\"\nimage=pipe(prompt=prompt, num_inference_steps=2, guidance_scale=0).images[0]\nUnified LoRA (support 1 to 8 steps inference)\nYou can flexibly adjust the number of inference steps and eta value to achieve best performance.\nimport torch\nfrom diffusers import DiffusionPipeline, TCDScheduler\nfrom huggingface_hub import hf_hub_download\nbase_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo_name = \"ByteDance/Hyper-SD\"\nckpt_name = \"Hyper-SDXL-1step-lora.safetensors\"\n# Load model.\npipe = DiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\npipe.load_lora_weights(hf_hub_download(repo_name, ckpt_name))\npipe.fuse_lora()\n# Use TCD scheduler to achieve better image quality\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n# Lower eta results in more detail for multi-steps inference\neta=1.0\nprompt=\"a photo of a cat\"\nimage=pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0, eta=eta).images[0]\n1-step SDXL Unet\nOnly for the single step inference.\nimport torch\nfrom diffusers import DiffusionPipeline, UNet2DConditionModel, LCMScheduler\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\nbase_model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\nrepo_name = \"ByteDance/Hyper-SD\"\nckpt_name = \"Hyper-SDXL-1step-Unet.safetensors\"\n# Load model.\nunet = UNet2DConditionModel.from_config(base_model_id, subfolder=\"unet\").to(\"cuda\", torch.float16)\nunet.load_state_dict(load_file(hf_hub_download(repo_name, ckpt_name), device=\"cuda\"))\npipe = DiffusionPipeline.from_pretrained(base_model_id, unet=unet, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n# Use LCM scheduler instead of ddim scheduler to support specific timestep number inputs\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n# Set start timesteps to 800 in the one-step inference to get better results\nprompt=\"a photo of a cat\"\nimage=pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0, timesteps=[800]).images[0]\nSD1.5-related models\n2-Steps, 4-Steps, 8-steps LoRA\nTake the 2-steps LoRA as an example, you can also use other LoRAs for the corresponding inference steps setting.\nimport torch\nfrom diffusers import DiffusionPipeline, DDIMScheduler\nfrom huggingface_hub import hf_hub_download\nbase_model_id = \"runwayml/stable-diffusion-v1-5\"\nrepo_name = \"ByteDance/Hyper-SD\"\n# Take 2-steps lora as an example\nckpt_name = \"Hyper-SD15-2steps-lora.safetensors\"\n# Load model.\npipe = DiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\npipe.load_lora_weights(hf_hub_download(repo_name, ckpt_name))\npipe.fuse_lora()\n# Ensure ddim scheduler timestep spacing set as trailing !!!\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\nprompt=\"a photo of a cat\"\nimage=pipe(prompt=prompt, num_inference_steps=2, guidance_scale=0).images[0]\nUnified LoRA (support 1 to 8 steps inference)\nYou can flexibly adjust the number of inference steps and eta value to achieve best performance.\nimport torch\nfrom diffusers import DiffusionPipeline, TCDScheduler\nfrom huggingface_hub import hf_hub_download\nbase_model_id = \"runwayml/stable-diffusion-v1-5\"\nrepo_name = \"ByteDance/Hyper-SD\"\nckpt_name = \"Hyper-SD15-1step-lora.safetensors\"\n# Load model.\npipe = DiffusionPipeline.from_pretrained(base_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\npipe.load_lora_weights(hf_hub_download(repo_name, ckpt_name))\npipe.fuse_lora()\n# Use TCD scheduler to achieve better image quality\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n# Lower eta results in more detail for multi-steps inference\neta=1.0\nprompt=\"a photo of a cat\"\nimage=pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0, eta=eta).images[0]\nControlNet Usage\nSDXL-related models\n2-Steps, 4-Steps, 8-steps LoRA\nTake Canny Controlnet and 2-steps inference as an example:\nimport torch\nfrom diffusers.utils import load_image\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL, DDIMScheduler\nfrom huggingface_hub import hf_hub_download\n# Load original image\nimage = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\")\nimage = np.array(image)\n# Prepare Canny Control Image\nlow_threshold = 100\nhigh_threshold = 200\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncontrol_image = Image.fromarray(image)\ncontrol_image.save(\"control.png\")\ncontrol_weight = 0.5  # recommended for good generalization\n# Initialize pipeline\ncontrolnet = ControlNetModel.from_pretrained(\n\"diffusers/controlnet-canny-sdxl-1.0\",\ntorch_dtype=torch.float16\n)\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", controlnet=controlnet, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\npipe.load_lora_weights(hf_hub_download(\"ByteDance/Hyper-SD\", \"Hyper-SDXL-2steps-lora.safetensors\"))\n# Ensure ddim scheduler timestep spacing set as trailing !!!\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\npipe.fuse_lora()\nimage = pipe(\"A chocolate cookie\", num_inference_steps=2, image=control_image, guidance_scale=0, controlnet_conditioning_scale=control_weight).images[0]\nimage.save('image_out.png')\nUnified LoRA (support 1 to 8 steps inference)\nTake Canny Controlnet as an example:\nimport torch\nfrom diffusers.utils import load_image\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL, TCDScheduler\nfrom huggingface_hub import hf_hub_download\n# Load original image\nimage = load_image(\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png\")\nimage = np.array(image)\n# Prepare Canny Control Image\nlow_threshold = 100\nhigh_threshold = 200\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncontrol_image = Image.fromarray(image)\ncontrol_image.save(\"control.png\")\ncontrol_weight = 0.5  # recommended for good generalization\n# Initialize pipeline\ncontrolnet = ControlNetModel.from_pretrained(\n\"diffusers/controlnet-canny-sdxl-1.0\",\ntorch_dtype=torch.float16\n)\nvae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n\"stabilityai/stable-diffusion-xl-base-1.0\",\ncontrolnet=controlnet, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n# Load Hyper-SD15-1step lora\npipe.load_lora_weights(hf_hub_download(\"ByteDance/Hyper-SD\", \"Hyper-SDXL-1step-lora.safetensors\"))\npipe.fuse_lora()\n# Use TCD scheduler to achieve better image quality\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n# Lower eta results in more detail for multi-steps inference\neta=1.0\nimage = pipe(\"A chocolate cookie\", num_inference_steps=4, image=control_image, guidance_scale=0, controlnet_conditioning_scale=control_weight, eta=eta).images[0]\nimage.save('image_out.png')\nSD1.5-related models\n2-Steps, 4-Steps, 8-steps LoRA\nTake Canny Controlnet and 2-steps inference as an example:\nimport torch\nfrom diffusers.utils import load_image\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom diffusers import ControlNetModel, StableDiffusionControlNetPipeline, DDIMScheduler\nfrom huggingface_hub import hf_hub_download\ncontrolnet_checkpoint = \"lllyasviel/control_v11p_sd15_canny\"\n# Load original image\nimage = load_image(\"https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/input.png\")\nimage = np.array(image)\n# Prepare Canny Control Image\nlow_threshold = 100\nhigh_threshold = 200\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncontrol_image = Image.fromarray(image)\ncontrol_image.save(\"control.png\")\n# Initialize pipeline\ncontrolnet = ControlNetModel.from_pretrained(controlnet_checkpoint, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16).to(\"cuda\")\npipe.load_lora_weights(hf_hub_download(\"ByteDance/Hyper-SD\", \"Hyper-SD15-2steps-lora.safetensors\"))\npipe.fuse_lora()\n# Ensure ddim scheduler timestep spacing set as trailing !!!\npipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\")\nimage = pipe(\"a blue paradise bird in the jungle\", num_inference_steps=2, image=control_image, guidance_scale=0).images[0]\nimage.save('image_out.png')\nUnified LoRA (support 1 to 8 steps inference)\nTake Canny Controlnet as an example:\nimport torch\nfrom diffusers.utils import load_image\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom diffusers import ControlNetModel, StableDiffusionControlNetPipeline, TCDScheduler\nfrom huggingface_hub import hf_hub_download\ncontrolnet_checkpoint = \"lllyasviel/control_v11p_sd15_canny\"\n# Load original image\nimage = load_image(\"https://huggingface.co/lllyasviel/control_v11p_sd15_canny/resolve/main/images/input.png\")\nimage = np.array(image)\n# Prepare Canny Control Image\nlow_threshold = 100\nhigh_threshold = 200\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncontrol_image = Image.fromarray(image)\ncontrol_image.save(\"control.png\")\n# Initialize pipeline\ncontrolnet = ControlNetModel.from_pretrained(controlnet_checkpoint, torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16).to(\"cuda\")\n# Load Hyper-SD15-1step lora\npipe.load_lora_weights(hf_hub_download(\"ByteDance/Hyper-SD\", \"Hyper-SD15-1step-lora.safetensors\"))\npipe.fuse_lora()\n# Use TCD scheduler to achieve better image quality\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\n# Lower eta results in more detail for multi-steps inference\neta=1.0\nimage = pipe(\"a blue paradise bird in the jungle\", num_inference_steps=1, image=control_image, guidance_scale=0, eta=eta).images[0]\nimage.save('image_out.png')\nComfyui Usage\nHyper-SDXL-Nsteps-lora.safetensors: text-to-image workflow\nHyper-SD15-Nsteps-lora.safetensors: text-to-image workflow\nHyper-SDXL-1step-Unet-Comfyui.fp16.safetensors: text-to-image workflow\nREQUIREMENT / INSTALL for 1-Step SDXL UNet: Please install our scheduler folder into your ComfyUI/custom_nodes to enable sampling from 800 timestep instead of 999.\ni.e. making sure the ComfyUI/custom_nodes/ComfyUI-HyperSDXL1StepUnetScheduler folder exist.\nFor more details, please refer to our technical report.\nHyper-SD15-1step-lora.safetensors: text-to-image workflow\nHyper-SDXL-1step-lora.safetensors: text-to-image workflow\nREQUIREMENT / INSTALL for 1-Step Unified LoRAs: Please install the ComfyUI-TCD into your ComfyUI/custom_nodes to enable TCDScheduler with support of different inference steps (1~8) using single checkpoint.\ni.e. making sure the ComfyUI/custom_nodes/ComfyUI-TCD folder exist.\nYou're encouraged to adjust the eta parameter in TCDScheduler to get better results.\nCitation\n@misc{ren2024hypersd,\ntitle={Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image Synthesis},\nauthor={Yuxi Ren and Xin Xia and Yanzuo Lu and Jiacheng Zhang and Jie Wu and Pan Xie and Xing Wang and Xuefeng Xiao},\nyear={2024},\neprint={2404.13686},\narchivePrefix={arXiv},\nprimaryClass={cs.CV}\n}",
    "microsoft/Phi-3-mini-4k-instruct-gguf": "Model Summary\nIntended Uses\nChat Format:\nHow to download GGUF files\nHow to use with Ollama\nBuilding from Modelfile\nHow to use with Llamafile:\nHow to run on Python:\nResponsible AI Considerations\nTraining\nModel\nDatasets\nSoftware\nLicense\nTrademarks\nModel Summary\nThis repo provides the GGUF format for the Phi-3-Mini-4K-Instruct.\nThe Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) it can support.\nThe model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.\nResources and Technical Documentation:\nPhi-3 Microsoft Blog\nPhi-3 Technical Report\nPhi-3 on Azure AI Studio\nPhi-3 on Hugging Face\nPhi-3 ONNX: 4K and 128K\nThis repo provides GGUF files for the Phi-3 Mini-4K-Instruct model.\nName\nQuant method\nBits\nSize\nUse case\nPhi-3-mini-4k-instruct-q4.gguf\nQ4_K_M\n4\n2.2 GB\nmedium, balanced quality - recommended\nPhi-3-mini-4k-instruct-fp16.gguf\nNone\n16\n7.2 GB\nminimal quality loss\nIntended Uses\nPrimary use cases\nThe model is intended for commercial and research use in English. The model provides uses for applications which require\nmemory/compute constrained environments\nlatency bound scenarios\nstrong reasoning (especially math and logic)\nlong context\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\nUse case considerations\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios.\nDevelopers  should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\nChat Format:\nGiven the nature of the training data, the Phi-3-Mini-4K-instruct model is best suited for prompts using the chat format as follows.\nYou can provide the prompt as a question with a generic template as follow:\n<|user|>\\nQuestion <|end|>\\n<|assistant|>\nFor example:\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|>\nwhere the model generates the text after \"<|assistant|>\" . In case of few-shots prompt, the prompt can be formatted as the following:\n<|user|>\nI am going to Paris, what should I see?<|end|>\n<|assistant|>\nParis, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"<|end|>\n<|user|>\nWhat is so great about #1?<|end|>\n<|assistant|>\nHow to download GGUF files\nInstall Hugging Face CLI:\npip install huggingface-hub>=0.17.1\nLogin to Hugging Face:\nhuggingface-cli login\nDownload the GGUF model:\nhuggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Phi-3-mini-4k-instruct-q4.gguf --local-dir . --local-dir-use-symlinks False\nHow to use with Ollama\nInstall Ollama:\ncurl -fsSL https://ollama.com/install.sh | sh\nRun the phi3 model:\nollama run phi3\nBuilding from Modelfile\nAssuming that you have already downloaded GGUF files, here is how you can use them with Ollama:\nGet the Modelfile:\nhuggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Modelfile_q4 --local-dir /path/to/your/local/dir\nBuild the Ollama Model:\nUse the Ollama CLI to create your model with the following command:\nollama create phi3 -f Modelfile_q4\nRun the phi3 model:\nNow you can run the Phi-3-Mini-4k-Instruct model with Ollama using the following command:\nollama run phi3 \"Your prompt here\"\nReplace \"Your prompt here\" with the actual prompt you want to use for generating responses from the model.\nHow to use with Llamafile:\nAssuming that you already have GGUF files downloaded. Here is how you can use the GGUF model with Llamafile:\nDownload Llamafile-0.7.3\nwget https://github.com/Mozilla-Ocho/llamafile/releases/download/0.7.3/llamafile-0.7.3\nRun the model with chat format prompt:\n<|user|>\\nHow to explain Internet for a medieval knight?\\n<|end|>\\n<|assistant|>\n./llamafile-0.7.3 -ngl 9999 -m Phi-3-mini-4k-instruct-q4.gguf --temp 0.6 -p \"<|user|>\\nHow to explain Internet for a medieval knight?\\n<|end|>\\n<|assistant|>\"\nRun with a chat interface:\n./llamafile-0.7.3 -ngl 9999 -m Phi-3-mini-4k-instruct-q4.gguf\nYour browser should open automatically and display a chat interface. (If it doesn't, just open your browser and point it at http://localhost:8080)\nHow to run on Python:\nInstall llama-cpp-python:\n! CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\nRun the model:\nfrom llama_cpp import Llama\nllm = Llama(\nmodel_path=\"./Phi-3-mini-4k-instruct-q4.gguf\",  # path to GGUF file\nn_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\nn_threads=8, # The number of CPU threads to use, tailor to your system and the resulting performance\nn_gpu_layers=35, # The number of layers to offload to GPU, if you have GPU acceleration available. Set to 0 if no GPU acceleration is available on your system.\n)\nprompt = \"How to explain Internet to a medieval knight?\"\n# Simple inference example\noutput = llm(\nf\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\",\nmax_tokens=256,  # Generate up to 256 tokens\nstop=[\"<|end|>\"],\necho=True,  # Whether to echo the prompt\n)\nprint(output['choices'][0]['text'])\nResponsible AI Considerations\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\nQuality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.\nRepresentation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\nInappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.\nInformation Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\nLimited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:\nAllocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\nHigh-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\nMisinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\nGeneration of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\nMisuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\nTraining\nModel\nArchitecture: Phi-3 Mini has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidlines.\nInputs: Text. It is best suited for prompts using chat format.\nContext length: 4K tokens\nGPUS: 512 H100-80G\nTraining time: 7 days\nTraining data: 3.3T tokens\nOutputs: Generated text in response to the input\nDates: Our models were trained between February and April 2024\nStatus: This is a static model trained on an offline dataset with cutoff date October 2023. Future versions of the tuned models may be released as we improve models.\nDatasets\nOur training data includes a wide variety of sources, totaling 3.3 trillion tokens, and is a combination of\npublicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\nnewly created synthetic, ‚Äútextbook-like‚Äù data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\nhigh quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\nSoftware\nPyTorch\nDeepSpeed\nTransformers\nFlash-Attention\nLicense\nThe model is licensed under the MIT license.\nTrademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow‚ÄØMicrosoft‚Äôs Trademark & Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies."
}